var tipuesearch = {"pages":[{"title":"About This Website","text":"Welcome to Stats Works! This website is dedicated to statistical approaches to various problems. Statistics and the analysis of data are becoming more and more relevant in the world today. With the introduction of machine learning techniques, statistics and computer science have become essential to the ability of business and governments' success. Unleashing the potential of information is more crucial than ever, and is having a remarkable impact on the world. This website is dedicated to personal projects surrounding both statistics and machine learning and shows how we can leverage the wealth of data in the world today to make actionable insights to further improve the world we live in. Parts of this website are also dedicated to explaining some basic statistical concepts to those who may have no background in statistics, or have a background in STEM but need a refresher on these topics. The author, Sean Ammirati, is a Data Scientist with a deep love and passion for statistics, programming and machine learning. The intention of creating this website is to demonstrate practical implementations of theoretical statistics as well as demystify some of the more complex topics in the dicipline. Sean has a Master's degree in Statistics from Hunter College.","tags":"About","url":"https://seanammirati.github.io/pages/about/","loc":"https://seanammirati.github.io/pages/about/"},{"title":"Hidden Markov Models","text":"In my previous project, I developed the reasoning behind Hidden Markov Models (HMM) and developed a way of determining the most likely sequence of states that resulted in an observed sequence of emissions using the Viterbi algorithm. This project will explore extensions to the Hidden Markov Model, such as the Forwards-Backwards algorithm and the Baum-Welch algorithm. Consider a discrete time, discrete state-space Hidden Markov chain with: $\\pi_{t}$ : the state at time t (unknown in the HMM) of n possible states $\\pi_{t} = 1,2,\\ldots,n$ $x_{\\text{t}}$: the emission at time t of m possible emissions, $x_{\\text{t}} = 1,2,\\ldots,m$ P(0) : initial probabilities of being in the states. P : a nxn transition matrix of transitions between states, giving P($\\pi_{t} = i\\left| \\pi_{t - 1} = j \\right)$ for the ith row and jth column. E : a nxm matrix containing P($x_{\\text{t}} = j\\ \\left| \\ \\pi_{t} = i \\right)$ in the ith row, jth column, the emission probabilities. Note that the row sums must sum to 1. Given a set of observed emissions, $x_{1,\\ldots,\\ T}$, the Viterbi algorithm gives us the most likely sequence of states, $\\pi_{1,\\ldots,\\ T}$ that resulted in the observed emissions, given P and E. Although this decoding algorithm is very useful, there may be other aspects of the HMM we are interested in. One such problem is, given a set of observed emissions $x_{1,\\ldots,\\ T}$, what is the probability of the observed emissions? This problem can be solved using the law of total probability, i.e.: $$P\\left( x_{1},\\ \\ldots,x_{T} \\right) = \\ \\sum_{\\mathbf{\\pi}}&#94;{}{P(x_{1},\\ \\ldots,x_{T}}\\left| \\ \\mathbf{\\pi} \\right)P(\\mathbf{\\pi})\\ $$ where $\\mathbf{\\pi}$ refers to any vector of state sequences. This can be computationally intensive -- for many steps, this will involve summing over an exponentially large number of possible state sequences. Thus, an algorithmic approach will be useful to make this result practical to obtain for large T. This is a similar problem to the Viterbi algorithm, but not exactly the same. This can be solved using the forward algorithm , as described below. Note that we can also write the above as: $$P\\left( x_{1},\\ \\ldots,x_{T} \\right) = \\ \\sum_{\\mathbf{\\pi}}&#94;{}{P(x_{1},\\ \\ldots,x_{T}},\\mathbf{\\pi})\\ $$ Let's call $\\alpha_{t}(\\pi_{t})$ = $P\\left( x_{1},\\ \\ldots,x_{t},\\ \\pi_{t} \\right) = \\ \\sum_{\\pi_{t - 1}}&#94;{}{P(x_{1},\\ \\ldots,x_{T},\\ \\pi_{t},\\pi_{t - 1})}$ It follows that our desired probability of the sequence is equal to the sum over all states of the above quantity. $$\\alpha_{t}(\\pi_{t}) = \\ \\sum_{\\mathbf{\\pi}_{t - 1}}&#94;{}{P(x_{t}|x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t},\\ \\pi_{t - 1})P(\\pi_{t}|x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t - 1}})P(x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t - 1})$$ But $x_{t}$ is conditionally independent of everything but $\\pi_{t},\\ $and $\\pi_{t}$conditionally independent on everything but $\\pi_{t - 1}$. So we can write this simply as: $$\\alpha_{t}(\\pi_{t}) = \\ P(x_{t}|\\pi_{t})\\sum_{\\mathbf{\\pi}_{t - 1}}&#94;{}{P(\\pi_{t}|\\pi_{t - 1}})P(x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t - 1})$$$$= \\ P(x_{t}|\\pi_{t})\\sum_{\\mathbf{\\pi}_{t - 1}}&#94;{}{P(\\pi_{t}|\\pi_{t - 1}})\\alpha_{t - 1}(\\pi_{t - 1})$$ It follows, then, that we can compute each $\\alpha_{t}$ recursively. This will eliminate the need to look at the probability of all states. This has a computation time of $O(\\text{nm}&#94;{2})$ which is far faster than doing it for each possible sequence, which is$\\ O(\\text{nm}&#94;{n})$. After computing this for each possible state at time T, we sum the probabilities to get the probability of the observed sequence. Note that $\\alpha_{1}\\left( \\pi_{1} \\right) = P(x_{1}|\\pi_{1})\\sum_{x_{0}}&#94;{}{P(\\pi_{1}|\\pi_{0}})P(\\pi_{0}) = \\ \\ P(x_{1}|\\pi_{1})P(\\pi_{1})$, where $P\\left( \\pi_{1} \\right)$ is to be considered the initial probabilities of entering the chain, predefined by the problem. Consider the following example of a fair and biased coin, where: Initial probabilities are equal, .5 for fair and .5 for biased $$\\mathbf{P = \\ }\\begin{matrix} .9 & .1 \\\\ .05 & .95 \\\\ \\end{matrix}$$$$\\mathbf{E = \\ }\\begin{matrix} .5 & .5 \\\\ .25 & .75 \\\\ \\end{matrix}$$ We observe the sequence HTHHTTHH and want to determine the probability of this sequence occurring given our HMM. First, $$\\alpha_{1}\\left( \\pi_{1} = F \\right) = \\ P\\left( x_{1} \\middle| \\pi_{1} = F \\right)P\\left( \\pi_{1} = F \\right) = \\left( 0.5 \\right)\\left( 0.5 \\right) = \\ .25$$$$\\alpha_{1}\\left( \\pi_{1} = B \\right) = P\\left( x_{1} \\middle| \\pi_{1} = B \\right)P\\left( \\pi_{1} = B \\right) = \\left( 0.25 \\right)\\left( 0.5 \\right) = \\ .135$$ So it follows that the probability of a head being observed, alone, is the sum of these partial probabilities, or .385. To determine the probability of the full sequence, we continue this recursively, considering the results of the function at the last time step. We can also simplify this using matrix multiplication. If we consider $\\mathbf{\\alpha}_{\\mathbf{t}}$ as a nx1 vector pertaining to each of the n states, we can calculate the following: $${\\mathbf{\\alpha}_{\\mathbf{t}}\\mathbf{= \\ E}}_{\\mathbf{.,j}} x \\mathbf{P'\\alpha}_{\\mathbf{t - 1}}$$ Where j refers to a row vector of the jth column pertaining to the observed value at time t . This can help to simplify the calculations. Here, x refers to component-wise multiplication. I have included a user-defined function to determine this probability recursively for the problem at hand: In [6]: forwardalgor <- function ( obs , trans , emissions , init ){ alphas <- list ( 1 , init * emissions [, obs [ 1 ]]) for ( i in 2 : ( length ( obs ) + 1 )){ alphasum <- as.numeric ( trans %*% alphas [[ i ]]) alphas [[ i + 1 ]] <- emissions [, obs [ i -1 ]] * alphasum } print ( \"Sequences:\" ) print ( alphas ) print ( \"Final Probability\" ) print ( sum ( alphas [[ length ( obs ) +1 ]])) return ( sum ( alphas [[ length ( obs ) +1 ]])) } forwardalgor ( c ( 1 , 2 , 1 , 1 , 2 , 2 , 1 , 1 ), rbind ( c ( . 9 , . 1 ), c ( . 95 , . 05 )), rbind ( c ( . 5 , . 5 ), c ( . 25 , . 75 )), c ( . 5 , . 5 )) [1] \"Sequences:\" [[1]] [1] 1 [[2]] [1] 0.250 0.125 [[3]] [1] 0.1187500 0.0609375 [[4]] [1] 0.05648438 0.08689453 [[5]] [1] 0.02976270 0.01450122 [[6]] [1] 0.014118274 0.007249905 [[7]] [1] 0.006715719 0.010331142 [[8]] [1] 0.003538630 0.005172367 [[9]] [1] 0.0018510021 0.0009050793 [[10]] [1] 0.0008782049 0.0004509265 [1] \"Final Probability\" [1] 0.002756081 0.00275608136978746 This gives us our result -- there is about a .2% chance of this observation being output by this particular HMM. This can be useful for numerous applications: consider if one was testing against n possible HMMs. We could then find which HMM is the more likely fit by considering $\\text{argmax}_{i}(P\\left( \\text{HMM}_{i} \\middle| x_{1},\\ \\ldots,x_{T} \\right))$. Bayes' Rule tells us that: $$P\\left( \\text{HMM}_{i} \\middle| x_{1},\\ \\ldots,x_{T} \\right) \\propto \\ P\\left( x_{1},\\ \\ldots,x_{T} \\middle| \\text{HMM}_{i} \\right)P(\\text{HMM}_{i})$$ If we consider our n models as equally likely, we can then determine which model is most probable given the observations simply by considering the likelihood in the fashion calculated above. That is, $\\operatorname{}\\left( P\\left( \\text{HMM}_{i} \\middle| x_{1},\\ \\ldots,x_{T} \\right) \\right) = \\text{argmax}_{i}(\\ P\\left( x_{1},\\ \\ldots,x_{T} \\middle| \\text{HMM}_{i} \\right))$. This relatively simple result and simple algorithm gives us a method of comparing and optimizing our fit. Also, since the probabilities here can become quite small, the fact that a logarithmic function is monotonically increasing means we can also compare the logarithms, in order to avoid machine zeros. The closely related, but not exactly equivalent, Forwards-Backwards algorithm attempts to find probability of a particular state of a particular time point given all of the observations, i.e. : $$P({\\pi_{k}|x}_{1},\\ \\ldots,x_{T})$$$$P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k,\\ }x_{k + 1},\\ldots,x_{T} \\right) = \\ \\frac{P\\left( x_{k + 1},\\ldots,x_{T}|{\\pi_{k},x}_{1},\\ \\ldots,x_{k} \\right)P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k} \\right)}{P\\left( x_{k + 1},\\ldots,x_{T} \\right)}$$ $$\\propto P\\left( x_{k + 1},\\ldots,x_{T}|\\pi_{k} \\right)P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k} \\right)$$ This is due to the fact that $x_{k + 1},\\ldots,x_{T}$ is conditionally independent of $x_{1},\\ \\ldots,x_{k}$ given $\\pi_{k}$ and that $P\\left( x_{k + 1},\\ldots,x_{T} \\right)$ is a multiplicative constant which does not change with a selection of $\\pi_{k}$. This means we can normalize this probability at each point to ensure that $\\sum_{}&#94;{}{P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k,\\ }x_{k + 1},\\ldots,x_{T} \\right) = 1}$. Notice that $P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k} \\right)\\ \\propto \\ P\\left( x_{1},\\ \\ldots,x_{k},\\ \\pi_{k} \\right) = \\ \\alpha_{k}\\mathbf{(}\\pi_{k})$ as described earlier. Notice that $P\\left( x_{k + 1},\\ldots,x_{T}|\\pi_{k} \\right)$ is a similar proposition, but backwards. That is, we can call $$b_{k}(\\pi_{k})$$ $$= \\sum_{\\pi_{k + 1}}&#94;{}{P\\left( \\pi_{k + 1},x_{k + 1},\\ldots,x_{T}|\\pi_{k} \\right)}$$ $$= \\sum_{\\pi_{k + 1}}&#94;{}{P\\left( x_{k + 2},\\ldots,x_{T}|{x_{k + 1},\\pi}_{k + 1},\\pi_{k} \\right)P(x_{k + 1}|\\pi_{k + 1},\\pi_{k})P(\\pi_{k + 1}|\\pi_{k})}$$ $$= \\sum_{\\pi_{k + 1}}&#94;{}{P\\left( x_{k + 2},\\ldots,x_{T}|\\pi_{k + 1} \\right)P(x_{k + 1}|\\pi_{k + 1})P(\\pi_{k + 1}|\\pi_{k})}$$ This is obtained using the chain rule and the conditional independencies of the HMM. Notice now that $P\\left( x_{k + 2},\\ldots,x_{T}|\\pi_{k + 1} \\right) = \\ b_{k + 1}\\left( \\pi_{k + 1} \\right)$ and we can now see the recursive nature of this function. That is, $$b_{k}\\left( \\pi_{k} \\right) = \\ \\sum_{\\pi_{k + 1}}&#94;{}{b_{k + 1}\\left( \\pi_{k + 1} \\right)P(x_{k + 1}|\\pi_{k + 1})P(\\pi_{k + 1}|\\pi_{k})}$$ We take $b_{T}\\left( \\pi_{T} \\right) = 1$ for all states, as this can be considered the exit probability. Intuitively, this means that we are considering, for some intermediate time step, the probabilities of getting there from zero and the probabilities of \"back tracking\" from T. This gives us the total probability of an intermediate state based on our observations. The implementation is as follows: Compute the forward probabilities (by the previously stated algorithm) until time step k. For each step, normalize the results so that the probabilities sum to 1. Then, compute the backward probabilities from T to k. Normalize these at each step as well. Then, multiply the two results and normalize to get the probabilities of being in each state after k time steps. To calculate this, we may use the following formula: $${\\mathbf{b}_{\\mathbf{t}}\\mathbf{= \\ P(E}}_{\\mathbf{.,j}} x \\mathbf{b}_{\\mathbf{t + 1}}\\mathbf{)}$$ Notice that this is similar to the formula from before for the forwards portion, but the multiplications are \"flipped.\" Thus we can implement the Forward-Backwards algorithm as follows: In [7]: bkforwardalgor <- function ( obs , k , trans , emissions , init ) { alphas <- list ( 1 , init * emissions [, obs [ 1 ]] / sum ( init * emissions [, obs [ 1 ]])) betas <- list ( rep ( 1 , nrow ( trans ))) for ( i in 3 : ( k + 1 )) { alphasum <- as.numeric ( t ( trans ) %*% alphas [[ i - 1 ]]) alphas [[ i ]] <- emissions [, obs [ i - 1 ]] * alphasum alphas [[ i ]] <- alphas [[ i ]] / sum ( alphas [[ i ]]) } for ( i in 2 : ( length ( obs ) - k + 1 )) { betasum <- emissions [, obs [ length ( obs ) - i + 2 ]] * betas [[ i - 1 ]] betas [[ i ]] <- as.numeric ( trans %*% betasum ) betas [[ i ]] <- betas [[ i ]] / sum ( betas [[ i ]]) } pointalph <- alphas [[( k + 1 )]] pointbet <- betas [[( length ( obs ) - k + 1 )]] return ( list ( forward = alphas , backward = betas , probstates = pointalph * pointbet / sum ( pointalph * pointbet ))) } bkforwardalgor ( c ( 1 , 2 , 1 , 1 , 2 , 2 , 1 , 1 ), 3 , rbind ( c ( 0.9 , 0.1 ), c ( 0.95 , 0.05 )), rbind ( c ( 0.5 , 0.5 ), c ( 0.25 , 0.75 )), c ( 0.5 , 0.5 )) $forward 1 0.666666666666667 0.333333333333333 0.88 0.12 0.950682056663169 0.0493179433368311 $backward 1 1 0.493506493506494 0.506493506493506 0.493683851143735 0.506316148856265 0.50646857940524 0.49353142059476 0.505577910274165 0.494422089725835 0.493357500634704 0.506642499365296 $probstates 0.949421205954081 0.0505787940459189 So, at the third time step, we are close to 95% certain that we were in the first state (Heads), given all of the observations. This all leads us to the Baum-Welch algorithm, which is a way of estimating parameters of a HMM given a sequence of emissions. Let $\\gamma_{i}\\left( t \\right) = P\\left( \\pi_{t} \\middle| \\theta,x_{1},\\ \\ldots,x_{T} \\right)$ where $\\theta$ is the parameters of the distribution (the transition matrix, the emission matrix and the initial probabilities.) Note that we have already calculated this in the previous exercise using the forwards backwards algorithm. Further, let $$\\xi_{i,j}\\left( t \\right) = \\ P\\left( \\pi_{t}\\ ,\\pi_{t + 1}\\ \\middle| \\theta,x_{1},\\ \\ldots,x_{T} \\right) = \\frac{\\alpha_{t}\\left( \\pi_{t} = i \\right)\\mathbf{P}_{\\text{ij}}b_{t + 1}\\left( \\pi_{t + 1} = j \\right)\\mathbf{E}_{{j,x}_{t + 1}}}{\\sum_{}&#94;{}{\\alpha_{t}\\left( \\pi_{t} = i \\right)\\mathbf{P}_{\\text{ij}}b_{t + 1}\\left( \\pi_{t + 1} = j \\right)\\mathbf{E}_{{j,x}_{t + 1}}}}$$ This represents the probability of entering state i from the left, going from state i to j, exiting state j from the right, and emitting the emission at the t+1 time step from state j. All of this culminates in the joint probability of two consecutive states. Then, using an initial set of parameters we can calculate these values and update our parameters to find a local maxima. Our updates are as follows: Initial probabilities = $\\gamma_{i}\\left( 1 \\right)$. This is the expected frequency spent in state i at time 1. The new transition matrix, P' , will have entries: $$\\mathbf{P'}_{\\text{ij}} = \\frac{\\sum_{t = 1}&#94;{T - 1}{\\ \\xi_{i,j}\\left( t \\right)}\\ }{\\sum_{t = 1}&#94;{T - 1}{\\gamma_{i}\\left( t \\right)}}$$ This is the expected number of transitions from state i to j compared to the expected total number of transitions away from state i (including itself). The new emission matrix, E' , will have entries: $\\mathbf{E'}_{\\text{ij}} = \\frac{\\sum_{t = 1}&#94;{T}1_{x_{t} = j}\\gamma_{i}\\left( t \\right)}{\\sum_{t = 1}&#94;{T}{\\gamma_{i}\\left( t \\right)}}$ Where $1_{x_{t} = j}$ is an indicator function which is 1 if the emission was the jth emission and zero otherwise. This calculates the average amount of time an emission j was emitted in state i divided by the average amount of time it was in state i in general. We then iterate this process until we are within a threshold to determine the optimal parameters for the model given only an observed sequence. While I have not implemented this algorithm directly here, I have used the HMM package in R to determine this. Here is the output: In [8]: library ( HMM ) chk <- initHMM ( States = c ( \"F\" , \"B\" ), Symbols = c ( \"H\" , \"T\" ), startProbs = c ( . 5 , . 5 ), transProbs = rbind ( c ( . 9 , . 1 ), c ( . 95 , . 05 )), emissionProbs = rbind ( c ( . 5 , . 5 ), c ( . 25 , . 75 ))) baumWelch ( chk , c ( \"H\" , \"T\" , \"H\" , \"H\" , \"T\" , \"T\" , \"H\" , \"H\" )) $hmm $States 'F' 'B' $Symbols 'H' 'T' $startProbs F 0.5 B 0.5 $transProbs F B F 0.8356626 1.643374e-01 B 1.0000000 4.663436e-19 $emissionProbs H T F 0.530374 4.696260e-01 B 1.000000 8.442496e-29 $difference 0.508902752112227 0.240556572158092 0.175367150726267 0.125799629747644 0.0789866075573572 0.0430462501906644 0.0220242002384686 0.0120457248068965 0.00797054223724487 0.00648938284089056 0.00596923716449372 0.00573434947770014 0.00556884788402301 0.00541435766962771 0.00525534145586094 0.0050878669619669 0.00491154731418667 0.0047272548854208 0.00453636210810545 0.00434044024235019 0.00414111692008325 0.0039399973466488 0.00373861517021827 0.00353839967199841 0.00334065357605126 0.00314653876626508 0.00295706838232949 0.00277310422503745 0.00259535857201991 0.00242439957307494 0.00226065943795158 0.00210444467891964 0.00195594773481778 0.00181525938160172 0.00168238142175776 0.00155723923619973 0.00143969387155143 0.0013295534195225 0.00122658352013238 0.00113051688536663 0.00104106179381376 0.000957909549977724 0.000880740935187623 0.000809231701136347 0.000743057173680575 0.00068189604460203 0.000625433433944128 0.000573363306621804 0.000525390324947385 0.00048123121464055 0.000440615716505618 0.000403287189561262 0.000369002924884535 0.0003375342226381 0.000308666278318049 0.00028219791808662 0.000257941217349493 0.000235721031633014 0.000215374464079931 0.000196750289868947 0.000179708354205615 0.000164118957457245 0.000149862238244094 0.000136827563139715 0.000124912929580535 0.000114024387063381 0.000104075480274582 9.49867167999181e-05 8.66850610378905e-05 7.91034553396986e-05 7.21803686502851e-05 6.58593726685694e-05 6.00887450076778e-05 5.48210986672737e-05 5.00130368923791e-05 4.56248323759889e-05 4.16201296418831e-05 3.79656693828655e-05 3.46310335586279e-05 3.1588409960111e-05 2.88123750393403e-05 2.62796938294031e-05 2.39691357210852e-05 2.18613050690297e-05 1.99384854986017e-05 1.81844969456332e-05 1.65845644250734e-05 1.51251977163732e-05 1.37940810142869e-05 1.25799718653171e-05 1.14726085741031e-05 1.04626254579159e-05 9.54147526764379e-06 8.70135822614897e-06 7.93515712464892e-06 7.23637797002243e-06 6.59909573466095e-06 6.0179047717908e-06 5.48787350674592e-06 5.00450304905593e-06 Here, we see that the algorithm suggests changing the transition probabilities to almost never transition when in the Biased state, and to transition more frequently when in the fair state. It also suggests that the emission probabilities be heads nearly in all cases when it is biased. This is troublesome, since we knew (by our definition) that the emissions would be fair for the fair coin and biased otherwise. Still, this generalizes the problem and allows us to attempt to fit optimal HMMs to a given set of emissions. Using the Viterbi algorithm is known as \"decoding\" -- we are attempting to determine the most likely state sequence given a sequence of observed emissions. Determining the most likely state at a time step is known as \"filtering\" (when the step is the final step, T) or \"smoothing\" (for some intermediate step, k ). This is what the forward-backward algorithm was used for. Attempting to determine the best fitting parameters is known as \"training\". This is the motivation of the Baum-Welch algorithm. Together, these three algorithms provide the basis for many applications of the HMM model.","tags":"Markov Chains","url":"https://seanammirati.github.io/category/markov-chains/hidden-markov-models.html","loc":"https://seanammirati.github.io/category/markov-chains/hidden-markov-models.html"},{"title":"Heteroskedacity with Weighted Least Squares","text":"An Approach to Solving Heteroskedacity when the Variance is a Multiplicative Constant One of the most widely known assumptions of linear regression is homoskedacity , that is that the variance of the errors are constant for all of the values (they do not depend on the particular observation). Weighted least squares is a way of dealing with this problem. We will detail the intuition and reasoning behind why this is the case. Consider a scale that measures two objects. For each object, we have some randomized error. Consider the weights of two objects, first alone and then together. That is, consider the following model: $$ y_{1} = \\alpha_{1} + \\varepsilon_{1} $$$$ y_{2} = \\alpha_{2} + \\varepsilon_{2} $$$$ y_{3} = \\alpha_{1} + \\alpha_{2}\\ + \\ \\varepsilon_{3} $$ This is like a regression model, in that we can solve for the alphas (the true weights) by the method of least squares. $${\\mathbf{y = X\\alpha + \\ \\varepsilon} } $$$$\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{X} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\\\ \\end{bmatrix}$$$$ {(\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{X)}}&#94;{- 1} = \\frac{1}{3} \\begin{bmatrix} 2 & - 1 \\\\ -1 & 2 \\\\ \\end{bmatrix} $$$$ {(\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{X)})}&#94;{- 1}\\mathbf{X}&#94;{\\mathbf{T}}=\\frac{1}{3} \\begin{bmatrix} 2 & -1 & 1 \\\\ -1 & 2 & 1 \\\\ \\end{bmatrix} $$$$ \\widehat{\\mathbf{\\alpha}} = \\frac{1}{3}\\begin{bmatrix} 2y_{1} - y_{2} + y_{3} \\\\ -y_{1} + 2y_{2} + y_{3} \\\\ \\end{bmatrix} $$ Now consider that the random element is not equally variant for the two objects -- that is, that for both weights combined there is some multiplicative factor on the error $> 1$. That is, $$ \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3} \\\\ \\end{bmatrix} = \\mathbf{MVN}(0, \\Sigma) \\quad \\text{where } \\Sigma = \\begin{bmatrix} \\sigma&#94;{2} & 0 & 0 \\\\ 0 & \\sigma&#94;{2} & 0 \\\\ 0 & 0 & {k&#94;{2}\\sigma}&#94;{2} \\\\ \\end{bmatrix} $$ Note that this violates the least squares assumptions. One method to fix this issue is to transform the variables so they have equal variance, since $k$ is a numeric value $> 0$. Note that we can transform the third variable as follows: $$\\frac{y_{3}}{k} = \\frac{\\alpha_{1}}{k} + \\frac{\\alpha_{2}}{k}\\ + \\ \\frac{\\varepsilon_{3}}{k}$$ Now, $\\frac{\\varepsilon_{3}}{k}$ has variance $\\sigma&#94;{2}$. Let $$\\mathbf{y}&#94;{\\mathbf{*}} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3}/k \\\\ \\end{bmatrix} $$ $$ \\mathbf{X}&#94;{\\mathbf{*}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1/k & 1/k \\\\ \\end{bmatrix} $$$$ \\mathbf{\\alpha}&#94;{\\mathbf{*}} = \\begin{bmatrix} \\alpha_{1} \\\\ \\alpha_{2} \\\\ \\end{bmatrix} $$$$ \\mathbf{\\varepsilon}&#94;{\\mathbf{*}} = \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3}/k \\\\ \\end{bmatrix} $$ Then we can solve as in the case of OLS: $${{\\widehat{\\mathbf{\\alpha}}}&#94;{*} = \\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1}{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{y}&#94;{\\mathbf{*}} }{{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}}\\mathbf{= \\ }\\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{1}{k&#94;{2}} \\\\ \\frac{1}{k&#94;{2}} & \\frac{k&#94;{2} + 1}{k&#94;{2}} \\\\ \\end{bmatrix}}$$$$\\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1} = \\frac{k&#94;{4}}{k&#94;{4} + 2k&#94;{2}}\\ \\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{- 1}{k&#94;{2}} \\\\ \\frac{- 1}{k&#94;{2}} & \\frac{k&#94;{2} + 1}{k&#94;{2}} \\\\ \\end{bmatrix}$$$$\\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1}{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{=}\\frac{k&#94;{4}}{k&#94;{4} + 2k&#94;{2}}\\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{- 1}{k&#94;{2}} & \\frac{1}{k} \\\\ \\frac{- 1}{k&#94;{2}} & \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{1}{k} \\\\ \\end{bmatrix}$$$${\\widehat{\\mathbf{\\alpha}}}&#94;{*}\\mathbf{=}\\frac{k&#94;{4}}{k&#94;{4} + 2k&#94;{2}}\\mathbf{\\ }\\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}}y_{1} - \\frac{y_{2}}{k&#94;{2}} + \\frac{y_{3}}{k&#94;{2}} \\\\ \\frac{{- y}_{1}}{k&#94;{2}} + \\frac{k&#94;{2} + 1}{k&#94;{2}}y_{2} + \\frac{y_{3}}{k&#94;{2}} \\\\ \\end{bmatrix}$$$${\\widehat{\\mathbf{\\alpha}}}&#94;{*}\\mathbf{=}\\frac{k&#94;{2}}{k&#94;{4} + 2k&#94;{2}}\\mathbf{\\ }\\begin{bmatrix} \\mathbf{(}k&#94;{2} + 1)y_{1} - y_{2} + y_{3} \\\\ \\mathbf{-}y_{1}\\mathbf{+ (}k&#94;{2} + 1)y_{2} + y_{3} \\\\ \\end{bmatrix}$$ However, this is not a general solution. Consider instead the following: $$\\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3} \\\\ \\end{bmatrix}\\mathbf{\\ \\sim\\ MVN(0, \\Sigma)\\ \\ } \\quad \\text{where } \\Sigma = \\begin{bmatrix} {a&#94;{2}\\sigma}&#94;{2} & 0 & 0 \\\\ 0 & b&#94;{2}\\sigma&#94;{2} & 0 \\\\ 0 & 0 & {c&#94;{2}\\sigma}&#94;{2} \\\\ \\end{bmatrix}$$ We could attempt to perform the same procedure, that is, divide $y_{1}$ by $a&#94;{2}$, $y_{2}$ by $b&#94;{2}$ and so on. However, this is arduous, and it would be convenient to use a matrix of weights to solve this. Let W be a matrix of the reciprocals of these constants. Using the same procedure as above, we can express $\\mathbf{y}&#94;{\\mathbf{*}}$ as $\\mathbf{W}&#94;{\\mathbf{1/2}}\\mathbf{\\ }\\mathbf{y}$ and $\\mathbf{X}&#94;{\\mathbf{*}}$ as $\\mathbf{W}&#94;{\\mathbf{1/2}}\\mathbf{X}$ Then our result is: $${\\widehat{\\mathbf{\\alpha}}}&#94;{*} = \\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1}{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{y}&#94;{\\mathbf{*}}$$$${\\widehat{\\mathbf{\\alpha}}}&#94;{*} = \\left( \\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{\\text{WX}} \\right)&#94;{- 1}\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{W}\\mathbf{y}&#94;{\\mathbf{*}}$$ We can see that if W = $\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & {1/k}&#94;{2} \\\\ \\end{bmatrix}\\mathbf{\\ }$ we get the same result as earlier. Code Implementation In [3]: ols_preds <- function ( X , y ) { return ( solve ( t ( X ) %*% X ) %*% t ( X ) %*% y ) } weighted_ols_preds <- function ( X , y , W ) { return ( solve ( t ( X ) %*% W %*% X ) %*% t ( X ) %*% W %*% y ) } In [6]: y <- c ( 41 , 53 , 97 ) X <- matrix ( c ( 1 , 0 , 1 , 0 , 1 , 1 ), nrow = 3 ) alphahat <- solve ( t ( X ) %*% X ) %*% t ( X ) alphahat %*% y 42 54 This calculates the predictions in the first case. In [7]: k <- 1.25 ystar <- y ystar [ 3 ] <- y [ 3 ] / k xstar <- matrix ( c ( 1 , 0 , 1 / k , 0 , 1 , 1 / k ), nrow = 3 ) alphahat <- solve ( t ( xstar ) %*% xstar ) %*% t ( xstar ) alphahat %*% ystar 41.84211 53.84211 This calculates the predictions in the second case Using the functions we get the same results: In [8]: ols_preds ( X , y ) W <- diag ( c ( rep ( 1 , 2 ), 1 / k &#94; 2 )) weighted_ols_preds ( X , y , W ) 42 54 41.84211 53.84211","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/heteroskedacity-with-weighted-least-squares.html","loc":"https://seanammirati.github.io/category/linear-models/heteroskedacity-with-weighted-least-squares.html"},{"title":"Constrained Least Squares","text":"Consider the following: We wish to constrain the parameters of a linear model to have certain properties, for example, that the last coefficient is equal to 1, or that the sum of the coefficients are equal to one. How can we estimate the Least Squares Estimates using these constraints? Thus is constrained least squares, and this article will explore the theoretical basis for choosing a selection of coefficients that will minimize the least squares given some linear constraints on the coefficients. Constrained Least Squares -- Coefficient Calculation and Simulation We want to maximize the likelihood equation of $\\vec{\\beta}$. We know that $\\vec{y}$ in this context is normally distributed with mean $\\mathbf{X}\\vec{\\beta}$ and variance $\\sigma&#94;2$. This means that the distribution of $y_i$ given a vector of values $\\vec{x_i}$ is as follows: $$f_{Y}\\left( y_i \\middle|\\ \\vec{x}_i \\right) = {(2\\pi\\sigma&#94;{2})}&#94;{- 1/2}e&#94;{\\frac{- (y_i-\\vec{x}_i \\vec{\\beta})}{2\\sigma&#94;{2}}&#94;{2}}$$ So the likelihood of $\\vec{\\beta}$ is therefore: $$ L(\\vec{\\beta}, \\sigma&#94;{2} | \\vec{y}, \\mathbf{X}) = ( 2\\pi\\sigma&#94;{2})&#94;{- \\frac{N}{2}}e&#94;{- \\frac{\\sum_{i}&#94;{N}{(y_{i}-\\vec{x}_{i} \\vec{\\beta})}}{2\\sigma&#94;{2}}&#94;{2}} $$ Taking the natural logarithm gives us the log likelihood: $$l\\left( \\vec{\\beta},\\sigma&#94;{2} \\middle| y,\\mathbf{X} \\right) = - \\frac{1}{2\\sigma&#94;{2}}\\sum_{i}&#94;{N}{{(y_{i}-\\vec{x}_{i}\\vec{\\beta})}&#94;{2}-\\frac{N}{2}\\ln\\left( \\sigma&#94;{2} \\right)+ C}= - \\frac{1}{2\\sigma&#94;{2}}\\left( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right) - \\frac{N}{2}\\ln\\left( \\sigma&#94;{2} \\right)+ C$$ Here, I have truncated the result to only those involving $\\vec{\\beta}$ and $\\sigma&#94;{2}$, as we are only interested in maximizing with respect to $\\vec{\\beta}$ . Note that I have transitioned $\\sigma&#94;{2}$ to a constant, as we are assuming the errors are iid with constant variance $\\sigma&#94;{2}$. Here, we obtain the same result as minimizing the least squares estimator, so the MLE and the least squares estimators are the same. If we were unconstrained, we would simply take the partial derivative here with respect to $\\vec{\\beta}$ to estimate $\\vec{\\beta}$ using maximum likelihood. Since $\\sigma&#94;{2}$ is unconstrained, we can find the maximum likelihood estimator directly and use this to maximize with respect to $\\vec{\\beta}$. To do this, we differentiate the above equation with respect to $\\sigma&#94;2$. Doing so yields the normal estimate for $\\sigma&#94;{2}$: $${\\widehat{\\sigma}}&#94;{2} = - \\frac{1}{N}\\sum_{i}&#94;{N}{(y_{i}-x_{i}\\vec{\\beta}&#94;{c})}&#94;{2}=\\frac{1}{N}\\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)$$ Where $\\vec{\\beta}&#94;{c}$ is our new constrained estimates. Plugging this back into the log likelihood equation yields: $$ l(\\vec{\\beta}&#94;{c} | \\vec{y},\\mathbf{\\mathbf{X}}) = - \\frac{N}{2}[{(\\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c})}&#94;{\\intercal}( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} )]&#94;{-1} ( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c})&#94;{\\intercal} \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c}) - \\frac{N}{2}\\ln( \\frac{1}{N}( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c})&#94;{\\intercal}( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c}))+ C $$$$l\\left( \\vec{\\beta} \\middle| \\vec{y},\\mathbf{X} \\right) = - \\frac{N}{2} - \\frac{N}{2}\\ln\\left( \\frac{1}{N}\\left( \\vec{y} -\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left( \\vec{y} -\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right) \\right)+ C$$ However, we want to maximize the above equation with the constraint that: M$\\vec{\\beta}$ = $\\vec{d}$, where d is a known vector and M is an r x p matrix of rank $r < p$. This is an equality constraint. We can use the Lagrange multipliers to solve this maximization problem with such a constraint. We can see that, since the natural logarithm is a monotonically increasing function, and the division within the above logarithm is a constant, and that all other terms do not involve $\\beta$, maximizing the above equation with respect to$\\ \\vec{\\beta}&#94;{c}$ is equivalent to minimizing: $$f = \\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right) - \\vec{\\lambda}&#94;{\\intercal}\\left( M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d} \\right)$$ Where $\\vec{\\lambda}$ is the Lagrange multiplier (and so $f$ is the Lagrange function). The Lagrange multiplier is especially useful in this case, as we are trying to minimize the least square error with some constraint on the parameters. Multiplying the error by the transpose gives: $$f =\\vec{y}&#94;{\\intercal}\\vec{y} -\\vec{y}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-{{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\vec{y} + {{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-\\vec{\\lambda}&#94;{\\intercal}\\left( M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d} \\right) = =\\vec{y}&#94;{\\intercal}\\vec{y} -2{{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\vec{y} + {{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-\\vec{\\lambda}&#94;{\\intercal}\\left( M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d} \\right)$$ Now we take derivatives with respect to $\\vec{\\beta}&#94;{c}$ and $\\vec{\\lambda}$ to obtain $$\\frac{\\partial{f}}{\\partial{B}}= - 2\\mathbf{X}&#94;{\\intercal}\\vec{y}+2\\mathbf{X}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-M&#94;{\\intercal}\\vec{\\lambda}$$$$\\frac{\\partial{f}}{\\partial{\\lambda}}=M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d}$$ We can now set both of these equal to zero and solve this system of equations to find the appropriate MLE estimate of $\\vec{\\beta}&#94;{c}$ . However, there is a few simplifying steps which can make the result more useable. The first is to get the expression in terms of the original least squares estimator. If we multiply the first equation above by M(${\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}$, we obtain: $$- \\ 2M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\mathbf{X}&#94;{\\intercal}\\vec{y}+\\ 2M{\\widehat{\\beta}}&#94;{c}-\\ M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = 0$$ The first term on the right is the familiar $\\vec{\\beta}$ from unconstrained OLS (I will denote this $\\vec{\\beta}&#94;{u}$). Substituting, we get $$- \\ 2M\\vec{\\beta}&#94;{u}+\\ 2M{\\widehat{\\beta}}&#94;{c}-\\ M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = 0$$$$- \\ 2M\\vec{\\beta}&#94;{u}+\\ 2M{\\widehat{\\beta}}&#94;{c}-\\ M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = 0$$$$M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = \\ 2M{\\widehat{\\beta}}&#94;{c} - \\ 2M{\\widehat{\\beta}}&#94;{u}$$ Solving for lambda: $$\\vec{\\lambda} = {(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\left\\lbrack 2M{\\widehat{\\beta}}&#94;{c} - \\ 2M{\\widehat{\\beta}}&#94;{u} \\right\\rbrack= 2{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ Plugging this back into the original equation and solving for ${\\widehat{\\beta}}&#94;{c}$, $$0= - 2\\mathbf{X}&#94;{\\intercal}\\vec{y}+2\\mathbf{X}&#94;{\\intercal}\\mathbf{X}{\\widehat{\\beta}}&#94;{c}-M&#94;{\\intercal}\\vec{\\lambda}$$$$0= - 2\\mathbf{X}&#94;{\\intercal}\\vec{y}+2\\mathbf{X}&#94;{\\intercal}\\mathbf{X}{\\widehat{\\beta}}&#94;{c}-{2(M}&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack)$$ $$\\mathbf{X}&#94;{\\intercal}\\mathbf{X}{\\widehat{\\beta}}&#94;{c}=\\mathbf{X}&#94;{\\intercal}\\vec{y}+M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ $${\\widehat{\\beta}}&#94;{c}= ({{\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\mathbf{X}}&#94;{\\intercal}\\vec{y}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ $${\\widehat{\\beta}}&#94;{c}=\\vec{\\beta}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ This is the desired result, the maximum likelihood estimator given the constraint M$\\vec{\\beta} =d$. Now, let's look at an example. $$\\vec{\\beta}_{p} = 0$$ Is equivalent to the following constraint. $\\vec{\\alpha}&#94;{\\intercal}\\vec{\\beta} = 0$ where $\\vec{\\alpha}$ is a vector of zeros in all but the last row, which is a 1. So, $$\\vec{\\alpha} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$ Plugging it into the derived result, we get: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha}&#94;{\\intercal}{(\\vec{\\alpha}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha})}&#94;{- 1}\\lbrack - \\ \\vec{\\alpha}'{\\widehat{\\beta}}&#94;{u}\\rbrack$$$${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha}&#94;{\\intercal}{(\\vec{\\alpha}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha})}&#94;{- 1}\\lbrack - \\ {{\\widehat{\\beta}}&#94;{u}}_{p}\\rbrack$$ Since $\\vec{\\alpha}&#94;{- 1}= \\vec{\\alpha}$ in this case, we know that $\\vec{\\alpha}&#94;{\\intercal}{(\\vec{\\alpha}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha})}&#94;{- 1} = \\vec{\\alpha}&#94;{\\intercal}\\left( \\vec{\\alpha}&#94;{\\intercal}\\left(\\mathbf{\\mathbf{X}}&#94;{\\intercal}\\mathbf{X} \\right)\\vec{\\alpha} \\right) = {\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}$ which is a scalar that is the bottom right element of the $\\mathbf{\\mathbf{X}}&#94;{\\intercal}\\mathbf{X}$ matrix. So we can simplify this to: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}{\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}\\lbrack - \\ {{\\widehat{\\beta}}&#94;{u}}_{p}\\rbrack$$ Since ${\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}\\lbrack 1 - \\ {\\beta&#94;{u}}_{p}\\rbrack$ is a constant: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}-{\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}(\\left\\lbrack \\ {\\beta&#94;{u}}_{p} \\right\\rbrack){{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}$$ So, if we compute the unconstrained ${\\widehat{\\beta}}&#94;{u}$ and ${{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}$, we can solve directly for the constrained $\\vec{\\beta}$. Now, for the second part of part a), this is equivalent to M being a vector of 1s, lets call it ϕ. Then $d=1$ and $$ \\vec{\\phi} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$ Plugging it into the derived result, we get: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\phi}&#94;{\\intercal}{(\\vec{\\phi}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\phi})}&#94;{- 1}\\lbrack 1 - \\vec{\\phi}'{\\widehat{\\beta}}&#94;{u}\\rbrack$$ The residual sum of squares is defined as: $$\\text{RSS} = \\ {(\\vec{y} -\\widehat{y})}&#94;{\\intercal}(\\vec{y} -\\widehat{y})$$ In this case, $$ \\widehat{y} =\\mathbf{\\mathbf{X}}{\\widehat{\\beta}}&#94;{c} = \\vec{\\beta}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}) =\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{u}+\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack $$ So it follows that $$ \\left(\\vec{y}-\\widehat{y_{c}} \\right) =\\vec{y}- \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{u}+\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack =\\vec{\\varepsilon} +\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack $$ where $\\vec{\\varepsilon}$ is the error in the unconstrained case. So $$\\text{RSS}_c - \\text{RSS} =\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack)$$ Each of the errors are normally distributed with mean 0 and variance sigma squared. This implies that: $$ (y - \\check{y})\\ I\\frac{1}{\\sigma}\\sim\\text{MVN}(0,I) \\text{, i.i.d} $$ Where the estimator is the estimator from the restrained equation. We can write the sum of squares of the above as: $$\\left(\\vec{y}- \\widehat{y} \\right)&#94;{\\intercal}(I\\frac{1}{\\sigma&#94;{2}})\\left(\\vec{y}- \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} \\right)&#94;{\\intercal}(I\\frac{1}{\\sigma&#94;{2}})\\left(\\vec{y}- \\check{y} \\right) + \\ \\left( \\check{y}-\\widehat{y} \\right)&#94;{\\intercal}\\left( I\\frac{1}{\\sigma&#94;{2}} \\right)\\left( \\check{y}-\\widehat{y} \\right)+ 2(\\check{y}-\\widehat{y})'\\left( I\\frac{1}{\\sigma&#94;{2}} \\right)(y -\\check{y})$$ Where$\\ {\\check{y}}_{i}\\ $is the estimator of the unconstrained model. To show this, we remove the identical identity matrices and see that: $$\\left(\\vec{y}- \\widehat{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} + \\check{y} - \\widehat{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\check{y} + \\check{y} - \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\check{y} \\right) + \\ \\left( \\check{y} - \\widehat{y} \\right)&#94;{\\intercal}\\left( \\check{y} - \\widehat{y} \\right) + 2(y - \\check{y})'\\left( \\check{y} - \\widehat{y} \\right)$$ Now, $$\\left( \\check{y} - \\widehat{y} \\right) = \\left(\\vec{y}- \\widehat{y} - \\left(\\vec{y}- \\check{y} \\right) \\right) = \\left( \\ \\left(\\vec{y}- \\widehat{y} \\right) - \\vec{\\varepsilon} \\right)$$ which we know from the above is $(\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$)' Rewriting, we have: $$\\left(\\vec{y}- \\check{y} \\right)'\\left( \\left(\\vec{y}- \\widehat{y} \\right) - \\vec{\\varepsilon} \\right) = \\vec{\\varepsilon}'(\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\left\\lbrack \\vec{d} - \\ M{\\widehat{\\beta}}&#94;{u} \\right\\rbrack)$$ Using the fact that $\\vec{\\varepsilon}&#94;{\\intercal}\\mathbf{X} = 0$ , we can remove this to obtain. $${\\left(\\vec{y}- \\widehat{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} \\right)}&#94;{\\intercal}\\left(\\vec{y}- \\check{y} \\right) + \\ \\left( \\check{y} - \\widehat{y} \\right)&#94;{\\intercal}\\left( \\check{y} - \\widehat{y} \\right)$$ Thus the original standardized normal variable can be represented as a sum of $Q_1$, and $Q_2$ where: $$Q_1 = \\left(\\vec{y}-\\check{y} \\right)&#94;{\\intercal}\\left(\\vec{y}-\\check{y} \\right)$$$$Q_2 = \\left( \\check{y}-\\widehat{y} \\right)&#94;{\\intercal}\\left( \\check{y}-\\widehat{y} \\right) $$ $Q_1$ is the sum of squared errors for the unconstrained model, and therefore has rank $n - p$. $Q_2$ has rank $p - r$, as it is the differences between the two estimators. Thus rank($Q_1$) + rank ($Q_2$) = $n-r$ which is the rank of the errors of the constrained model, since $r<p$. Thus by Cochran's Theorem, the two are independently distributed. I have also produced simulations to prove that this is the case. I create three normal random variables, two of which are related to the others linearly and randomly. I compute the beta unconstrained, and then use this to compute the beta constrained. I then produce the errors for the constrained beta, and check the correlations in 100 samples of 100 each. The mean correlation is very close to zero, which is the expected result. This is included below. In [2]: const <- vector () unconst <- vector () correl <- vector () for ( j in 1 : 100 ) { for ( i in 1 : 100 ) { y <- rnorm ( 100 ) x1 <- y + rnorm ( 100 , 0 , . 2 ) x2 <- rnorm ( 100 , 0 , . 01 ) - . 3 * y Xmat <- matrix ( c ( rep ( 1 , length ( x1 )), x1 , x2 ), ncol = 3 ) data2 <- data.frame ( y , x1 , x2 ) mod.sim.1 <- lm ( y ~ x1 + x2 , data = data2 ) constraint <- matrix ( 1 , nrow = nrow ( data2 ), ncol = 1 ) d = 1 M = matrix ( rep ( 1 , 3 ), ncol = 1 ) mod.sim.1 BetaUnconstr <- summary ( mod.sim.1 ) $ coef [, 1 ] b <- d - t ( M ) %*% BetaUnconstr a <- t ( Xmat ) %*% Xmat c <- solve ( t ( M ) %*% solve ( a ) %*% M ) const [ i ] <- b * c * b unconst [ i ] <- sum ( resid ( mod.sim.1 ) &#94; 2 ) } correl [ j ] <- cor ( const , unconst ) } correl mean ( correl ) -0.0531688267764585 -0.114464495425658 0.030905848126332 -0.0804090923830123 -0.0282214216966975 0.194785864859665 0.0830930039710045 0.0724914248601695 0.0829089181916613 0.0288906516772889 0.0279474715925631 -0.0533135707611509 0.0658332419742044 -0.0799946756911054 -0.0941441499229167 -0.024909668515204 0.0714084573952652 -0.0455394491591305 0.166193108785476 -0.077429232110221 -0.128495293842719 -0.0786313087721033 -0.135204615732129 0.00402444738581924 -0.210414921426846 -0.0466002337555276 -0.0744914482523485 0.0253724403628865 -0.144593000144071 -0.0558067022331745 0.0941802108323231 -0.0326002696533416 0.1191407256456 0.0742989188901125 0.0498008210293263 0.02980912239177 -0.132207185159464 -0.154762140608869 -0.0311445812733598 0.0213674326991292 -0.0563900846752629 0.113499475702925 -0.125719333220632 0.0434611514397736 0.202868180728299 -0.113919008868015 -0.0382369212754716 0.0491389841646633 -0.0198221517858243 0.0175352131496739 0.0264657952051942 0.119278436715872 -0.218521793800235 0.0159366944834975 0.194276086012615 -0.121180977909389 -0.165054100432551 -0.0651733446635078 -0.07680468077657 -0.0503302517530004 0.00823376132659068 0.00388666785680694 -0.00915532185917745 -0.0930607404292774 -0.171362738860478 -0.0310668331585724 -0.0210004347148357 0.0549900820264029 -0.0468877624516252 0.0868626310356565 0.0532565684252016 -0.168838788927617 0.0339257077316963 0.0762184886512164 -0.0890754231460831 -0.0647727147695691 -0.0889513202649563 0.000667073625725037 0.0509714346818621 -0.0940193584932185 -0.0839669876254321 0.106982624440368 0.145030507757848 -0.0757386310783636 0.161739345883223 0.214585243128979 -0.0764856927072532 0.0477290142665558 0.0126102486334496 -0.0741476392314046 -0.0725679361079652 -0.0807334358109478 0.0186774645788979 0.0234449247050084 0.0176989561150896 0.0516204767156866 0.120367846574953 0.0233333600272549 -0.00113490805336294 0.118557812809982 -0.00814363230874542","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/constrained-least-squares.html","loc":"https://seanammirati.github.io/category/linear-models/constrained-least-squares.html"},{"title":"HMMs: Viterbi Algorithm","text":"This investigates HMMs (Hidden Markov Models) with two possible states and six possible emissions using the viterbi algorithm. Given a Markov process with two states (state 1 and state 2) and an arbitrary number of possible observed emissions , with transition probabilities and emission probabilities given for each state, we can find the most likely sequence of states given some observed emissions using the viterbi algorithm. This algorithm is an example of dynamic programming which produces results far faster than a brute force approach. This takes less time to run than considering all possible sequences of states -- we only need to consider the state probabilities forwards, rejecting paths that are unlikely at each point. The example which the following function defaults to is the case of a fair and loaded die, each with some probability of values 1-6. The roller does not show you what die he is using, but we know that he will generally switch between the two die with some transition probs. The goal is then to find the most likely sequence of fair and loaded die given the observed sequence of emissions (for example 1,2,2,1,3). In [12]: viterbialgorithm <- function ( values , initprobstates = c ( . 5 , . 5 ), probvectorstate1 = c ( rep ( 1 / 6 , 6 )), probvectorstate2 = c ( rep ( 1 / 10 , 5 ), 1 / 2 ), Ps1s2 = 0.05 , Ps2s1 = . 1 ){ ## values (vector): a series of observed values of two emissions. ## initprobstates (vector, len 2): the probabilities of entering the system in either state. (assuming .5, .5 is the least informative) ## probvectorstate1 (vector): the emission probabilities of the first state. ## probvectorstate2 (vector): the emission probabilities of the second state. ## Ps1s2 (numeric): probability of entering state 2 from state 1. ## Ps2s1 (numeric): probability of entering state 1 from state 2. iterations <- length ( values ) initprobstate1 = initprobstates [ 1 ] initprobstate2 = initprobstates [ 2 ] P11 <- 1 - Ps1s2 P22 <- 1 - Ps2s1 if ( iterations < 1 ) { stop ( \"Length of vector must be a positive integer.\" ) } # Initializes the ret list for the first sequence. ret <- list ( c ( s0 = 0 , s1 = - Inf , s2 = - Inf , st = 0 , prevst = NA )) ## Creates the transition matrix. transmatrix <- matrix ( c ( 0 , 0 , 0 , initprobstate1 , P11 , Ps2s1 , initprobstate2 , Ps1s2 , P22 ), ncol = 3 ) maxfun <- function ( potentialstate , prevvector ){ ## potentialstate (integer): integer referring to the potential state (1 or 2) ## prevvector (vector): previous probability of states. trans <- transmatrix [, potentialstate + 1 ] mx <- max ( prevvector [ 1 : 3 ] + log ( trans )) prevst <- which.max ( prevvector [ 1 : 3 ] + log ( trans )) - 1 return ( list ( val = mx , state = prevst )) } for ( i in 1 : iterations ) { maxst1 <- maxfun ( 1 , ret [[ i ]]) maxst2 <- maxfun ( 2 , ret [[ i ]]) ekst1 <- log ( probvectorstate1 [ values [ i ]]) ekst2 <- log ( probvectorstate2 [ values [ i ]]) V1 <- maxst1 [[ 'val' ]] + ekst1 V2 <- maxst2 [[ 'val' ]] + ekst2 end <- which.max ( c ( V1 , V2 )) ret [[ i + 1 ]] <- c ( s0 = - Inf , s1 = V1 , s2 = V2 , st = end , prevst = c ( maxst1 [[ 'state' ]], maxst2 [[ 'state' ]])) } finalstate <- ret [[ iterations + 1 ]][ 'st' ] path <- c ( rep ( 0 , iterations + 1 )) path [ iterations + 1 ] <- finalstate for ( i in ( iterations ) : 2 ) { path [ i ] <- ret [[ i + 1 ]][ 5 : 6 ][ path [ i + 1 ]] } res <- as.data.frame ( ret [ -1 ]) colnames ( res ) <- c ( values ) return ( list ( steps = res , path = path )) } In [13]: observed <- c ( 6 , 5 , 1 , 1 , 6 , 6 , 4 , 5 , 3 , 1 , 3 , 2 , 6 , 5 , 1 , 2 , 4 , 5 , 3 , 6 , 6 , 6 , 4 , 6 , 3 , 1 , 6 , 3 , 6 , 6 , 6 , 3 , 1 , 6 , 2 , 3 , 2 , 6 , 4 , 5 , 5 , 2 , 3 , 6 , 2 , 6 , 6 , 6 , 6 , 6 , 6 , 2 , 5 , 1 , 5 , 1 , 6 , 3 , 1 ) results <- viterbialgorithm ( observed ) resultswords <- c ( \"Fair\" , \"Loaded\" )[ results $ path ] resultswords 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' So this tells us that given the observed die rolls, the most likely sequence of states is as shown above. This did not have to be done manually -- we can use the HMM package to do this. It will serve as a check. In [14]: require ( HMM ) mod <- initHMM ( c ( \"Fair\" , \"Loaded\" ), c ( 1 , 2 , 3 , 4 , 5 , 6 ), startProbs = c ( . 5 , . 5 ), transProbs = rbind ( c ( . 95 , . 05 ), c ( . 1 , . 90 )), emissionProbs = rbind ( c ( rep ( 1 / 6 , 6 )), c ( rep ( 1 / 10 , 5 ), 1 / 2 ))) viterbi ( mod , observed ) == resultswords ## :) TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE","tags":"Markov Chains","url":"https://seanammirati.github.io/category/markov-chains/hmms-viterbi-algorithm.html","loc":"https://seanammirati.github.io/category/markov-chains/hmms-viterbi-algorithm.html"},{"title":"Hotelling's T-Test Example","text":"Here's an example of the Hotelling's MV T-test with responses and hypothesized means. Hotelling's MV T-test allows us to use a multivariate analogue of the t-test, comparing the observed values with some hypothesized mean. In this, similar to the univariate case, we assume that the population is Multivariate Normally Distributed, that is: $$ \\vec{x} \\sim MVN(\\vec{\\mu}, \\Sigma) $$ where $\\Sigma$ is a square, symmetric matrix. This test is implemented below. In [4]: ## Hotelling's t-test for multivariate normal. prob_null <- function ( data , hypothesized_mean_vector ) { n = nrow ( data ) p = ncol ( data ) tsqobs <- hotellings_tsq_statistic ( data , hypothesized_mean_vector ) f_dist <- hotellings_f_statistic_trans ( tsqobs , n , p ) return ( 1 - pf ( f_dist [ 'f' ], f_dist [ 'df1' ], f_dist [ 'df2' ])) } hotellings_f_statistic_trans <- function ( tsqobs , n , p ) { f <- (( n - p ) / ( p * ( n - 1 ))) * tsqobs df1 <- p df2 <- n - p return ( c ( f = f , df1 = df1 , df2 = df2 )) } hotellings_tsq_statistic <- function ( data , hypothesized_mean_vector ) { sample_mean <- apply ( data , 2 , mean ) sample_covar <- cov ( data ) S_inv <- solve ( sample_covar ) n <- nrow ( data ) mu <- hypothesized_mean_vector tsqobs <- n * t ( sample_mean - mu ) %*% S_inv %*% ( sample_mean - mu ) return ( tsqobs ) } find_discriminant <- function ( data , hypothesized_mean_vector ) { sample_mean <- apply ( data , 2 , mean ) sample_covar <- cov ( data ) discriminant <- solve ( sample_covar ) %*% ( sample_mean - hypothesized_mean_vector ) return ( discriminant ) } In [5]: response_data <- matrix ( c ( 51 , 27 , 37 , 42 , 27 , 43 , 41 , 38 , 36 , 26 , 29 , 36 , 20 , 22 , 36 , 18 , 32 , 22 , 21 , 23 , 31 , 20 , 50 , 26 , 41 , 32 , 33 , 43 , 36 , 31 , 27 , 31 , 25 , 35 , 17 , 37 , 34 , 14 , 35 , 25 , 20 , 25 , 32 , 26 , 42 , 27 , 30 , 27 , 29 , 40 , 38 , 16 , 28 , 36 , 25 ), ncol = 5 ) hypothesized_mean <- c ( 30 , 25 , 40 , 25 , 30 ) response_data prob_null ( response_data , hypothesized_mean ) find_discriminant ( response_data , hypothesized_mean ) 51 36 50 35 42 27 20 26 17 27 37 22 41 37 30 42 36 32 34 27 27 18 33 14 29 43 32 43 35 40 41 22 36 25 38 38 21 31 20 16 36 23 27 25 28 26 31 31 32 36 29 20 25 26 25 f: 0.00669952528414886 0.5298893 -0.2659554 -0.6946549 0.1483265 0.3206404 From this, because the p value is less than .05, we reject the hypothesis that this data comes from a normally distributed population with mean vector: $$ \\vec{\\mu} = \\begin{bmatrix} 30 \\\\ 25 \\\\ 40 \\\\ 25 \\\\ 30 \\end{bmatrix} $$ The discriminant indicates that the third variable contributes most to the difference between the hypothesized and sample mean. In [6]: require ( MASS ) ex <- mvrnorm ( 11 , mu = hypothesized_mean , Sigma = cov ( response_data )) prob_null ( ex , hypothesized_mean ) find_discriminant ( ex , hypothesized_mean ) f: 0.684532555387565 -0.03199313 -0.07721548 0.09417471 0.09898568 -0.13109563 Now we see that a multivariate normal distribution sampled with mean equal to the hypothesized mean and Sigma equal to the covariance of the observed data provides unsignificant results -- as expected. Now, let's see the t-test for the equality of means, assuming an equal covariance matrix and sample sizes. In [7]: differences <- response_data - ex prob_null ( differences , rep ( 0 , 5 )) find_discriminant ( differences , rep ( 0 , 5 )) f: 0.00818771246890448 0.35457592 -0.09934875 -0.70226005 0.01886289 0.33948696 We see the expected result: that the two samples produce significant results -- meaning that there is evidence to reject the null hypothesis that they come from a distribution with the same mean vector.","tags":"Multivariate Analysis","url":"https://seanammirati.github.io/category/multivariate-analysis/hotellings-t-test-example.html","loc":"https://seanammirati.github.io/category/multivariate-analysis/hotellings-t-test-example.html"},{"title":"Generalized Linear Models","text":"Here we will consider the marathon runners listed in this wikipedia page . This post will consider various generalized linear models (GLMS) with different families. In [10]: timesm <- c ( 123.98 , 128.633 , 134.266 , 139.484 , 139.533 , 156.5 , 161.95 , 174.8 , 184.9 , 195.9 , 236.567 , 340.017 , 505.283 ) timesf <- c ( 139.316 , 144.893 , 149.00 , 151.083 , 170.55 , 181.5 , 192.95 , 215.483 , 233.7 , 252.733 , 314.433 , 533.133 ) agem <- c ( 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 100 ) agef <- c ( 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 ) agemsq <- agem &#94; 2 time <- c ( timesf , timesm ) age <- c ( agef , agem ) gender <- c ( rep ( 1 , 12 ), rep ( 0 , 13 )) marathon <- data.frame ( time , age , gender ) head ( marathon ) time age gender 139.316 35 1 144.893 40 1 149.000 45 1 151.083 50 1 170.550 55 1 181.500 60 1 First, let's do an ordinary linear regression model with a Gaussian response variable assumption. Linear Regression (Gaussian) In [3]: lin1 <- lm ( timesm ~ agem , data = marathon ) lin2 <- lm ( timesm ~ agem + agemsq , data = marathon ) summary ( lin1 ) summary ( lin2 ) Call: lm(formula = timesm ~ agem, data = marathon) Residuals: Min 1Q Median 3Q Max -70.80 -47.41 -15.95 28.83 149.61 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -89.203 62.054 -1.438 0.17841 agem 4.449 0.910 4.889 0.00048 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 63.52 on 11 degrees of freedom Multiple R-squared: 0.6848, Adjusted R-squared: 0.6562 F-statistic: 23.9 on 1 and 11 DF, p-value: 0.0004801 Call: lm(formula = timesm ~ agem + agemsq, data = marathon) Residuals: Min 1Q Median 3Q Max -47.811 -15.503 5.468 19.742 40.354 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 498.57207 98.79145 5.047 0.000502 *** agem -14.89307 3.13305 -4.754 0.000776 *** agemsq 0.14557 0.02335 6.233 9.72e-05 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 30.14 on 10 degrees of freedom Multiple R-squared: 0.9355, Adjusted R-squared: 0.9226 F-statistic: 72.49 on 2 and 10 DF, p-value: 1.118e-06 Based on the summary, it appears that the quadratic term is quite important, it increases the R-squared significantly. So there does appear to be a quadratic effect of age on the time (or at the very least, a non-linear one). In [4]: lin3 <- lm ( time ~ age + gender ) summary ( lin3 ) Call: lm(formula = time ~ age + gender) Residuals: Min 1Q Median 3Q Max -74.18 -42.85 -14.10 26.96 181.20 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -104.3254 48.9768 -2.130 0.0446 * age 4.6801 0.6979 6.706 9.68e-07 *** gender 35.0534 25.7576 1.361 0.1873 --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 64.15 on 22 degrees of freedom Multiple R-squared: 0.6749, Adjusted R-squared: 0.6454 F-statistic: 22.84 on 2 and 22 DF, p-value: 4.285e-06 Here, the model shows that gender has some part in determining the time (in seconds) of completion, but it is not significant. If the coefficient is correct, it means that, holding age constant, a man will have 35.0534 faster time (or a woman will take 35.0534 additional seconds) to finish the race. Comparing the intercepts in the models shows a pretty large difference in the predictions. The reason why the male-only intercept is different from the intercept in the second model is because the second model is not considering the interaction of age and gender. When you account for this in the model, the intercepts will be the same. The gender coefficient in the model assumes that you are holding everything else constant, that is, the difference between a 35 year old male and female, for instance. When you consider the interaction term as well, this problem is eliminated. Gamma Distribution In [5]: gammalin1 <- glm ( timesm ~ agem , family = \"Gamma\" ) summary ( gammalin1 ) Call: glm(formula = timesm ~ agem, family = \"Gamma\") Deviance Residuals: Min 1Q Median 3Q Max -0.169540 -0.067254 0.001897 0.092610 0.116811 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 1.275e-02 5.748e-04 22.17 1.76e-10 *** agem -1.057e-04 6.839e-06 -15.46 8.30e-09 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for Gamma family taken to be 0.009790868) Null deviance: 2.42365 on 12 degrees of freedom Residual deviance: 0.10997 on 11 degrees of freedom AIC: 116.42 Number of Fisher Scoring iterations: 4 The link function here is the canonical link function for the Gamma distribution, which is the inverse ($\\frac{1}{\\mu}$). All variables come up as significant. The model can be written as follows $Y = X\\beta + \\epsilon$ where Y follows a Gamma distribution with link function $g(\\mu) = \\frac{1}{\\mu}$ Inverse-Gaussian distribution The exponential family has the following form: $$ f(y,\\theta ,\\phi ) = e&#94;{\\frac{y \\theta - b(\\theta )}{a(\\phi )} + c(y, \\phi)} $$ The inverse-Gaussian distribution is defined as: $$ f(y) = \\exp (-\\frac{\\lambda (y - \\mu )&#94;2)}{2\\mu &#94;2y} - \\frac{1}{2} \\log (\\frac{2\\pi y&#94;3}{\\lambda})) $$ $$ f(y) = \\exp (-\\frac{\\lambda (y&#94;2 - 2y\\mu + \\mu &#94;2)}{2\\mu &#94;2y} - \\frac{1}{2} \\log (\\frac{2\\pi y&#94;3}{\\lambda})) $$ $$ f(y) = \\exp (\\lambda [-\\frac{y}{2\\mu &#94;2} + \\frac{1}{\\mu}] - \\frac{\\lambda }{2y} - \\frac {1}{2}\\log(\\frac {2\\pi y&#94;3}{\\lambda})) $$ $$ f(y) = \\exp (\\frac {[-\\frac{y}{2\\mu &#94;2} + \\frac{1}{\\mu}]}{\\frac {1}{\\lambda}} - \\frac{\\lambda }{2y} - \\frac {1}{2}\\log(\\frac {2\\pi y&#94;3}{\\lambda})) $$ This is in the exponential form, that is, $$\\theta = - \\frac {1} {2 \\mu &#94;2}$$ $$a(\\phi) = \\frac {1}{\\lambda} = { \\phi}$$ $$b(\\theta) = - \\frac {1}{\\mu} = -\\sqrt{2 \\theta}$$ $$c(y,\\theta) = - \\frac{\\lambda}{2y} - \\frac{1}{2}\\log(\\frac{2\\pi y&#94;3}{\\lambda}) = - \\frac{1}{y\\phi} - \\frac{1}{2}\\log(2\\pi \\phi y&#94;3)$$ $b'(\\theta) = {\\sqrt {\\frac{2}{\\theta }}} = \\mu$ , which is $\\text{E}(Y)$. The canonical link function is $g(\\text{E}(Y))$ = $\\theta$. So, the canonical link is proportional to $\\frac {1}{\\mu &#94;2}$, which is equivalent to $X\\beta$. The model is then $Y = X\\beta + \\epsilon$ where Y follows a inverse Gaussian distribution with link function $g(\\text{E}(Y))$ = 1/$\\mu &#94;2$ In [7]: invgauslin1 <- glm ( timesm ~ agem , family = \"inverse.gaussian\" ) summary ( invgauslin1 ) Call: glm(formula = timesm ~ agem, family = \"inverse.gaussian\") Deviance Residuals: Min 1Q Median 3Q Max -0.0048497 -0.0023005 -0.0000681 0.0005923 0.0123111 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 9.958e-05 3.892e-06 25.59 3.75e-11 *** agem -9.617e-07 4.179e-08 -23.01 1.18e-10 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for inverse.gaussian family taken to be 2.180685e-05) Null deviance: 0.01111018 on 12 degrees of freedom Residual deviance: 0.00020857 on 11 degrees of freedom AIC: 102.69 Number of Fisher Scoring iterations: 4 Both are quite difficult to interpret by themselves, as the response variables are transformed via the link function. The predictions for the gamma are as follows: In [8]: predict ( gammalin1 ) pred1 <- 1 / predict ( gammalin1 ) pred2 <- 1 / (( predict ( invgauslin1 ))) &#94; . 5 pred3 <- predict ( lin1 ) sum (( pred1 - timesm ) &#94; 2 ) sum (( pred2 - timesm ) &#94; 2 ) sum (( pred3 - timesm ) &#94; 2 ) sum (( predict ( lin2 ) - timesm ) &#94; 2 ) 1 0.00904503120484234 2 0.00851640593952094 3 0.00798778067419954 4 0.00745915540887814 5 0.00693053014355673 6 0.00640190487823533 7 0.00587327961291393 8 0.00534465434759253 9 0.00481602908227113 10 0.00428740381694972 11 0.00375877855162832 12 0.00323015328630692 13 0.00217290275566411 6398.61022699518 5598.12999367698 44389.3243635469 9086.8812639335 Here, we can see the sum squared error of the transformed predictions from the models. We can see that the model using the gamma distribution does an exceptional job at predicting the response variable, time. It is better than both the linear and quadratic simple linear regression models. The inverse-gaussian performs the best out of the three candidate distributions. Below, I do the same thing, except using the identity function. This produces more easily interpretable coefficients, but they are less efficient at predicting the model. That is, when we take the sum of squares, the identity links are less successful at determining the response. In [9]: gammalin2 <- glm ( timesm ~ agem , family = \"Gamma\" ( link = \"identity\" )) summary ( gammalin2 ) invgauslin2 <- glm ( timesm ~ agem , family = \"inverse.gaussian\" ( link = identity )) summary ( invgauslin2 ) predict ( gammalin2 ) predict ( invgauslin2 ) sum (( predict ( gammalin2 ) - timesm ) &#94; 2 ) sum (( predict ( invgauslin2 ) - timesm ) &#94; 2 ) Call: glm(formula = timesm ~ agem, family = Gamma(link = \"identity\")) Deviance Residuals: Min 1Q Median 3Q Max -0.21209 -0.18853 -0.09241 0.08858 0.54268 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -8.3100 38.5463 -0.216 0.833256 agem 3.1548 0.6898 4.574 0.000799 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for Gamma family taken to be 0.06523248) Null deviance: 2.42365 on 12 degrees of freedom Residual deviance: 0.60806 on 11 degrees of freedom AIC: 138.73 Number of Fisher Scoring iterations: 9 Call: glm(formula = timesm ~ agem, family = inverse.gaussian(link = identity)) Deviance Residuals: Min 1Q Median 3Q Max -0.012827 -0.011721 -0.006183 0.004403 0.032956 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 10.6841 32.6336 0.327 0.749516 agem 2.7958 0.6252 4.472 0.000944 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for inverse.gaussian family taken to be 0.000289055) Null deviance: 0.0111102 on 12 degrees of freedom Residual deviance: 0.0024039 on 11 degrees of freedom AIC: 134.47 Number of Fisher Scoring iterations: 10 1 102.107169679271 2 117.881054509177 3 133.654939339083 4 149.428824168989 5 165.202708998895 6 180.976593828801 7 196.750478658707 8 212.524363488613 9 228.29824831852 10 244.072133148426 11 259.846017978332 12 275.619902808238 13 307.16767246805 1 108.535713328251 2 122.514520425568 3 136.493327522885 4 150.472134620202 5 164.45094171752 6 178.429748814837 7 192.408555912154 8 206.387363009471 9 220.366170106788 10 234.344977204105 11 248.323784301422 12 262.302591398739 13 290.260205593373 52728.0806590428 58577.3835961494","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/generalized-linear-models.html","loc":"https://seanammirati.github.io/category/linear-models/generalized-linear-models.html"},{"title":"MANOVA","text":"Here we will be doing MANOVA for the dataset below. We want to know if the means of tear, gloss and opacity are statistically significantly different from one another depending on the rate groups -- Low or High. Data from manova example: Krzanowski (1998, p. 381) In evaluating MANOVA (Multivariate Analysis of Variance), we can consider a few different approaches to estimating the \"size\" of deviance from the between (the matrix $H$) and the within (the matrix $E$) distances. Each matrix represents something similar to the univariate ANOVA example, but we are now comparing the differences in matrices rather than in vectors. This makes the task more difficult, as we cannot simply use the F-distribution as a comparison of squared differences -- our results will also look like matrices, not single numbers. There have been four proposed solutions to this problem, named after Wilks, Roys, Pillai and Hotelling respectively. Each of these attempts to quantify the magnitude of the between matrix divided by the within matrix using the determinant or the eigenvalues of the product of these two matrices. If this value is greater than a certain threshold, we conclude that the groups are different, via a similar intuition with ANOVA. However, unlike ANOVA, no such simple distribution exists to describe the two error matrices. We also implement here the F approximations for each of these statistics to quantify our results. In [2]: return_MANOVA_results <- function ( dataset , group_column ) { F_dists <- find_approx_F_dist ( dataset , group_column ) results <- 1 - sapply ( X = F_dists [ c ( 'Wilks' , 'Roy' , 'Pillai' , 'Hotelling' )], FUN = function ( obj ) pf ( obj [ 1 ], obj [ 2 ], obj [ 3 ])) results <- rbind ( p_f = results , statistic = F_dists $ untransformed [ 1 : 4 ], data.frame ( F_dists [ c ( 'Wilks' , 'Roy' , 'Pillai' , 'Hotelling' )], row.names = c ( 'F Approx.' , 'df1' , 'df2' ))) return ( results ) } find_approx_F_dist <- function ( dataset , group_column ) { # dataset (dataframe): dataset with groupings as integer values. # group_column (string): string name of column with group entries, integer values. MANOVA_stat_lst <- find_MANOVA_statistics ( dataset , group_column ) Vh <- MANOVA_stat_lst [ 'n_groups' ] - 1 p <- ncol ( dataset ) - 1 Ve <- nrow ( dataset ) - Vh - 1 s <- min ( Vh , p ) Wilks_approx_F <- FforWilks ( Lambda = MANOVA_stat_lst [ 'Wilks' ], Vh = Vh , p = p , Ve = Ve ) Roy_approx_F <- FBoundforRoy ( R = MANOVA_stat_lst [ 'Roys' ], Vh = Vh , p = p , Ve = Ve ) Pillai_approx_F <- FapproxforPillai ( P = MANOVA_stat_lst [ 'Pillai' ], Vh = Vh , p = p , Ve = Ve , s = s ) Hotelling_approx_F <- FapproxforHotelling ( H = MANOVA_stat_lst [ 'Hotelling' ], Vh = Vh , p = p , Ve = Ve ) return ( list ( untransformed = MANOVA_stat_lst , Wilks = Wilks_approx_F , Roy = Roy_approx_F , Pillai = Pillai_approx_F , Hotelling = Hotelling_approx_F )) } find_MANOVA_statistics <- function ( dataset , group_column ){ # dataset (dataframe): dataset with groupings as integer values. # group_column (string): string name of column with group entries, integer values. grouped_ds_lst <- lapply ( 1 : length ( unique ( dataset [, group_column ])), function ( i ) dataset [ dataset [, group_column ] == i , ! names ( dataset ) %in% c ( group_column )]) H <- Reduce ( \"+\" , lapply ( grouped_ds_lst , function ( g ) hypothesis_iv ( g , dataset ))) E <- Reduce ( \"+\" , lapply ( grouped_ds_lst , function ( g ) error_iv ( g , dataset ))) Einv <- solve ( E ) HoverE <- H %*% Einv eigen_det <- eigen ( HoverE ) ev <- eigen_det $ values Wilks <- 1 / ( prod ( 1 + ev )) Roys <- ( max ( ev )) / ( 1 + max ( ev )) Pillai <- sum ( ev / ( 1 + ev )) Hotelling <- sum ( ev ) return ( c ( Wilks = Wilks , Roys = Roys , Pillai = Pillai , Hotelling = Hotelling , n_groups = length ( grouped_ds_lst ))) } hypothesis_iv <- function ( group , dataset ) { A <- sapply ( group , mean ) B <- sapply ( dataset [, 1 : ( ncol ( dataset ) - 1 )], mean ) dif <- A - B return (( dif %*% t ( dif )) * nrow ( group )) } error_iv <- function ( group , dataset ) { A <- sapply ( group , mean ) mattimestranspose <- function ( A ) return ( A %*% t ( A )) mylist <- lapply ( 1 : nrow ( group ), function ( i ) mattimestranspose ( t ( as.matrix ( group [ i ,] - A )))) a1 <- Reduce ( '+' , mylist ) return ( a1 ) } FforWilks <- function ( Lambda , Vh , p , Ve ) { t <- ((( p &#94; 2 ) * ( Vh &#94; 2 ) - 4 ) / (( p &#94; 2 ) + ( Vh &#94; 2 ) - 5 )) &#94; . 5 df1 <- p * Vh w <- Ve + Vh - (( p + Vh + 1 ) / 2 ) df2 <- w * t - ((( p * Vh ) - 2 ) / 2 ) Fapprox <- (( 1 - ( Lambda &#94; ( 1 / t ))) / ( Lambda &#94; ( 1 / t ))) * ( df2 / df1 ) return ( c ( Fapprox = Fapprox , df1 = df1 , df2 = df2 )) } FBoundforRoy <- function ( R , Vh , p , Ve ) { v1 <- ( Ve - p - 1 ) / 2 v2 <- ( p - Vh - 1 ) / 2 return ( c ( Fapprox = R * v1 / v2 , df1 = 2 * v1 + 2 , df2 = 2 * v2 + 2 )) } FapproxforPillai <- function ( P , Vh , p , Ve , s ) { N <- ( Ve - Vh + s ) / 2 m <- ( abs ( Vh - p ) - 1 ) / 2 num <- ( 2 * N + s + 1 ) * P den <- ( 2 * m + s + 1 ) * ( s - P ) Fapprox <- num / den df1 <- s * ( 2 * m + s + 1 ) df2 <- s * ( 2 * N + s + 1 ) return ( c ( Fapprox = Fapprox , df1 = df1 , df2 = df2 )) } FapproxforHotelling <- function ( H , Vh , p , Ve ) { a <- p * Vh B <- (( Ve + Vh - p - 1 ) * ( Ve - 1 )) / (( Ve - p - 3 ) * ( Ve - p )) b <- 4 + (( a + 2 ) / ( B - 1 )) cnum <- a * ( b - 2 ) cden <- b * ( Ve - p + 1 ) Fapprox <- ( H * cden ) / cnum return ( c ( Fapprox = Fapprox , df1 = a , df2 = b )) } In [3]: tear <- c ( 6.5 , 6.2 , 5.8 , 6.5 , 6.5 , 6.9 , 7.2 , 6.9 , 6.1 , 6.3 , 6.7 , 6.6 , 7.2 , 7.1 , 6.8 , 7.1 , 7.0 , 7.2 , 7.5 , 7.6 ) gloss <- c ( 9.5 , 9.9 , 9.6 , 9.6 , 9.2 , 9.1 , 10.0 , 9.9 , 9.5 , 9.4 , 9.1 , 9.3 , 8.3 , 8.4 , 8.5 , 9.2 , 8.8 , 9.7 , 10.1 , 9.2 ) opacity <- c ( 4.4 , 6.4 , 3.0 , 4.1 , 0.8 , 5.7 , 2.0 , 3.9 , 1.9 , 5.7 , 2.8 , 4.1 , 3.8 , 1.6 , 3.4 , 8.4 , 5.2 , 6.9 , 2.7 , 1.9 ) rate <- gl ( 2 , 10 , labels = c ( \"Low\" , \"High\" )) Y <- data.frame ( cbind ( tear , gloss , opacity , rate )) return_MANOVA_results ( Y , 'rate' ) Wilks Roy Pillai Hotelling p_f 0.002273044 0.05409917 4.282064e-04 0.001220981 statistic 0.413619230 0.58638077 5.863808e-01 1.417682561 F Approx. 7.560973658 8.20933078 9.451217e+00 8.641112752 df1 3.000000000 16.00000000 3.000000e+00 3.000000000 df2 16.000000000 3.00000000 2.000000e+01 16.000000000 We see that all four methods approximately agree that this would be unlikely if the two groups followed the same Multivariate Normal Distribution.","tags":"Multivariate Analysis","url":"https://seanammirati.github.io/category/multivariate-analysis/manova.html","loc":"https://seanammirati.github.io/category/multivariate-analysis/manova.html"},{"title":"To T-Test or Not to T-Test","text":"An Exploration of Using Bayesian Principles to Generalize the T-Test Procedure Standard T-Test Consider the t-test of equality of means. Using the standard procedure for a t-test: In [ ]: test_equality_means <- function ( norm_vec1 , norm_vec2 ){ sx <- var ( norm_vec1 ) sy <- var ( norm_vec2 ) xbar <- mean ( norm_vec1 ) ybar <- mean ( norm_vec2 ) n = length ( norm_vec1 ) m = length ( norm_vec2 ) tstat <- ( xbar - ybar ) / ( sqrt (( 1 / n + 1 / m ) * (( n - 1 ) * sx + ( m - 1 ) * sy ) / ( n + m - 2 ))) p_tstat <- pt ( tstat , n + m - 2 ) return ( list ( tstat = tstat , prob = p_tstat )) } In [2]: vec1 <- c ( 120 , 107 , 110 , 116 , 114 , 111 , 113 , 117 , 114 , 112 ) vec2 <- c ( 110 , 111 , 107 , 108 , 110 , 105 , 107 , 106 , 111 , 111 ) t_test_res = test_equality_means ( vec1 , vec2 ) t_test_res $tstat 3.48432421316996 $prob 0.998676366290831 We can see that the t-test gives us a significant result -- the two means would have a difference smaller than the observed difference with probability r t_test_res$prob . We then would reject the null hypothesis with 95% confidence (or even 99% confidence.) Bayesian Approach Now let's try to solve this problem with our Bayesian hats! Assuming that we have a prior distribution of $\\mu$ and $\\sigma&#94;2$ such that $x \\mid \\mu, \\sigma&#94;2 \\sim N(\\mu, \\sigma&#94;2)$ , we want the posterior $f(\\mu \\mid x_1, x_2, ..., x_n, \\sigma&#94;2)$ . Assuming a prior of $\\mu \\mid \\sigma&#94;2 \\sim N(\\beta, \\frac{\\sigma}{\\sqrt{n}})$ and $\\frac{S}{\\sigma&#94;2} \\sim \\chi&#94;2(k)$, we start with prior parameters $\\beta_0, n, S_0, k$ and update for each value of x. In [1]: simulatepostprob <- function ( vec , prior_params = c ( n = 0 , S = 0 , k = -1 , B = 0 )){ nnew = prior_params [ 'n' ] Snew = prior_params [ 'S' ] knew = prior_params [ 'k' ] Bnew = prior_params [ 'B' ] lst <- list () for ( i in 1 : length ( vec )) { obs <- vec [ i ] ninit = nnew Binit = Bnew Sinit = Snew kinit = knew knew = kinit + 1 nnew = ninit + 1 Bnew <- ( ninit * Binit + obs ) / nnew Snew <- Sinit + ninit * Binit &#94; 2 + obs &#94; 2 - nnew * Bnew &#94; 2 if ( knew == 0 ) { sampchi <- 0 sigma <- 0 } else { ## We randomly sample a value for sigma|x first sampchi <- sum ( rnorm ( knew , 0 , 1 ) &#94; 2 ) sigma <- sqrt ( Snew / sampchi ) } ## Using sigma, we sample for mu|x, sigma. mu <- rnorm ( 1 , obs + Bnew , sigma / sqrt ( nnew )) lst [[ i ]] <- mu } return ( unlist ( lst )) } rpost <- function ( n , vec , prior_params = c ( n = 0 , S = 0 , k = -1 , B = 0 )){ replicate ( n , expr = simulatepostprob ( vec )[ length ( vec )]) } In [3]: simulatepostprob ( vec = vec1 ) - simulatepostprob ( vec = vec2 ) 20 -0.503416712324565 1.45988251782074 17.6073011837844 7.22926620488519 9.24411515038176 8.35747543643981 15.4506594648444 8.61721035342939 6.22213384120991 By iterating for each observation, we arrive at a posterior distribution for $\\mu \\mid \\sigma&#94;2$ and $\\frac{S}{\\sigma&#94;2}$. We then sample randomly from these theoretical distributions to sample from the posterior distributions. If we replicate this many times, we will be able to estimate the differences between the posterior distributions of the two vectors. Using uninformative prior gives similar results to frequentist t-test method In [4]: sum ( rpost ( 1000 , vec1 ) > rpost ( 1000 , vec2 )) / 1000 0.999 We can then use our prior information about the sample to produce a mixed, updated result. For instance, with prior parameters: In [5]: prior_params = c ( n = 10 , S = 2 , k = 9 , B = 30 ) We can see our posterior: In [6]: sum ( rpost ( 1000 , vec1 , prior_params ) > rpost ( 1000 , vec2 , prior_params )) / 1000 0.999","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/t test or not t test.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/t test or not t test.html"},{"title":"Bayesian Analysis: Poisson Discrete","text":"A Simple Illustration of Bayesian Probability with a Binomial Distribution Suppose we have a discrete set of possible rates of occurrence $l$ of an event E with some prior beliefs about which rate is the true rate of occurrence, $\\lambda$, of the event E. That is, we have a probability mass function that takes discrete values within $l$ and maps them to a discrete probability space, where $$ P(\\lambda=w) = 0, \\forall{w \\notin l} $$$$ \\sum_{w}{P(\\lambda=w)} = 1, w \\in l $$ We then observe many events (that is, $n$ is very large, and $p$ is very small) and daily rates for these events and wish to update our prior beliefs accordingly. We observe a daily rate of $r$, and wish to update our probabilities accordingly. From Bayes' Rule, we know that $$ P(p=w | r) = \\frac{P(r | \\lambda=w)P(\\lambda=w)}{P(r)} $$ We can now use this information to calculate our posterior of $\\lambda$ by using the fact that this situation (the probability events with a constant rate) follows a Poisson distribution. Using this for $P(r|\\lambda=w)$, we can now solve for our posterior for each defined value of $\\lambda$. This is implemented below: In [4]: ## Poisson Discrete Updater ## Basic Bayesian analysis on discrete random variables. ## Given our data follows a Poisson distribution, finds the probability of various ## lambdas given an observed value using discrete probabilities for discrete lambda values. ## In this case I have generalized this to where lambda_new = lambda * t where t is the number of days. ## Consider that the number of events, k, in t days follows a Poisson distribution with lambda = t*l. ## For example, consider that the number of accidents in t days is distributed Poisson(t*l) ## Given prior beliefs about various l values (can be seen as the daily rate), we find the posterior distribution ## of these lambdas given new information. # Variables: ## pos_lambda (vector): a vector of hypothesized lambdas to check in posterior distribution. ## prior_probs (vector): a vector of prior probabilities for the hypothesized lambdas ## X (dataframe): a dataframe of new observed values, where the first column is the total number of events and t is the ## total number of days. For instance k=4, t=2 means there were two accidents per day, or 4 accidents in two days. k is the first column and t ## the second. ManualPoisson <- function ( k , t , l ){ # Same as dpois(x=k,lambda = l*t), or explictly return ((( t * l ) &#94; k ) * exp ( - t * l ) / factorial ( k )) } PosteriorFinder <- function ( k , t , plam , pp ){ # Finds the posterior and multiplicative constant for given probabilities and values. const <- sum ( mapply ( function ( l , p ) ManualPoisson ( k , t , l ) * p , plam , pp )) post <- mapply ( function ( l , p ) ManualPoisson ( k , t , l ) * p / const , plam , pp ) return ( list ( const = const , post = post )) } find_posterior <- function ( pos_lambda , prior_probs , X ){ ## sanity checks if ( sum ( pos_lambda > 0 ) != length ( pos_lambda )) { stop ( \"Lambdas must all be greater than zero.\" ) } if ( sum ( prior_probs ) != 1 ) { stop ( 'Priors must sum to 1, as they are probabilities.' ) } if ( ncol ( X ) != 2 ) { stop ( \"X can only have two columns (number of incidents and time)\" ) } if ( length ( pos_lambda ) != length ( prior_probs )) { stop ( \"The lambda vector and probability vector must be the same length.\" ) } ## Initialize probabilities as prior probabilities. probs <- prior_probs # Updates the posteriors for each observed value for ( i in nrow ( X )) { k <- X [ i , 1 ] t <- X [ i , 2 ] probs <- PosteriorFinder ( k , t , pos_lambda , probs ) $ post } posterior = probs return ( posterior ) } Example : Allergic Reactions Consider that we believe that people in most counties will have a severe allergic reaction around 1.5 times per day. We have an expert who has just moved to XYZ county who thinks that this is not the case in the county. He wants to test this hypothesis on stretches of the last two weeks, where he believes we have seen a lower number of allergic reactions. There was 12 incidents in the first 6 days, and zero in the last seven. Suppose that we know (by some empirical estimation) that counties in the US approximately have probability mass function: \\begin{equation} P(\\lambda = w) = \\begin{cases} .1 ,& \\text{if } w=0.5\\\\ .2 ,& \\text{if } w=1\\\\ .3 ,& \\text{if } w=1.5\\\\ .2 ,& \\text{if } w=2\\\\ .15 ,& \\text{if } w=2.5\\\\ .05 ,& \\text{if } w=4\\\\ 0, & \\text{otherwise} \\end{cases} \\end{equation} We believe that the distribution of the number of severe allergic reactions follows a Poisson distribution with rate $\\lambda$. That is, the rate is constant in a given county, and each event occurs independently of one another (i.e., one person having an allergic reaction does not effect the probability of another having one). Given these assumptions, we want to check the probability of the rates given our new data. In [5]: poslam_example <- c ( . 5 , 1 , 1.5 , 2 , 2.5 , 4 ) probs_example <- c ( . 1 , . 2 , . 3 , . 2 , . 15 , . 05 ) X_example <- data.frame ( k = c ( 12 , 6 ), t = c ( 0 , 7 )) find_posterior ( poslam_example , probs_example , X_example ) 0.14075345211024 0.544049403057648 0.280702619102117 0.031750965697214 0.00274313760334297 4.22429437384091e-07 What is the conclusion? The expert seems to be on to something -- there appears to be more evidence at the current time that lambda is actually 1 rather than 1.5. More trials would find tune this result further.","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/Poisson Discrete.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/Poisson Discrete.html"},{"title":"Bayesian Analysis: Binomial Discrete","text":"A Simple Illustration of Bayesian Probability with a Binomial Distribution Suppose we have a discrete set of possible probabilities $q$ of an event E with some prior beliefs about which probability is the true probability of the event E. That is, we have a probability mass function that takes discrete values within $q$ and maps them to a discrete probability space, where $$ P(p=w) = 0, \\forall{w \\notin q} $$$$ \\sum_{w}{P(p=w)} = 1, w \\in q $$ We then observe some independent events and wish to update our prior beliefs accordingly. We observe $k$ occurences over $n$ trials, and wish to update our probabilities accordingly. From Bayes' Rule, we know that $$ P(p=w | k, n) = \\frac{P(k | p=w, n)P(p=w|n)}{P(k|n)} $$ We can now use this information to calculate our posterior of $p$ by using the fact that independent events occuring with some probability $p$ follow a Bernoulli distribution, and so the total number of events follow a binomial distribution. Using this for $P(n, k|p=w)$, we can now solve for our posterior for each defined value of p. This is implemented below: Code to Find Posterior Distribution Given Binomial Prior In [1]: ## Finds posterior distribution of observed values assuming that k follows a binomial distribution. p is discrete in this case. numerator <- function ( k , n , posprobs , probsofprobs ){ ## finds the numerator -- ie P(k|p)P(p) ## k (integer) : the total number of observed successes. ## n (integer) : the total number of observed trials. ## posprobs (vector): possible p values associated with the random variable k. ## probsofprobs (vector): this is our prior, mapping a probability to each possible probability function ( p ){ ## For a particular p, this will find the posterior probability of p given observed values. This is a function generator -- given ## values of k and n, we can then find the probability of any particular p. x <- posprobs [ p ] px <- probsofprobs [ p ] res <- ( x &#94; k ) * (( 1 - x ) &#94; ( n - k )) * px return ( res ) } } denominator <- function ( k , n , posprobs , probsofprobs ){ ## Finds the denominator, the multiplicative constant. Same for all values of p. P(k) sum ( sapply ( X = 1 : length ( posprobs ), FUN = numerator ( k , n , posprobs , probsofprobs ))) } probabilityfinder <- function ( k , n , p , posprobs , probsofprobs ){ ## Finds the posterior probability of a particular p value. numerator ( k , n , posprobs , probsofprobs )( p ) / denominator ( k , n , posprobs , probsofprobs ) } posterior <- function ( k , n , posprobs , probsofprobs ){ ## Finds the posterior probability mass function of the p values. sapply ( X = 1 : length ( posprobs ), FUN = function ( p ) probabilityfinder ( k , n , p , posprobs , probsofprobs )) } log_transf_numerator <- function ( k , n ){ ## finds the numerator -- ie P(k|p)P(p), using log transform to avoid machine zeros ## k (integer) : the total number of observed successes. ## n (integer) : the total number of observed trials. ## posprobs (vector): possible p values associated with the random variable k. ## probsofprobs (vector): this is our prior, mapping a probability to each possible probability function ( p ){ ## For a particular p, this will find the posterior probability of p given observed values. This is a function generator -- given ## values of k and n, we can then find the probability of any particular p. x <- posprobs [ p ] px <- probsofprobs [ p ] mx <- max ( sapply ( posprobs , function ( pr ) k * log ( pr ) + ( n - k ) * log ( 1 - pr ))) res <- exp ( k * log ( x ) + ( n - k ) * log ( 1 - x ) - mx ) * probsofprobs [ p ] return ( res ) } } log_transf_denominator <- function ( k , n ){ ## Finds the denominator, the multiplicative constant, using log transform for machine zeros. Same for all values of p. P(k) sum ( sapply ( X = 1 : length ( posprobs ), FUN = log_transf_numerator ( k , n ))) } log_transf_probabilitydist <- function ( k , n , p ){ ## Finds the posterior probability of a particular p value using log transform. log_transf_numerator ( k , n )( p ) / log_transf_denominator ( k , n ) } log_transf_posterior <- function ( k , n ){ ## Finds the posterior probability mass function of the p values using log transform sapply ( X = 1 : length ( posprobs ), FUN = function ( x ) log_transf_probabilitydist ( k , n , x )) } Example: Thunderstorms Consider if we observed 11 positive events in 27 days -- as an example, the number of thunderstorms in a month. We want to estimate the probability of a thunderstorm given the new information. Assuming weather each day is independent (which is a simplifying assumption), we can assume that the number of thunderstorms in $n$ days can be modeled by a binomial distribution. Suppose we have the following prior probability mass function about the discrete values of $p$: \\begin{equation} P(p = w) = \\begin{cases} .07 ,& \\text{if } w=.05\\\\ .13 ,& \\text{if } w=.15\\\\ .26 ,& \\text{if } w=.25\\\\ .26 ,& \\text{if } w=.35\\\\ .13 ,& \\text{if } w=.45\\\\ .07 ,& \\text{if } w=.55\\\\ .02 ,& \\text{if } w=.65\\\\ .02 ,& \\text{if } w=.75\\\\ .02 ,& \\text{if } w=.85\\\\ .02 ,& \\text{if } w=.95\\\\ 0, & \\text{otherwise} \\end{cases} \\end{equation} If we believe prior to this that the distribution looks as it does above (with the highest belief in 35 and 45%), we can now find the posterior of the probabilities after observing the 11 thunderstorms as follows: In [2]: posprobs <- c ( . 05 , . 15 , . 25 , . 35 , . 45 , . 55 , . 65 , . 75 , . 85 , . 95 ) probsofprobs <- c ( . 07 , . 13 , . 26 , . 26 , . 13 , . 07 , . 02 , . 02 , . 02 , . 02 ) pos <- posterior ( 11 , 27 , posprobs , probsofprobs ) round ( pos , 3 ) 0 0.002 0.128 0.524 0.287 0.057 0.002 0 0 0 So we now believe with 0.524 probability that the actual $p$ is .35. This means the data supports the assumption of .35 chance of thunderstorms a day, given our prior beliefs and the independence of events. Behaviors for different $n$ and $k$ Now, let us investigate the behaviors for various sample sizes, $n$, and number of observed thunderstorms $k$. In [3]: n <- c ( 10 , 50 , 100 , 150 , 200 , 300 , 500 ) k <- lapply ( n , function ( n ) seq ( n / 10 , n , n / 10 )) col_names = as.factor ( 1 : 10 / 10 ) row_names = paste ( 'P(p=' , posprobs , ')' ) list_names = paste ( 'n=' , n ) iter_j = 1 : 10 ; names ( iter_j ) = col_names iter_i = 1 : 7 ; names ( iter_i ) = list_names dimthreedfs = lapply ( iter_i , function ( i ) data.frame ( sapply ( iter_j , function ( j ) posterior ( k [[ i ]][ j ], n [ i ], posprobs , probsofprobs )), check.names = FALSE )) In [4]: dimthreedfs = lapply ( dimthreedfs , function ( y ) round ( y , 3 )) In [5]: dimthreedfs $`n= 10` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.160 0.030 0.004 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.328 0.209 0.096 0.035 0.010 0.002 0.000 0.000 0.000 0.000 0.354 0.427 0.371 0.255 0.141 0.062 0.020 0.004 0.000 0.000 0.137 0.266 0.373 0.415 0.372 0.263 0.136 0.043 0.006 0.000 0.020 0.058 0.123 0.208 0.283 0.304 0.239 0.116 0.026 0.003 0.002 0.009 0.030 0.075 0.152 0.245 0.287 0.208 0.070 0.010 0.000 0.000 0.002 0.009 0.029 0.070 0.124 0.137 0.070 0.015 0.000 0.000 0.000 0.002 0.011 0.043 0.123 0.219 0.182 0.064 0.000 0.000 0.000 0.000 0.002 0.012 0.064 0.215 0.338 0.225 0.000 0.000 0.000 0.000 0.000 0.000 0.005 0.058 0.306 0.683 $`n= 50` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.232 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.703 0.292 0.009 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.065 0.646 0.471 0.054 0.001 0.000 0.000 0.000 0.000 0.000 0.001 0.061 0.489 0.613 0.135 0.005 0.000 0.000 0.000 0.000 0.000 0.000 0.031 0.311 0.555 0.160 0.006 0.000 0.000 0.000 0.000 0.000 0.000 0.023 0.299 0.640 0.179 0.003 0.000 0.000 0.000 0.000 0.000 0.000 0.010 0.180 0.408 0.047 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.016 0.392 0.499 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.015 0.451 0.602 0.004 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.370 0.996 $`n= 100` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.168 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.828 0.288 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.004 0.706 0.479 0.005 0.000 0.000 0.000 0.000 0.000 0 0.000 0.006 0.517 0.655 0.019 0.000 0.000 0.000 0.000 0 0.000 0.000 0.004 0.337 0.637 0.026 0.000 0.000 0.000 0 0.000 0.000 0.000 0.003 0.343 0.762 0.028 0.000 0.000 0 0.000 0.000 0.000 0.000 0.001 0.211 0.505 0.005 0.000 0 0.000 0.000 0.000 0.000 0.000 0.002 0.467 0.548 0.002 0 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.447 0.725 0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.273 1 $`n= 150` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.111 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.889 0.269 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.000 0.730 0.471 0.000 0.000 0.000 0.000 0.000 0.000 0 0.000 0.001 0.529 0.656 0.002 0.000 0.000 0.000 0.000 0 0.000 0.000 0.001 0.343 0.648 0.004 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.349 0.782 0.004 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 0.214 0.527 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 0.000 0.469 0.575 0.000 0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.424 0.812 0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.188 1 $`n= 200` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.071 0.00 0.000 0.000 0.00 0.000 0.000 0.0 0.000 0 0.929 0.25 0.000 0.000 0.00 0.000 0.000 0.0 0.000 0 0.000 0.75 0.461 0.000 0.00 0.000 0.000 0.0 0.000 0 0.000 0.00 0.538 0.653 0.00 0.000 0.000 0.0 0.000 0 0.000 0.00 0.000 0.346 0.65 0.000 0.000 0.0 0.000 0 0.000 0.00 0.000 0.000 0.35 0.787 0.000 0.0 0.000 0 0.000 0.00 0.000 0.000 0.00 0.212 0.538 0.0 0.000 0 0.000 0.00 0.000 0.000 0.00 0.000 0.461 0.6 0.000 0 0.000 0.00 0.000 0.000 0.00 0.000 0.000 0.4 0.876 0 0.000 0.00 0.000 0.000 0.00 0.000 0.000 0.0 0.124 1 $`n= 300` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.028 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.972 0.214 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.786 0.442 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.558 0.647 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.353 0.65 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.35 0.793 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.207 0.558 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.442 0.648 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.352 0.949 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.051 1 $`n= 500` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.004 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.996 0.153 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.847 0.405 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.595 0.633 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.367 0.65 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.35 0.802 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.198 0.595 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.405 0.734 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.266 0.992 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.008 1 In [6]: library ( ggplot2 ) library ( reshape2 ) create_gg_heatmap <- function ( df ){ mat = as.matrix ( df ) melted = melt ( mat ) colnames ( melted ) = c ( 'k' , 'p' , 'posterior' ) plt = ggplot ( data = melted , aes ( x = k , y = p , fill = posterior )) + geom_tile () + scale_fill_gradientn ( limits = c ( 0 , 1 ), colors = c ( \"navyblue\" , \"darkmagenta\" , \"darkorange1\" )) return ( plt ) } In [7]: for ( df in dimthreedfs ){ print ( create_gg_heatmap ( df )) } Each list represents $n$ for $n \\in \\{10, 50, 100, 150, 200, 300, 500\\}$, and each column represents possible $k$ values from $\\frac{n}{10}$ to $n$. The heatmaps shows us that the probabilities become more polarized as $n$ increases, which is what we would expect. We see that with small $n$ the colors are fuzzy, reflecting our uncertainty concerning our estimate. We see what is expected with Bayesian probabilities: as $n$ increases, our confidence in our posterior probability becomes stronger. This is reflected in the fact that we get values that are nearly 1 for the observations which have p = k/n and 0 otherwise. This shows that as we get more observations, the data will overwhelm the prior. For small values of $n$, we see the opposite: it is highly skewed by our prior, so that we are not so confident that $p$ = $\\frac{k}{n}$ if $p$ was not deemed probable prior. Machine Zeros problem: In [8]: posterior ( 1000 , 2000 , posprobs , probsofprobs ) NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Notice that we have many machine zeros for large n. This interferes with our calculations, and gives us NaNs for the calculations. The following uses a log transformation to avoid machine zeros: In [9]: posterior ( 1000 , 2000 , posprobs , probsofprobs ) log_transf_posterior ( 1000 , 2000 ) NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0 5.59622100226468e-289 3.46746548211335e-121 3.31238703307168e-37 0.650000000000052 0.349999999999948 2.54799002543976e-38 2.66728114008719e-122 8.60957077271489e-290 0","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/Binomial Discrete.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/Binomial Discrete.html"},{"title":"Determining Sampling Schemes for Hierarchies","text":"First, we will treat the csv \"schools_ds\" as the population that follows a random intercept model based on the group. We will use this to investigate the sampling strategies that one could use and see how the models compare. This data considers scores from various schools, with the score representing a students' score on a test, and the grpID represeting the school. In [2]: data <- read.csv ( \"./schools_ds.csv\" ) head ( data ) require ( lme4 ) mod1 <- lmer ( score ~ 1 + ( 1 | grpID ), data = data ) summary ( mod1 ) X score grpID 1 100 1 2 110 1 3 96 1 4 80 1 5 83 1 6 76 1 Loading required package: lme4 Loading required package: Matrix Linear mixed model fit by REML ['lmerMod'] Formula: score ~ 1 + (1 | grpID) Data: data REML criterion at convergence: 9142.6 Scaled residuals: Min 1Q Median 3Q Max -2.9995 -0.6230 -0.0388 0.6663 3.7416 Random effects: Groups Name Variance Std.Dev. grpID (Intercept) 32.01 5.658 Residual 233.57 15.283 Number of obs: 1100, groups: grpID, 10 Fixed effects: Estimate Std. Error t value (Intercept) 101.298 1.876 53.99 This exercise will be investigating the effect of sampling schema on the effectiveness of HLMS by looking at school data. Each school has a different number of students, and we wish to fit a random intercept model based on the school. I first create some functions that will be useful for me in creating the samples. The first creates a sample from a group of n elements. The second transfers a list into a data frame. I then create the samples by first randomly selecting 11 from each group, then selecting i*2 from each group (so 2 from group 1, 4 from group 2, etc.). Helper functions In [3]: samplefromgroup <- function ( grp , n ){ j = sample ( 1 : length ( data [ data $ grpID == grp , 1 ]), n ) return ( data [ data $ grpID == grp ,][ j ,]) } lt2df <- function ( l ){ return ( as.data.frame ( do.call ( rbind.data.frame , l ))) } Simulation In [4]: simresults <- list () for ( sim in 1 : 10 &#94; 3 ) { sample1 <- list () nschool <- length ( unique ( data $ grpID )) for ( i in 1 : nschool ){ sample1 [[ i ]] <- samplefromgroup ( i , 11 ) } sample1 <- lt2df ( sample1 ) sample1 sample2 <- list () for ( i in 1 : nschool ){ sample2 [[ i ]] <- samplefromgroup ( i , 2 * i ) } sample2 <- lt2df ( sample2 ) sample2 sample3 <- list () randomschool <- sample ( 1 : nschool , 5 ) trigger <- any ( randomschool == 1 ) if ( trigger ){ remaining <- ! randomschool == 1 remsch <- randomschool [ remaining ] randomselect23 <- sample ( remsch , 2 ) schoolselect22 <- remsch [ remsch != randomselect23 [ 1 ] & remsch != randomselect23 [ 2 ]] sample3 [[ 1 ]] <- data [ data $ grpID == 1 ,] for ( i in 1 : 2 ){ sample3 [[ i +1 ]] <- samplefromgroup ( randomselect23 [ i ], 23 ) sample3 [[ i +3 ]] <- samplefromgroup ( schoolselect22 [ i ], 22 ) } } else { for ( i in 1 : length ( randomschool )){ sample3 [[ i ]] <- samplefromgroup ( randomschool [ i ], 22 ) } } sample3 <- lt2df ( sample3 ) sample3 sample4 <- list () weights <- vector () for ( i in 1 : nschool ){ weights [ i ] <- length ( data [ data $ grpID == i , 1 ]) / nrow ( data ) } randomschool2 <- sample ( 1 : nschool , 5 , prob = weights ) trigger2 <- any ( randomschool2 == 1 ) if ( trigger2 ){ remaining2 <- ! randomschool2 == 1 remsch2 <- randomschool2 [ remaining2 ] randomselect232 <- sample ( remsch2 , 2 ) schoolselect222 <- remsch2 [ remsch2 != randomselect232 [ 1 ] & remsch2 != randomselect232 [ 2 ]] sample4 [[ 1 ]] <- data [ data $ grpID == 1 ,] for ( i in 1 : 2 ){ sample4 [[ i +1 ]] <- samplefromgroup ( randomselect232 [ i ], 23 ) sample4 [[ i +3 ]] <- samplefromgroup ( schoolselect222 [ i ], 22 ) } } else { for ( i in 1 : length ( randomschool )){ sample4 [[ i ]] <- samplefromgroup ( randomschool2 [ i ], 22 ) } } sample4 <- as.data.frame ( do.call ( rbind.data.frame , sample4 )) sample4 listofsamples <- list ( sample1 , sample2 , sample3 , sample4 ) parameters <- matrix ( nrow = 3 , ncol = 4 ) rownames ( parameters ) <- c ( \"Gamma00\" , \"Sigma\" , \"Tao0\" ) colnames ( parameters ) <- c ( \"Model 1\" , \"Model 2\" , \"Model 3\" , \"Model 4\" ) models <- list () for ( i in 1 : length ( listofsamples )){ models [[ i ]] <- lmer ( score ~ 1 + ( 1 | grpID ), data = listofsamples [[ i ]]) parameters [ 1 , i ] <- summary ( models [[ i ]]) $ coef [ 1 ] parameters [ 2 , i ] <- summary ( models [[ i ]]) $ sigma parameters [ 3 , i ] <- as.data.frame ( VarCorr ( models [[ i ]]))[ 1 , 5 ] } simresults [[ sim ]] <- parameters } fullmodelparameters <- matrix ( rep ( c ( summary ( mod1 ) $ coef [ 1 ], summary ( mod1 ) $ sigma , as.data.frame ( VarCorr ( mod1 ))[ 1 , 5 ]), 4 ), nrow = 3 ) simresults [[ 1 ]] - fullmodelparameters Warning message in optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp), : \"convergence code 3 from bobyqa: bobyqa -- a trust region step failed to reduce q\" Model 1 Model 2 Model 3 Model 4 Gamma00 -0.0801288 -2.981430985 -0.3801288 -0.9437652 Sigma 0.7517267 -0.005658724 -0.8614719 1.1448531 Tao0 2.2040381 -2.554488338 -0.4333093 -0.3124444 I then create a list of samples, and use each of these samples to sample using the models. I create a matrix that will store the parameters in parameters , and it stores the fixed effect ($\\gamma_{0,0}$) and the variances of the random effects ($\\sigma&#94;2$ and $\\tau&#94;2$) for each model, using each sample. It then stores this information for each simulation, which is done $10&#94;5$ times. This means the samples are created differently each time, and then the model is fit using each sample, and the information is stored in a list for each simulation. I then compare this to the model using the full population, which are the full model parameters. As an example, we can see the bias from the first simulation of each model, for each variable. I then compute the mean bias, mean squared error, and mean variance for each of the samples. I do this by summing all of the individual biases, squared errors, and variances, and dividing by the total number of simulations. In [5]: indbias <- lapply ( simresults , function ( x ) x - fullmodelparameters ) indsquarederror <- lapply ( simresults , function ( x ) ( x - fullmodelparameters ) &#94; 2 ) mean <- Reduce ( '+' , simresults ) / 10 &#94; 5 indvariance <- lapply ( simresults , function ( x ) ( x - mean ) &#94; 2 ) mse <- Reduce ( '+' , indsquarederror ) / 10 &#94; 5 variance <- Reduce ( '+' , indvariance ) / 10 &#94; 5 fullbias <- Reduce ( '+' , indbias ) / 10 &#94; 5 mse variance fullbias Model 1 Model 2 Model 3 Model 4 Gamma00 0.01748752 0.02072217 0.05179054 0.04740217 Sigma 0.01140245 0.01069759 0.01055099 0.01016488 Tao0 0.03968227 0.02583796 0.04847282 0.04262270 Model 1 Model 2 Model 3 Model 4 Gamma00 100.2610814 101.440250 100.5597129 101.5455572 Sigma 2.2049334 2.290984 2.2154111 2.2687420 Tao0 0.3043728 0.366400 0.2977824 0.3723673 Model 1 Model 2 Model 3 Model 4 Gamma00 -0.001651197 0.0042711199 -0.000320029 0.004667134 Sigma -0.003194664 -0.0002994215 -0.002816646 -0.001024023 Tao0 -0.004419130 0.0024195585 -0.005803453 0.001443499 Discussion The components of the MSE can be decomposed into the variance and the bias (MSE = $b&#94;2$ + $\\sigma&#94;2$). This discussion is especially important to application because it is often expensive or impossible to gather data from the entire population (thus why we sample in the first place), and in the same vein, it may be expensive or difficult to gather data from all of the subsequent groups. Sampling the schools may be seen as desirable in this instance, as it will likely require less resources to acquire information from 5 groups rather than 10. However, we can see from the above that the effects of this on the models are somewhat large, and sampling from the number of schools does not produce the same results as sampling from each school directly. While it may not be immediately obvious from this problem, if you were to try to sample from, say, 100,000 schools, 1,000 students in each school, this could become extremely expensive and difficult to do. Thus the importance of the results here cannot be stressed enough - it is not equivalent to randomly sample 5 (or any number) groups and select a comparable number of students. Due to the structure of the HLMs, a great deal of information is lost if we neglect to look at all of the groups. This is reflected in the results above. Models 3 and 4 show considerably worse results than the first two models. This suggest that sampling from the number of schools, which may be less expensive, is less accurate at predicting the true model underlying the data. That is, using more groups will produce more favorable results in the application of HLM, even though the number of students is exactly the same in all cases. We see that the estimates of $\\sigma$ are more accurate in terms of mse. This makes sense, as we can more accurately account for the individual differences, as there are less groups. However, this is a consequence of the sampling - and therefore, the $\\tau_{0}$ estimates are much, much worse for the later two models. Since we have constructed the HLM's to account for variations within groups, and we are often interested in the level 2 variations as well, gaining a small amount of accuracy in sigma for a large drop in accuracy of $\\tau_0$ is unfavorable. If we were that much more concerned with estimating $\\sigma$ than $\\tau$, we wouldn't be using a HLM in the first place. We also see a larger difference in the estimates for $\\gamma_0$ for the later two models. The other structure that is included here involves weighting the models by the number of students in each school. Thus Model 2 and Model 4 weigh the results by the number of students, while Model 1 and Model 3 treat them all as if they are equivalent. We see that using the weighing strategy produces worse results for the fixed coefficients, but in general is better for the random coefficients. This makes sense, since this weighing is based on the groups themselves, which will introduce more bias and variance in the fixed coefficients for the intercept. However, there is a trade off here, as the mse for both $\\sigma$ and $\\tau_0$ decreases, which suggests that the random components of the models are closer to the true random parameters. The weighed models produce more variance in estimating sigma, but less bias, and produce better results for both for $\\tau_0$. This makes sense, as weighing the groups will produce less biased estimates for the individual random effect, but produce a higher variance as these results have different amounts of students sampled. $\\tau_0$ will be more accurate in terms of both variance and bias, as we are accounting for the differences in the groups, which is reflected in the full model in a similar way.","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/determining-sampling-schemes-for-hierarchies.html","loc":"https://seanammirati.github.io/category/linear-models/determining-sampling-schemes-for-hierarchies.html"},{"title":"Approaches to Contingency Tables","text":"Frequentist and Bayesian Approaches to Contingency Tables Chi-Squared Test Here we will be investigating a few ways of dealing with contingency tables. The first two are standard procedures in frequentist statistics, the chi-squared test and the Fischer's Exact test. Consider a contingency table that looks like this: In [4]: row_names = c ( 'Has traveled outside US' , 'Has not traveled outside US' ) column_names = c ( 'Non-Artist' , 'Artist' ) cont_table <- matrix ( c ( 5 , 15 , 30 , 20 ), nrow = 2 , dimnames = list ( row_names , column_names )) cont_table Non-Artist Artist Has traveled outside US 5 30 Has not traveled outside US 15 20 Consider if we ask the question: are artists and non-artists equally likely to travel outside of the US? The first approach we can take is the chi-squared test, which is an asymptotically accurate. If each was equally likely to travel, we would expect a cont_table like this : In [7]: cont_table_exp <- matrix ( c ( 10 , 10 , 25 , 25 ), nrow = 2 , dimnames = list ( row_names , column_names )) cont_table_exp Non-Artist Artist Has traveled outside US 10 25 Has not traveled outside US 10 25 If we assume that artistry and travel likelihood are independent, our sum of the differences from this expectation divided by the square root of the expectation should be roughly normally distributed and so the squared differences will be approximately $\\chi &#94;{2}\\left ( 1 \\right )$. Here, we assume that the observed counts are Poisson distributed, so that their expectation and variance are defined by $E_{i}$ and $\\frac{1}{\\sqrt{E_{i}}}$. As $n \\to \\infty$, this is asymptotically normal, given the hypothesis of independence. Because we know the row and column totals, in this case we have a degree of freedom of 1: when one value is known on this contingency table, given the row and column totals we can deduce the others. In order to calculate an (approximate) probability of this example given independence (the null hypothesis), we perform the following calculation. In [8]: zsq <- sum (( cont_table - cont_table_exp ) &#94; 2 / cont_table_exp ) 1 - pchisq ( zsq , 1 ) 0.00815097159350264 So we would reject the notion of independence based on these observations. More generally: In [15]: chi_squared_test <- function ( cont_table , cont_table_exp = NULL ){ if ( cont_table_exp == NULL ) { cont_table_exp = t ( as.matrix ( rep ( apply ( cont_table , 2 , mean )))) } cont_table_exp df <- ( nrow ( cont_table ) - 1 ) * ( ncol ( cont_table ) - 1 ) zsq <- sum (( cont_table - cont_table_exp ) &#94; 2 / cont_table_exp ) probability <- 1 - pchisq ( zsq , df ) return ( probability ) } Fischer Exact Test However, the chi-squared test is only asympotically accurate. If we consider the counts to be distributed as a hyper-geometric distribution, we can use Fisher's Exact Test to calculate the probability of independence. Given some values in the contingency table, the probability of any distribution of values given independence is as follows: In [16]: probfun <- function ( i , j , n , m ){ # i: particular coordinate (for instance, non-artist, has not traveled) # j: the column total for the particular coordinate (for instance, non-artists) # n: the row total for particular coordinate (for instance, people who have traveled) # m: the row total for the other coordinate (for instance, people who have not traveled) choose ( m , i ) * choose ( n ,( j - i )) / choose ( m + n , j ) } This will give us the exact probability of each coordinate. If we wanted to find the probability that non-artist, people who traveled is less than or equal to five, we would do the following in this case: In [17]: sum ( sapply ( 0 : 5 , function ( x ) probfun ( x , 20 , 35 , 35 ))) 0.008027370811152 This shows that the probability of what we have observed is quite small if the columns and rows are independent. We again reject independence with 1% significance. The results of this test and the chisquared test are also quite close, which is as expected. As n goes to infinity, the chisquared test converges to the Fischer's Exact test. This is the hyper geometric distribution, which is included in R. The results are the same: In [18]: phyper ( 5 , 35 , 35 , 20 ) ##same result 0.00802737081115199 These are well known tests we can make. However, we can add some bayesian analysis to the mix here! Bayesian Approach -- using Beta distributions Let us assume that the probability of having traveled for artists and non-artists can be modeled using a Beta distribution. That is [P(travel \\mid nonartist) = p \\sim \\ {Beta}(\\alpha {1}, \\beta {1})] and [P(travel \\mid artist) = q \\sim \\ {Beta}(\\alpha {2}, \\beta {2})] We can now make an approximation to determine the probability that $p < q$. That is, we want the posterior distribution of $p$ and $q$ given our contingency table. Because our likelihood is multinomial, we know that the posterior distribution is also Beta distributed. After doing some manipulations, we can write the posterior distribution of $p$ and $q$. If we assume uninformative priors, we set $\\alpha = \\beta = 1$ to get the following: In [50]: posteriorb <- function ( p , q , cont_table , num = FALSE ){ if ( num == TRUE ) { if ( p >= q ) { return ( 0 ) } } obs_row_totals = apply ( cont_table , 1 , sum ) obs_coordinates = c ( cont_table ) obs_rows_w_prior = cont_table - 1 constant <- ( prod ( gamma ( obs_row_totals )) / ( prod ( gamma ( obs_coordinates )))) p_mat <- matrix ( c ( p , q , 1 - p , 1 - q ), nrow = 2 ) ret <- constant * ( prod ( p_mat &#94; obs_rows_w_prior )) return ( ret ) } Note that this will only work for 2x2 contingency tables and assuming an (invalid) prior of $\\alpha = \\beta = 1$ for each prior. If we wanted to include prior information, we could change these for each prior, which would result in a different number being added/subtracted above. This joint distribution is somewhat difficult to determine analytically. There are alternative here to estimate this probability (gtools, MCMCpack, etc), but I have included a numerical approximation. In [53]: approx <- function ( f , num = FALSE , ... ) { sum ( sapply ( seq ( 0.01 , 0.99 , 0.01 ), function ( p ) sapply ( seq ( 0.01 , 0.99 , 0.01 ), function ( q ) f ( p , q , num , ... )))) } estim <- approx ( posteriorb , TRUE , cont_table = cont_table ) / approx ( posteriorb , cont_table = cont_table ) estim 0.996557074981106 We estimate ${P}(p< q) = .9966$. Here, we individual values of p,q and compare their pdf evaluations at each point. Bayesian Approach -- Assuming Dependence Structure Let's use another formulation that allows $p$ and $q$ to depend on one another. That is, it is unlikely that $p$ and $q$ are distributed independently -- there is an underlying 'willing to travel' factor that is prevalent in both groups. We saw this when we considered the earlier tests. If we assume then that $\\ln \\frac{p}{1-p} - \\ln \\frac{q}{1-q}$, the difference in their log odds ratios is normally distributed with mean 0 and some arbitrary variance, we are allowing for some dependencies based on the value of sigma squared that we select. Selecting a small value of sigmasquared introduces a higher dependency structure. We can then test, for a certain level of expected variance in their difference, whether they are independent or not. In [42]: posteriorc <- function ( p , q , cont_table , num = FALSE , sigmasq = 1 ){ if ( num == TRUE ) { if ( p >= q ) { return ( 0 ) } } obs_rows_w_prior = cont_table - 1 logit1 <- log (( p / ( 1 - p ))) logit2 <- log (( q / ( 1 - q ))) expo <- exp (( - ( logit1 - logit2 ) &#94; 2 ) / ( 2 * sigmasq )) p_mat <- matrix ( c ( p , q , 1 - p , 1 - q ), nrow = 2 ) ret <- expo * prod (( p_mat &#94; obs_rows_w_prior )) return ( ret ) } We then approximate ${P}(p< q)$ given various assumed sigmas in the prior. In [52]: estimb1 <- approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 1 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 1 ) estimb2 <- approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 0.25 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 0.25 ) estimb3 <- approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 4 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 4 ) # sigmasq = 1 estimb1 # sigmasq = .25 estimb2 # sigma sq = 4 estimb3 # sigma sq = 400000 (close to complete independence) approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 4e+05 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 4e+05 ) 0.990231766938391 0.961061419233541 0.995239851507228 0.996557062782334 We see that the larger the variance, the higher ${P}(p< q)$ is. When we condsidered them to be independent, we got an estimate of .9965571. It makes sense, then, that as we increase the variance, and therefore the two become closer to being what we would consider as independent from one another, it approaches the value when they are assumed to be independent explicitly. Considering them to have a small variance (.25) will lead to the probability that $p < q$ to be smaller than any other method, including the Chi-Squared and Fischer Exact test. When we have a high variance (even at sigma sq = 4), we obtain a higher probability that the two probabilities are different. The conclusion then is that the two groups are different from one another. When the variance is small, we are less confident that the two groups are different from one another, both in prior and posterior distribution.","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/Approaches to Contingency Tables.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/Approaches to Contingency Tables.html"},{"title":"Time Series Analysis: Lake Erie","text":"As a preface, I will define here some functions that will be useful later on in the analysis. In [1]: ## Functions used in Time Series Project -- Lake Erie Water Levelss library ( RColorBrewer ) qual_col_pals = brewer.pal.info [ brewer.pal.info $ category == 'qual' ,] col_vector = unlist ( mapply ( brewer.pal , qual_col_pals $ maxcolors , rownames ( qual_col_pals ))) sse <- function ( pred , data ){ true <- as.vector ( data ) sum (( pred - true ) &#94; 2 ) } colors2 <- col_vector addplot <- function ( pred , range = 1 : length ( lkerie ), color1 ){ lines ( x = c ( time ( lkerie ))[ range ], y = c ( lkerietrain , pred )[ range ], col = color1 , lty = \"dashed\" ) } get.best.arima <- function ( x_ts , maxord = c ( 1 , 1 , 1 , 1 , 1 , 1 )) { best.aic <- 1e8 n <- length ( x_ts ) for ( p in 0 : maxord [ 1 ]) for ( d in 0 : maxord [ 2 ]) for ( q in 0 : maxord [ 3 ]) for ( P in 0 : maxord [ 4 ]) for ( D in 0 : maxord [ 5 ]) for ( Q in 0 : maxord [ 6 ]) { fit <- arima ( x_ts , order = c ( p , d , q ), seas = list ( order = c ( P , D , Q ), frequency ( x_ts )), method = \"CSS\" ) fit.aic <- -2 * fit $ loglik + ( log ( n ) + 1 ) * length ( fit $ coef ) if ( fit.aic < best.aic ) { best.aic <- fit.aic best.fit <- fit best.model <- c ( p , d , q , P , D , Q ) } } list ( best.aic , best.fit , best.model ) } In [2]: library ( forecast ) library ( prophet ) library ( tidyr ) library ( TSA ) library ( lubridate ) Loading required package: Rcpp Loading required package: rlang Attaching package: ‘TSA' The following objects are masked from ‘package:stats': acf, arima The following object is masked from ‘package:utils': tar Attaching package: ‘lubridate' The following object is masked from ‘package:base': date The dataset I have chosen is a monthly recording of water levels in Lake Erie from the period between 1921 and 1970. The dataset is available at the following link: https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line&numberformat=n1 . Below is a plot of the time series over time: Importing the data In [4]: #lkerie<-dmseries(\"https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line\") #lkerie <- as.ts(lkerie,frequency=12) #Note: update as of 2019, datamarket is no longer availiable. As an alternative, I am using data from a different source # that has the lake erie water levels as well. lkerie = read.csv ( 'http://lre-wm.usace.army.mil/ForecastData/GLHYD_data_english.csv' , skip = 12 ) mask = ( lkerie $ year <= 1970 ) & ( lkerie $ year >= 1921 ) lkerie = lkerie [ mask , 'Erie' ] lkerie <- ts ( lkerie , frequency = 12 , start = c ( 1921 , 1 )) In [5]: plot ( lkerie ) We can see that the water levels seem to jump somewhat cyclically over time. This is typical of physical processes, and thus it is an interesting dataset to analyze. We see that over time the water levels decreased initially, and then began to increase again somewhat steadily from the 1940s onwards. While there is clearly a seasonal component at work here, it is not so clear how this seasonal component operates initially from the plot above. I first begin by fitting a seasonal means with linear trend model to summarize and get an idea of the seasonal component of the data. I then built four models – the Holt Winters model, an SARIMA model, a model decomposing the seasonal component with spectral analysis, and a model from the prophet package. I then compare the models using the sum squared errors and plotting them against the true data values. I have separated the time series into two sets, as shown below. These are the training and test sets I will be using to evaluate how effective each of the models I fit are at estimating the true time series. The following code initializes the dataset as a time series, and creates a training and test dataset to perform analysis on. The test set is the last 12 observations, or one year, of the time series. In [6]: lkerietrain <- ts ( lkerie [ 1 : ( length ( lkerie ) -12 )], frequency = 12 ) lkerietest <- lkerie [( length ( lkerie ) -11 ) : length ( lkerie )] tail ( lkerietrain ) 600 Jul Aug Sep Oct Nov Dec 49 573.13 572.93 572.44 571.92 571.65 571.65 After this initialization, the dataset is in good condition and doesn't need to be cleaned any further. The first 6 values of the time series are shown below: Seasonal Means Model The first model I fit was a seasonal means model, to get a sense of the seasonality of the dataset. Although this is a simplistic model that will not be used to model the data itself, it provides good insight into the seasonal nature of the time series. Below is a plot of the time series with the months added to the plot: In [7]: season_ <- season ( lkerie ) time_ <- time ( lkerie ) We can see that the time series tends to peak around the summer time, and has troughs in the colder months. The seasonal means model gives us more insight into how this is occurring: In [8]: help ( plot ) In [9]: seasonmean <- lm ( lkerie ~ season_ + time_ ) summary ( seasonmean ) plot ( lkerie ) points ( y = lkerie , x = time ( lkerie ), pch = as.vector ( season ( lkerie ))) Call: lm(formula = lkerie ~ season_ + time_) Residuals: Min 1Q Median 3Q Max -2.0887 -0.6366 -0.0174 0.6178 2.3333 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 526.266512 4.902892 107.338 < 2e-16 *** season_February -0.045883 0.178105 -0.258 0.796791 season_March 0.177633 0.178105 0.997 0.319006 season_April 0.725750 0.178106 4.075 5.24e-05 *** season_May 1.055267 0.178107 5.925 5.33e-09 *** season_June 1.165384 0.178108 6.543 1.31e-10 *** season_July 1.125900 0.178109 6.321 5.13e-10 *** season_August 0.918617 0.178111 5.158 3.43e-07 *** season_September 0.612734 0.178113 3.440 0.000623 *** season_October 0.274650 0.178115 1.542 0.123616 season_November 0.024167 0.178117 0.136 0.892120 season_December -0.017116 0.178120 -0.096 0.923479 time_ 0.022599 0.002519 8.971 < 2e-16 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.8905 on 587 degrees of freedom Multiple R-squared: 0.2928, Adjusted R-squared: 0.2784 F-statistic: 20.26 on 12 and 587 DF, p-value: < 2.2e-16 Indeed, we can see that the seasonal means model has quite a bit of significant components, suggesting that there is a good amount of seasonality in the data. However, this is not consistent, with some months having insignificant values. In general, we can see that the summer months have positive coefficients, while the colder months (which are not nearly as significant) sit somewhere close to zero. This agrees with the earlier findings from the time series plot. Next, I perform a decomposition of the time series, to see how this package can break up the time series into seasonal, trend and random components. In [10]: madecomp <- decompose ( lkerie ) plot ( madecomp ) madecomp $ seasonal [ 1 : 12 ] -0.50421626984126 -0.547915249433118 -0.325653344671198 0.221778628117899 0.550673185941034 0.664431689342403 0.627271825396811 0.420052437641727 0.11478883219956 -0.224964569160989 -0.477234977324255 -0.519012188208614 We can see again that there is clearly a seasonal component to the time series. We can also see the general trend of decreasing and increasing. When these components are removed, we see a relatively stationary random process. This cyclical nature of the seasonality lead me to consider spectral analysis. However, first we will look at two other models which can deal with seasonality – the Holt Winters' Model and a seasonal ARIMA model. Holt Winters' Model I use two Holt-Winters' Models (an additive and multiplicative model) to model the data. Below, I fit the two models, and plot the resulting fits. The Holt-Winters models both consider seasonal components, although the multiplicative model and additive model come up with different quantities for the coefficients (see the appendix for the summaries). However, the smoothing parameters are relatively the same in character, with alpha=0.8973698 beta=0.003048736 and gamma: 1. This indicates that the level and seasonality are smoothing based on observations in the distant past (with gamma = 1 meaning it is considering only the previous observations), while a small beta indicates that it is using the trend of only the nearest values. I have plotted one of the models below, but the other is very similar in nature. However, as we will see later, the prediction error is quite high – (and the prediction error for the multiplicative model is, predictably, much higher). In [11]: hw1 <- HoltWinters ( lkerietrain ) hw1 plot ( hw1 ) Holt-Winters exponential smoothing with trend and additive seasonal component. Call: HoltWinters(x = lkerietrain) Smoothing parameters: alpha: 0.8987307 beta : 0.002926917 gamma: 1 Coefficients: [,1] a 5.720476e+02 b -1.549754e-04 s1 -4.205078e-01 s2 -3.699342e-01 s3 -2.099025e-01 s4 3.027650e-01 s5 5.968531e-01 s6 6.435499e-01 s7 6.181102e-01 s8 4.454104e-01 s9 1.618096e-01 s10 -2.064551e-01 s11 -3.922436e-01 s12 -3.976096e-01 In [12]: hw2 <- HoltWinters ( lkerietrain , beta = NULL , seasonal = \"multiplicative\" ) hw2 plot ( hw2 ) hw1pred <- predict ( hw1 , n.ahead = 12 , prediction.interval = TRUE ) hw2pred <- predict ( hw2 , n.ahead = 12 , prediction.interval = TRUE ) plot ( forecast ( hw1 , 12 )) plot ( forecast ( hw2 , 12 )) ssehw1 <- sse ( hw1pred [, 1 ], lkerietest ) ssehw2 <- sse ( hw2pred [, 1 ], lkerietest ) Holt-Winters exponential smoothing with trend and multiplicative seasonal component. Call: HoltWinters(x = lkerietrain, beta = NULL, seasonal = \"multiplicative\") Smoothing parameters: alpha: 0.8986116 beta : 0.002922608 gamma: 1 Coefficients: [,1] a 5.720483e+02 b -1.676177e-04 s1 9.992631e-01 s2 9.993512e-01 s3 9.996315e-01 s4 1.000530e+00 s5 1.001045e+00 s6 1.001127e+00 s7 1.001082e+00 s8 1.000780e+00 s9 1.000284e+00 s10 9.996387e-01 s11 9.993134e-01 s12 9.993037e-01 SARIMA Modeling Next, we will turn to using an ARIMA (or in this case, a seasonal ARIMA model) to model and predict the data. The above time series is not stationary, so we must transform it. The transformation which creates an optimal situation for ARIMA modeling is one that is roughly stationary. After looking through various transformations (including logarithmic, differencing and BoxCox transformations), I settled upon a differencing of logs. By differencing the logarithms of the model, we get a result that is very close to stationary. The resulting plot is plotted below. In [13]: ts.plot ( lkerie ) ts.plot ( log ( lkerie )) ts.plot ( diff ( lkerie )) ts.plot ( diff ( log ( lkerie ))) BxCx <- BoxCox.ar ( lkerie ) root <- BxCx $ mle ts.plot ( diff ( lkerie &#94; ( root ))) Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\" In [14]: lkerietrain.trans <- diff ( log ( lkerie )) plot ( lkerietrain.trans ) Except for the one outlier just before 1940, we can see that this time series is now stationary, and we can proceed to fit ARIMA models to it. Using the ACF, PACF and EACF plots, we can see that it will be difficult to find a fitting ARMA (without a seasonal component) model that fits the data well. Since we believe there is a good degree of seasonality to this data, this is not surprising. In the plots below of the ACF and PACF, we see some sine and cosine patterns in the autocorrelations. This indicates a seasonal model may be more appropriate than an ordinary ARMA or ARIMA model. In [15]: mean ( lkerie ) mean ( diff ( lkerie )) mean ( diff ( log ( lkerie )), lag = 12 ) mean ( diff ( lkerie &#94; root )) 570.745583333333 0.00160267111853076 2.80638701397251e-06 1.83050684474109 In [16]: acf ( lkerietrain.trans ) pacf ( lkerietrain.trans ) Using the arima.subsets function gives us a good idea of the seasonality and we can better infer the best models to use based on this. In [17]: a <- armasubsets ( lkerietrain.trans , nar = 12 , nma = 12 ) plot ( a ) We can see the most significant lags are those at times 1, 11, and 12 for the AR component, and lag 12 for the MA component. Using the auto.arima and get.best.arima functions, we can get the optimal model based on the AIC. In [18]: auto.arima ( lkerietrain.trans ) get.best.arima ( lkerietrain.trans , maxord = c ( 2 , 2 , 2 , 2 , 2 , 2 )) Series: lkerietrain.trans ARIMA(3,0,0)(2,1,0)[12] Coefficients: ar1 ar2 ar3 sar1 sar2 0.3252 -0.0717 -0.0758 -0.7303 -0.3584 s.e. 0.0413 0.0432 0.0412 0.0388 0.0385 sigma&#94;2 estimated as 1.525e-07: log likelihood=3772.65 AIC=-7533.29 AICc=-7533.15 BIC=-7507.04 Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\" [[1]] [1] -7761.115 [[2]] Call: arima(x = x_ts, order = c(p, d, q), seasonal = list(order = c(P, D, Q), frequency(x_ts)), method = \"CSS\") Coefficients: ma1 sar1 sma1 intercept 0.2951 0.9918 -0.9040 2e-04 s.e. 0.0375 0.0049 0.0208 3e-04 sigma&#94;2 estimated as 1.315e-07: part log likelihood = 3895.35 [[3]] [1] 0 0 1 1 0 1 The get.best.arima's model produces the minimal AIC, and therefore is the optimal model. We can see it is very similar to the models determined by the subsets, and adds an MA component to the seasonal part of the ARIMA model. I consider both models found (that is, SARIMA[1,0,0]x[2,0,2] and SARIMA[0,0,1]x[2,0,2]). In both models, all of the terms are significant, so there is no need to remove any. Looking at the residuals of these ARIMA models, we can see that they are roughly normally distributed, with the outliers skewing it somewhat. This is encouraging. Unfortunately, the residuals (and squared residuals) do seem to still exhibit autocorrelation. However, the ACF and PACF squared do not appear to have drastically more correlated values in the squared case. I have included the Q-Q plot and the ACF of the residuals below. For brevity, I have omitted the PACF, but it exhibits similar results. Regardless, this result is disappointing. This led me to use spectral analysis to interpret the seasonality, as I believed there was some underlying seasonal pattern that could not be found using these standard SARIMA processes. In [19]: eacf ( lkerietrain.trans ) sarima1 <- arima ( log ( lkerietrain ), order = c ( 0 , 1 , 1 ), seasonal = list ( order = c ( 2 , 0 , 2 ), frequency ( lkerietrain )), method = \"CSS\" ) sarima2 <- arima ( log ( lkerietrain ), order = c ( 1 , 0 , 1 ), seasonal = list ( order = c ( 0 , 1 , 1 ), frequency ( lkerietrain )), method = \"CSS\" ) sarima1pred <- predict ( sarima1 , n.ahead = 12 ) sarima2pred <- predict ( sarima2 , n.ahead = 12 ) predarima1 <- exp ( sarima1pred $ pred ) predarima2 <- exp ( sarima2pred $ pred ) searima1 <- exp ( sarima1pred $ se ) searima2 <- exp ( sarima2pred $ se ) ts.plot ( lkerietrain , predarima1 , col = \"red\" , main = \"SARIMA 1\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima1 +1.96 * searima1 )), col = \"red\" , lty = \"dashed\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima1 -1.96 * searima1 )), col = \"red\" , lty = \"dashed\" ) lines ( lkerietrain ) ts.plot ( lkerietrain , predarima2 , col = \"blue\" , main = \"SARIMA 2\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 +1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 -1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( lkerietrain ) ssesarima1 <- sse ( predarima1 , lkerietest ) ssesarima2 <- sse ( predarima2 , lkerietest ) plot ( residuals ( sarima1 )) plot ( residuals ( sarima2 )) AR/MA 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 x x o x x x x x o x x x x x 1 x x o x x x x x o o x x x x 2 x x o o o o o o o o o x o o 3 x x o o o o o o o x o x o o 4 x x x o o o o o o o o o o o 5 x x x x o o o o o o o x o o 6 x x x x o o o o o o o x o o 7 x x o x o x o o o o o x o o In [20]: qqnorm ( y = residuals ( sarima1 )) acf ( residuals ( sarima1 )) acf ( residuals ( sarima1 ) &#94; 2 ) In [21]: pacf ( residuals ( sarima1 )) pacf ( residuals ( sarima1 ) &#94; 2 ) acf ( residuals ( sarima2 )) acf ( residuals ( sarima2 ) &#94; 2 ) pacf ( residuals ( sarima2 )) pacf ( residuals ( sarima2 ) &#94; 2 ) Spectral Analysis + ARIMA The cyclical component of the time series led me to believe that using spectral analysis to analyze the seasonality of the data may be fruitful. The method I used here was to find the highest frequencies on the periodogram, and use these to model the seasonal components. Using the ten frequencies with this highest spectrum on the log of the time series, I created twenty variables with a sin/cosine function pertaining to each. I then performed linear regression on the transformed time series with these ten values to create a harmonic model. In [22]: spec.per <- spec ( log ( lkerietrain )) frequencies <- spec.per $ freq [ order ( spec.per $ spec , decreasing = TRUE )] t <- 1 : length (( log ( lkerietrain ))) length ( t ) cc1 <- as.numeric ( cos ( 2 * pi * frequencies [ 1 ] * t )) d1 <- sin ( 2 * pi * frequencies [ 1 ] * t ) c2 <- cos ( 2 * pi * frequencies [ 2 ] * t ) d2 <- sin ( 2 * pi * frequencies [ 2 ] * t ) c3 <- cos ( 2 * pi * frequencies [ 3 ] * t ) d3 <- sin ( 2 * pi * frequencies [ 3 ] * t ) c4 <- cos ( 2 * pi * frequencies [ 4 ] * t ) d4 <- sin ( 2 * pi * frequencies [ 4 ] * t ) c5 <- cos ( 2 * pi * frequencies [ 5 ] * t ) d5 <- sin ( 2 * pi * frequencies [ 5 ] * t ) c6 <- cos ( 2 * pi * frequencies [ 6 ] * t ) d6 <- sin ( 2 * pi * frequencies [ 6 ] * t ) c7 <- cos ( 2 * pi * frequencies [ 7 ] * t ) d7 <- sin ( 2 * pi * frequencies [ 7 ] * t ) c8 <- cos ( 2 * pi * frequencies [ 8 ] * t ) d8 <- sin ( 2 * pi * frequencies [ 8 ] * t ) c9 <- cos ( 2 * pi * frequencies [ 9 ] * t ) d9 <- sin ( 2 * pi * frequencies [ 9 ] * t ) c10 <- cos ( 2 * pi * frequencies [ 10 ] * t ) d10 <- sin ( 2 * pi * frequencies [ 10 ] * t ) spec.m1 <- lm ( log ( lkerietrain ) ~ cc1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9 + d10 + t ) summary ( spec.m1 ) 588 Call: lm(formula = log(lkerietrain) ~ cc1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9 + d10 + t) Residuals: Min 1Q Median 3Q Max -2.371e-03 -4.185e-04 -4.084e-05 4.940e-04 1.747e-03 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.346e+00 1.536e-04 41304.497 < 2e-16 *** cc1 -4.134e-04 3.943e-05 -10.483 < 2e-16 *** c2 -1.104e-03 3.827e-05 -28.837 < 2e-16 *** c3 6.794e-04 3.941e-05 17.237 < 2e-16 *** c4 1.714e-04 3.933e-05 4.358 1.56e-05 *** c5 8.460e-04 3.921e-05 21.575 < 2e-16 *** c6 6.117e-04 3.928e-05 15.574 < 2e-16 *** c7 3.266e-04 3.881e-05 8.415 3.22e-16 *** c8 -3.230e-04 3.914e-05 -8.252 1.10e-15 *** c9 -2.637e-04 3.906e-05 -6.752 3.62e-11 *** c10 6.280e-05 3.836e-05 1.637 0.10221 d1 -5.615e-04 1.048e-04 -5.356 1.24e-07 *** d2 -2.710e-04 3.830e-05 -7.075 4.44e-12 *** d3 1.012e-03 6.148e-05 16.458 < 2e-16 *** d4 -6.820e-04 4.453e-05 -15.314 < 2e-16 *** d5 -1.193e-04 4.073e-05 -2.929 0.00354 ** d6 -2.688e-04 4.208e-05 -6.386 3.55e-10 *** d7 -3.401e-04 3.876e-05 -8.775 < 2e-16 *** d8 6.069e-05 3.992e-05 1.520 0.12897 d9 -6.551e-05 3.941e-05 -1.662 0.09706 . d10 2.577e-04 3.855e-05 6.685 5.55e-11 *** t 2.417e-06 5.135e-07 4.708 3.16e-06 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.0006562 on 566 degrees of freedom Multiple R-squared: 0.8769, Adjusted R-squared: 0.8724 F-statistic: 192 on 21 and 566 DF, p-value: < 2.2e-16 The results here are very significant, with a very high R-squared (.8804). We can see that the plot below produced a moderately good fit, considering it is just a sum of sine and cosine waves. However, the danger of overfitting is present. To mitigate this issue, I used the training data and a ARIMA (1,0,1) to find the sum squared errors related to different amounts of frequencies. We can see that the error plateaus at 2 frequencies. I used frequencies of 2 and 4 to create these fits. I then used these frequencies as a prediction of the seasonal components. In [23]: plot ( exp ( fitted ( spec.m1 )), type = \"l\" , col = \"blue\" ) lines ( t , lkerietrain ) In [24]: test <- cbind ( c ( 589 : 600 ), cos ( 2 * pi * frequencies [ 1 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 1 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 2 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 2 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 3 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 3 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 4 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 4 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 5 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 5 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 6 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 6 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 7 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 7 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 8 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 8 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 9 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 9 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 10 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 10 ] * ( 589 : 600 ))) colnames ( test ) = c ( \"t\" , \"cc1\" , \"d1\" , \"c2\" , \"d2\" , \"c3\" , \"d3\" , \"c4\" , \"d4\" , \"c5\" , \"d5\" , \"c6\" , \"d6\" , \"c7\" , \"d7\" , \"c8\" , \"d8\" , \"c9\" , \"d9\" , \"c10\" , \"d10\" ) plot ( lkerietrain ) models <- list () In [25]: for ( i in 1 : 10 ){ beg <- \"cc1+d1\" if ( i > 1 ){ for ( s in 2 : i ){ beg <- paste ( beg , paste ( \"c\" , s , sep = \"\" ), paste ( \"d\" , s , sep = \"\" ), sep = \"+\" ) } } end <- paste ( beg , \"t\" , sep = \"+\" ) models [[ i ]] <- lm ( formula ( paste ( \"log(lkerietrain)\" , \"~\" , end , sep = \"\" ))) pre <- exp ( fitted ( models [[ i ]])) sub <- lkerietrain - exp ( fitted ( models [[ i ]])) arimasub <- Arima ( sub , order = c ( 1 , 0 , 1 )) print ( i ) print ( sum ((( arimasub $ fitted + pre ) - lkerietrain ) &#94; 2 )) } [1] 1 [1] 42.29154 [1] 2 [1] 27.36564 [1] 3 [1] 26.89214 [1] 4 [1] 26.63287 [1] 5 [1] 25.86952 [1] 6 [1] 25.1265 [1] 7 [1] 24.43297 [1] 8 [1] 24.10715 [1] 9 [1] 23.83364 [1] 10 [1] 23.4628 In [26]: summary ( models [[ 2 ]]) summary ( models [[ 4 ]]) ##c4 is insignificant, remove it models [[ 4 ]] <- lm ( log ( lkerietrain ) ~ cc1 + c2 + c3 + d1 + d2 + d3 + d4 + t ) Call: lm(formula = formula(paste(\"log(lkerietrain)\", \"~\", end, sep = \"\"))) Residuals: Min 1Q Median 3Q Max -0.0030522 -0.0009396 -0.0000317 0.0008021 0.0046639 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.347e+00 1.822e-04 34832.880 < 2e-16 *** cc1 -5.413e-04 8.497e-05 -6.371 3.83e-10 *** d1 -1.220e-03 1.389e-04 -8.786 < 2e-16 *** c2 -1.100e-03 8.384e-05 -13.122 < 2e-16 *** d2 -2.828e-04 8.386e-05 -3.373 0.000793 *** t -1.070e-06 5.851e-07 -1.829 0.067857 . --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.001437 on 582 degrees of freedom Multiple R-squared: 0.3927, Adjusted R-squared: 0.3875 F-statistic: 75.27 on 5 and 582 DF, p-value: < 2.2e-16 Call: lm(formula = formula(paste(\"log(lkerietrain)\", \"~\", end, sep = \"\"))) Residuals: Min 1Q Median 3Q Max -0.0027355 -0.0007332 -0.0001049 0.0008003 0.0036466 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.346e+00 2.050e-04 30958.858 < 2e-16 *** cc1 -4.664e-04 6.620e-05 -7.044 5.32e-12 *** d1 -4.834e-04 1.442e-04 -3.351 0.000857 *** c2 -1.104e-03 6.497e-05 -16.991 < 2e-16 *** d2 -2.688e-04 6.501e-05 -4.136 4.07e-05 *** c3 6.267e-04 6.618e-05 9.470 < 2e-16 *** d3 1.055e-03 9.067e-05 11.641 < 2e-16 *** c4 1.202e-04 6.608e-05 1.819 0.069362 . d4 -6.514e-04 7.133e-05 -9.133 < 2e-16 *** t 2.811e-06 6.783e-07 4.144 3.92e-05 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.001114 on 578 degrees of freedom Multiple R-squared: 0.6378, Adjusted R-squared: 0.6321 F-statistic: 113.1 on 9 and 578 DF, p-value: < 2.2e-16 Using the models with 2 and 4 frequencies, I subtracted these fitted spectral models from the original time series. As the plots are very similar in nature, I will only produce one of them here: In [27]: sub1 <- lkerietrain - exp ( fitted ( models [[ 2 ]])) sub2 <- lkerietrain - exp ( fitted ( models [[ 4 ]])) plot ( sub1 ) plot ( sub2 ) This produced two time series that are very reminiscent of random noise, which is what is expected, as this was to remove the seasonal component from the data. After a similar analysis to this data to the above SARIMA models, I arrived at two models to fit this ‘white noise' like data, SARIMA[1,0,1]x[0,1,1] for the first and SARIMA[2,1,1]x[1,0,1] for the second (the code is included in the appendix, but this result had the smallest AIC from the get.best.arima() function.) In [28]: acf ( sub1 ) pacf ( sub1 ) acf ( sub2 ) pacf ( sub2 ) eacf ( sub1 ) eacf ( sub2 ) plot ( armasubsets ( sub1 , nar = 10 , nma = 10 )) plot ( armasubsets ( sub2 , nar = 10 , nma = 10 )) auto.arima ( sub1 ) auto.arima ( sub2 ) get.best.arima ( sub1 , c ( 2 , 2 , 2 , 2 , 2 , 2 )) get.best.arima ( sub2 , c ( 2 , 2 , 2 , 2 , 2 , 2 )) a1spec1 <- Arima ( sub1 , c ( 1 , 0 , 1 )) a2spec1 <- auto.arima ( sub1 ) a1spec2 <- Arima ( sub2 , c ( 1 , 0 , 1 )) a2spec2 <- auto.arima ( sub2 ) a3spec1 <- Arima ( y = sub1 , order = c ( 1 , 0 , 1 ), seasonal = c ( 0 , 1 , 1 ), method = \"CSS\" ) a3spec2 <- Arima ( y = sub2 , order = c ( 2 , 1 , 1 ), seasonal = c ( 1 , 0 , 1 ), method = \"CSS\" ) AR/MA 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 x x x x x x x x x x x x x x 1 x x x o o o o o o x x x o o 2 x x o o o o o o o o o x x o 3 x x o o o o o o o o o x x o 4 x x o o o o o o o o o o o o 5 o x o o o o o o o o o o o o 6 x o x o o o o o o o o x o o 7 x x o o o o o o o o o x o o AR/MA 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 x x x x x x x x x x x x x x 1 x o x o o o o o o x x x x o 2 x x o o o o o o o o o x x o 3 o x o o o o o o o o o x x o 4 x x o o o o o o o o o o o o 5 x x o o o o o o o o o o o o 6 x o x o o o o o o o o x o o 7 o x o x o o o o o o o o o o Series: sub1 ARIMA(1,0,2)(2,0,0)[12] with zero mean Coefficients: ar1 ma1 ma2 sar1 sar2 0.9309 0.3420 0.0113 0.1461 0.1965 s.e. 0.0172 0.0457 0.0474 0.0412 0.0415 sigma&#94;2 estimated as 0.04377: log likelihood=85.88 AIC=-159.77 AICc=-159.63 BIC=-133.51 Series: sub2 ARIMA(1,0,1)(2,0,0)[12] with zero mean Coefficients: ar1 ma1 sar1 sar2 0.9032 0.3462 0.1385 0.1797 s.e. 0.0187 0.0399 0.0409 0.0416 sigma&#94;2 estimated as 0.04297: log likelihood=91.14 AIC=-172.28 AICc=-172.17 BIC=-150.39 Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\"Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\" [[1]] [1] -169.6077 [[2]] Call: arima(x = x_ts, order = c(p, d, q), seasonal = list(order = c(P, D, Q), frequency(x_ts)), method = \"CSS\") Coefficients: ar1 ar2 sar1 sma1 intercept 1.2697 -0.3110 0.9526 -0.8858 0.3867 s.e. 0.0393 0.0394 0.0149 0.0307 0.5862 sigma&#94;2 estimated as 0.04121: part log likelihood = 103.25 [[3]] [1] 2 0 0 1 0 1 Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\"Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\" [[1]] [1] -185.2687 [[2]] Call: arima(x = x_ts, order = c(p, d, q), seasonal = list(order = c(P, D, Q), frequency(x_ts)), method = \"CSS\") Coefficients: ar1 ar2 sar1 sma1 intercept 1.2491 -0.3189 0.9536 -0.8916 0.0008 s.e. 0.0391 0.0391 0.0138 0.0277 0.3225 sigma&#94;2 estimated as 0.04013: part log likelihood = 111.08 [[3]] [1] 2 0 0 1 0 1 The plots below show that the residuals for the first model are relatively normal and that they do not exhibit very high auto-correlation or partial auto-correlation in the squared cases, which suggests that a these ARIMA models are sufficient, and a GARCH model is unneccessary. In [29]: resid1 <- residuals ( a3spec1 ) resid2 <- residuals ( a3spec2 ) In [30]: plot ( resid1 ) acf ( resid1 ) acf ( resid1 &#94; 2 ) In [31]: plot ( resid2 ) pacf ( resid2 ) pacf ( resid2 &#94; 2 ) In [32]: # Predictions specpred1 <- exp ( predict ( models [[ 2 ]], newdata = as.data.frame ( test ))) specpred2 <- exp ( predict ( models [[ 4 ]], newdata = as.data.frame ( test ))) arimapred1 <- predict ( a3spec1 , n.ahead = 12 ) $ pred arimapred2 <- predict ( a3spec2 , n.ahead = 12 ) $ pred arimase1 <- predict ( a3spec1 , n.ahead = 12 ) $ se arimase2 <- predict ( a3spec2 , n.ahead = 12 ) $ se finalpred1 <- specpred1 + arimapred1 finalpred2 <- specpred2 + arimapred2 ssesp1 <- sse ( finalpred1 , lkerietest ) ssesp2 <- sse ( finalpred2 , lkerietest ) ssesp1 / 12 ssesp2 / 12 Warning message in predict.Arima(a3spec2, n.ahead = 12): \"MA part of model is not invertible\"Warning message in predict.Arima(a3spec2, n.ahead = 12): \"MA part of model is not invertible\" 0.146456905447528 0.127569058857754 Prophet The final model that I considered was one produced by Facebook's prophet package for R. All that is needed is to initialize the data and set it up appropriately. Although the prophet package is somewhat of a \"black box\" – and it is most effective on daily data – I thought it would be interesting to include it and see how it performs as compared with the other models. In [40]: ds <- as.Date ( lkerietrain ) y <- as.numeric (( log ( lkerietrain ))) df <- data.frame ( ds , y ) prop <- prophet ( df , weekly.seasonality = TRUE ) future <- make_future_dataframe ( prop , period = 12 , freq = \"m\" ) forecast <- prophet ::: predict.prophet ( prop , future ) fitproph <- exp (( forecast $ yhat )[ 1 : ( length ( forecast $ yhat ) -12 )]) predproph <- exp (( forecast $ yhat )[( length ( forecast $ yhat ) -11 ) : length ( forecast $ yhat )]) confintup <- exp (( forecast $ yhat_upper )[( length ( forecast $ yhat ) -11 ) : length ( forecast $ yhat )]) confintlow <- confintup <- exp (( forecast $ yhat_lower )[( length ( forecast $ yhat ) -11 ) : length ( forecast $ yhat )]) sseproph <- sse ( predproph , lkerietest ) Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. Results Below is a table representing the sum squared errors of each of the models when forecasting for the next 12 months. These are the errors of each model in comparison to the test data. In [41]: results <- data.frame ( c ( ssehw1 , ssehw2 , ssesarima1 , ssesarima2 , ssesp1 , ssesp2 , sseproph )) colnames ( results ) <- \"SSE\" rownames ( results ) <- ( c ( \"HW 1\" , \"HW 2\" , \"SARIMA 1\" , \"SARIMA 2\" , \"Spec 1\" , \"Spec 2\" , \"Prophet\" )) results SSE HW 1 2.216213 HW 2 2.222284 SARIMA 1 2.842606 SARIMA 2 1.682859 Spec 1 1.757483 Spec 2 1.530829 Prophet 2.811629 We can see that the models which use the frequencies from spectral analysis to remove the seasonal components are the best at predicting the data. This makes sense, as many geophysical processes can be modeled well using the sin and cosine waves in spectral analysis. Besides these two models, the SARIMA 2 model predicted with the least error. Below, I produce a plot which shows all the models' predictions as well as the true values for the time series. In [46]: length ( lkerie ) range <- 560 : 600 plot ( time ( lkerie )[ range ], lkerie [ range ], ty = \"l\" , xlab = \"Time\" , ylab = \"Water Level\" , main = \"Overview of Models Used\" ) addplot ( finalpred1 , range , colors2 [ 1 ]) addplot ( finalpred2 , range , colors2 [ 2 ]) addplot ( predarima1 , range , colors2 [ 3 ]) addplot ( predarima2 , range , colors2 [ 4 ]) addplot ( hw1pred , range , colors2 [ 5 ]) addplot ( hw2pred , range , colors2 [ 6 ]) addplot ( predproph , range , colors2 [ 7 ]) lines ( lkerie [ range ]) legend ( 'topleft' , c ( \"True\" , \"Spec 1\" , \"Spec 2\" , \"SARIMA 1\" , \"SARIMA 2\" , \"HW 1\" , \"HW 2\" , \"Prophet\" ), col = c ( \"black\" , colors2 ), lty = \"dashed\" , bty = 'n' , cex = . 75 ) 600 Finally, I produce graphs with confidence intervals for the most effective models of each type. In [61]: plot ( forecast ( hw1 , 12 )) plot ( x = time ( lkerie ), c ( lkerietrain , finalpred2 ), ty = \"l\" , main = \"Forecast of Spec 2\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain , finalpred1 +1.96 * arimase1 ), lty = \"dashed\" , col = \"blue\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain , finalpred1 -1.96 * arimase1 ), lty = \"dashed\" , col = \"blue\" ) lines ( lkerietrain , col = \"black\" ) plot ( time ( lkerie ), c ( lkerietrain , predarima2 ), ty = 'l' , col = \"blue\" , main = \"SARIMA 2\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 +1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 -1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( lkerietrain )","tags":"Projects","url":"https://seanammirati.github.io/category/projects/tsa_lake_erie.html","loc":"https://seanammirati.github.io/category/projects/tsa_lake_erie.html"},{"title":"ANOVA","text":"A common argument against ANOVA is non-normality -- when the variables in question do not follow a normal distribution. This exercise will explore that contention in more detail. I will be comparing simulated exponential and beta distributed variables to results of the F distribution of ANOVA to determine how inaccurate ANOVA is when used on variables that are independent and identically distributed to a non-Normal distribution. Helper Functions First, let's define some helper functions -- these define the sum of squares for the treatment and error. In [2]: sst <- function ( a , n , r ) { a <- matrix ( a , n , r ) rowmean <- apply ( a , 2 , mean ) sum ( n * ( rowmean - mean ( rowmean )) &#94; 2 ) } sse <- function ( a , n , r ) { a <- matrix ( a , n , r ) rowmean <- apply ( a , 2 , mean ) b <- numeric () for ( i in 1 : n ) { b [ i ] <- sum (( a [ i ,] - rowmean ) &#94; 2 ) } sum ( b ) } The following code creates $10&#94;5$ sets of 4 groups of 3, finds the $F$ statistic associated with each and stores it in an R object. So exemp1 has the empirical F statistics for $10&#94;5$ such sets using an exponential distribution, and bemp has the empirical $F$ statistics for $10&#94;5$ sets using a beta distribution. In [4]: help ( rexp ) In [3]: expemp1 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rexp ( 12 , 1 ) f <- ( sst ( a , 3 , 4 ) / 3 ) / ( sse ( a , 3 , 4 ) / 8 ) expemp1 [ l ] <- f } expemp2 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rexp ( 120 , 1 ) f <- ( sst ( a , 30 , 4 ) / 3 ) / ( sse ( a , 30 , 4 ) / 116 ) expemp2 [ l ] <- f } bemp1 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rbeta ( 12 , 5 , 2 ) f <- ( sst ( a , 3 , 4 ) / 3 ) / ( sse ( a , 3 , 4 ) / 8 ) bemp1 [ l ] <- f } bemp2 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rbeta ( 120 , 5 , 2 ) f <- ( sst ( a , 30 , 4 ) / 3 ) / ( sse ( a , 30 , 4 ) / 116 ) bemp2 [ l ] <- f } This has created all of the $F$ statistics needed to determine. This then uses $exp(12)$ with 4 groups of 3 and 30, and $beta(5,2)$ with 4 groups of 3 and 30. Below I create a table F to compare them with the true mean of the $F$ distribution ($\\frac{df_2}{df_2-2}$) and the quantiles (the median and 95th quantile.) In [5]: f1mean <- 9 / 7 f2mean <- 116 / 114 table_F <- matrix ( c ( f1mean , mean ( expemp1 ), mean ( bemp1 ), f2mean , mean ( expemp2 ), mean ( bemp2 ), qf ( . 5 , 3 , 9 ), median ( expemp1 ), median ( bemp1 ), qf ( . 5 , 3 , 116 ), median ( expemp2 ), median ( bemp2 ), qf ( . 95 , 3 , 9 ), quantile ( expemp1 , . 95 ), quantile ( bemp1 , . 95 ), qf ( . 95 , 3 , 116 ), quantile ( expemp2 , . 95 ), quantile ( bemp2 , . 95 )), 6 ) colnames ( table_F ) <- c ( \"Mean\" , \"Median\" , \"95th Percentile\" ) rownames ( table_F ) <- c ( \"F,3,9\" , \"Empirical Exp,3,4\" , \"Empirical Beta,3,4\" , \"F,3,116\" , \"Empirical Exp,30,4\" , \"Empirical Beta,30,4\" ) table_F Mean Median 95th Percentile F,3,9 1.285714 0.8516840 3.862548 Empirical Exp,3,4 1.319407 0.8701090 3.778306 Empirical Beta,3,4 1.353786 0.8590901 4.150790 F,3,116 1.017544 0.7933195 2.682809 Empirical Exp,30,4 1.013416 0.8021038 2.628236 Empirical Beta,30,4 1.017539 0.7929869 2.691883 Discussion We can see that, for the mean, the true F distribution has a lower mean than the empirical beta and exponential using 4 groups of 3. This suggests that if we were to use ANOVA with data with an exponential or beta distribution, the mean would be underestimated. That is, the true mean would be higher than that given by the F-distribution. For the median, the true F distribution again would underestimate the empirical results from the beta and exponential distributions. This implies that if we were to use ANOVA and found a P-value of .5 using the F-distribution, the true p-value would be lower than this. So our estimations would be skewed in favor of not rejecting the null hypothesis if we were to set the significance to .5. The 95th percentile is perhaps the most interesting case, where we can see some difference between the two empirical distributions. In the case of the exponential distribution, we can see that the empirical exponential F statistic produced lower 95th quantiles than the true F distribution. Suppose we were to run ANOVA on data that was exponentially distributed and got an F statistic of 3.8. Using the true F distribution, we would conclude that we cannot reject the null hypothesis that the two groups behave the same with 95% confidence. However, the true exponential distribution would have a 95th percentile of 3.72, meaning we would reject the null hypothesis that the groups are the same. It results in a type II error - we fail to reject the null hypothesis when it is false. The reverse is true for the beta distribution. Let's say we run ANOVA with data coming from an underlying beta(5,2) distribution and we get an F statistic of 4. The true F distribution would suggest that we reject the null that the groups are the same with 95% confidence - however, the empirical F using beta has a 95th percentile of 4.12, leading us to a Type I error, we have rejected the null hypothesis when it is true. When we increase the value of n, we see that the discrepancies between the true F distribution and the empirical distributions shrink. This makes sense intuitively; due to the central limit theorem, we would expect this to occur as we increase the number in each group. There are still some discrepancies, but the difference is not very large and would likely not give very different results except at the margin (when it is exactly one of the values of the other distributions). Interestingly, the beta distribution produces a lower F empirical than the true distribution for the 95th percentile and the median in this case. This seems preferable, because it means that Type I error (a false positive) is no more likely (in fact, less likely) using the true F distribution, as it is in fact more conservative than the actual distributions. So if we have a large sample size (n=120), and we are able to reject the null hypothesis, knowing only that the original distribution of each data point was either exponential, beta(5,2) or normally distributed (as is the normal case in ANOVA), we can be confident that our conclusion is correct regardless of which distribution it originally followed. It is in this sense that the original assumption of normality for ANOVA is perhaps not so important - compared to the independent and identically distributed assumption which has a larger effect if it is not satisfied.","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/anova.html","loc":"https://seanammirati.github.io/category/linear-models/anova.html"},{"title":"Factor Analysis","text":"Introduction Note: All of the code associated with this project can be found on my GitHub repository located here . What are the core factors of human personality? It's a hard question to answer! Data doesn't give us a direct answer to this question -- we may, using some sort of model , be able to determine how individuals are clustered into groups based on their responses to surveys. This is an example of unsupervised learning . We can use this information to cluser similar people together, and infer that they have similar personalities based on this. However, what we really want to know when we answer this question may be things that data cannot tell us. For example, what kinds of questions pertain to aggression? Introversion? Reliability? We can determine clusters of similar behaviors -- for instance, the answer to the questions \"Do you enjoy parties?\" and \"Do you have a large social network?\" will likely be correlated , so we may believe that these are influenced by some factor called extroversion. Most of the time, however, we do not model these explicitly -- we only wish to cluster individuals together, to make predictions of some measure (say, a happiness score), or make inferences about the questions themselves. For instance, people who answered question A and B in a certain way are likely to be similar to others in their responses to questions. This distinction is important -- we can speculate that there is some unknown factor involved, but it is not necessarily the case. In clustering algorithms like K-Means, SVD, LDA and Gaussian Mixture Models, we do not concern ourselves with what the actual underlying factors actually are -- we only use them as a way to categorize the data in order to make inferences or to predict another variable of interest. We can see that these approaches are data-driven . We assume that the data contains relevant information by which we can group people together. But does this truly answer the question we wish to solve? In most cases, the answer is no, at least not explicitly. Factor Analysis attempts to model these underlying constructs explicitly. In this way, it is hypothesis driven , i.e., we model for the hypothesis we wish to verify or reject. Hypothesis testing is a common exercise in Statistics, but in this case our hypothesis is quite specific -- we hyptohesize that there are some unknown, unseen, unobservable factors underlying the patterns we see. It is for this reason that is particularly controversial -- who is to say that such factors even exist, and if they do, how can we categorize them in abstract terms? This is an open ended question -- but factor analysis can begin to give us a sense of how these factors may interact. Here's the objective : from our concrete features, we wish to derive information about unknown, and generally unknowable, latent variables. Let's dive in! Overview Factor analysis is similar to principal components analysis in that we are working only with a single group of variables (in this case, the response) and we wish to in some way reduce the number of variables in order to simplify our data. This gives us a sense of what the underlying nature of the data is, and how it is organized. In factor analysis, we wish to reduce the redundancy of the variables by limiting them to a smaller number of factors which can adequately explain the variation in the full set of variables. Factor analysis is considered somewhat controversial by statisticians and is not encouraged by all schools of thought. This is because factor analysis is often difficult to validate in practice, as the number of factors or the interpretations are not always clear from the analysis itself. Although at first glance factor analysis and principal component analysis may seem very similar, there are key differences which separate the two methods. In principal component analysis we aim to maximize the total variance of the variables in question, but in factor analysis we wish to account for the covariance between the variables. While principal components analysis uses linear combinations of the variables themselves, in factor analysis we create a linear combination of factors , which are unobserved, supposed latent variables. The factors that we are looking at are often some underlying attributes of the variables that we believe to be \"seperate\" in some way. For instance, let's say there we have students' test scores for different classes: physics, chemistry, statistics, English, history, etc. We may expect that there would be some underlying factors that would affect an individual's ability to perform well in these classes. Perhaps this would be something like quantitative reasoning skills, critical thinking ability, or reading level and skill. We wish to reduce the variables to a smaller subset that can use these factors (which are not observed but can be derived from the data using factor analysis) to simplify our dataset. We would then use factor analysis to determine both the \"correct\" number of factors and the effects these factors have on each of the variables. It is quite easy to see how in practice this could be quite difficult to implement. This contributes to the skepticism of some statisticians to its use in the first place. Often times, the data doesn't easily lend itself to such a simplistic interpretation, and it is unclear what these factors can be and how they should be interpreted. The Model Our model is constructed as follows: For each observation vector of $p$ variables, we have $$ y_1 - \\mu_1 = \\lambda_{1 1}f_1 + \\lambda_{1 2}f_2 + ... + \\lambda_{1 m}f_m + \\epsilon_1 \\\\ y_2 - \\mu_2 = \\lambda_{2 1}f_1 + \\lambda_{2 2}f_2 + ... + \\lambda_{2 m}f_m + \\epsilon_2 \\\\ ... \\\\ y_p - \\mu_p = \\lambda_{p 1}f_1 + \\lambda_{p 2}f_2 + ... + \\lambda_{p m}f_m + \\epsilon_p $$ On the left side of the equations, $y_1, ..., y_p$ are the variables in the dataset, and $\\mu_1, ..., \\mu_p$ are the corresponding means of these variables. We do this in order to center the variables. On the right side of the equations, $\\lambda_{i j}$ are the loadings for the $ith$ variable and $jth$ factor, $f_1, f_2, ..., f_m$ are the $m$ factors, and $\\epsilon_1, \\epsilon_2, ..., \\epsilon_p$ are the $p$ error terms associated with each variable. Our goal in doing factor analysis is to find some $m << p$ such that the factors specified above are appropriate for the $p$ variables of interest. In doing this, we are defining the original $p$ variables into a linear combination of $m$ factors. The $f$s in the above model are the $m$ factors themselves, while the lambdas are the loadings, which serve as weights for each factor for each of the $p$ variables. While this may seem similar to a more typical multiple regression model , there are key differences. Perhaps the most important thing to note here is that the $fs$ are unobserved random variables, not fixed effects like in multiple regression. Factor analysis only represents one observation vector, while multiple regression represents all of the observations simultaneously -- in this way, factor analysis is looking more to individual variation as opposed to aggregate or population level aggregation. Our model can be written more simply in matrix notation as: $$ \\vec{y} - \\vec{\\mu} = \\Lambda\\vec{f} + \\vec{\\epsilon} $$ where $$ \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_p \\end{bmatrix} \\\\ \\vec{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{bmatrix} \\\\ \\vec{f} = \\begin{bmatrix} f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_m \\end{bmatrix} \\\\ \\vec{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_p \\end{bmatrix} \\\\ \\Lambda = \\begin{bmatrix} \\lambda_{1 1} ... \\lambda_{1 m} \\\\ \\vdots \\ddots\\vdots \\\\ \\lambda_{p 1} ... \\lambda_{p m} \\end{bmatrix} $$ Assumptions : In performing factor analysis, we assume that the following holds: $\\mathbb{E}[\\vec{f}] = \\vec{0}$ $\\Sigma_{\\vec{f}} = I_{mxm}$ $\\mathbb{E}[\\vec{\\epsilon}] = \\vec{0}$ $\\Sigma_{\\vec{\\epsilon}} = \\Phi_{mxm} \\text{, where } \\Phi{pxp} = \\begin{pmatrix} \\sigma&#94;2_{\\epsilon_1} & 0 & \\dots & 0 \\\\ 0 & \\sigma&#94;2_{\\epsilon_2} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\sigma&#94;2_{\\epsilon_p} \\end{pmatrix}$ $\\Sigma_{\\vec{f}, \\vec{\\epsilon}} = 0_{mxm}$ Notationally, I mean $\\Sigma_{\\vec{X}}$ to be the square covariance matrix of a vector $\\vec{X}$ with itself, where the $i, j$th element is the covariance of the $i$th entry in $\\vec{X}$ with the $j$th entry. It is therefore a positive semi-definite symmetric matrix with variances in the diagonal. I mean $\\Sigma_{\\vec{X}, \\vec{Y}}$ to be the covariances of each element in $\\vec{X}$ with each element in $\\vec{Y}$. This is not a square matrix, but relates the covariance of the individual elements of the two vectors. These assumptions follow naturally from the model itself. For instance, as $\\mathbb{E}[\\vec{y} - \\vec{\\mu}] = \\vec{0}$, it follows that both episilon and the factors themselves must also have an expected value of zero. We assume that the factors are uncorrelated, as we wish to minimize the number of variables used (so, we wish to have no correlation between the factors themselves.) The $\\Phi$ matrix shows that there are no covariance between the errors, only a specific variance for each error term, meaning the factors account for the correlations amongst the $y$s. This is exactly the goal of factor analysis, so these assumptions are self-checking -- if in recreating the model it is not clear what the factors should be or what number of them there should be, it is saying that these assumptions have not been met. In achieving our goal in factor analysis of reducing the variables used, we wish to express the covariance of $\\vec{y}$ in terms of the loadings $\\Lambda$ and the variances of the errors, $Phi$ using some number of factors $m$ which is less than the original number of variables $p.$ We can show that the population covariance matrix $\\Sigma_{\\vec{y}-\\vec{\\mu}}$ can be written as follows: $$ \\Sigma_{\\vec{y}-\\vec{\\mu}} = \\Lambda \\Lambda{'} + \\Phi$$ This follows from assumption 2 and 5 above. We can see then that the variance in this case will be a combination of some signal -- or explained -- variance in the latent variable loadings, and some random or unexplained variance in the error terms. This seperates Factor analysis from Principle Component Analysis -- in PCA, we make no distinction between explained and unexplained variance except in terms of the number of components selected. Here, we make this distinction for a number of latent variables. It is also of note that we can rotate the loadings by an orthogonal matrix without effecting their ability to reproduce this population covariance matrix. As such the loadings are not unique. We will use these rotations when we perform our analysis. The rotations prove useful as the results given in the loadings may not make it clear which variables are effected by which factors. By rotating the coordinate axis, we can more easily interpret the results from the factor analysis. The variances of each of the individual $y$s can be written as follows: $$ Var[y_i] = h&#94;2_i + \\phi_i $$ where $h_i&#94;2 = \\lambda_{i 1}&#94;2 + \\lambda_{i 2}&#94;2 + ... + \\lambda_{i m}&#94;2$ and $\\phi_i$ is the specific variance for the ith error term, that is, the ith diagonal of $\\Phi$. Thus we can separate the variance of each of the $y$s into a communal and specific part, which is $h_i&#94;2$ and $\\phi_i$ respectively. As such, the diagonal elements can be easily estimated using the loadings and the specific variance, while the off diagonal elements depend on the selection of the loadings alone. Since factor analysis is largely dealing with the estimations of the loadings, it accounts for the covariations between the variables, rather than the total variance as in principal component analysis. There are four different methods to obtain the loadings $\\Lambda$ from a sample. These are the principal component method, the principal factor method, the iterated principal factor method, and the maximum likelihood method. These will be explored later in the coding. Another difficulty is determining the number of factors, $m$. There are four approaches: we can select the number of variables $m$ which accounts for a prespecified amount of variance of the variables, we can choose $m$ to be the number of eigenvalues of the correlation matrix $R$ which is greater than the average of the eigenvalues, we can use a scree plot to determine where there is a leveling of the eigenvalues of $R$, or we can test the hypothesis using a chi-squared distribution if the number of factors is the true number of factors. Again, this will be shown further in the coding. Data The dataset I will be using to illustrate the benefits and limitations of factor analysis is collected data from an online personality questionnaire. The link can be found here . In this study, 1,005 participants were prompted with 40 statements to rate on a scale of one to five. This scale is used frequently and is known as the likert scale. It ranges from 1 (strongly disagree) to 5 (strongly agree). The data was partitioned into four sections, with every ten statements designed to be related to some attributes of the population; in particular, it was looking for their assertiveness, social confidence, adventurousness, and dominance. The questions were designed to discover the prevalence of these attributes through the responses to the statements in the survey. This seems like an ideal dataset to use factor analysis on, because the goal of the dataset seems to be precisely what factor analysis is used for. That is, we have some variables (the questions) that have some underlying, unobservable factor (initially hypothesized to be assertiveness, social confidence, adventurousness and dominance) that ties them all together. We are not very interested in the individual answers to the questions themselves, but of the behavior of the underlying factors that exist which are unobservable. Some examples of questions are printed below: AS2 I try to lead others. SC4 I express myself easily. AD6 I dislike changes. DO8 I challenge others' points of view. By looking at the coding markers of each of the questions, we can get a sense of how this survey was distributed and why. If, after doing factor analysis, there are four factors (that is, $m = 4$) and these factors are inclusive of each of the ten questions, we can then say that the data reflects these attributes well. Therefore, the goal of this analysis is to see if the data supports the idea that these factors are well separated in the variables, and if not, is there a better set of factors that can describe the individual more effectively. After doing some research, it seems that many personality scales fail to truly describe individuals completely. A very popular scale, the Myers Briggs Type Indicator (MBTI), has been purported by many to effectively separate all personalities into one of sixteen types. However, when researchers used factor analysis on data testing for the four underling dimensions of the Myers Briggs tests, they found conflicting results. One of these analyses confirmed the four-dimensionality of the data, while the other suggested there to be six, rather than the four purported by the Myers Briggs test.The motivation for choosing this topic and subsequent dataset was to test these findings for myself. Although I was unable to find an adequate dataset with questions relating to the Myers Briggs tests, I used this dataset to see how well factor analysis can truly separate personality types from a series of related statements. Analysis In [7]: require ( psych ) tmp <- tempfile () download.file ( \"http://personality-testing.info/_rawdata/AS+SC+AD+DO.zip\" , tmp ) data <- read.csv ( unz ( tmp , 'AS+SC+AD+DO/data.csv' )) unlink ( tmp ) data <- data [, 1 : 40 ] data [ 1 : 5 , c ( 1 , 11 , 21 , 31 )] R <- cor ( data ) Loading required package: psych Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : \"there is no package called ‘psych'\" AS1 SC1 AD1 DO1 4 5 5 1 4 4 2 4 5 4 4 4 4 2 4 3 4 2 4 4 In [5]: round ( R , 2 )[ 1 : 5 , 1 : 5 ] AS1 AS2 AS3 AS4 AS5 AS1 1.00 0.41 0.37 0.44 0.32 AS2 0.41 1.00 0.63 0.42 0.45 AS3 0.37 0.63 1.00 0.38 0.49 AS4 0.44 0.42 0.38 1.00 0.30 AS5 0.32 0.45 0.49 0.30 1.00 I begin by importing the dataset and only using the variables pertaining to the questions. The next step is to find the correlation matrix. This is the preferred matrix to work with, as it makes many of the computations easier to do (as compared with the covariance matrix itself). We can see immediately that this is a covariance matrix, as there are ones in the diagonal. However, it does not appear to have particularly strong or easily discernible groups of correlations between the variables, which implies that factor analysis may not perform as well as we had hoped. I did not produce the entire matrix, as it is a 40x40 matrix. Instead, I only look at the first five rows and columns for illustrative purposes. There were forty questions on the questionaire, with every ten representing a \"factor\" of one's personality. We are using factor analysis to determine whether there is statistical evidence that these groupings represent latent 'factors'. Selecting $m$ As mentioned before, it often is not clear how many factors, $m$, we should use in the factor analysis. In this example, it would be preferable to use four, as that is how the data is designed. However, as we will see, four factors are not sufficient to separate the variables. In order to perform factor analysis, we must find the correlation matrix of the dataset. This makes many of the calculations quite simple. The next step will be determining how many factors we should use. In [108]: R <- cor ( data ) Once we have the correlation matrix, the eigenvalues will tell us the optimal number of ways to reduce the features. In [17]: head ( round ( eigen ( R ) $ values , 2 ), 15 ) 9.21 4.25 2.71 2.12 1.59 1.4 1.25 1.07 0.98 0.93 0.88 0.81 0.79 0.77 0.73 There are numerous methods to select the number of eigenvectors/eigenvalues we want to include. Each eigenvalue represents the proportion of variance explained by that eigenvector (a linear combination of the variables). We want to select eigenvectors/values such that we can explain a good deal of the variance with a smaller number of linear combinations of the features. The first method involves choosing an arbitrary threshold. In this example, at 19 latent variables (that is, reducing the forty variables nearly in half) we have acheieved a threshold of variance explained greater than 60%. In [110]: sum ( eigen ( R ) $ values [ 1 : 19 ]) / 40 ##choose 19 0.80183840681608 Another method is to select eigenvectors such that each corresponding eigenvalue is greater than one. This means that the eigenvector of variables contributes more to the variance of the features than any single feature alone. Using this method, we come up with 8 linear combinations. In [111]: sum ( eigen ( R ) $ values > 1 ) ##choose 8 8 The final methodology is using a scree plot and determining where the eigenvalues level off. This is at 9 linear combinations of the features. In [112]: scree ( R ) ##choose 9 fa.parallel ( data ) Parallel analysis suggests that the number of factors = 9 and the number of components = 7 Three different methods were explored to determine the amount of factors, $m$, to use. Unfortunately, it seems right from the beginning that four factors will not be enough to adequately describe the variables. The three methods provide very different numbers: 19, 8 and 9. I will use 9, as it appears to be a middle ground between the two, and the scree plot does flatten out significantly. Also, we do not wish to use nearly half of the number of variables as factors! Even 9 factors is quite high. This is where factor analysis and principle component analysis begin to differ. In PCA, we simply use the eigenvectors of the correlation matrix as variables, or 'principle components'. This does not account for the noise in the principle components themselves -- it is not trying to find any signal here. Factor analysis considers that these components are representations of some latent variables, that which are the true 'signal' in the correlation matrix, while the remainder is noise. Factor analysis is controversial because it attempts to estimate this latency by approximating correlation matrices, as we saw above. Now that we have determined m, the number of factors, we will aim to create the loadings matrix to estimate the covariance matrix, as explained above. There are four methods by which we can estimate the loadings matrices. The loadings matrix in each case will be a $p x m$ matrix, and will use the eigenvalues of the correlation matrix. The first method we will use is the principal components method. Principal Components Method In [16]: eigenvectors <- eigen ( R ) $ vectors [, 1 : 9 ] loadings <- eigenvectors %*% ( diag ( 1 , nrow = 9 ) * ( eigen ( R ) $ values [ 1 : 9 ]) &#94; . 5 ) head ( round ( loadings , 2 )) -0.63 -0.12 -0.33 -0.05 0.20 -0.26 -0.01 -0.11 -0.22 -0.66 0.13 -0.12 0.22 0.22 0.13 -0.25 0.00 0.07 -0.64 0.20 -0.06 0.23 0.24 0.11 -0.31 0.07 -0.01 -0.60 0.00 -0.25 0.03 0.20 -0.10 0.22 0.39 0.18 -0.56 0.08 0.04 0.09 0.32 0.10 -0.34 0.03 0.09 -0.56 0.20 0.02 0.13 0.30 0.07 -0.28 -0.07 0.09 We first determine the loadings by multiplying the first $m$ eigenvectors (in this case, 9) by the square roots of their corresponding eigenvalues. This produces the above matrix, a unsightly $40x9$ matrix. Each column corresponds to a different loading. To determine the communalities and specific variances of the variables, we can square the entries in each of the rows of the loadings matrix. This will give us the $h&#94;2_i$ vector, from which we can also determine the specific variances. In [21]: hisq <- apply ( apply ( loadings , 2 , function ( x ) x &#94; 2 ), 1 , sum ) psi <- rep ( 1 , 40 ) - hisq cbind ( communalities = round ( hisq , 3 ), specific = round ( psi , 3 )) communalities specific 0.684 0.316 0.654 0.346 0.675 0.325 0.713 0.287 0.567 0.433 0.559 0.441 0.635 0.365 0.532 0.468 0.610 0.390 0.485 0.515 0.707 0.293 0.736 0.264 0.539 0.461 0.697 0.303 0.745 0.255 0.713 0.287 0.634 0.366 0.711 0.289 0.685 0.315 0.506 0.494 0.528 0.472 0.602 0.398 0.625 0.375 0.590 0.410 0.563 0.437 0.701 0.299 0.661 0.339 0.573 0.427 0.362 0.638 0.511 0.489 0.774 0.226 0.806 0.194 0.501 0.499 0.621 0.379 0.594 0.406 0.529 0.471 0.559 0.441 0.556 0.444 0.573 0.427 0.574 0.426 The above are the communalities and the specific variances of each of the variables. To determine the amount of the total variance of which each loading contributes, we can divide the eigenvalues of $R$ by the number of variables, $p = 40$. Thus, all nine factors account for about 60% of the variance of the variables. This is not very good, especially considering there were supposed to be only four underlying factors! Rotating the Loadings Because we have $m = 9$, I will use the varimax rotations from the psych package to rotate the loadings we have above. This will make it easier to see which variables correspond to the factors. In [115]: ## varimax includes a print method which makes the loading distributions easier to see. loadingsrot <- print ( varimax ( loadings ) $ loadings , cutoff = . 3 , sort = T ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] -0.681 [2,] -0.713 [3,] -0.681 [4,] -0.655 [5,] 0.506 0.486 [6,] -0.657 [7,] -0.742 [8,] -0.765 [9,] -0.616 [10,] -0.642 [11,] -0.623 [12,] -0.685 [13,] -0.698 [14,] 0.716 [15,] 0.784 [16,] 0.769 [17,] 0.731 [18,] 0.643 [19,] -0.397 -0.534 [20,] -0.718 [21,] -0.747 [22,] -0.688 [23,] -0.313 -0.728 [24,] -0.753 [25,] 0.759 [26,] 0.727 0.301 [27,] 0.722 [28,] 0.794 [29,] -0.349 0.786 [30,] 0.744 [31,] 0.745 [32,] 0.338 -0.618 [33,] 0.302 0.681 [34,] -0.408 -0.522 [35,] 0.578 [36,] -0.396 0.337 -0.501 [37,] -0.407 0.481 [38,] -0.478 0.355 [39,] 0.360 0.327 0.457 [40,] 0.391 0.326 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.970 4.308 3.507 2.557 3.954 1.746 2.289 1.437 1.819 Proportion Var 0.074 0.108 0.088 0.064 0.099 0.044 0.057 0.036 0.045 Cumulative Var 0.074 0.182 0.270 0.334 0.432 0.476 0.533 0.569 0.615 We can see from the above that the factors do not separate so easily. The values of the loadings below .3 have been omitted to make it clearer what the underling relationships are. Although they do not separate cleanly into the four theorized factors, we can see that the first factor, for instance, largely is between the first and tenth variables (which was assertiveness), the second factor between the 30th and 40th variables (which was dominance). Similarly, the fifth factor is between the 20th and 30th variables (social confidence) and the third and factor is largely between the 30th and 40th variables (adventurousness). However, the split is not so simple as we assumed before the analysis, as the other factors show relationships between variables which are not in the same groups. The second function simply sorts the variables by factor, so we can get a better sense of how they separate. We can see that towards the top there are larger values for the loading matrix, which indicates a stronger association with the factor. We can also see that the 6th-9th factors also have some large values, which means that their correlations cannot be ignored. This suggests that the initial separation into only four factors may have been overly simplistic. Principle Factors Method In the previous method, we had ignored the structure of the specific variances. Now, using the principal factors method, we will take the $\\Phi$ matrix into account. We first make an initial estimate of the $h_i$s, and use this to account for the specific variances. Below I create a diagonal matrix which has the specific variances in each of the diagonal elements. We will then subtract this matrix from the covariance matrix, and use the new matrix to compute our loadings. In [25]: hinit <- 1 - 1 / diag ( solve ( R )) psiint <- 1 - hinit cbind ( communalities = round ( hinit , 3 ), specific = round ( psiint , 3 )) communalities specific AS1 0.703 0.297 AS2 0.548 0.452 AS3 0.551 0.449 AS4 0.505 0.495 AS5 0.391 0.609 AS6 0.418 0.582 AS7 0.479 0.521 AS8 0.327 0.673 AS9 0.268 0.732 AS10 0.219 0.781 SC1 0.578 0.422 SC2 0.656 0.344 SC3 0.457 0.543 SC4 0.696 0.304 SC5 0.572 0.428 SC6 0.639 0.361 SC7 0.486 0.514 SC8 0.580 0.420 SC9 0.572 0.428 SC10 0.378 0.622 AD1 0.362 0.638 AD2 0.382 0.618 AD3 0.402 0.598 AD4 0.413 0.587 AD5 0.428 0.572 AD6 0.611 0.389 AD7 0.579 0.421 AD8 0.393 0.607 AD9 0.230 0.770 AD10 0.361 0.639 DO1 0.619 0.381 DO2 0.658 0.342 DO3 0.424 0.576 DO4 0.539 0.461 DO5 0.439 0.561 DO6 0.415 0.585 DO7 0.442 0.558 DO8 0.462 0.538 DO9 0.458 0.542 DO10 0.485 0.515 In [27]: New <- R - diag ( psiint , nrow = 40 ) eigs <- eigen ( New ) head ( round ( eigs $ values , 2 )) round ( eigs $ vectors , 3 )[ 1 : 5 , 1 : 5 ] 8.72 3.73 2.22 1.56 1.13 0.95 -0.212 0.064 0.217 -0.022 -0.256 -0.219 -0.065 0.061 0.177 -0.133 -0.211 -0.098 0.024 0.184 -0.150 -0.199 0.000 0.147 0.036 -0.166 -0.183 -0.036 -0.032 0.068 -0.188 We will then use this new matrix to compute our eigenvalues and vectors, and subsequently create our new loadings matrix. In a similar way, we will determine the specific variances and communalities, and then rotate the loadings matrix. In [28]: eigenvectors2 <- eigen ( New ) $ vectors [, 1 : 9 ] loadings2 <- eigenvectors2 %*% ( diag ( 1 , nrow = 9 ) * ( eigen ( New ) $ values [ 1 : 9 ]) &#94; . 5 ) hisq2 <- apply ( apply ( loadings2 , 2 , function ( x ) x &#94; 2 ), 1 , sum ) psi <- rep ( 1 , 40 ) - hisq loadings2rot <- print ( varimax ( loadings2 ) $ loadings , cutoff = . 35 ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] 0.713 [2,] -0.635 [3,] -0.662 [4,] -0.574 [5,] -0.549 [6,] -0.554 [7,] 0.500 [8,] [9,] 0.368 [10,] [11,] 0.636 [12,] 0.729 [13,] 0.460 [14,] 0.717 [15,] -0.620 [16,] -0.739 [17,] -0.360 0.505 [18,] -0.650 [19,] -0.688 [20,] 0.371 [21,] -0.466 [22,] -0.625 [23,] -0.627 [24,] -0.597 [25,] 0.649 [26,] 0.754 [27,] 0.732 [28,] 0.635 [29,] 0.362 [30,] 0.578 [31,] 0.727 [32,] -0.375 0.730 [33,] -0.609 [34,] -0.711 [35,] -0.686 [36,] -0.588 [37,] -0.582 [38,] -0.581 [39,] -0.631 [40,] -0.651 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.508 3.963 3.089 1.957 3.511 1.279 1.401 1.402 0.995 Proportion Var 0.063 0.099 0.077 0.049 0.088 0.032 0.035 0.035 0.025 Cumulative Var 0.063 0.162 0.239 0.288 0.376 0.408 0.443 0.478 0.503 We can see a similar pattern to that of the principal component method above. The variance is not as well explained using this method, but when we perform the next step (using iterations of the above method), it will become a better representation of the data. Again, we can see similar relationships between the first five factors and the variables, but it is not all encompassing. Principal Factors Method We will now use a similar method to above, but iterate it each time and update it with the new initial squared loadings. That is, when we calculate the new $h&#94;2_i$ values, we do the iterate until it converges. For completeness, I perform the process 100 times below. In [30]: hinit2 <- 1 - 1 / diag ( solve ( R )) for ( i in 1 : 100 ) { New2 <- R - diag (( 1 - hinit2 ), nrow = 40 ) eigenvectors3 <- eigen ( New2 ) $ vectors [, 1 : 9 ] loadings3 <- eigenvectors3 %*% ( diag ( 1 , nrow = 9 ) * ( eigen ( New2 ) $ values [ 1 : 9 ]) &#94; . 5 ) hinit2 <- apply ( apply ( loadings3 , 2 , function ( x ) x &#94; 2 ), 1 , sum ) } hisq3 = hinit2 round ( loadings3 , 3 )[ 1 : 5 , 1 : 5 ] -0.634 0.130 0.342 -0.038 -0.299 -0.650 -0.126 0.095 0.225 -0.120 -0.629 -0.191 0.040 0.241 -0.147 -0.591 0.003 0.226 0.040 -0.169 -0.540 -0.070 -0.047 0.089 -0.185 In [32]: head ( round ( hisq3 , 3 )) 0.829 0.602 0.638 0.568 0.43 0.454 In [35]: loadings3rot <- print ( varimax ( loadings3 ) $ loadings , cutoff = . 3 ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] 0.328 0.771 [2,] -0.639 [3,] -0.688 -0.300 [4,] 0.593 [5,] -0.551 [6,] -0.555 [7,] 0.487 -0.362 [8,] -0.357 [9,] -0.376 [10,] -0.326 [11,] 0.605 [12,] 0.755 [13,] 0.463 [14,] 0.324 0.749 [15,] 0.300 0.703 [16,] -0.757 [17,] -0.361 -0.509 -0.340 [18,] -0.624 [19,] -0.706 [20,] -0.327 -0.398 [21,] -0.350 -0.455 [22,] -0.647 [23,] -0.662 [24,] -0.606 [25,] 0.653 [26,] 0.771 [27,] 0.734 [28,] 0.641 [29,] 0.347 [30,] 0.573 [31,] 0.730 [32,] -0.367 0.834 [33,] -0.606 [34,] -0.717 [35,] -0.698 [36,] -0.592 [37,] -0.575 [38,] -0.578 [39,] -0.628 [40,] -0.649 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.456 3.967 3.049 1.971 3.529 1.417 1.485 1.511 1.206 Proportion Var 0.061 0.099 0.076 0.049 0.088 0.035 0.037 0.038 0.030 Cumulative Var 0.061 0.161 0.237 0.286 0.374 0.410 0.447 0.485 0.515 Again, we can see a similar pattern to the two above methods. The iterated method also created loadings which are closer to the principal component method. There are a few differences; for instance, in the iterated method we see that the third variable is part of both the 1st and 2nd factor, which wasnâ€™t present in the principal component method nor the principal factor method. Still, the analysis from before holds: much of the expected separation is contained in the factors (or some combination of them), but there are other correlations which those four factors cannot account for. I will use the rotated matrix above to make some considerations about the level of effectiveness of the analysis. We know that the main goal of factor analysis is to acquire factors for which each variable can be loaded into only one factor. If we can do this, we have essentially reached our goal â€\" we have separated the variables completely into a smaller number of factors which accounts for the covariates between them. However, in practice, this is very difficult to achieve. In the loading matrix above, there are many instance where the variables correlate strongly with more than one factor at once -- that is there is no clear distinction between what these questions relate to in terms of our factors. This indicates that factor analysis is not doing a very good job at separating the factors with the number of factors we have used. Some Considerations In an ideal situation, we would use a very small value of $m$ relative to the amount of variables $p$. Although $m = 9$ performed decently well, it is quite a large number of factors. Even with nine factors, we see that the factor analysis was only able to account for around 60% of the covariation of the original variables. A fundamental issue with factor analysis is that the correlation matrix $R$ contains both structure and error, and factor analysis is not able to separate the two. Thus, the original assumptions of no relationships between the errors and the factors are too optimistic, as this rarely happens in practice. As a result, the loadings above are quite difficult to interpret. It is difficult to say what the exact factors are in plain English. Here, we can see some of the difficulty in working with factor analysis in practice â€\" it is often questioned if these factors truly exist. Conclusions and Relation to Psychological Tests In [37]: print ( varimax ( loadings ) $ loadings , cutoff = . 3 ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] -0.408 -0.522 [2,] -0.681 [3,] -0.713 [4,] 0.744 [5,] -0.681 [6,] -0.655 [7,] 0.506 0.486 [8,] 0.578 [9,] 0.302 0.681 [10,] -0.407 0.481 [11,] -0.313 -0.728 [12,] -0.753 [13,] -0.478 0.355 [14,] -0.396 0.337 -0.501 [15,] 0.745 [16,] 0.759 [17,] 0.338 -0.618 [18,] 0.727 0.301 [19,] 0.722 [20,] 0.360 0.327 0.457 [21,] 0.397 -0.534 [22,] -0.718 [23,] -0.747 [24,] -0.688 [25,] -0.716 [26,] -0.784 [27,] -0.769 [28,] -0.731 [29,] -0.391 0.326 [30,] -0.643 [31,] 0.794 [32,] 0.349 0.786 [33,] 0.657 [34,] 0.742 [35,] 0.765 [36,] 0.616 [37,] 0.642 [38,] 0.623 [39,] 0.685 [40,] 0.698 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.970 4.308 3.507 2.557 3.954 1.746 2.289 1.437 1.819 Proportion Var 0.074 0.108 0.088 0.064 0.099 0.044 0.057 0.036 0.045 Cumulative Var 0.074 0.182 0.270 0.334 0.432 0.476 0.533 0.569 0.615 We know that the original variables were constructed considering four groups (assertiveness, social confidence, adventurousness, and dominance). These were the intended characteristics that the statements were testing for. Above, we can see how factors 1-5 capture a lot of the variables in each set. So, we could say that factor 1 represents assertiveness, factor 2 represents dominance, factor 3 and 4 represent adventurousness, and factor 5 represents social confidence. However, this would not be so accurate, as there are other factors to consider as well. We can see that factors 6-9 have members from all the different groups. This suggests that the relationship between the variables was not so simple as we once thought. Additionally, variable 10 failed to have a loading greater than .3 into any of the factors, which suggests that even 9 factors are inadequate to capture the variation in the data. In this example, which seemed relatively straightforward, we see that problems emerged quite quickly. What, for instance, is factor six? And why are factors 3 and 4 separated? It is not so clear that our prior ideas about the factors are accurate. One thing is clear, though. The four factors used to create the questionnaire are insufficient to truly describe the structure of the data. To say that these statements measure these factors would be jumping to conclusions too quickly. Doing so is, in a way, oversimplifying the data. In conclusion, we have learned that this dataset cannot be so simply separated into the four purported factors. In fact, even separating the variables into 9 factors can only account for 60% of the variation of the variables. We can see that while factor analysis is a powerful technique, it is very dependent on the dataset which it is performed on. The self-checking property of the assumptions of factor analysis ensures that only models which fit the assumptions will produce results which separate the variables adequately to the different factors.","tags":"Projects","url":"https://seanammirati.github.io/category/projects/factor_analysis.html","loc":"https://seanammirati.github.io/category/projects/factor_analysis.html"},{"title":"Summary Statistics Part 2: Measures of Spread","text":"Measures of Spread In the last post on central tendency , we discussed the basics of what a statistic and a population is as well as the common measures of central tendency, the mean , median and mode . In this post, we will dive deeper into how to describe sample and population data using Measures of Spread. When we talked about central tendency, we discussed the idea behind the \"average\" observation in a pool of data. When it comes to describing a dataset, the central tendency is a great place to start -- it gives us a sense of the location of our data. For instance, if we had the weight of a variety of individuals, we can get a sense of what a \"typical\" weight might be using measures of central tendency. Alien Brains Imagine you are an observer who knows nothing about a particular subject. Let's say this subject is the number of neurons in a sample of aliens' brains. Or, you know, something. Based on this information alone, an ordinary, human observer would have absolutely no sense of where to begin. That is, we have absolutely no prior information about aliens' brains. Let's say the sample of neurons in 100 aliens found in the first UFO crashing ever known to man look something like this: In [1]: import numpy as np import pandas as pd import scipy.stats import plotly.offline as plt import plotly.graph_objs as go plt . init_notebook_mode ( connected = True ) In [47]: alien_neurons = pd . Series ( np . random . negative_binomial ( 424242 , . 0042 , 100 )) In [48]: data = [ go . Histogram ( x = alien_neurons . tolist ())] layout = go . Layout ( title = 'Alien Brains' , xaxis = dict ( title = 'Number of Neurons' ), yaxis = dict ( title = 'Number of Aliens' ), bargap = 0.2 , bargroupgap = 0.1 ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Woah! These values are in the hundreds of millions! Well actually, the human brain has over 100,000,000,000 neurons! So, this alien is not the most sophisticated in terms of neurons. Is there a way to see this, the location, more quantitatively? Your answer should be ... yes! Using the techniques we learned last time, we can find the mean and median of this distribution. To spare the agony of performing this arithmetic on such huge numbers, we can use pandas to find these values. In [49]: alien_mean , alien_median = alien_neurons . aggregate ([ 'mean' , 'median' ]) print ( '''Mean of Alien Neurons: {} , Median of Alien Neurons: {} ''' . format ( alien_mean , alien_median )) Mean of Alien Neurons: 100544873.04, Median of Alien Neurons: 100546471.0 This means that the mean of this sample is 100,584,186.03 and the median is 100,592,749. These values are quite \"close\", which is expected because we see that the distribution is somewhat symmetric. But... wait a minute! How are we certain that these two values are \"close\"? We can look at our histogram to see if this assertion is correct. In [50]: alien_mean - alien_median Out[50]: -1597.9599999934435 In [51]: data = [ go . Histogram ( x = alien_neurons . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : alien_mean , 'y0' : 0 , 'x1' : alien_mean , 'y1' : 20 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : alien_median , 'y0' : 0 , 'x1' : alien_median , 'y1' : 20 , }] layout = go . Layout ( title = 'Alien Brains' , xaxis = dict ( title = 'Number of Neurons' ), yaxis = dict ( title = 'Number of Aliens' ), bargap = 0.2 , bargroupgap = 0.1 , shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Well, they look pretty close. However, this is far too hand-wavy of an answer. In determining the \"closeness\" of these two statistics, we would like to have an objective measure relative to scale. The actual difference is around {{alien_mean - alien_median}}! With just that information alone, we would likely not say that the two are \"close\". If the difference between what you got in a paycheck was \\$8,563 less than what you expected, you would definitely be upset! The more we think about it, relative differences like these are reliant of the <b> scale </b> of the numbers we're talking about. 8,000 stars may be astronomically small compared to the total number of stars, but \\$8,000 is significant to most people! However, because of the scale of this distribution, this is not a large difference relative to the differences in observations. Imagine we had a distribution that looked like this instead: In [52]: log_norm_sigma = 1.158 log_norm_mu = np . log (( alien_mean - alien_median ) / ( 1 - np . exp ( ( log_norm_sigma ** 2 ) / 2 ))) sim = pd . Series ( np . random . lognormal ( mean = log_norm_mu , sigma = log_norm_sigma , size = 100 )) In [53]: data = [ go . Histogram ( x = sim . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : sim . mean (), 'y0' : 0 , 'x1' : sim . mean (), 'y1' : 50 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : sim . median (), 'y0' : 0 , 'x1' : sim . median (), 'y1' : 50 , }] layout = go . Layout ( title = 'Not Alien Brains' , xaxis = dict ( title = 'Counts' ), yaxis = dict ( title = 'Volume' ), bargap = 0.2 , bargroupgap = 0.1 , shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) As a sidenote, the way this distribution was simulated was using the fact that we want a relative difference in the mean and median in the population to be equal to 4,692, but on a completely different scale. This was using the lognormal distribution, which is a skewed distribution, so it works well for this purpose! In [54]: sim . mean () - sim . median () Out[54]: 1260.074316668763 We can now clearly see that the absolute differences in the mean and median (and in fact, any of the observations) is not sufficient to describe their \"closeness\" to one another. We have an idea of the location, but we need a sense of scale. Why does spread matter? We can already see why the measures of central tendency are insufficient to give us an idea of what a distribution looks like. To investigate further, suppose we know only that the mean of some observations is 100. What else can we say? Well, not much. Consider the two distributions below. Both populations have mean 100, and we sample 100 values from them. The resulting samples have means very close to one another (and to 100), but they are quite different from one another. In [55]: ex_1 = np . random . normal ( 100 , 2 , 1000 ) ex_2 = np . random . normal ( 100 , 10 , 1000 ) data = [ go . Histogram ( x = ex_1 . tolist (), name = 'Less Variant' ), go . Histogram ( x = ex_2 . tolist (), name = 'More Variant' ) ] layout = go . Layout ( title = 'Mean of 100' , xaxis = dict ( title = 'Value' ), yaxis = dict ( title = 'Volume' ), ) figure = go . Figure ( data = data , layout = layout ) plt . iplot ( figure ) In [56]: ex_1 . mean () Out[56]: 100.06843195281844 In [57]: ex_2 . mean () Out[57]: 99.93551822004906 We can now see how the spread of these distributions are quite different. How can we measure spread? Range The simplest measure of spread is known as the range . This is the largest value in the sample subtracted from the smallest value in the sample. This can give us an idea of where the possible values of the sample lay. That is: $$ \\bar{S}_{range} = x_{max} - x_{min} $$ For instance, in our alien example, we have: In [58]: max_alien_neurons , min_alien_neurons = alien_neurons . agg ([ 'max' , 'min' ]) rng = max_alien_neurons - min_alien_neurons print ( '''Maximum: {} Minimum: {} Range: {} ''' . format ( max_alien_neurons , min_alien_neurons , rng )) Maximum: 100856570 Minimum: 100223744 Range: 632826 From this, we know the difference between the largest and smallest values. To see how the original difference compares to this, we can look at the ratio of the difference and the range. In [59]: abs ( alien_mean - alien_median ) / rng Out[59]: 0.002525117488841235 Here we can get a sense of how \"close\" the median and mean are. This ratio is necessarily between 0 and 1, with 0 representing the same point, and 1 representing the largest difference in the sample (that is $x_1 - x_2 = x_{range}$). Although simple, the range suffers from some problems in that it only can measure the differences between the largest and smallest points. It is therefore as variable as those points. Suppose we had a dataset that looked like the following: 0, 50, 50, 50, 90, 95, 100. Then, the mean is: In [60]: example_ds = pd . Series ([ 0 , 50 , 50 , 50 , 90 , 95 , 100 ]) example_ds . mean () Out[60]: 62.142857142857146 In [61]: example_ds . median () Out[61]: 50.0 Using the ratio calculation as before, we would calculate: In [62]: ( example_ds . mean () - example_ds . median ()) / ( example_ds . max () - example_ds . min ()) Out[62]: 0.12142857142857146 In [63]: data = [ go . Histogram ( x = example_ds . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . mean (), 'y0' : 0 , 'x1' : example_ds . mean (), 'y1' : 5 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . median (), 'y0' : 0 , 'x1' : example_ds . median (), 'y1' : 5 , }] layout = go . Layout ( title = 'Dummy' , xaxis = dict ( title = 'Number' ), yaxis = dict ( title = 'Volume' ), shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Now let's do something crazy -- let's change the last value of the series to 1 million, and see what happens to the \"ratio\" of the difference between the median and mean to the range. In [64]: example_ds . replace ({ 100 : 1000000 }, inplace = True ) In [65]: example_ds . median () Out[65]: 50.0 In [66]: ( example_ds . mean () - example_ds . median ()) / ( example_ds . max () - example_ds . min ()) Out[66]: 0.142855 In [67]: data = [ go . Histogram ( x = example_ds . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . mean (), 'y0' : 0 , 'x1' : example_ds . mean (), 'y1' : 6 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . median (), 'y0' : 0 , 'x1' : example_ds . median (), 'y1' : 6 , }] layout = go . Layout ( title = 'Dummy' , xaxis = dict ( title = 'Number' ), yaxis = dict ( title = 'Volume' ), shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Huh. Here, the mean and median are very far apart -- and the mean is quite a bit off from the \"average\" person, as only one observation pushed it up into the thousands. This is, again, due to the lack of robustness of the mean. However, our ratio measure does not seem to think there is much of a difference between the difference in this case and in the previous case. We would expect that our most extreme values in the sample may be more variable then values closer to the center. In this case, we can take ranges of specific quartiles , as discussed in the previous post, to determine variability. Interquartile range In a similar way, we can define a range between any two percentiles of the data. In this case, the range is a specific case of this, where we take the 0th and 100th percentile of the data. As discussed earlier, this may be quite variant -- these are the most extreme values of our distribution. Suppose instead we took the difference between the 25th and 75th percentiles, or the 1st and 3rd quantiles. It would follow that this measure would be more robust then the max-min range. This is known as the inter-quartile range and is (unsurprisingly) robust to outliers. That is, we can say that the interquartile range in a finite sample is: $$ \\tilde{S}_{IQR} = Q_x(.75) - Q_x(.25) $$ Where $Q_x(p)$ is the quantile function, i.e. the function that finds data which is the above $p%$ of the data. Arithmetically, we can determine this by finding the medians of the values above and below the median. Most software packages contain functions which can determine this automatically. In the case of the alien brains, we have: In [68]: iqr = alien_neurons . quantile ( . 75 ) - alien_neurons . quantile ( . 25 ) iqr Out[68]: 210608.0 This represents how distant the closest two points which account for 50% of the data are. In other words, we have determined a measure of how far away things are. Taking a similar ratio, we can determine how distant two points are. For example: In [69]: abs ( alien_mean - alien_median ) / iqr Out[69]: 0.007587366101921311 This value has a range between 0 and infinity. In general, each \"unit\" of this ratio measures how far away the two points are relative to the center 50% of the data . A small value indicates that two points are relatively \"close\", while a large value (>1) indicates they are \"distant\" relative to the center 50%. For more information on this robust measure of spread, see here Boxplots Using the information we have learned about the measures of spread and measures of central tendency, we can now culminate it together into a plot called a box and whisker plot. This plot shows the median, the inter-quartile range, and the total range in one figure. This can give us a good \"overview\" of what the data looks like in terms of quantiles. In [70]: data = go . Box ({ 'x' : alien_neurons , 'name' : 'Alien Neurons' }) plt . iplot ([ data ]) The center line represents the median, the two lines surrounding in the 25th and 75th quartiles, respectively, and the endpoints (and whiskers) represent the minimum and maximum. To compare to the non-symmetric simulated plot we discussed before, we can see large differences between the two: In [71]: data = go . Box ({ 'x' : sim , 'name' : 'Not Alien Neurons' }) plt . iplot ([ data ]) We can see now how much of the inferences about spread can be understood through the use of quantiles and differences between them. These measures are quite robust, but suffer in the sense of efficiency. In a manner equivalent to the median and mean, these measures of variability do not take into account every data point, as the mean does, but only considers the quantiles. The next measure of spread, and the most common historically and in practice, is the variance which is defined in terms of the mean instead of in terms of quantiles. But first, we will discuss the degrees of freedom implicit in the mean. Freedom -- the dream, and the degree to which we have it It is at this point that we should discuss degrees of freedom , before jumping into variance. Degrees of freedom are a concept which many struggle with in elementary statistics, and is often glossed over in respect to statistics courses. I believe that understanding the degrees of freedom are essential in understanding how randomness ties into statistical treatment of data. It is for this reason that I have included this here. If this feels too math heavy, feel free to disregard it -- it is not necessary to understand, but helpful. If you're fond of mathematics, read on. Consider it this way. Let's say I told you we have 100 points. How many degrees of freedom do these points have? When I ask this, I essentially am asking the question how many different ways can these points vary? Well, we only know that there are 100 points -- we know nothing of how these points are distributed. Therefore, they can conceivably be any 100 points, since we have no prior information. Now, suppose that I tell you that the mean of these points is 100. Have the degrees of freedom changed? The answer is yes. We can no longer vary the numbers absolutely freely -- we are constrained in the fact that the number must have some property -- the mean -- which equals some known value. In this case, n - 1 of the points can vary, after which the final number is fixed. Let's consider an example. Imagine we pick some random numbers out of a hat and get the following: 9, 8, 2, 42, 100, -2. Suppose there is only one paper left in the hat and I told you that I, an omniscient observer, know that the mean of all the values in the hat is 1000. What is the final value? Well, we know that $$ \\bar{X}_{mean} = \\frac{\\sum_{1}&#94;{n} x_i}{n} = \\frac{\\sum_{1}&#94;{n - 1}}{n} + \\frac{x_n}{n} $$ So, it follows that: $$ x_n = n\\bar{X}_{mean} - \\sum_{1}&#94;{n-1} x_i $$ This is only using elementary algebra. In our case, the final value is $$ 7 \\bar{X}_{mean} - \\sum_{1}&#94;{6} x_i\\\\ 7 \\times 1000 - (9 + 8 + 2 + 42 + 100 - 2) $$ This is: In [72]: points_picked = pd . Series ([ 9 , 8 , 2 , 42 , 100 , - 2 ]) last_value = 7000 - points_picked . sum () last_value Out[72]: 6841 To confirm that this is correct, lets see what the mean looks like. In [73]: points_picked . append ( pd . Series ( last_value )) . mean () Out[73]: 1000.0 But we could do this for any mean! Suppose the mean was instead -1000. Then In [74]: last_value = - 7000 - points_picked . sum () last_value Out[74]: -7159 In [75]: points_picked . append ( pd . Series ( last_value )) . mean () Out[75]: -1000.0 This describes what the mean tells us about the data -- if we know the mean, one of the data points essentially becomes redundant. Now, suppose I wanted to create my own hat game, where I wanted the mean to be 42. How could I do this? There are infinitely many sets of numbers I could choose that have a mean of 42. However, they all have the constraint that if we were to remove one value, we could reconstruct it in the way above ! That is, the final value is not random but determined . This means that we have n - 1 degrees of freedom -- or that the dimensionality of our random vector is now n -1. Then, I could randomly select n - 1 numbers, and select the final number deterministically such that $$ x_7 = 7 \\times 42 - \\sum_{1}&#94;{6} x_i $$ This will ensure that my mean is always 42. In fact, there is no such other number I could choose that has a mean of 42. Conversely, the number I select will uniquely determine the mean. So the mean has one degree of freedom -- it can be determined entirely from one number, if the other numbers are known. This becomes important because the more degrees of freedom we have, the more we allow random variation to play a part in our estimates. Although randomness may seem bad, in fact having more degrees of freedom can be a good thing, as is the case in linear regression -- a topic for a different day. In general, we have In [76]: def find_point_from_mean ( series_of_less , mean ): n = len ( series_of_less ) + 1 n_final = n * mean - series_of_less . sum () return n_final In [77]: find_point_from_mean ( points_picked , 1000 ) Out[77]: 6841 Freedom isn't free Now let's tackle the first case -- we have 100 points with a mean of 100. How many different ways can we select these 100 points? Consider the following: knowing only the mean of the distribution, we can select any n - 1 points, in this case 99, arbitrarily and completely at random, and then select the final point such that: $$ x_{final} = n\\bar{X}_{mean} - \\sum_{i=1}&#94;{n-1} x_i $$ In this case, this is $$ x_{final} = 10,000 - \\sum_{i=1}&#94;{99} x_i $$ Taken another way, we can say the following: Knowing the mean, we can calculate: $$ \\tilde{x}_i = x_i - \\bar{X}_{mean}\\\\ \\forall x_i \\in S $$ Only n -1 of these values are allowed to vary -- once we know n-1 of them, the last is determined by the calculation of the mean. In general, we have that the degrees of freedom can be decomposed as follows: $$ \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} = \\begin{bmatrix}\\bar{X}_{mean} \\\\ \\bar{X}_{mean}\\\\ ... \\\\ \\bar{X}_{mean}\\end{bmatrix} + \\begin{bmatrix} x_1 - \\bar{X}_{mean} \\\\ x_2 - \\bar{X}_{mean}\\\\ ... \\\\ x_n - \\bar{X}_{mean}\\end{bmatrix} $$ Note that this is in vector notation, but it makes the results easier to see. The vector on the left, our data points $x_1, x_2, ..., x_n$ have n degrees of freedom -- given no additional information, they are free to vary randomly. The first vector on the right, the vector of means, has 1 degree of freedom -- if we do not know it, it can vary in one dimension. That means it can be any single number. The final vector on the right now has n - 1 degrees of freedom (the degrees of freedom of the original vector minus the degrees of freedom of the mean). This means that given a mean, our data points can vary with dimension n - 1. The final dimension is then a linear combination of the other data points. Variance But how does this have any connection to the spread of a dataset? Happy you asked! One way we can consider spread is: how far away are the observations from the mean? In order to do this, we could take the differences between each point and the mean. This would look something like this: In [78]: x_differences = alien_neurons - alien_mean x_differences Out[78]: 0 -37191.04 1 56131.96 2 -208377.04 3 78291.96 4 209167.96 5 311696.96 6 157524.96 7 184399.96 8 -133574.04 9 11597.96 10 152764.96 11 -3920.04 12 -133156.04 13 -66259.04 14 29614.96 15 -45663.04 16 307873.96 17 106871.96 18 153068.96 19 -60630.04 20 -147816.04 21 7425.96 22 -156595.04 23 240394.96 24 1339.96 25 -22053.04 26 -118010.04 27 -262313.04 28 -146881.04 29 45270.96 ... 70 23517.96 71 234281.96 72 -129149.04 73 -226458.04 74 -48473.04 75 -61272.04 76 -32908.04 77 -146455.04 78 68237.96 79 -76984.04 80 151819.96 81 144539.96 82 111319.96 83 -132128.04 84 3298.96 85 -125061.04 86 59073.96 87 86979.96 88 -91383.04 89 112969.96 90 -253920.04 91 -157460.04 92 -32293.04 93 -321129.04 94 -107901.04 95 -152860.04 96 291216.96 97 91285.96 98 -4254.04 99 123267.96 Length: 100, dtype: float64 This will give us differences for each of the points. It would be nicer to have one number to measure the variability of the data, so we can take the average of these values: In [79]: x_differences . mean () Out[79]: -6.556510925292969e-09 Hm. This value is very close to zero. This is actually expected, simply because of the definition of the arithmetic mean . Consider the following: $$ \\bar{X}_{mean} = \\frac{\\sum_{i=1}&#94;{n} x_i}{n}\\\\ \\sum_{i=1}&#94;{n}(x_i - \\bar{X}_{mean}) =\\\\ \\sum_{i=1}&#94;{n}x_i - n\\bar{X}_{mean} = \\\\ \\sum_{i=1}&#94;{n}x_i - n\\frac{\\sum_{i=1}&#94;{n} x_i}{n} =\\\\ 0 $$ Intuitively, the mean is the value which is in the \"center\" of the distribution, in the sense that the differences above the mean are equal to the differences below the mean. The mean is specifically selected such that this sum will equal to zero. But in our case, we are not very happy with this. We are not as concerned with the sign of the distance from the mean, but how far away it is. We are not interested in the direction but the magnitude . For example, if the mean is 2 and we have two points around it, 1 and 3, we would say that they are equally distanced from the mean -- the sign of the difference (-1 and 1) does not matter. How can we eliminate the sign? Mathematically, there are two general approaches to this -- we can take the absolute value or we can take the squares . The absolute value preserves the exact magnitude, while squaring does not. Why might we want to take the squares? Squaring the distance values will essentially put more weight on values which are far from the mean, and less on those close. In this sense, we get a penalization in the case of extreme values -- extreme values will make our variance larger than \"common\" values. Additionally, historically the squares have been used because they are analytically \"nicer\" -- they are smooth functions for which we can take the derivative, while the absolute value is not. In the computing age, this has become less of an issue as numerical approaches to optimization problems have taken hold, but I digress. Since the squared version is used the most in statistical literature, we will define this first. We can define the sum of squares as the following: $$ SS = \\sum_{i=1}&#94;{n}(x_i - \\bar{X}_{mean})&#94;2 $$ This gives us a sense of the total amount of squared deviations from the mean. This is a measure of spread. It may be useful to get an average of these values, i.e. the \"typical\" squared deviation from the mean. This is known as the mean squared deviation from the mean, or the variance . In the case of a known population mean , we can calculate this variance directly from our sample. In this case, all of our values are allowed to vary freely -- they are not reliant on this known population mean. So we can take the mean as follows: $$ \\hat\\sigma&#94;2 = \\frac{1}{n}\\sum_{i=1}&#94;{n}(x_i - \\mu)&#94;2 $$ Of course, this would not happen often in practice. But since the mean was known, our values are not linearly computable directly from it -- in this sense, they can be any values. If we do not, however, we must make a correction known as the Bessel's correction . The sample variance is the variance which takes this into account, as follows: $$ \\hat{s}&#94;2 = \\frac{1}{n-1}\\sum_{i=1}&#94;{n}(x_i - \\bar{X}_{mean})&#94;2 $$ Here, we divide by n-1 because the vector of $x_i - \\bar{x}_i$ is only allowed to vary in n-1 dimensions -- because it is determined once the other points are calculated. This is how degrees of freedom relates to the sample variance.","tags":"Beginner Statistics","url":"https://seanammirati.github.io/category/beginner-statistics/sumstatsmeasuresofspread.html","loc":"https://seanammirati.github.io/category/beginner-statistics/sumstatsmeasuresofspread.html"},{"title":"Summary Statistics Part 1: Central Tendency","text":"Summary Statistics (and what they mean) Welcome! In this (very first) post, we are going to be going over a few basic statistical concepts involving summary statistics and how we can use them. This is a great place to start for getting some idea on how statistics can work for you! Statistics courses usually start with a discussion of summary statistics, and for good reason. Summary statistics are the essential building blocks of statistics. When we talk about expectation, or prediction, we are actually talking about a summary statistic -- in many cases, and algorithms, the mean. For the most part, summary statistics can give us a good idea of the underlying distribution and, in some cases, are sufficient in describing a set of data points for a particular algorithm. Don't Be Another Statistic The first question that will come up immediately when discussing this is what exactly a statistic is. In everyday language many people use statistic to mean a way of summarizing data into a single data point. When people say things like \"You are not just a statistic!\", what they are generally referring to is how statistics are aggregates of data. That is, statistics cannot capture all of the truth, but only an average. So is it fair to say that statistics are purporting to be cold-hard facts gathered from data? Not exactly. Statistics are gathered from sample data, and are often used as an estimation of the population. In statistician speak, a population is the theoretical overarching group that we wish to infer about. For instance, the population can be men living in the United States, all of the atoms in the universe, or the set of all possible combinations of people into couples. A sample is a subset of the population, a smaller group derived from it. We use statistics derived from the sample to make inferences about the population, but by definition these are not exact -- they depend on the sample that we choose. Almost surely, if we were to choose another sample we would come up with a different statistic. Therefore, a statistician would not take a summary statistic to be the hard, cold \"truth\", but a signal, or estimation, of the \"cold hard truth.\" Statistics vs Statistic Unfortunately, much of the confusion about the nature of a statistic has to do with the field known as statistics. Confusing, but true. Statistics is generally what people are referring to when they talk about the hand-wavy idea -- \"gathering insights from data.\" But this treats data as though it is a fixed, constant thing. This couldn't be further from the case! This is exactly what makes statistics such an interesting field -- statistics deals with random variation. Statistics isn't used for cases of absolute certainty, like many of the sciences do, but revels in the uncertainty of real life. Indeed, nearly everything has random variation involved in it, and therefore nearly everything is in the scope of statistics. The way we can quantify uncertainty is by using the theories of probability. But that is for another post! In general, we do not say that a statistic is the \"ground truth\" but a description of sample data. This sample statistic is then used to estimate the </b> population parameters.</b> This comes from the initial distinction that many people make -- samples themselves are not a representation of the ground truth. When a statistic is used as a statement about the population, we call this statistic an estimator of the population parameter. In fact, population parameters are not nearly as interesting as sample statistics. A population parameter is not probabilistic at all -- if it is obtainable, then it will be a single value with no variation. Sample statistics, however, do vary, and sometimes considerably based on how the data is sampled and how large the sample is. Throughout this exercise, I am going to be using some code to give examples of how these things work. I will only be displaying code that I believe is helpful for the task at hand, and I will create a future article describing how this code is written using python. In this post, which I will be calling a \"framework\" post, I will only be describing how these things work. Then, in a \"application\" post, I will show how to write code to solve these problems with real-world data. Class Grades Now, onto an example. Suppose we have a class of 7th graders who had gotten the following grades on the previous exam: 95, 80, 50, 80, 100, 94. Let's start with the situation where this is the entire class. In this case, we are dealing with the population. That means, if we take the common 'statistics', we will not have a statistic at all but a parameter. This distinction makes it clear that we are not estimating anything, but are looking at the \"cold, hard truth.\" The population is then \"the grades of 7th graders on math test last week.\" If, however, we were trying to estimate their performance in the class overall from this test alone, then this is a sample , presuming there were other tests. Thus, the phrasing of the question is a large distinction. In [1]: from itertools import combinations import numpy as np import pandas as pd from scipy import stats import matplotlib as plt % matplotlib inline In [2]: class_grades = pd . Series ({ 'John' : 95 , 'Mary' : 80 , 'Jacob' : 50 , 'Jose' : 80 , 'Sarah' : 100 , 'Laura' : 94 }) class_grades Out[2]: John 95 Mary 80 Jacob 50 Jose 80 Sarah 100 Laura 94 dtype: int64 Central Tendency Given a set of data points, where do we start? A good start is determining the central tendency or what we often call the average. Here, I use the word average as a general case of central tendency -- we want to know what the average student in this class looks like. All of the following examples assumes that we have some finite sample of a finite population, but can be extended in the more general case. Arithmetic Mean One common example of an average is the arithmetic mean. We are all familiar with this from our schooling days: \\begin{equation} \\bar{X}_{mean} = \\frac{ \\sum_{i=0}&#94;{n} x_{i} }{n} \\end{equation} Here, we say that we sum over all of the possible values, and divide by the total number. We can see the results on our simple dataset above: $$ 95 + 80 + 50 + 80 + 100 + 94 = 499 = T \\\\ \\bar{x} = \\frac{T}{6} = 83.166 $$ In [3]: mean = class_grades . mean () mean Out[3]: 83.16666666666667 Median Another measure of central tendency is the median. The median is the value which half of the data is smaller and half is larger than. If there is an even number of points, we take the average of the two closest to the \"center\" of the ranked numbers. For example, we first list the points from smallest to largest: $$ 50, 80, 80, 94, 95, 100 $$ Then, we iteratively remove points from each side until we arrive at a single value (if the total number of points is odd) or two values (if it is even). This is shown below: $$ 50, 80, 80, 94, 95, 100\\\\ 80, 80, 94, 95\\\\ 80, 94\\\\ $$ Now that we have two values (since 6 = n is even), we take an average of these values. That is: $$ \\frac{(80 + 94)}{2} = 87 $$ Note that we can also express this as follows: Rank the $X_i$ from smallest to largest, where $i = 1, ..., n$. Define $X&#94;{(i)}$ as the $i$th order statistic , where $i$ is the rank in this list. Then: $$ \\tilde{X}_{median} = \\begin{cases} X&#94;{(\\frac{n+1}{2})}, & \\text{if n is odd}\\\\ \\frac{X&#94;{(\\frac{n+2}{2})} + X&#94;{(\\frac{n}{2})}}{2}, & \\text{if n is even} \\end{cases} $$ Then, $n$ in this case is 6, which is even. So we take the average of the 3rd and 4th order statistic, 80 and 94. This is the same result we had derived earlier. In [4]: median = class_grades . median () median Out[4]: 87.0 Mode Finally there is the mode. The mode is simply the most likely value, or the most recurring value. Thus there can be multiple modes if there are two values which are equally likely and are the most recurring in the dataset. We can summarize this by looking at the frequencies of each data point. For convenience, I have done this using pandas. In [5]: class_grades . value_counts () Out[5]: 80 2 95 1 94 1 100 1 50 1 dtype: int64 Because 80 occurs twice, it is the mode. In [6]: mode = class_grades . mode () mode Out[6]: 0 80 dtype: int64 For all unique values $u_j$ of $x_i$ in the sample, let $f(u_j) = \\sum_{x_i} 1, \\forall x_i = u_j$ Then, $f(u_j)$ is the frequency of unique value $u_j$. Then: $$ \\hat{X}_{mode} = \\{ u_j \\mid f(u_j) = \\max_{u_j}f(u_j) \\} $$ The set notation implies that it is possible for there to be multiple modes. If there is only one mode, such as in the case of the class grades, then we say that the sample is unimodal. Other Measures of Central Tendency These are all of the canonical measures of central tendency that you have likely heard about before. However, there are others as well such as those listed here . Check them out -- they're pretty interesting! For fun, I will show some examples below. Trimean The trimean is the weighted arithmetic mean of the median and two quartiles. That's a lot to unpack, so let me explain. By weighted, we mean that the observations are multiplied by some set of constants that place a magnitude on a particular observation. Higher weighted variables are considered more \"important\" to the calculation. Instead of dividing by the sample size, we divide instead by the sum of the weights . Another way of looking at this is that each observation is considered to have a probability of occurring, $\\frac{w_i}{\\sum_{i}w_i}$. Then, the weighted average is a generalization of the arithmetic average explained before, where the arithmetic average weighs each value equally, i.e, $w_i = c$ for all $i$. Then we arrive at the same formula we saw in equation (1). A quartile is the point that is greater then some multiple of 25% of the data. If there is no such discrete number, we take a weighted average to determine its value. It is called a quartile because it separates the data into four segments. The first quartile is the data point which is greater than 25% of the data, the second is the median (50% of the data), the third is for 75% of the data, and the forth quartile is also the maximum, greater than all of the data. Interestingly, this is often not considered a quartile, but by the same logic it can be considered one. The trimean is then defined as: $$ TM = \\frac{Q_1 + 2M + Q_3}{4} $$ Intuitively, this is the average of the Median (M) and the midhinge, defined as: $$ MH = \\frac{Q_1 + Q_3}{2} $$ This is calculated below: In [7]: def trimean ( series ): T = series . quantile ( . 25 ) + 2 * series . median () + series . quantile ( . 75 ) TM = T / 4 return ( TM ) TM = trimean ( class_grades ) TM Out[7]: 87.1875 In practice, it turns out that this estimator is a remarkably efficient estimator of the population, meaning it can accurately estimate the population mean if calculated on a sample with relatively fewer points than other estimators. For a symmetric distribution like the normal distribution, it is the most efficient as compared with the median, and other L-3 estimators. See here for more information. It is also is more robust than the mean, which will be discussed further below. Winsorized Mean This will be a good segue into the next section, which examines the most appropriate method of central tendency to use. The Winsorized Mean attempts to mitigate the effect of outliers -- values that are very far from what we would call the \"average\" value. If this doesn't make sense yet, don't worry. I'll be discussing it further later in the section on robustness. For now, we will just examine what this will look like. In the Winsorized mean, we will take the most extreme values of our sample and replace them with the most extreme remaining values. In this case, this is 50 and 100. So, our new, adjusted, sample to calculate the central tendency is: $$ \\tilde{S} = {80, 80, 80, 94, 95, 95} $$ Here, we replaced 50 with 80 -- the next smallest value -- and 100 with 95 -- the next largest value. In this case, the data is quite small, so we would likely not want to do this, but for a large dataset this can be useful. We then take the average of the remaining values. $$ \\frac{ \\sum_i \\tilde{s}_{i} }{n},\\\\ \\forall \\tilde{s}_i \\in \\tilde{S} $$ If you don't understand the notation, don't worry. We're just taking an average of our \"new\" sample. I just did it to be precise. In general, we can take the largest and smallest p% of data and replace it with the most extreme values within this range. This value is arbitrary, but is often taken to be 10 to 25% of the ends replaced. In this example, we take the top and bottom 10%, which trims the most extreme values on both sides. In [8]: def winsorized_mean ( series , percent_removed ): ## Taking the value which is greater than/less than percent_removed bot_quant = series . quantile ( percent_removed ) top_quant = series . quantile ( 1 - percent_removed ) #Finding the values to replace bottom = series [ series < bot_quant ] top = series [ series > top_quant ] #Replacements rep_bottom = min ( series [ series > bot_quant ]) rep_top = max ( series [ series < top_quant ]) replacement_dic = { b : rep_bottom for b in bottom } replacement_dic . update ({ t : rep_top for t in top }) # Make replacements, find mean S_tilde = series . replace ( replacement_dic ) winsorized_mean = S_tilde . mean () return ( winsorized_mean ) wm = winsorized_mean ( class_grades , . 1 ) wm Out[8]: 87.33333333333333 But wait! How can you just replace values like that! I know, it seems dishonest. The idea is that these values are extremely unlikely. In the case of 6 people, this is probably not the case. But imagine we had the following values: $$ 0, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 100 $$ In [9]: ex_outliers = pd . Series ( np . concatenate ([[ 0 ], np . repeat ( 95 , 18 ), np . repeat ( 96 , 12 ), [ 100 ]])) ex_outliers . mean () Out[9]: 92.5625 But, would we say that the average student received a grade of 92? This seems like an underestimation. Naively, if we choose the Winsorized mean, we would get the following: In [10]: winsorized_mean ( ex_outliers , . 1 ) Out[10]: 95.40625 This seems like a more accurate representation of the data. In the cases of outliers, one must be careful to examine the distribution of the data well before naively using one measure of central tendency, as the results may be very misleading. Which to choose? This is not always clear, as we saw above for the Windsorized mean. If we look at a histogram of the values, we can get a sense of the shape of the distribution. It turns out that the shape can help us determine which measure of central tendency best describes the distribution. In [11]: plt . pyplot . figure ( figsize = ( 15 , 10 )) plot = class_grades . hist () plot . axvline ( x = mean , color = 'red' , label = 'Arithmetic Mean' ) plot . axvline ( x = median , color = 'black' , label = 'Median' ) plot . axvline ( x = mode [ 0 ], color = 'green' , label = 'Mode' ) plot . axvline ( x = TM , color = 'yellow' , label = 'Trimean' ) plot . axvline ( x = wm , color = 'purple' , label = 'Winsorized Mean' ) plot . legend () Out[11]: <matplotlib.legend.Legend at 0x11662c5da58> All of these represent central tendency. To see this more easily, let's consider it on a larger dataset. In [12]: likelihood_estim = stats . beta . fit ( class_grades / 100 ) random_var = pd . Series ( np . random . beta ( likelihood_estim [ 0 ], likelihood_estim [ 1 ], 100 )) * 100 random_var = random_var . floordiv ( 1 ) C:\\Users\\ginge\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:437: RuntimeWarning: invalid value encountered in sqrt sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b) In [13]: plt . pyplot . figure ( figsize = ( 15 , 10 )) plot = random_var . hist ( bins = 30 ) plot . axvline ( x = random_var . mean (), color = 'red' , label = 'Arithmetic Mean' ) plot . axvline ( x = random_var . median (), color = 'black' , label = 'Median' ) plot . axvline ( x = random_var . mode ()[ 0 ], color = 'green' , label = 'Mode' ) plot . axvline ( x = winsorized_mean ( random_var , . 1 ), color = 'yellow' , label = 'Winsorized Mean' ) plot . axvline ( x = trimean ( random_var ), color = 'purple' , label = \"Trimean\" ) plot . legend () Out[13]: <matplotlib.legend.Legend at 0x11662e02d30> We see that this is a skewed distribution -- in particular it is left skewed . A majority of the students did well on the exam, but there were some who did particularly poorly, with some extreme values on the far left. In this case, where there are many extreme values, the mean is not a good \"average\" -- it is dragged down by the very small values in the tail. We can see this as the mean is close to 70, but it appears that the \"average\" student did better than this. The median is a more robust measure, because it is less sensitive to anomalies in the sample. The mode is not very useful in this case, as it is at the bin with the largest values. The \"average\" student likely did worse than this. In general, as a rule of thumb, for a unimodal distribution (meaning there is only one mode), it is often but not always the case that: Let $\\bar{X}_{median}$ be the median of a distribution and $\\bar{X}_{mean}$ be the arithmetic mean. Then: if $\\bar{X}_{median} << \\bar{X}_{mean}$, X is right skewed if $\\bar{X}_{median} \\approx \\bar{X}_{mean}$, X is approximately symmetric if $\\bar{X}_{median} >> \\bar{X}_{mean}$, X is left skewed This can be used as a guideline to see how skewed our distribution is. However, it is not foolproof - in some cases this fails. See this wikipedia article for more information. For instance, consider -6, -4, 0, 0, 2, 8. The mean and median are zero, but this distribution is skewed to the left, i.e. not symmetric. In [14]: plt . pyplot . figure ( figsize = ( 15 , 10 )) example = pd . Series ([ - 6 , - 4 , 0 , 0 , 2 , 8 ]) example . hist () plot . axvline ( x = example . mean (), color = 'red' , label = 'Arithmetic Mean' ) plot . axvline ( x = example . median (), color = 'black' , label = 'Median' ) plot . legend () Out[14]: <matplotlib.legend.Legend at 0x11662ea32e8> In general, since the median is a more robust measure, if we are given a sample and wish to estimate the behavior of the population and we believe that the skewed values are anomalies , the median is a better \"guess\" of the central tendency. Interestingly, the two \"non-typical\" central tendency estimators, the Windsorized mean and the Trimean, lay between the mean and the median. The Windsorized mean and Trimean are both more robust then the mean, meaning they are less sensitive to extreme values. This is directly dealt with in the Windsorized mean and is implicit in the Trimean in a method similar to taking the median. Notice also that the median and mean tell us different things about the data. The mean uses all of the information available, and each data point contributes equally to the evaluation of it. On the other hand, the median \"cuts away\" all of the data not at the 50% mark. In this way, the median can be seen as an extreme version of the Windsorized Mean where we cut out the bottom 49% and 51%. In this case, we are seeing a trade-off between robustness which is the variability of the estimator under anomalies and varying sample distributions, and power (or efficiency ), which can determine a more stable solution with less data assuming certain assumptions are met. For instance, if we know that the population distribution follows some \"known\" distribution, it can often be shown that the mean is a sufficient, unbiased estimator of the true population mean. In the case of the normal distribution, it is the most efficient estimator of the population mean, while the median is relatively inefficient. This means that the median will require more data to obtain the same level of accuracy as the mean. To sum it all up -- the mean is often the most efficient estimator in certain simplifying conditions, but is not robust. On the other hand, the median is robust , but not as efficient. Samples of samples of samples, oh my! Why do we care about these measures of central tendency? Well, let's take a look at how they function in terms of predicting a population parameter. Suppose, as we talked about before, the population in a simplistic example is 95, 80, 50, 80, 100, 94. Let's consider all possible samples of 3 of this data. There are 6 data points and we wish to choose 3. This means there are 6 choose 3, or 20 possible samples. The possible samples are: In [15]: possible_samples = combinations ( class_grades , 3 ) list ( possible_samples ) Out[15]: [(95, 80, 50), (95, 80, 80), (95, 80, 100), (95, 80, 94), (95, 50, 80), (95, 50, 100), (95, 50, 94), (95, 80, 100), (95, 80, 94), (95, 100, 94), (80, 50, 80), (80, 50, 100), (80, 50, 94), (80, 80, 100), (80, 80, 94), (80, 100, 94), (50, 80, 100), (50, 80, 94), (50, 100, 94), (80, 100, 94)] We remember from before that the population mean is 83.16666. This is a population parameter -- it is not a statistic. However, suppose we sampled randomly from this population and retrieved the sample data: 80, 94, 80. Then our estimate of the population mean would be 84.6. Remember when I said that the statistic is never the \"cold hard\" truth? Here is a direct example. However, there are some nice properties of this. Consider the means and medians of all the samples: In [16]: sample_dict = {} for i , x in enumerate ( combinations ( class_grades , 3 )): sample_dict [ i ] = x sample_results = pd . DataFrame ( sample_dict ) . T sample_results [ 'mean' ] = sample_results . mean ( axis = 1 ) sample_results [ 'median' ] = sample_results . iloc [:, 0 : 3 ] . median ( axis = 1 ) sample_results Out[16]: 0 1 2 mean median 0 95 80 50 75.000000 80.0 1 95 80 80 85.000000 80.0 2 95 80 100 91.666667 95.0 3 95 80 94 89.666667 94.0 4 95 50 80 75.000000 80.0 5 95 50 100 81.666667 95.0 6 95 50 94 79.666667 94.0 7 95 80 100 91.666667 95.0 8 95 80 94 89.666667 94.0 9 95 100 94 96.333333 95.0 10 80 50 80 70.000000 80.0 11 80 50 100 76.666667 80.0 12 80 50 94 74.666667 80.0 13 80 80 100 86.666667 80.0 14 80 80 94 84.666667 80.0 15 80 100 94 91.333333 94.0 16 50 80 100 76.666667 80.0 17 50 80 94 74.666667 80.0 18 50 100 94 81.333333 94.0 19 80 100 94 91.333333 94.0 We see that both the mean and the median vary around the true population mean for each sample. This is the sampling distribution of the sample mean. This is what has a distribution and can vary! Remember, our population parameter is fixed, but the variation implicit in each sample is what controls the distribution here. Since this too is a distribution, we can examine statistics about it. In [17]: sample_results . drop ([ 0 , 1 , 2 ], axis = 1 , inplace = True ) sample_results . mean ( axis = 0 ) Out[17]: mean 83.166667 median 87.200000 dtype: float64 In [18]: mean Out[18]: 83.16666666666667 Hmm, that seems odd. It turns out that the expectation, or arithmetic mean, of the sampling distribution of the sample mean is equal to the true population mean! This property actually follows directly from the definition of the arithmetic mean. We're going to go into some maths here, but feel free to skip over it if you don't want to. The point here is that the sample mean is an unbiased estimator of the population mean . Theorum 1: If $S$ is a sample of size $k$ from a population of size $N$ drawn randomly and without replacement , then the mean of the sample, $\\bar{X}_S$, is an unbiased estimator of the population mean. Let $\\bar{X}_P$ be the population mean. Then $$ \\bar{X}_P = \\frac{ \\sum_{i=1}&#94;{N} x_{i} }{N}\\\\ $$ Let $\\bar{X}_s$ be the mean of a sample of size k, called $S_s$. Then, $$ \\bar{X}_s = \\frac{ \\sum_j x_{j, s} }{k}\\\\ $$ Where $X_{j,s}$ is the $j$th observation from sample $s$. It follows that there are ${N}\\choose{k}$ possible samples, ie, $s$ runs from $1$ to ${N}\\choose{k}$. Let $M =$ ${N}\\choose{k}$. Then: $$ \\sum_{s=1}&#94;{M} \\frac{\\bar{X}_s}{M} =\\frac{\\sum_{s=1}&#94;{M} \\sum_j \\frac{x_{j, s}}{k}}{M}\\\\ =\\frac{\\sum_{s=1}&#94;{M} \\sum_j x_{j, s}}{kM} $$ Each value of the population will appear in ${N}\\choose{k}$ - ${N-1}\\choose{k}$ = $M - \\frac{M(N-k)}{N}$ samples. We can say this because it is the total number of possible samples ${N}\\choose{k}$ minus the total number of samples of size $k$ which do not contain observation $j$, ${N-1}\\choose{k}$. So then we have: $$ \\frac{M - \\frac{M(N-k)}{N}}{kM} \\sum_i x_i $$ Expanding the left hand side: $$ \\frac{M - \\frac{M(N-k)}{N}}{kM} = \\frac{1}{k} - \\frac{N-k}{kN} =\\frac{1}{N} $$ so $$ \\sum_{s=1}&#94;{M} \\frac{\\bar{X}_s}{M} = \\frac{ \\sum_{i=1}&#94;{N} x_{i} }{N}\\\\ $$ But, we now see that this is exactly equal to the population mean, $\\bar{X}_P$! Since we weighed each sample equally in this expectation, this is equivalent to sampling randomly without replacement. This means that we have proven the result -- the sample mean is an unbiased estimator of the population mean. This is a powerful result! Hopefully you can begin to see why statistics is so exciting now! We have proven that any inference about the population mean using a sample mean will, on average, be accurate. However, it is important to note that while the sample mean is unbiased, it does not make it the most accurate estimator. This will be covered in the next post, when we discuss the Measures of Spread. We've seemed to disregard the median in this case. Why is this? Well, the median is not an unbiased estimator of the population mean. However, we see that the case changes quite a bit when we take the medians! In [19]: sample_results . median ( axis = 0 ) Out[19]: mean 83.166667 median 87.000000 dtype: float64 In [20]: median Out[20]: 87.0 In this case we see that both the mean and the median are equal to the population values. What gives?! Well, it turns out that the sample median is a median unbiased estimator of the population median. This one is a bit tougher to prove, so I will leave it for now. Perhaps we can get back to it when we discuss probability density functions, as it requires some use of this. What about the mean? Why is the mean a median unbiased estimator of the population mean? The reason is because of the Central Limit Theorum, but this is something that will take some time to discuss! However, because the sampling distribution of the sample mean will tend to a normal distribution relatively quickly, it also becomes symmetric quickly. In this case, the median converges towards the mean. That is why we see that the mean in this case is again equal to the median. We can look at a histogram to see the details: In [21]: plt . pyplot . figure ( figsize = ( 15 , 10 )) sample_results [ 'mean' ] . hist () Out[21]: <matplotlib.axes._subplots.AxesSubplot at 0x11662e70278> Conclusion In this first post, we went over what a statistic is and the distinction between a sample and a population. We also discussed measures of central tendencies and their relation to sampling distributions. In the next post, we will be discussing the Measures of Spread which will shed light further on the sampling distribution of the sample mean. Hope you enjoyed!","tags":"Beginner Statistics","url":"https://seanammirati.github.io/category/beginner-statistics/sumstatscentraltendency.html","loc":"https://seanammirati.github.io/category/beginner-statistics/sumstatscentraltendency.html"},{"title":"Bootstrapping Exercise","text":"This is a simple example of using the boot function in R to produce bootstrapped estimates of the bias and standard error of the parameters. In [2]: install.packages ( 'ISLR' ) Updating HTML index of packages in '.Library' Making 'packages.html' ... done In [3]: library ( ISLR ) library ( boot ) attach ( Default ) head ( Default ) glm.fit <- glm ( default ~ income + balance , family = \"binomial\" , data = Default ) summary ( glm.fit ) boot.fn <- function ( data , index ) { return ( glm ( default ~ income + balance , family = \"binomial\" , data = data , subset = index ) $ coefficients ) } boot.fn ( Default , 1 : 100 ) boot ( Default , boot.fn , 1000 ) summary ( glm.fit ) default student balance income No No 729.5265 44361.625 No Yes 817.1804 12106.135 No No 1073.5492 31767.139 No No 529.2506 35704.494 No No 785.6559 38463.496 No Yes 919.5885 7491.559 Call: glm(formula = default ~ income + balance, family = \"binomial\", data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4725 -0.1444 -0.0574 -0.0211 3.7245 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.154e+01 4.348e-01 -26.545 < 2e-16 *** income 2.081e-05 4.985e-06 4.174 2.99e-05 *** balance 5.647e-03 2.274e-04 24.836 < 2e-16 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1579.0 on 9997 degrees of freedom AIC: 1585 Number of Fisher Scoring iterations: 8 (Intercept) -26.5660685235383 income 9.0174230280541e-20 balance -3.21887798899407e-20 ORDINARY NONPARAMETRIC BOOTSTRAP Call: boot(data = Default, statistic = boot.fn, R = 1000) Bootstrap Statistics : original bias std. error t1* -1.154047e+01 -4.009952e-02 4.329822e-01 t2* 2.080898e-05 1.740632e-07 4.721751e-06 t3* 5.647103e-03 1.964705e-05 2.331222e-04 Call: glm(formula = default ~ income + balance, family = \"binomial\", data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4725 -0.1444 -0.0574 -0.0211 3.7245 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.154e+01 4.348e-01 -26.545 < 2e-16 *** income 2.081e-05 4.985e-06 4.174 2.99e-05 *** balance 5.647e-03 2.274e-04 24.836 < 2e-16 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1579.0 on 9997 degrees of freedom AIC: 1585 Number of Fisher Scoring iterations: 8 As we can see above, the boot function found standard errors that were greater than the standard errors of the summary function. This makes sense, as the bootstrap method does not depend on estimates of the deviations of the error terms -- it simply samples observations repeatedly and creates models with them, and uses the difference between that and the full model to determine the standard error. So, as expected, the true standard error is likely larger than that given by the summary, since the summary is dependent on the type of model itself, while the bootstrap is not.","tags":"Model Validation","url":"https://seanammirati.github.io/category/model-validation/bootstrapping-exercise.html","loc":"https://seanammirati.github.io/category/model-validation/bootstrapping-exercise.html"},{"title":"Mixed Models vs GEE","text":"This is an example using 'fake' data for the difference between lmer (which assumes a constant (independent) covariance matrix for the error terms) and geepack (general estimating equations, which can handle an unknown convariance structure)) This example highlights the power of using a GEE model over a HLM model when the covariance structure differs at different levels of the data. The GEE model can be seen as a population estimating model -- i.e., it is preferable when we are trying to make inferences about the population rather than the individuals in the sample. We do not explicitly parametrize over the groups -- instead, we assign some covariance structure between members of each group and find the marginal distribution. The problem is formulated as follows: consider 6 athletes with characteristics given by: In [1]: library ( lme4 ) library ( geepack ) Loading required package: Matrix In [2]: athlete <- c ( 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 6 , 6 , 6 ) age <- c ( 38 , 40 , 43 , 53 , 55 , 56 , 58 , 37 , 40 , 42 , 41 , 45 , 46 , 54 , 58 , 57 , 60 , 62 ) club <- c ( 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 1 ) time <- c ( 95 , 94 , 93 , 96 , 98 , 91 , 93 , 83 , 82 , 82 , 91 , 94 , 99 , 105 , 111 , 90 , 89 , 95 ) logtime <- log ( time ) This is an example of a repeated measures problem. We create a dataframe from these elements: In [3]: df <- data.frame ( athlete , age , club , time , logtime ) df athlete age club time logtime 1 38 0 95 4.553877 1 40 0 94 4.543295 1 43 1 93 4.532599 2 53 1 96 4.564348 2 55 1 98 4.584967 2 56 1 91 4.510860 2 58 1 93 4.532599 3 37 1 83 4.418841 3 40 1 82 4.406719 3 42 0 82 4.406719 4 41 0 91 4.510860 4 45 1 94 4.543295 4 46 0 99 4.595120 5 54 1 105 4.653960 5 58 1 111 4.709530 6 57 1 90 4.499810 6 60 1 89 4.488636 6 62 1 95 4.553877 Now, we want to estimate the (log) time based on age and club. However, the data is clearly correlated -- each individual will have different specifications. Consider that we believe that the average (log) time will be different for each individual, as a baseline. This is a mixed model more specifically a Random Intercept model. This is highlighted by the following equations. In [4]: mod1 <- lmer ( logtime ~ club + log ( age ) + ( 1 | athlete ), data = df , REML = FALSE ) mod2 <- lmer ( logtime ~ log ( age ) + ( 1 | athlete ), data = df , REML = FALSE ) summary ( mod1 ) summary ( mod2 ) Linear mixed model fit by maximum likelihood ['lmerMod'] Formula: logtime ~ club + log(age) + (1 | athlete) Data: df AIC BIC logLik deviance df.resid -49.2 -44.7 29.6 -59.2 13 Scaled residuals: Min 1Q Median 3Q Max -1.3445 -0.7551 -0.1840 0.7895 1.2651 Random effects: Groups Name Variance Std.Dev. athlete (Intercept) 0.0042173 0.06494 Residual 0.0008858 0.02976 Number of obs: 18, groups: athlete, 6 Fixed effects: Estimate Std. Error t value (Intercept) 3.78158 0.45714 8.272 club -0.01049 0.02101 -0.499 log(age) 0.19753 0.11837 1.669 Correlation of Fixed Effects: (Intr) club club 0.184 log(age) -0.998 -0.216 Linear mixed model fit by maximum likelihood ['lmerMod'] Formula: logtime ~ log(age) + (1 | athlete) Data: df AIC BIC logLik deviance df.resid -50.9 -47.4 29.5 -58.9 14 Scaled residuals: Min 1Q Median 3Q Max -1.3343 -0.6924 -0.3215 0.7936 1.2927 Random effects: Groups Name Variance Std.Dev. athlete (Intercept) 0.0042948 0.06554 Residual 0.0008962 0.02994 Number of obs: 18, groups: athlete, 6 Fixed effects: Estimate Std. Error t value (Intercept) 3.8241 0.4527 8.448 log(age) 0.1846 0.1164 1.586 Correlation of Fixed Effects: (Intr) log(age) -0.998 However, this assumes that the covariance structure of the errors at each level are independent. This assumption is too simplifying -- it is likely that the variables are correlated across individuals (for instance, it appears that the clubs are for particular age ranges). To account for this, we could add more random effects to the model (which would subsequently break the data down into even smaller groups), or we could use a GEE to forgo the subject-level estimations and estimate on the populations, assuming some covariance structure between groups. In [5]: mod3 <- geeglm ( logtime ~ club + log ( age ), data = df , id = athlete , family = gaussian , corstr = \"exchangeable\" ) mod4 <- geeglm ( logtime ~ log ( age ), data = df , id = athlete , family = gaussian , corstr = \"exchangeable\" ) summary ( mod3 ) summary ( mod4 ) Call: geeglm(formula = logtime ~ club + log(age), family = gaussian, data = df, id = athlete, corstr = \"exchangeable\") Coefficients: Estimate Std.err Wald Pr(>|W|) (Intercept) 3.67345 0.42962 73.109 <2e-16 *** club -0.01429 0.01422 1.010 0.3149 log(age) 0.22584 0.11276 4.011 0.0452 * --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Estimated Scale Parameters: Estimate Std.err (Intercept) 0.004443 0.001609 Correlation: Structure = exchangeable Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.6112 0.1829 Number of clusters: 6 Maximum cluster size: 4 Call: geeglm(formula = logtime ~ log(age), family = gaussian, data = df, id = athlete, corstr = \"exchangeable\") Coefficients: Estimate Std.err Wald Pr(>|W|) (Intercept) 3.742 0.488 58.84 1.7e-14 *** log(age) 0.206 0.127 2.61 0.11 --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Estimated Scale Parameters: Estimate Std.err (Intercept) 0.00458 0.00154 Correlation: Structure = exchangeable Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.624 0.181 Number of clusters: 6 Maximum cluster size: 4 The main shift in inference here is from likelihood models (which are generally prefered) to a semi-parametric model that depends only on the first two moments. GEEs are better estimators of a population-level variance -- they cannot account for individual differences explicitly, as a GLMM could. In this example, however, we see that the GEE finds the log(age) value significant where the previous model had not. Depending on the goal of the analysis, the GEE will produce better estimates asymptotically at the cost of sample-level inference.","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/mixed-models-vs-gee.html","loc":"https://seanammirati.github.io/category/linear-models/mixed-models-vs-gee.html"},{"title":"NYPD Stop and Frisk","text":"The Results of Stop and Frisk: What Contributes Most to Arrests? Note: All of the code associated with this project, as well as the data used, is accessible at my GitHub repository here . The code is omitted here for brevity. For the complete code used in this project, please visit the GitHub repository listed above, or check out this article which also includes the R code as well as some unannotated intermediate results. Introduction In this project I will be looking at a dataset available from the city of New York about the results of the Stop, Question and Frisk policy. Which factors lead to the highest likelihood of arrest? The data is a collection of 22,564 cases of Stop, Question and Frisk in the City of New York in 2015. There are 112 variables which were measured. A majority of the variables in the dataset are categorical variables relating to the incident itself, while a few represent the age, race, height, weight, and sex of the person who was on the receiving end of the policy. Stop and Frisk has been a controversial policy in New York City for quite some time. Many feel that it is a violation of privacy, that it is unconstitutional, or that the policy is inherently racist - that it disproportionately affects minorities and people of color. Meanwhile, there are others who support the policy, believing stop and frisk to keep people safe from terrorists and other wrongdoers. As a result of this and other recent controversies involving police across the country, the NYPD has been getting a lot of negative press over the way it addresses these racial problems. What I aim to discover by analyzing this dataset are the main contributing factors to whether a person is arrested or not in a stop and frisk incident. I will be using a few different methods of analysis to determine the best model to use in predicting whether a person will be arrested or not. Because the response variable, arstmade, is a qualitative, binary variable, I will be using categorical methods. These include logistic regression, K-Nearest Neighbors analysis and tree methods (like bagging, random forests, boosting). Data The data comes from the City of New York. It is a collection of 22,564 observations of 112 variables. Some of these variables had to be removed, as they did not contribute any information (for instance, they were the same value for all observations) or they had too many missing values. After removing these variables from the dataset, I had 75 variables to work with, with 22,563 observations. For example, lineCM was a redundant variable, as it had 1's for all observations. I also removed apt number, state and zip, as many of these values were left blank. Overall, none of the variables I was very interested in had any missing values, so removing these variables likely had very little effect on the analysis, if any. There were two variables for height (ht_inch and ht_feet) which I combined into one (height). The response variable I used for all of the following analyses was arstmade, which was a categorical variable that had values of \"Y\" for yes and \"N\" for no. Many of the categorical variables in the dataset followed this pattern. The percentage of arrests in all cases was 0.18, meaning that around 18% of the stops resulted in an arrest. The mean age of people who were stopped and frisked was around 29 years. Most of the people who were stopped were males (92.42%). Force was used in about 27% of the cases. In investigating whether blacks were targeted more frequently and if they were arrested more frequently, I created a categorical variable that was 1 if the person was black and 0 otherwise. Below is a bar graph of the arrests, with the colorings indicating the percentage who were black and non-black. It does not seem from these graphs alone that blacks were more likely to be arrested, since they have roughly half of the bars in both instances. However, it does seem like there are a disproportionate number of blacks who are stopped and frisked in the first place, which is shown in the second graph. With blacks making up roughly 23% of the population, it is surprising that almost double that (53%) of the people who were stopped were black In [4]: ggplot ( mydata , aes ( arstmade )) + geom_bar ( aes ( fill = black )) + ggtitle ( \"Bar Graph of Arrests for Blacks vs. Non-Blacks\" ) + xlab ( \"Was the person arrested?\" ) + ylab ( \"Numer of Incidences\" ) + scale_fill_discrete ( name = \"Race\" , labels = c ( \"Non-Blacks\" , \"Blacks\" )) In [5]: ggplot ( mydata , aes ( black )) + geom_bar ( fill = \"red\" ) + ggtitle ( \"Blacks vs. Non-Blacks Who Were Stopped and Frisked\" ) + xlab ( \"Was the person black?\" ) + ylab ( \"Number of Incidents\" ) Some more interesting things found in the descriptive statistics: Around 6.7 times more people were arrested than given summonses. That is, only 2% of the people were given summonses. None of the people who were stopped had a machine gun, so I removed this variable from the data. I removed any variable which had levels that were exactly one, as this indicated that there was no variation between observations. Although the suspected crime variable (crimsusp) was of interest to me in my analysis, I found the data to be inconsistent in how it was recorded. For instance, if it was a felony, the officer had data entered that had both \"FEL\" and \"FELONY\" in two different cases. Of course, R would not be able to recognize the fact that these are the same, and would treat them as separate variables. As such, I decided to remove any variables which had levels larger than 10, as this would indicate that the data was too separated to be meaningful. Since much of this information is contained in the categorical variables (such as the rs variables, which had different values for different reasons for stopping the person), it was relatively inconsequential in my analysis to do so. There were also groupings which could raise doubt as to whether the information recorded was accurate. Of primary concern to me was pf_other (physical force used: other), rs_other (reason for search: other) and ac_other (additional circumstances: other). These are not very informative, and could be raise a speculative point about whether these were truly extraordinary cases or if they were omitted for other reasons. Of course, that is merely speculation, but it is important to note that this is a source of potential inconsistency and unreliability from the dataset. Even worse, some of these variables contributed a lot to the models which I have fit. Below shows a bar graph of the number of arrests conditional on whether the reason for search was classified as \"other.\" In [6]: ggplot ( mydata , aes ( sb_other )) + geom_bar ( aes ( fill = arstmade )) + ggtitle ( \"Arrests Conditional on if Reason for Search classified as OTHER\" ) + xlab ( \"Was the reason for search classifed as other?\" ) + ylab ( \"Number of Incidents\" ) + scale_fill_discrete ( name = \"Arrest Status\" , labels = ( c ( \"Not Arrested\" , \"Arrested\" ))) This confirms the results of the analysis below, as this variable has a large effect on whether the person was arrested or not. If the person was classified as being searched for other reasons not specified, they were arrested quite often. This leads to a limitation of the data and interpretation - we can not determine what the true reasons were. sb_other accounted for around 10.7% of all the cases, and 57.6% of all the people who were searched. Analysis and Model Fitting Logistic Regression I separated the data into two sets - test data and training data. The training data was randomly sampled from ¾ of the data. I began my analysis by using a logistic regression model to predict arstmade on all of the other variables. This was because the response variable was categorical, so it was necessary to use a logistic model instead of a linear model with normal errors. I then used stepwise reduction to determine the best fit and to reduce the amount of variables used. Since there were a great deal of variables that came up insignificant in the full model, it reduced the model quite a lot. I used the glm function using the binomial family to model the data. Below is a summary of the results of the models: In [11]: table1 arrest.test fit1 N Y 0 4513 397 1 126 590 In [12]: misclasserrorsfull table2 misclasserrorstepwise 0.0945609669392108 arrest.test fitted2 N Y 0 4508 401 1 131 586 0.0929612513330963 Using the stepwise regression model determined by the AIC produced a marginally better misclassification error, from .095 to .093 (or 9.5 % to 9.3%). This isn't a very big improvement, though. Surprisingly, the model does not find any of the race factors to be significant in determining whether an arrest was made or not besides Q, which stands for the White-Hispanics. That is, all of the levels of the \"race\" variable other than \"White-Hispanic\" came up insignificant. All of these coefficients are positive besides \"U\", which stood for \"unknown\", which essentially means that if the person's race was known, the odds of arrest are higher. The full summary of the stepwise regression is included in the appendix(1). When the stepwise regression was performed, we could see that contraband and knife/cutting instrument variables were highly significant in determining whether a person would get arrested or not. These variables are both very significant in the reduced logistic model. The variables that represent the different boroughs were all found to be significant, all with negative values. This suggests that the variable that was not included in the factors (the Bronx) had a higher likelihood of being arrested than any of the other four boroughs. Since the largest coefficient is that of Staten Island it means that, all else equal, a person from Staten Island who was stopped and frisked was less likely to be arrested than a person from Manhattan. The model also indicates that people who had a weapon were more likely to be arrested during a stop and frisk. This is logical, as if the police had found a weapon, it is quite likely that the person would be arrested. If the officer was in uniform, interestingly, a person would be less likely to be arrested, according to the model. If the police officer stopped the person because they knew them (indicated by rf_knowlY), the person was less likely to be arrested. If the reason the officer used force was to defend themselves, the person was less likely to be arrested than if the reason for force was that the person was suspected of running away. If the person is searched, they had a higher chance of being arrested than if they were not searched. KNN The next model I used was the K-Nearest Neighbors approach. K-Nearest Neighbors is a non-parametric method, so it does not make assumptions about the underlying distribution of the variables. K-Nearest Neighbors works by finding the nearest data points to the ones you are testing them against and assigning a classification based on the nearest k points. I have used k=5 and k=10 in this model to see if they can accurately predict the chances of arrest. Below are the tables for predicted vs actual arrest values for the test set, as well as the misclassification error, for the 10-nearest neighbor method (as this was the most successful.) In [15]: table3 round ( KNNMisclass , 3 ) arrest.test knn.predict N Y N 4479 749 Y 160 238 0.162 The 10-Nearest Neighbor model was the most successful of the different k's I used (I tried 1, 5, 10, and 20). Still, it doesn't produce very good outcomes compared to the logistic model. In fact, if I were to guess that none of the people would get arrested, I would have an error of about 18%, so this model is only marginally better than the trivial method. The failure of this method is due to the fact that the variables cannot exactly be scaled, and the distances are affected by this. As there are many categorical variables and few continuous ones, high numbers of the continuous variables may have influenced the predictions, since the KNN function in R uses Euclidian distances to predict the outcomes. To rectify this, I perform the K-NN method using only the categorical variables. The predictions are slightly better in this model, as shown by the confusion matrix and misclassification rate below. In [16]: table4 round ( KNNMisclass2 , 3 ) arrest.test knn.predict2 N Y N 4547 582 Y 92 405 0.12 Interestingly, while the error is still larger than the logistic models, or in fact any of the models I used in the analysis, it is more accurate at predicting an arrest when an arrest was made than any of the earlier models, suggested by the second row of the confusion matrix above. That is, while it produces more overall error than any of the models I've used, it also produces the smallest type I error out of any of the models (the false positives). Tree-Based Methods I will now use tree-based methods (a single tree, Random Forest, Bagging and Boosting) in order to predict whether a person will be arrested or not. The first model I use will be an unpruned tree using all the variables. The tree selects groupings of the variables which produces the least errors in predictions, and lists decision trees based on the conditional results of each tree. It determines these factors by a popular vote in the case of classification trees, so that the most common outcome is the one which is predicted by the tree. The tree is posted below. In [17]: tree . arrest <- tree ( arstmade ~ . , mydata [ train , ]) In [18]: plot ( tree . arrest , main = \"Unpruned Tree\" ) text ( tree . arrest , cex = 0.9 ) This tree has six nodes, so it is relatively simple. If the statement is true, you go to the right, and if it is false you go to the left. The top node is whether the person was searched or not. This makes sense, as if a person is searched, the police are more likely to find a reason to arrest the person (because of illegal goods, etc). It is already evident that this tree can be pruned. For the trhsloc and pf_hcuff variables, either decision results in the same response. This means that the variable is redundant, and can be removed from the tree. The values that are included are: searched, repcmd (reporting officers command, which takes values from 1 to 999), contrabn (contraband), sb_hdobj (basis of search being a hard object), trhsloc P,T (whether the location was a transit location or housing location), and pf_hcuff(force used was handcuffing). Some of these are quite interesting: the fact that the officers command has an influence on whether the person was arrested or not, and also the fact that regardless of whether the person was handcuffed or not, they were not reported as arrested if they reach the bottom left node. This defies our common sense - as we often expect someone who is handcuffed to be arrested. Based on the prior conditional factors, the tree says that if someone is handcuffed given they don't have contraband, they weren't searched, and the reporting officers command was less than 805, they would be classified as not being arrested. Since we already have seen that we should prune this tree, I will find the correct number of nodes to prune to by plotting the cross-validation errors. In [19]: cv . arrest <- cv . tree ( tree . arrest , FUN = prune . misclass ) In [20]: plot ( cv . arrest $ size , cv . arrest $ dev , type = \"b\" , xlab = \"Number of Nodes\" , ylab = \"Deviance\" , main = \"Determining Best Reduction of Trees by Deviance\" ) points ( 3 , cv . arrest $ dev [ 3 ], col = \"red\" ) There is a clear leveling off at 3, marked with a red point, so we will prune the next tree to three nodes. In [21]: prune . arrest <- prune . misclass ( tree . arrest , best = 3 ) In [22]: plot ( prune . arrest ) text ( prune . arrest ) This tree is extremely small, and only considers two variables - whether a person was searched and whether the reason for the search was that they had a hard object. Below, I post the tables for the pruned and unpruned trees, along with their misclassification errors. In [23]: tree . pred <- predict ( tree . arrest , newdata = mydata . test , type = \"class\" ) table5 <- table ( tree . pred , arrest . test ) misclassificationunprune <- 1 - sum ( diag ( table5 )) / nrow ( mydata . test ) pruned . pred <- predict ( prune . arrest , newdata = mydata . test , type = \"class\" ) table6 <- table ( pruned . pred , arrest . test ) misclassificationprune <- 1 - sum ( diag ( table6 )) / nrow ( mydata . test ) In [24]: table5 round ( misclassificationunprune , 3 ) table6 round ( misclassificationprune , 3 ) arrest.test tree.pred N Y N 4426 447 Y 213 540 0.117 arrest.test pruned.pred N Y N 4466 486 Y 173 501 0.117 The misclassification errors are quite high for the single trees, which is expected since one tree will rarely be sufficient in predicting the outcome. They perform roughly the same on the data, which indicates that the pruning was effective - we were able to reduce the nodes without significantly effecting the accuracy of the models. It is notable that the pruned tree reduced Type I error, which is more desirable in this case. Still, both trees performed better on the overall error than the KNN approach. Next, I will use the bagging method. The bagging method, or bootstrap aggregation, uses bootstrap samples repeatedly to create many trees, and then averages these trees to make predictions. In the case of a classification problem such as this one, bagging will predict by the majority vote. Performing bagging with 300 trees reports the following confusion matrix and misclassification error: In [25]: bag . arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 300 ) bag . pred <- predict ( bag . arrest , newdata = mydata . test , type = \"class\" ) table7 <- table ( bag . pred , arrest . test ) misclassificationbagging <- 1 - sum ( diag ( table7 )) / nrow ( mydata . test ) In [26]: table7 round ( misclassificationbagging , 3 ) arrest.test bag.pred N Y N 4505 285 Y 134 702 0.074 This is a large improvement over any of the previous methods we have used. In order to make the results more easily interpretable and prevent overfitting, I will reduce the number of trees used. Looking at the plot below can help us determine the correct number of trees to use : In [27]: plot ( bag . arrest , main = \"Trees vs Errors\" ) This plot determines the errors produced by each amount of trees. Using this, we can see visually where the line appears to stabilize and use this information to minimize the loss in accuracy from removing the trees. The green line shows the errors for the affirmative case (which is more likely), the red line for the negative case, and the black line for the overall error (the same as the misclassification error above). The plot appears to level off at around n=25. When I recreated the model using only 25 trees, I found the misclassification error to be 0.076. The error and confusion matrix will be included in the appendix. Although this is slightly higher than the model using 300 trees, it it a very small reduction in accuracy for a rather large increase in the interpretability and utility of the model. Now that we have determined a good amount of trees, we can look at an importance plot to see which variables create the most variations in the errors. Since this is a categorical variable, it will be most useful to use the right graph, which uses the Gini index. In [28]: bag . arrest2 <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 25 ) bag . pred2 <- predict ( bag . arrest2 , newdata = mydata . test , type = \"class\" ) table8 <- table ( bag . pred2 , arrest . test ) misclassificationbagging2 <- 1 - sum ( diag ( table8 )) / nrow ( mydata . test ) varImpPlot ( bag . arrest2 ) The variable with the most importance is sb_other, which is reason for search:other. This is a troubling thing, as described in the data section, because this is not a very informative variable. We can see that race now does have some importance in determining the arrests, as well as some other variables that were ommited from the logistic regression model. Interestingly, age, weight and height are included quite highly as well. Once again, whether the person was searched, had contraband, or was handcuffed is determined as important, which is consistent with our findings from the logistic regression. By using a sample of the variables in each tree and performing the bagging method on a different subset for each iteration, we can implement a Random Forest method. This method is useful since it allows some variation in the trees, since they will not only be dominated by the most important variables. Using two numbers for the number of variables used, I produced two random forest models. Below are their confusion matrices and misclassification errors. In [29]: table7 misclassificationbagging table8 misclassificationbagging2 arrest.test bag.pred N Y N 4505 285 Y 134 702 0.0744756487735514 arrest.test bag.pred2 N Y N 4490 277 Y 149 710 0.0757198720227515 The misclassification errors for the two are nearly identical. They perform very similarly to the bagging method. I would prefer the second model, as it predicts better in the case that the prediction and observed value are both yes than they are both no. By using a similar method to the bagging method earlier, we determine a good reduction in the amount of trees is to 40. Using 40 trees, we get the following confusion matrix and misclassification error. In [31]: table11 misclassRF3 arrest.test RF3.pred N Y N 4515 293 Y 124 694 0.0741201564166371 In this case, the error actually was reduced. This can be accounted for by the fact that it simplified the model, which resulted in the random forest model with 40 trees actually predicting better than the more complex model. This suggests that the increase in bias by using a simpler model with 40 trees was less than the decrease in variance of the model on the testing data. This model performs the best of all the models I have used, and is the most successful at predicting whether a person is arrested or not when they are stopped and frisked, with about a 7.2% misclassification rate. Conclusions Below is a table summarizing all of the models that I've used, and their misclassifcation errors on the test data. In [32]: Method <- c ( \"Full Logistic Model\" , \"Reduced Logistic Model\" , \"10-NN Full Model\" , \"5-NN Reduced Model\" , \"Pruned Tree\" , \"Unpruned Tree\" , \"Bagging (n=300)\" , \"Bagging (n=25)\" , \"Random Forest I (n=300)\" , \"Random Forest II (n=300)\" , \"Random Forest III (n=40)\" ) TestError <- c ( misclasserrorsfull , misclasserrorstepwise , KNNMisclass , KNNMisclass2 , misclassificationprune , misclassificationunprune , misclassificationbagging , misclassificationbagging2 , misclassRF , misclassRF2 , misclassRF3 ) TestError <- round ( TestError , 3 ) data . frame ( Method , TestError ) # error: Yes when No Er1 <- table1 [ 2 , 1 ] / sum ( table1 [ 2 , ]) Er2 <- table2 [ 2 , 1 ] / sum ( table2 [ 2 , ]) Er3 <- table3 [ 2 , 1 ] / sum ( table3 [ 2 , ]) Er4 <- table4 [ 2 , 1 ] / sum ( table4 [ 2 , ]) Er5 <- table5 [ 2 , 1 ] / sum ( table5 [ 2 , ]) Er6 <- table6 [ 2 , 1 ] / sum ( table6 [ 2 , ]) Er7 <- table7 [ 2 , 1 ] / sum ( table7 [ 2 , ]) Er8 <- table8 [ 2 , 1 ] / sum ( table8 [ 2 , ]) Er9 <- table9 [ 2 , 1 ] / sum ( table9 [ 2 , ]) Er10 <- table10 [ 2 , 1 ] / sum ( table10 [ 2 , ]) Er11 <- table11 [ 2 , 1 ] / sum ( table11 [ 2 , ]) # error: No when Yes Er21 <- table1 [ 1 , 2 ] / sum ( table1 [ 1 , ]) Er22 <- table2 [ 1 , 2 ] / sum ( table2 [ 1 , ]) Er23 <- table3 [ 1 , 2 ] / sum ( table3 [ 1 , ]) Er24 <- table4 [ 1 , 2 ] / sum ( table4 [ 1 , ]) Er25 <- table5 [ 1 , 2 ] / sum ( table5 [ 1 , ]) Er26 <- table6 [ 1 , 2 ] / sum ( table6 [ 1 , ]) Er27 <- table7 [ 1 , 2 ] / sum ( table7 [ 1 , ]) Er28 <- table8 [ 1 , 2 ] / sum ( table8 [ 1 , ]) Er29 <- table9 [ 1 , 2 ] / sum ( table9 [ 1 , ]) Er210 <- table10 [ 1 , 2 ] / sum ( table10 [ 1 , ]) Er211 <- table11 [ 1 , 2 ] / sum ( table11 [ 1 , ]) FalsePositive <- round ( c ( Er1 , Er2 , Er3 , Er4 , Er5 , Er6 , Er7 , Er8 , Er9 , Er10 , Er11 ), 3 ) FalseNegative <- round ( c ( Er21 , Er22 , Er23 , Er24 , Er25 , Er26 , Er27 , Er28 , Er29 , Er210 , Er211 ), 3 ) Method TestError Full Logistic Model 0.095 Reduced Logistic Model 0.093 10-NN Full Model 0.162 5-NN Reduced Model 0.120 Pruned Tree 0.117 Unpruned Tree 0.117 Bagging (n=300) 0.074 Bagging (n=25) 0.076 Random Forest I (n=300) 0.074 Random Forest II (n=300) 0.073 Random Forest III (n=40) 0.074 In [33]: data . frame ( Method , TestError , FalsePositive , FalseNegative ) Method TestError FalsePositive FalseNegative Full Logistic Model 0.095 0.176 0.081 Reduced Logistic Model 0.093 0.183 0.082 10-NN Full Model 0.162 0.402 0.143 5-NN Reduced Model 0.120 0.185 0.113 Pruned Tree 0.117 0.283 0.092 Unpruned Tree 0.117 0.257 0.098 Bagging (n=300) 0.074 0.160 0.059 Bagging (n=25) 0.076 0.173 0.058 Random Forest I (n=300) 0.074 0.137 0.064 Random Forest II (n=300) 0.073 0.146 0.060 Random Forest III (n=40) 0.074 0.152 0.061 I have also included the false positive and false negative results, as these can help to determine which models fit the best. The Random Forest model which used 40 trees and 39 variables performed the best out of all the methods used. It is clear that the bagging and random forest methods were superior to the logistic and KNN methods. Of the simpler methods, the reduced Logistic Model using stepwise reduction with the AIC performed the best. Despite my original inclinations, we can see from the importance plots on the bagging model and the summaries of the logistic model that race was not a very significant factor in determining whether a person would be arrested or not. However, as we saw before from the bar graphs, there was a significantly larger portion of blacks who were stopped and frisked than the general population. This suggests that a black person is more likely to be stopped, but not significantly more or less likely than other races to be arrested after they are stopped. Which raises the question: why stop more black people if they are no more likely to be arrested after the frisk than other races? Another variable of interest was sb_other. This variable indicates that there was an \"other reason for stopping the subject.\" As such, this variable is difficult to interpret. However, the variable was quite significant with a high coefficient in the logistic model, and was rated as the most important variable by a large margin in the bagging models. This is a point of difficulty in the interpretations. This may indicate that there needs to be more disclosure about the reason a person was searched, since the sb variables in their current state fail to capture much of the reasons for arrest. In both the logisitic and bagging methods, the variables for contraband and knife/cutting object came up as significant, and had a significant effect on whether the person was arrested or not. This is logical: if the officer found a weapon on contraband on the person, they were much more likely to be arrested. This was far more significant than other factors about the person, in particular the cs variables, which list the reasons why the person was stopped, or the age, weight, gender and race variables, which are the physical attributes of the person. This suggests that the physical attributes of a person are not nearly as important as criminal possession in predicting an arrest, which is in support of the stop and frisk's usage. For instance, being stopped due to the clothing you wear (cs_cloth) or being perceived as a lookout (cs_lkout) were not nearly as significant as carrying a suspicious object (cs_object) or if they were searched (searched), and actually had negative coefficients. So it appears that, most of the time, the probability of being arrested was largely reliant on whether the person was searched or not, or found to have an object. This matches up with the significance and postive coefficient of cs_drgtr, which was the variable representing the cause of search being for a suspected drug transaction. One would conclude that if a person was being suspected for a drug transaction, they would also be more likely to have contraband or perhaps a weapon than if they were not. Interestingly, the frisked variable was not found to be significant in the logistic regression model, suggesting that frisking, as compared to searching, was not very significant in arrest, and therefore not too effective a measure to stop criminal activity. In both models, searching had a stronger effect than frisking in determining whether a person was arrested or not. The success of the random forest/bagging methods as compared to the simpler methods suggests that the relationship between whether a person is arrested or not and the various variables in the dataset is quite complex, and that it cannot be estimated as well using the simpler techniques.","tags":"Projects","url":"https://seanammirati.github.io/category/projects/nypd_stop_frisk.html","loc":"https://seanammirati.github.io/category/projects/nypd_stop_frisk.html"},{"title":"NYPD Stop and Frisk -- Full Code","text":"The Results of Stop and Frisk: What Contributes Most to Arrests? Note: All of the code associated with this project, as well as the data used, is accessible at my GitHub repository here . This contains all of the code used in this project , unannotated. Introduction In this project I will be looking at a dataset available from the city of New York about the results of the Stop, Question and Frisk policy. Which factors lead to the highest likelihood of arrest? The data is a collection of 22,564 cases of Stop, Question and Frisk in the City of New York in 2015. There are 112 variables which were measured. A majority of the variables in the dataset are categorical variables relating to the incident itself, while a few represent the age, race, height, weight, and sex of the person who was on the receiving end of the policy. Stop and Frisk has been a controversial policy in New York City for quite some time. Many feel that it is a violation of privacy, that it is unconstitutional, or that the policy is inherently racist - that it disproportionately affects minorities and people of color. Meanwhile, there are others who support the policy, believing stop and frisk to keep people safe from terrorists and other wrongdoers. As a result of this and other recent controversies involving police across the country, the NYPD has been getting a lot of negative press over the way it addresses these racial problems. What I aim to discover by analyzing this dataset are the main contributing factors to whether a person is arrested or not in a stop and frisk incident. I will be using a few different methods of analysis to determine the best model to use in predicting whether a person will be arrested or not. Because the response variable, arstmade, is a qualitative, binary variable, I will be using categorical methods. These include logistic regression, K-Nearest Neighbors analysis and tree methods (like bagging, random forests, boosting). In [3]: set.seed ( 43 ) install.packages ( 'tree' ) require ( ggplot2 ) require ( tree ) require ( class ) require ( randomForest ) download_link <- \"http://www.nyc.gov/html/nypd/downloads/zip/analysis_and_planning/2015_sqf_csv.zip\" tmp <- tempfile () download.file ( download_link , tmp ) mydata <- read.csv ( unz ( tmp , \"2015_sqf_csv.csv\" )) unlink ( tmp ) sum ( is.na ( mydata )) nrow ( mydata ) * ncol ( mydata ) names ( mydata ) napercol <- list () for ( i in 1 : ncol ( mydata )) { napercol [[ i ]] <- sum ( is.na ( mydata [, i ])) } mydata <- mydata [, napercol == 0 ] lvllist <- list () for ( i in 1 : ncol ( mydata )) { lvllist [[ i ]] <- length ( levels ( mydata [, i ])) } attach ( mydata ) height <- 12 * ht_feet + ht_inch mydata <- data.frame ( mydata , height ) mydata <- mydata [, ( lvllist < 10 ) & ( lvllist != 1 )] mydata <- mydata [, - c ( 1 , 2 , 3 , 4 , 15 , 16 , 18 , 78 , 79 , 83 , 84 )] mydata <- na.omit ( mydata ) mydata <- mydata [ mydata $ age < 100 , ] train <- sample ( 1 : nrow ( mydata ), 3 * nrow ( mydata ) / 4 ) mydata.test <- mydata [ - train , ] arrest.test <- mydata $ arstmade [ - train ] attach ( mydata ) # descriptive sum ( arstmade == \"Y\" ) / nrow ( mydata ) sum ( sex == \"M\" ) / nrow ( mydata ) sum ( forceuse != \" \" ) / nrow ( mydata ) sum ( race == \"B\" ) / nrow ( mydata ) sum ( sumissue == \"Y\" ) / nrow ( mydata ) sum ( arstmade == \"Y\" ) / sum ( sumissue == \"Y\" ) sum ( sb_other == \"Y\" ) / nrow ( mydata ) sum ( sb_other == \"Y\" ) / sum ( searched == \"Y\" ) sum ( pf_other == \"Y\" ) / nrow ( mydata ) mean ( mydata $ age ) black <- as.factor ( race == \"B\" ) white <- as.factor ( race == \"W\" ) nonwhite <- as.factor ( race != \"W\" ) summary ( mydata ) Installing package into 'C:/Users/ginge/Anaconda3/Lib/R/library' (as 'lib' is unspecified) package 'tree' successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\\Users\\ginge\\AppData\\Local\\Temp\\RtmpMrc3nQ\\downloaded_packages Loading required package: ggplot2 Loading required package: tree Loading required package: class Loading required package: randomForest randomForest 4.6-12 Type rfNews() to see new features/changes/bug fixes. Attaching package: 'randomForest' The following object is masked from 'package:ggplot2': margin 173895 2527056 'year' 'pct' 'ser_num' 'datestop' 'timestop' 'recstat' 'inout' 'trhsloc' 'perobs' 'crimsusp' 'perstop' 'typeofid' 'explnstp' 'othpers' 'arstmade' 'arstoffn' 'sumissue' 'sumoffen' 'compyear' 'comppct' 'offunif' 'officrid' 'frisked' 'searched' 'contrabn' 'adtlrept' 'pistol' 'riflshot' 'asltweap' 'knifcuti' 'machgun' 'othrweap' 'pf_hands' 'pf_wall' 'pf_grnd' 'pf_drwep' 'pf_ptwep' 'pf_baton' 'pf_hcuff' 'pf_pepsp' 'pf_other' 'radio' 'ac_rept' 'ac_inves' 'rf_vcrim' 'rf_othsw' 'ac_proxm' 'rf_attir' 'cs_objcs' 'cs_descr' 'cs_casng' 'cs_lkout' 'rf_vcact' 'cs_cloth' 'cs_drgtr' 'ac_evasv' 'ac_assoc' 'cs_furtv' 'rf_rfcmp' 'ac_cgdir' 'rf_verbl' 'cs_vcrim' 'cs_bulge' 'cs_other' 'ac_incid' 'ac_time' 'rf_knowl' 'ac_stsnd' 'ac_other' 'sb_hdobj' 'sb_outln' 'sb_admis' 'sb_other' 'repcmd' 'revcmd' 'rf_furt' 'rf_bulg' 'offverb' 'offshld' 'forceuse' 'sex' 'race' 'dob' 'age' 'ht_feet' 'ht_inch' 'weight' 'haircolr' 'eyecolor' 'build' 'othfeatr' 'addrtyp' 'rescode' 'premtype' 'premname' 'addrnum' 'stname' 'stinter' 'crossst' 'aptnum' 'city' 'state' 'zip' 'addrpct' 'sector' 'beat' 'post' 'xcoord' 'ycoord' 'dettypCM' 'lineCM' 'detailCM' The following object is masked _by_ .GlobalEnv: height The following objects are masked from mydata (pos = 3): ac_assoc, ac_cgdir, ac_evasv, ac_incid, ac_inves, ac_other, ac_proxm, ac_rept, ac_stsnd, ac_time, age, arstmade, asltweap, build, city, contrabn, cs_bulge, cs_casng, cs_cloth, cs_descr, cs_drgtr, cs_furtv, cs_lkout, cs_objcs, cs_other, cs_vcrim, detailCM, explnstp, forceuse, frisked, inout, knifcuti, offshld, offunif, offverb, othpers, othrweap, perobs, pf_baton, pf_drwep, pf_grnd, pf_hands, pf_hcuff, pf_other, pf_pepsp, pf_ptwep, pf_wall, pistol, race, radio, recstat, repcmd, revcmd, rf_attir, rf_bulg, rf_furt, rf_knowl, rf_othsw, rf_rfcmp, rf_vcact, rf_vcrim, rf_verbl, riflshot, sb_admis, sb_hdobj, sb_other, sb_outln, searched, sex, sumissue, timestop, trhsloc, typeofid, weight 0.175984356946049 0.924317838414363 0.274686694516043 0.529419607146031 0.0261310105768376 6.73469387755102 0.107501555417296 0.577326968973747 0.026486534530264 27.6311883388143 timestop recstat inout trhsloc perobs typeofid Min. : 0 : 2845 I: 4211 H: 3357 Min. : 0.00 O: 432 1st Qu.: 507 1:14661 O:18291 P:18245 1st Qu.: 1.00 P:12960 Median :1623 9: 10 T: 900 Median : 1.00 R: 623 Mean :1377 A: 4986 Mean : 2.64 V: 8487 3rd Qu.:2052 3rd Qu.: 2.00 Max. :2359 Max. :535.00 explnstp othpers arstmade sumissue offunif frisked searched N: 28 N:15780 N:18542 N:21914 N: 9965 N: 7289 N:18312 Y:22474 Y: 6722 Y: 3960 Y: 588 Y:12537 Y:15213 Y: 4190 contrabn pistol riflshot asltweap knifcuti othrweap pf_hands N:21379 N:22337 N:22499 N:22501 N:21813 N:22228 N:18771 Y: 1123 Y: 165 Y: 3 Y: 1 Y: 689 Y: 274 Y: 3731 pf_wall pf_grnd pf_drwep pf_ptwep pf_baton pf_hcuff pf_pepsp N:21279 N:22191 N:22087 N:22234 N:22497 N:19111 N:22496 Y: 1223 Y: 311 Y: 415 Y: 268 Y: 5 Y: 3391 Y: 6 pf_other radio ac_rept ac_inves rf_vcrim rf_othsw ac_proxm N:21906 N:13142 N:15906 N:19561 N:17472 N:18858 N:14543 Y: 596 Y: 9360 Y: 6596 Y: 2941 Y: 5030 Y: 3644 Y: 7959 rf_attir cs_objcs cs_descr cs_casng cs_lkout rf_vcact cs_cloth N:20949 N:21580 N:14248 N:17837 N:20095 N:20834 N:21555 Y: 1553 Y: 922 Y: 8254 Y: 4665 Y: 2407 Y: 1668 Y: 947 cs_drgtr ac_evasv ac_assoc cs_furtv rf_rfcmp ac_cgdir rf_verbl N:21138 N:18194 N:20724 N:15766 N:19860 N:17541 N:22287 Y: 1364 Y: 4308 Y: 1778 Y: 6736 Y: 2642 Y: 4961 Y: 215 cs_vcrim cs_bulge cs_other ac_incid ac_time rf_knowl ac_stsnd N:20590 N:20804 N:14637 N:12148 N:15037 N:21242 N:21837 Y: 1912 Y: 1698 Y: 7865 Y:10354 Y: 7465 Y: 1260 Y: 665 ac_other sb_hdobj sb_outln sb_admis sb_other repcmd N:19947 N:21063 N:22204 N:22229 N:20083 Min. : 1 Y: 2555 Y: 1439 Y: 298 Y: 273 Y: 2419 1st Qu.: 66 Median :106 Mean :229 3rd Qu.:186 Max. :878 revcmd rf_furt rf_bulg offverb offshld forceuse sex Min. : 1.0 N:15913 N:20577 :15175 :12749 :16321 F: 1509 1st Qu.: 66.0 Y: 6589 Y: 1925 V: 7327 S: 9753 DO: 98 M:20799 Median :106.0 DS: 2145 Z: 194 Mean :229.4 OR: 284 3rd Qu.:186.0 OT: 1687 Max. :878.0 SF: 1461 SW: 506 race age weight build city B :11913 Min. : 0.00 Min. : 1.0 H: 2159 BRONX :4740 Q : 5082 1st Qu.:19.00 1st Qu.:150.0 M:10958 BROOKLYN :6334 W : 2505 Median :24.00 Median :170.0 T: 8861 MANHATTAN:3923 P : 1407 Mean :27.63 Mean :171.3 U: 191 QUEENS :5710 A : 1100 3rd Qu.:33.00 3rd Qu.:185.0 Z: 333 STATEN IS:1795 Z : 298 Max. :99.00 Max. :999.0 (Other): 197 detailCM height Min. : 6.00 Min. :36.00 1st Qu.: 20.00 1st Qu.:67.00 Median : 27.00 Median :69.00 Mean : 37.79 Mean :68.83 3rd Qu.: 46.00 3rd Qu.:71.00 Max. :113.00 Max. :95.00 Data The data comes from the City of New York. It is a collection of 22,564 observations of 112 variables. Some of these variables had to be removed, as they did not contribute any information (for instance, they were the same value for all observations) or they had too many missing values. After removing these variables from the dataset, I had 75 variables to work with, with 22,563 observations. For example, lineCM was a redundant variable, as it had 1's for all observations. I also removed apt number, state and zip, as many of these values were left blank. Overall, none of the variables I was very interested in had any missing values, so removing these variables likely had very little effect on the analysis, if any. There were two variables for height (ht_inch and ht_feet) which I combined into one (height). The response variable I used for all of the following analyses was arstmade, which was a categorical variable that had values of \"Y\" for yes and \"N\" for no. Many of the categorical variables in the dataset followed this pattern. The percentage of arrests in all cases was 0.18, meaning that around 18% of the stops resulted in an arrest. The mean age of people who were stopped and frisked was around 29 years. Most of the people who were stopped were males (92.42%). Force was used in about 27% of the cases. In investigating whether blacks were targeted more frequently and if they were arrested more frequently, I created a categorical variable that was 1 if the person was black and 0 otherwise. Below is a bar graph of the arrests, with the colorings indicating the percentage who were black and non-black. It does not seem from these graphs alone that blacks were more likely to be arrested, since they have roughly half of the bars in both instances. However, it does seem like there are a disproportionate number of blacks who are stopped and frisked in the first place, which is shown in the second graph. With blacks making up roughly 23% of the population, it is surprising that almost double that (53%) of the people who were stopped were black In [4]: ggplot ( mydata , aes ( arstmade )) + geom_bar ( aes ( fill = black )) + ggtitle ( \"Bar Graph of Arrests for Blacks vs. Non-Blacks\" ) + xlab ( \"Was the person arrested?\" ) + ylab ( \"Numer of Incidences\" ) + scale_fill_discrete ( name = \"Race\" , labels = c ( \"Non-Blacks\" , \"Blacks\" )) In [5]: ggplot ( mydata , aes ( black )) + geom_bar ( fill = \"red\" ) + ggtitle ( \"Blacks vs. Non-Blacks Who Were Stopped and Frisked\" ) + xlab ( \"Was the person black?\" ) + ylab ( \"Number of Incidents\" ) Some more interesting things found in the descriptive statistics: Around 6.7 times more people were arrested than given summonses. That is, only 2% of the people were given summonses. None of the people who were stopped had a machine gun, so I removed this variable from the data. I removed any variable which had levels that were exactly one, as this indicated that there was no variation between observations. Although the suspected crime variable (crimsusp) was of interest to me in my analysis, I found the data to be inconsistent in how it was recorded. For instance, if it was a felony, the officer had data entered that had both \"FEL\" and \"FELONY\" in two different cases. Of course, R would not be able to recognize the fact that these are the same, and would treat them as separate variables. As such, I decided to remove any variables which had levels larger than 10, as this would indicate that the data was too separated to be meaningful. Since much of this information is contained in the categorical variables (such as the rs variables, which had different values for different reasons for stopping the person), it was relatively inconsequential in my analysis to do so. There were also groupings which could raise doubt as to whether the information recorded was accurate. Of primary concern to me was pf_other (physical force used: other), rs_other (reason for search: other) and ac_other (additional circumstances: other). These are not very informative, and could be raise a speculative point about whether these were truly extraordinary cases or if they were omitted for other reasons. Of course, that is merely speculation, but it is important to note that this is a source of potential inconsistency and unreliability from the dataset. Even worse, some of these variables contributed a lot to the models which I have fit. Below shows a bar graph of the number of arrests conditional on whether the reason for search was classified as \"other.\" In [6]: ggplot ( mydata , aes ( sb_other )) + geom_bar ( aes ( fill = arstmade )) + ggtitle ( \"Arrests Conditional on if Reason for Search classified as OTHER\" ) + xlab ( \"Was the reason for search classifed as other?\" ) + ylab ( \"Number of Incidents\" ) + scale_fill_discrete ( name = \"Arrest Status\" , labels = ( c ( \"Not Arrested\" , \"Arrested\" ))) This confirms the results of the analysis below, as this variable has a large effect on whether the person was arrested or not. If the person was classified as being searched for other reasons not specified, they were arrested quite often. This leads to a limitation of the data and interpretation - we can not determine what the true reasons were. sb_other accounted for around 10.7% of all the cases, and 57.6% of all the people who were searched. Analysis and Model Fitting Logistic Regression I separated the data into two sets - test data and training data. The training data was randomly sampled from ¾ of the data. I began my analysis by using a logistic regression model to predict arstmade on all of the other variables. This was because the response variable was categorical, so it was necessary to use a logistic model instead of a linear model with normal errors. I then used stepwise reduction to determine the best fit and to reduce the amount of variables used. Since there were a great deal of variables that came up insignificant in the full model, it reduced the model quite a lot. I used the glm function using the binomial family to model the data. Below is a summary of the results of the models: In [7]: logistfullmodel <- glm ( arstmade ~ . , data = mydata [ train , ], family = binomial ) summary ( logistfullmodel ) # step(logistfullmodel) stepwiselogmodel <- glm ( formula = arstmade ~ recstat + inout + trhsloc + perobs + typeofid + sumissue + offunif + frisked + searched + contrabn + pistol + knifcuti + othrweap + pf_hands + pf_wall + pf_grnd + pf_drwep + pf_hcuff + pf_pepsp + pf_other + radio + ac_rept + rf_vcrim + rf_othsw + ac_proxm + rf_attir + cs_objcs + cs_casng + cs_lkout + cs_cloth + cs_drgtr + ac_evasv + ac_assoc + rf_rfcmp + rf_verbl + cs_vcrim + cs_bulge + cs_other + rf_knowl + ac_other + sb_hdobj + sb_other + revcmd + rf_furt + rf_bulg + forceuse + sex + race + age + weight + city + detailCM , family = binomial , data = mydata [ train , ]) summary ( stepwiselogmodel ) Call: glm(formula = arstmade ~ ., family = binomial, data = mydata[train, ]) Deviance Residuals: Min 1Q Median 3Q Max -3.7188 -0.3688 -0.2544 -0.1509 3.8891 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -3.603e-01 1.098e+00 -0.328 0.742736 timestop -5.247e-05 3.830e-05 -1.370 0.170732 recstat1 3.513e-01 1.029e-01 3.414 0.000640 *** recstat9 -1.984e+00 2.315e+00 -0.857 0.391350 recstatA -1.814e-01 1.186e-01 -1.530 0.126081 inoutO -8.448e-01 8.111e-02 -10.415 < 2e-16 *** trhslocP -8.829e-01 1.264e-01 -6.987 2.81e-12 *** trhslocT -1.433e+00 1.497e-01 -9.576 < 2e-16 *** perobs 8.119e-03 3.385e-03 2.398 0.016477 * typeofidP 4.639e-02 2.062e-01 0.225 0.822032 typeofidR -1.221e+00 3.381e-01 -3.612 0.000303 *** typeofidV -1.538e-01 2.087e-01 -0.737 0.461056 explnstpY -5.317e-01 7.951e-01 -0.669 0.503720 othpersY 8.174e-02 7.156e-02 1.142 0.253323 sumissueY -2.921e+00 2.354e-01 -12.411 < 2e-16 *** offunifY -2.842e-01 3.264e-01 -0.871 0.383957 friskedY 1.868e-01 1.029e-01 1.816 0.069326 . searchedY 1.743e+00 2.208e-01 7.893 2.95e-15 *** contrabnY 3.129e+00 1.397e-01 22.406 < 2e-16 *** pistolY 2.889e+00 3.064e-01 9.428 < 2e-16 *** riflshotY -9.084e+00 1.970e+02 -0.046 0.963216 asltweapY 9.640e+00 1.970e+02 0.049 0.960966 knifcutiY 2.090e+00 1.459e-01 14.327 < 2e-16 *** othrweapY 1.924e+00 2.089e-01 9.209 < 2e-16 *** pf_handsY 1.730e-01 9.642e-02 1.794 0.072817 . pf_wallY -2.660e-01 1.438e-01 -1.850 0.064294 . pf_grndY 7.378e-01 2.149e-01 3.433 0.000597 *** pf_drwepY -6.845e-01 2.622e-01 -2.610 0.009050 ** pf_ptwepY -1.893e-01 2.868e-01 -0.660 0.509344 pf_batonY 2.162e+00 1.520e+00 1.422 0.155040 pf_hcuffY 1.308e+00 9.645e-02 13.564 < 2e-16 *** pf_pepspY 3.710e+00 1.173e+00 3.162 0.001568 ** pf_otherY -2.753e-01 1.954e-01 -1.409 0.158894 radioY -4.752e-01 6.943e-02 -6.844 7.70e-12 *** ac_reptY 2.874e-01 7.863e-02 3.655 0.000257 *** ac_invesY 5.950e-02 9.443e-02 0.630 0.528644 rf_vcrimY -2.614e-01 9.061e-02 -2.885 0.003912 ** rf_othswY -2.411e-01 9.555e-02 -2.523 0.011633 * ac_proxmY 1.581e-01 6.608e-02 2.393 0.016729 * rf_attirY -3.667e-01 1.361e-01 -2.695 0.007042 ** cs_objcsY 5.588e-01 1.324e-01 4.219 2.45e-05 *** cs_descrY 1.136e-01 8.865e-02 1.281 0.200124 cs_casngY -1.454e-01 9.321e-02 -1.560 0.118861 cs_lkoutY -4.338e-01 1.205e-01 -3.599 0.000319 *** rf_vcactY -8.072e-02 1.335e-01 -0.605 0.545450 cs_clothY -3.673e-01 1.809e-01 -2.031 0.042294 * cs_drgtrY 3.498e-01 1.206e-01 2.901 0.003719 ** ac_evasvY 2.235e-01 8.377e-02 2.668 0.007640 ** ac_assocY -5.466e-01 1.345e-01 -4.063 4.84e-05 *** cs_furtvY -2.918e-02 8.263e-02 -0.353 0.723996 rf_rfcmpY -2.335e-01 1.029e-01 -2.270 0.023184 * ac_cgdirY 1.199e-01 8.096e-02 1.481 0.138624 rf_verblY -3.992e-01 2.927e-01 -1.364 0.172692 cs_vcrimY -3.357e-01 1.384e-01 -2.426 0.015256 * cs_bulgeY -4.862e-01 1.482e-01 -3.281 0.001033 ** cs_otherY -1.467e-01 7.321e-02 -2.004 0.045122 * ac_incidY 4.553e-02 7.148e-02 0.637 0.524196 ac_timeY -4.144e-02 7.612e-02 -0.544 0.586213 rf_knowlY -4.205e-01 1.562e-01 -2.692 0.007095 ** ac_stsndY -2.085e-01 1.926e-01 -1.083 0.278930 ac_otherY -3.115e-01 9.911e-02 -3.143 0.001673 ** sb_hdobjY -6.270e-01 2.079e-01 -3.016 0.002558 ** sb_outlnY 2.030e-01 2.446e-01 0.830 0.406604 sb_admisY 1.097e-01 2.662e-01 0.412 0.680126 sb_otherY 1.280e+00 2.151e-01 5.951 2.66e-09 *** repcmd -3.224e-04 6.206e-04 -0.520 0.603369 revcmd 8.321e-04 6.175e-04 1.348 0.177806 rf_furtY -1.473e-01 9.015e-02 -1.634 0.102289 rf_bulgY -3.050e-01 1.374e-01 -2.220 0.026416 * offverbV -2.356e-02 1.051e-01 -0.224 0.822675 offshldS -3.050e-02 3.115e-01 -0.098 0.922004 forceuseDO -7.150e-01 4.305e-01 -1.661 0.096764 . forceuseDS -6.897e-01 1.305e-01 -5.286 1.25e-07 *** forceuseOR -8.379e-02 2.432e-01 -0.345 0.730402 forceuseOT 3.347e-01 1.257e-01 2.664 0.007725 ** forceuseSF -5.223e-02 1.308e-01 -0.399 0.689594 forceuseSW -5.030e-01 2.170e-01 -2.318 0.020476 * sexM -2.052e-01 1.171e-01 -1.752 0.079809 . sexZ -7.004e-01 4.190e-01 -1.671 0.094626 . raceB 3.875e-02 1.589e-01 0.244 0.807283 raceI 8.685e-02 5.556e-01 0.156 0.875776 raceP 2.571e-01 1.889e-01 1.361 0.173603 raceQ 3.531e-01 1.623e-01 2.176 0.029566 * raceU 2.086e-01 4.130e-01 0.505 0.613431 raceW 1.073e-01 1.809e-01 0.593 0.552966 raceZ 5.422e-01 3.251e-01 1.668 0.095401 . age 8.302e-03 2.664e-03 3.117 0.001830 ** weight -1.064e-03 1.022e-03 -1.041 0.297954 buildM -4.270e-02 1.113e-01 -0.384 0.701347 buildT -1.117e-02 1.210e-01 -0.092 0.926426 buildU -1.004e-01 3.264e-01 -0.308 0.758299 buildZ -7.310e-01 2.973e-01 -2.459 0.013934 * cityBROOKLYN -8.185e-01 8.534e-02 -9.592 < 2e-16 *** cityMANHATTAN -6.850e-01 9.098e-02 -7.529 5.10e-14 *** cityQUEENS -5.335e-01 9.414e-02 -5.667 1.45e-08 *** citySTATEN IS -1.102e+00 1.562e-01 -7.053 1.75e-12 *** detailCM 8.680e-03 1.159e-03 7.492 6.78e-14 *** height -2.539e-03 1.019e-02 -0.249 0.803203 --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 15712.6 on 16875 degrees of freedom Residual deviance: 8277.2 on 16778 degrees of freedom AIC: 8473.2 Number of Fisher Scoring iterations: 10 Call: glm(formula = arstmade ~ recstat + inout + trhsloc + perobs + typeofid + sumissue + offunif + frisked + searched + contrabn + pistol + knifcuti + othrweap + pf_hands + pf_wall + pf_grnd + pf_drwep + pf_hcuff + pf_pepsp + pf_other + radio + ac_rept + rf_vcrim + rf_othsw + ac_proxm + rf_attir + cs_objcs + cs_casng + cs_lkout + cs_cloth + cs_drgtr + ac_evasv + ac_assoc + rf_rfcmp + rf_verbl + cs_vcrim + cs_bulge + cs_other + rf_knowl + ac_other + sb_hdobj + sb_other + revcmd + rf_furt + rf_bulg + forceuse + sex + race + age + weight + city + detailCM, family = binomial, data = mydata[train, ]) Deviance Residuals: Min 1Q Median 3Q Max -3.7416 -0.3693 -0.2555 -0.1531 3.8643 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.1394455 0.3563078 -3.198 0.001384 ** recstat1 0.3575529 0.1023959 3.492 0.000480 *** recstat9 -2.0037225 2.3393201 -0.857 0.391699 recstatA -0.1811463 0.1180715 -1.534 0.124978 inoutO -0.8198646 0.0803864 -10.199 < 2e-16 *** trhslocP -0.8738981 0.1249270 -6.995 2.65e-12 *** trhslocT -1.4117556 0.1472958 -9.584 < 2e-16 *** perobs 0.0085851 0.0033665 2.550 0.010767 * typeofidP 0.0553679 0.2048434 0.270 0.786934 typeofidR -1.2301359 0.3365018 -3.656 0.000257 *** typeofidV -0.1420737 0.2072662 -0.685 0.493051 sumissueY -2.9274054 0.2337174 -12.525 < 2e-16 *** offunifY -0.2300162 0.0709762 -3.241 0.001192 ** friskedY 0.1596321 0.0999233 1.598 0.110144 searchedY 1.8727029 0.1564972 11.966 < 2e-16 *** contrabnY 3.1108031 0.1387840 22.415 < 2e-16 *** pistolY 2.9397144 0.3037615 9.678 < 2e-16 *** knifcutiY 2.0781429 0.1408105 14.758 < 2e-16 *** othrweapY 1.9085458 0.2061663 9.257 < 2e-16 *** pf_handsY 0.1829775 0.0958719 1.909 0.056318 . pf_wallY -0.2684473 0.1426597 -1.882 0.059872 . pf_grndY 0.7069531 0.2111336 3.348 0.000813 *** pf_drwepY -0.7202203 0.2427153 -2.967 0.003004 ** pf_hcuffY 1.3038209 0.0958149 13.608 < 2e-16 *** pf_pepspY 3.6381377 1.1662366 3.120 0.001811 ** pf_otherY -0.2736084 0.1936878 -1.413 0.157766 radioY -0.4540335 0.0686508 -6.614 3.75e-11 *** ac_reptY 0.3203196 0.0716227 4.472 7.74e-06 *** rf_vcrimY -0.2302783 0.0885105 -2.602 0.009276 ** rf_othswY -0.2195720 0.0938505 -2.340 0.019305 * ac_proxmY 0.1764237 0.0639188 2.760 0.005778 ** rf_attirY -0.3508092 0.1347952 -2.603 0.009254 ** cs_objcsY 0.5277556 0.1306460 4.040 5.35e-05 *** cs_casngY -0.1567943 0.0897938 -1.746 0.080783 . cs_lkoutY -0.4306545 0.1190220 -3.618 0.000297 *** cs_clothY -0.3454175 0.1787932 -1.932 0.053367 . cs_drgtrY 0.3362326 0.1183804 2.840 0.004507 ** ac_evasvY 0.2346666 0.0809086 2.900 0.003727 ** ac_assocY -0.5555065 0.1340808 -4.143 3.43e-05 *** rf_rfcmpY -0.2040480 0.1014848 -2.011 0.044365 * rf_verblY -0.4083410 0.2922657 -1.397 0.162366 cs_vcrimY -0.3965895 0.1264841 -3.135 0.001716 ** cs_bulgeY -0.4881904 0.1440797 -3.388 0.000703 *** cs_otherY -0.1770344 0.0673632 -2.628 0.008587 ** rf_knowlY -0.4071034 0.1555024 -2.618 0.008845 ** ac_otherY -0.3129862 0.0982278 -3.186 0.001441 ** sb_hdobjY -0.7305307 0.1589738 -4.595 4.32e-06 *** sb_otherY 1.1601221 0.1583057 7.328 2.33e-13 *** revcmd 0.0004988 0.0001588 3.142 0.001681 ** rf_furtY -0.1420974 0.0805927 -1.763 0.077874 . rf_bulgY -0.2908946 0.1356321 -2.145 0.031974 * forceuseDO -0.7594367 0.4329658 -1.754 0.079425 . forceuseDS -0.6950447 0.1298365 -5.353 8.64e-08 *** forceuseOR -0.0864302 0.2400623 -0.360 0.718823 forceuseOT 0.3274358 0.1251247 2.617 0.008874 ** forceuseSF -0.0552919 0.1300915 -0.425 0.670820 forceuseSW -0.4997659 0.2145224 -2.330 0.019824 * sexM -0.2124173 0.1120586 -1.896 0.058014 . sexZ -0.6879402 0.4163837 -1.652 0.098498 . raceB 0.0408446 0.1577469 0.259 0.795693 raceI 0.0878653 0.5519119 0.159 0.873510 raceP 0.2519675 0.1881200 1.339 0.180441 raceQ 0.3582209 0.1613654 2.220 0.026423 * raceU 0.2148683 0.4095903 0.525 0.599866 raceW 0.1083503 0.1798309 0.603 0.546833 raceZ 0.5206662 0.3238037 1.608 0.107842 age 0.0073152 0.0026085 2.804 0.005042 ** weight -0.0012546 0.0008145 -1.540 0.123469 cityBROOKLYN -0.8072589 0.0847163 -9.529 < 2e-16 *** cityMANHATTAN -0.6764624 0.0904685 -7.477 7.58e-14 *** cityQUEENS -0.5136538 0.0932366 -5.509 3.61e-08 *** citySTATEN IS -1.0791310 0.1548114 -6.971 3.16e-12 *** detailCM 0.0090240 0.0011467 7.870 3.56e-15 *** --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 15712.6 on 16875 degrees of freedom Residual deviance: 8297.6 on 16803 degrees of freedom AIC: 8443.6 Number of Fisher Scoring iterations: 6 In [8]: plot ( stepwiselogmodel ) In [9]: fit1 <- predict ( stepwiselogmodel , newdata = mydata.test , type = \"response\" ) fit1 <- ifelse ( fit1 > 0.5 , 1 , 0 ) sum ( fit1 ) / nrow ( mydata.test ) table1 <- table ( fit1 , arrest.test ) misclasserrorstepwise <- 1 - sum ( diag ( table1 )) / nrow ( mydata.test ) round ( misclasserrorstepwise , 3 ) fitted3 <- ifelse ( fit1 > 0.17 , 1 , 0 ) sum ( fitted3 ) / nrow ( mydata.test ) fit2 <- predict ( logistfullmodel , newdata = mydata.test , type = \"response\" ) fitted2 <- ifelse ( fit2 > 0.5 , 1 , 0 ) sum ( fitted2 ) / nrow ( mydata.test ) table2 <- table ( fitted2 , arrest.test ) misclasserrorsfull <- 1 - sum ( diag ( table2 )) / nrow ( mydata.test ) round ( misclasserrorsfull , 3 ) 0.127266263775329 0.093 0.127266263775329 0.127444009953786 0.095 In [11]: table1 arrest.test fit1 N Y 0 4513 397 1 126 590 In [12]: misclasserrorsfull table2 misclasserrorstepwise 0.0945609669392108 arrest.test fitted2 N Y 0 4508 401 1 131 586 0.0929612513330963 Using the stepwise regression model determined by the AIC produced a marginally better misclassification error, from .095 to .093 (or 9.5 % to 9.3%). This isn't a very big improvement, though. Surprisingly, the model does not find any of the race factors to be significant in determining whether an arrest was made or not besides Q, which stands for the White-Hispanics. That is, all of the levels of the \"race\" variable other than \"White-Hispanic\" came up insignificant. All of these coefficients are positive besides \"U\", which stood for \"unknown\", which essentially means that if the person's race was known, the odds of arrest are higher. The full summary of the stepwise regression is included in the appendix(1). When the stepwise regression was performed, we could see that contraband and knife/cutting instrument variables were highly significant in determining whether a person would get arrested or not. These variables are both very significant in the reduced logistic model. The variables that represent the different boroughs were all found to be significant, all with negative values. This suggests that the variable that was not included in the factors (the Bronx) had a higher likelihood of being arrested than any of the other four boroughs. Since the largest coefficient is that of Staten Island it means that, all else equal, a person from Staten Island who was stopped and frisked was less likely to be arrested than a person from Manhattan. The model also indicates that people who had a weapon were more likely to be arrested during a stop and frisk. This is logical, as if the police had found a weapon, it is quite likely that the person would be arrested. If the officer was in uniform, interestingly, a person would be less likely to be arrested, according to the model. If the police officer stopped the person because they knew them (indicated by rf_knowlY), the person was less likely to be arrested. If the reason the officer used force was to defend themselves, the person was less likely to be arrested than if the reason for force was that the person was suspected of running away. If the person is searched, they had a higher chance of being arrested than if they were not searched. KNN The next model I used was the K-Nearest Neighbors approach. K-Nearest Neighbors is a non-parametric method, so it does not make assumptions about the underlying distribution of the variables. K-Nearest Neighbors works by finding the nearest data points to the ones you are testing them against and assigning a classification based on the nearest k points. I have used k=5 and k=10 in this model to see if they can accurately predict the chances of arrest. Below are the tables for predicted vs actual arrest values for the test set, as well as the misclassification error, for the 10-nearest neighbor method (as this was the most successful.) In [14]: train1 <- model.matrix ( arstmade ~ . , data = mydata [ train , ]) test1 <- model.matrix ( arstmade ~ . , data = mydata.test ) knn.predict <- knn ( train1 , test1 , arstmade [ train ], k = 10 ) table3 <- table ( knn.predict , arrest.test ) KNNMisclass <- 1 - sum ( diag ( table3 )) / nrow ( mydata.test ) round ( KNNMisclass , 3 ) train2 <- model.matrix ( arstmade ~ . , data = mydata [ train , ])[, 7 : 60 ] test2 <- model.matrix ( arstmade ~ . , data = mydata.test )[, 7 : 60 ] knn.predict2 <- knn ( train2 , test2 , arstmade [ train ], k = 5 ) table4 <- table ( knn.predict2 , arrest.test ) KNNMisclass2 <- 1 - sum ( diag ( table4 )) / nrow ( mydata.test ) round ( KNNMisclass2 , 3 ) 0.162 0.12 In [15]: table3 round ( KNNMisclass , 3 ) arrest.test knn.predict N Y N 4479 749 Y 160 238 0.162 The 10-Nearest Neighbor model was the most successful of the different k's I used (I tried 1, 5, 10, and 20). Still, it doesn't produce very good outcomes compared to the logistic model. In fact, if I were to guess that none of the people would get arrested, I would have an error of about 18%, so this model is only marginally better than the trivial method. The failure of this method is due to the fact that the variables cannot exactly be scaled, and the distances are affected by this. As there are many categorical variables and few continuous ones, high numbers of the continuous variables may have influenced the predictions, since the KNN function in R uses Euclidian distances to predict the outcomes. To rectify this, I perform the K-NN method using only the categorical variables. The predictions are slightly better in this model, as shown by the confusion matrix and misclassification rate below. In [16]: table4 round ( KNNMisclass2 , 3 ) arrest.test knn.predict2 N Y N 4547 582 Y 92 405 0.12 Interestingly, while the error is still larger than the logistic models, or in fact any of the models I used in the analysis, it is more accurate at predicting an arrest when an arrest was made than any of the earlier models, suggested by the second row of the confusion matrix above. That is, while it produces more overall error than any of the models I've used, it also produces the smallest type I error out of any of the models (the false positives). Tree-Based Methods I will now use tree-based methods (a single tree, Random Forest, Bagging and Boosting) in order to predict whether a person will be arrested or not. The first model I use will be an unpruned tree using all the variables. The tree selects groupings of the variables which produces the least errors in predictions, and lists decision trees based on the conditional results of each tree. It determines these factors by a popular vote in the case of classification trees, so that the most common outcome is the one which is predicted by the tree. The tree is posted below. In [17]: tree.arrest <- tree ( arstmade ~ . , mydata [ train , ]) In [18]: plot ( tree.arrest , main = \"Unpruned Tree\" ) text ( tree.arrest , cex = 0.9 ) This tree has six nodes, so it is relatively simple. If the statement is true, you go to the right, and if it is false you go to the left. The top node is whether the person was searched or not. This makes sense, as if a person is searched, the police are more likely to find a reason to arrest the person (because of illegal goods, etc). It is already evident that this tree can be pruned. For the trhsloc and pf_hcuff variables, either decision results in the same response. This means that the variable is redundant, and can be removed from the tree. The values that are included are: searched, repcmd (reporting officers command, which takes values from 1 to 999), contrabn (contraband), sb_hdobj (basis of search being a hard object), trhsloc P,T (whether the location was a transit location or housing location), and pf_hcuff(force used was handcuffing). Some of these are quite interesting: the fact that the officers command has an influence on whether the person was arrested or not, and also the fact that regardless of whether the person was handcuffed or not, they were not reported as arrested if they reach the bottom left node. This defies our common sense - as we often expect someone who is handcuffed to be arrested. Based on the prior conditional factors, the tree says that if someone is handcuffed given they don't have contraband, they weren't searched, and the reporting officers command was less than 805, they would be classified as not being arrested. Since we already have seen that we should prune this tree, I will find the correct number of nodes to prune to by plotting the cross-validation errors. In [19]: cv.arrest <- cv.tree ( tree.arrest , FUN = prune.misclass ) In [20]: plot ( cv.arrest $ size , cv.arrest $ dev , type = \"b\" , xlab = \"Number of Nodes\" , ylab = \"Deviance\" , main = \"Determining Best Reduction of Trees by Deviance\" ) points ( 3 , cv.arrest $ dev [ 3 ], col = \"red\" ) There is a clear leveling off at 3, marked with a red point, so we will prune the next tree to three nodes. In [21]: prune.arrest <- prune.misclass ( tree.arrest , best = 3 ) In [22]: plot ( prune.arrest ) text ( prune.arrest ) This tree is extremely small, and only considers two variables - whether a person was searched and whether the reason for the search was that they had a hard object. Below, I post the tables for the pruned and unpruned trees, along with their misclassification errors. In [23]: tree.pred <- predict ( tree.arrest , newdata = mydata.test , type = \"class\" ) table5 <- table ( tree.pred , arrest.test ) misclassificationunprune <- 1 - sum ( diag ( table5 )) / nrow ( mydata.test ) pruned.pred <- predict ( prune.arrest , newdata = mydata.test , type = \"class\" ) table6 <- table ( pruned.pred , arrest.test ) misclassificationprune <- 1 - sum ( diag ( table6 )) / nrow ( mydata.test ) In [24]: table5 round ( misclassificationunprune , 3 ) table6 round ( misclassificationprune , 3 ) arrest.test tree.pred N Y N 4426 447 Y 213 540 0.117 arrest.test pruned.pred N Y N 4466 486 Y 173 501 0.117 The misclassification errors are quite high for the single trees, which is expected since one tree will rarely be sufficient in predicting the outcome. They perform roughly the same on the data, which indicates that the pruning was effective - we were able to reduce the nodes without significantly effecting the accuracy of the models. It is notable that the pruned tree reduced Type I error, which is more desirable in this case. Still, both trees performed better on the overall error than the KNN approach. Next, I will use the bagging method. The bagging method, or bootstrap aggregation, uses bootstrap samples repeatedly to create many trees, and then averages these trees to make predictions. In the case of a classification problem such as this one, bagging will predict by the majority vote. Performing bagging with 300 trees reports the following confusion matrix and misclassification error: In [25]: bag.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 300 ) bag.pred <- predict ( bag.arrest , newdata = mydata.test , type = \"class\" ) table7 <- table ( bag.pred , arrest.test ) misclassificationbagging <- 1 - sum ( diag ( table7 )) / nrow ( mydata.test ) In [26]: table7 round ( misclassificationbagging , 3 ) arrest.test bag.pred N Y N 4505 285 Y 134 702 0.074 This is a large improvement over any of the previous methods we have used. In order to make the results more easily interpretable and prevent overfitting, I will reduce the number of trees used. Looking at the plot below can help us determine the correct number of trees to use : In [27]: plot ( bag.arrest , main = \"Trees vs Errors\" ) This plot determines the errors produced by each amount of trees. Using this, we can see visually where the line appears to stabilize and use this information to minimize the loss in accuracy from removing the trees. The green line shows the errors for the affirmative case (which is more likely), the red line for the negative case, and the black line for the overall error (the same as the misclassification error above). The plot appears to level off at around n=25. When I recreated the model using only 25 trees, I found the misclassification error to be 0.076. The error and confusion matrix will be included in the appendix. Although this is slightly higher than the model using 300 trees, it it a very small reduction in accuracy for a rather large increase in the interpretability and utility of the model. Now that we have determined a good amount of trees, we can look at an importance plot to see which variables create the most variations in the errors. Since this is a categorical variable, it will be most useful to use the right graph, which uses the Gini index. In [28]: bag.arrest2 <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 25 ) bag.pred2 <- predict ( bag.arrest2 , newdata = mydata.test , type = \"class\" ) table8 <- table ( bag.pred2 , arrest.test ) misclassificationbagging2 <- 1 - sum ( diag ( table8 )) / nrow ( mydata.test ) varImpPlot ( bag.arrest2 ) The variable with the most importance is sb_other, which is reason for search:other. This is a troubling thing, as described in the data section, because this is not a very informative variable. We can see that race now does have some importance in determining the arrests, as well as some other variables that were ommited from the logistic regression model. Interestingly, age, weight and height are included quite highly as well. Once again, whether the person was searched, had contraband, or was handcuffed is determined as important, which is consistent with our findings from the logistic regression. By using a sample of the variables in each tree and performing the bagging method on a different subset for each iteration, we can implement a Random Forest method. This method is useful since it allows some variation in the trees, since they will not only be dominated by the most important variables. Using two numbers for the number of variables used, I produced two random forest models. Below are their confusion matrices and misclassification errors. In [29]: table7 misclassificationbagging table8 misclassificationbagging2 arrest.test bag.pred N Y N 4505 285 Y 134 702 0.0744756487735514 arrest.test bag.pred2 N Y N 4490 277 Y 149 710 0.0757198720227515 The misclassification errors for the two are nearly identical. They perform very similarly to the bagging method. I would prefer the second model, as it predicts better in the case that the prediction and observed value are both yes than they are both no. By using a similar method to the bagging method earlier, we determine a good reduction in the amount of trees is to 40. Using 40 trees, we get the following confusion matrix and misclassification error. In [30]: RF.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = 9 , importance = TRUE , ntree = 300 ) RF.pred <- predict ( RF.arrest , newdata = mydata.test , type = \"class\" ) table9 <- table ( RF.pred , arrest.test ) misclassRF <- 1 - sum ( diag ( table9 )) / nrow ( mydata.test ) varImpPlot ( RF.arrest , main = \"Imporance of Variables\" ) RF2.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ) / 2 , importance = TRUE , ntree = 300 ) RF2.pred <- predict ( RF2.arrest , newdata = mydata.test , type = \"class\" ) table10 <- table ( RF2.pred , arrest.test ) misclassRF2 <- 1 - sum ( diag ( table10 )) / nrow ( mydata.test ) varImpPlot ( RF2.arrest ) plot ( RF2.arrest ) RF3.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ) / 2 , importance = TRUE , ntree = 40 ) RF3.pred <- predict ( RF3.arrest , newdata = mydata.test , type = \"class\" ) table11 <- table ( RF3.pred , arrest.test ) misclassRF3 <- 1 - sum ( diag ( table11 )) / nrow ( mydata.test ) In [31]: table11 misclassRF3 arrest.test RF3.pred N Y N 4515 293 Y 124 694 0.0741201564166371 In this case, the error actually was reduced. This can be accounted for by the fact that it simplified the model, which resulted in the random forest model with 40 trees actually predicting better than the more complex model. This suggests that the increase in bias by using a simpler model with 40 trees was less than the decrease in variance of the model on the testing data. This model performs the best of all the models I have used, and is the most successful at predicting whether a person is arrested or not when they are stopped and frisked, with about a 7.2% misclassification rate. Conclusions Below is a table summarizing all of the models that I've used, and their misclassifcation errors on the test data. In [32]: Method <- c ( \"Full Logistic Model\" , \"Reduced Logistic Model\" , \"10-NN Full Model\" , \"5-NN Reduced Model\" , \"Pruned Tree\" , \"Unpruned Tree\" , \"Bagging (n=300)\" , \"Bagging (n=25)\" , \"Random Forest I (n=300)\" , \"Random Forest II (n=300)\" , \"Random Forest III (n=40)\" ) TestError <- c ( misclasserrorsfull , misclasserrorstepwise , KNNMisclass , KNNMisclass2 , misclassificationprune , misclassificationunprune , misclassificationbagging , misclassificationbagging2 , misclassRF , misclassRF2 , misclassRF3 ) TestError <- round ( TestError , 3 ) data.frame ( Method , TestError ) # error: Yes when No Er1 <- table1 [ 2 , 1 ] / sum ( table1 [ 2 , ]) Er2 <- table2 [ 2 , 1 ] / sum ( table2 [ 2 , ]) Er3 <- table3 [ 2 , 1 ] / sum ( table3 [ 2 , ]) Er4 <- table4 [ 2 , 1 ] / sum ( table4 [ 2 , ]) Er5 <- table5 [ 2 , 1 ] / sum ( table5 [ 2 , ]) Er6 <- table6 [ 2 , 1 ] / sum ( table6 [ 2 , ]) Er7 <- table7 [ 2 , 1 ] / sum ( table7 [ 2 , ]) Er8 <- table8 [ 2 , 1 ] / sum ( table8 [ 2 , ]) Er9 <- table9 [ 2 , 1 ] / sum ( table9 [ 2 , ]) Er10 <- table10 [ 2 , 1 ] / sum ( table10 [ 2 , ]) Er11 <- table11 [ 2 , 1 ] / sum ( table11 [ 2 , ]) # error: No when Yes Er21 <- table1 [ 1 , 2 ] / sum ( table1 [ 1 , ]) Er22 <- table2 [ 1 , 2 ] / sum ( table2 [ 1 , ]) Er23 <- table3 [ 1 , 2 ] / sum ( table3 [ 1 , ]) Er24 <- table4 [ 1 , 2 ] / sum ( table4 [ 1 , ]) Er25 <- table5 [ 1 , 2 ] / sum ( table5 [ 1 , ]) Er26 <- table6 [ 1 , 2 ] / sum ( table6 [ 1 , ]) Er27 <- table7 [ 1 , 2 ] / sum ( table7 [ 1 , ]) Er28 <- table8 [ 1 , 2 ] / sum ( table8 [ 1 , ]) Er29 <- table9 [ 1 , 2 ] / sum ( table9 [ 1 , ]) Er210 <- table10 [ 1 , 2 ] / sum ( table10 [ 1 , ]) Er211 <- table11 [ 1 , 2 ] / sum ( table11 [ 1 , ]) FalsePositive <- round ( c ( Er1 , Er2 , Er3 , Er4 , Er5 , Er6 , Er7 , Er8 , Er9 , Er10 , Er11 ), 3 ) FalseNegative <- round ( c ( Er21 , Er22 , Er23 , Er24 , Er25 , Er26 , Er27 , Er28 , Er29 , Er210 , Er211 ), 3 ) Method TestError Full Logistic Model 0.095 Reduced Logistic Model 0.093 10-NN Full Model 0.162 5-NN Reduced Model 0.120 Pruned Tree 0.117 Unpruned Tree 0.117 Bagging (n=300) 0.074 Bagging (n=25) 0.076 Random Forest I (n=300) 0.074 Random Forest II (n=300) 0.073 Random Forest III (n=40) 0.074 In [33]: data.frame ( Method , TestError , FalsePositive , FalseNegative ) Method TestError FalsePositive FalseNegative Full Logistic Model 0.095 0.176 0.081 Reduced Logistic Model 0.093 0.183 0.082 10-NN Full Model 0.162 0.402 0.143 5-NN Reduced Model 0.120 0.185 0.113 Pruned Tree 0.117 0.283 0.092 Unpruned Tree 0.117 0.257 0.098 Bagging (n=300) 0.074 0.160 0.059 Bagging (n=25) 0.076 0.173 0.058 Random Forest I (n=300) 0.074 0.137 0.064 Random Forest II (n=300) 0.073 0.146 0.060 Random Forest III (n=40) 0.074 0.152 0.061 I have also included the false positive and false negative results, as these can help to determine which models fit the best. The Random Forest model which used 40 trees and 39 variables performed the best out of all the methods used. It is clear that the bagging and random forest methods were superior to the logistic and KNN methods. Of the simpler methods, the reduced Logistic Model using stepwise reduction with the AIC performed the best. Despite my original inclinations, we can see from the importance plots on the bagging model and the summaries of the logistic model that race was not a very significant factor in determining whether a person would be arrested or not. However, as we saw before from the bar graphs, there was a significantly larger portion of blacks who were stopped and frisked than the general population. This suggests that a black person is more likely to be stopped, but not significantly more or less likely than other races to be arrested after they are stopped. Which raises the question: why stop more black people if they are no more likely to be arrested after the frisk than other races? Another variable of interest was sb_other. This variable indicates that there was an \"other reason for stopping the subject.\" As such, this variable is difficult to interpret. However, the variable was quite significant with a high coefficient in the logistic model, and was rated as the most important variable by a large margin in the bagging models. This is a point of difficulty in the interpretations. This may indicate that there needs to be more disclosure about the reason a person was searched, since the sb variables in their current state fail to capture much of the reasons for arrest. In both the logisitic and bagging methods, the variables for contraband and knife/cutting object came up as significant, and had a significant effect on whether the person was arrested or not. This is logical: if the officer found a weapon on contraband on the person, they were much more likely to be arrested. This was far more significant than other factors about the person, in particular the cs variables, which list the reasons why the person was stopped, or the age, weight, gender and race variables, which are the physical attributes of the person. This suggests that the physical attributes of a person are not nearly as important as criminal possession in predicting an arrest, which is in support of the stop and frisk's usage. For instance, being stopped due to the clothing you wear (cs_cloth) or being perceived as a lookout (cs_lkout) were not nearly as significant as carrying a suspicious object (cs_object) or if they were searched (searched), and actually had negative coefficients. So it appears that, most of the time, the probability of being arrested was largely reliant on whether the person was searched or not, or found to have an object. This matches up with the significance and postive coefficient of cs_drgtr, which was the variable representing the cause of search being for a suspected drug transaction. One would conclude that if a person was being suspected for a drug transaction, they would also be more likely to have contraband or perhaps a weapon than if they were not. Interestingly, the frisked variable was not found to be significant in the logistic regression model, suggesting that frisking, as compared to searching, was not very significant in arrest, and therefore not too effective a measure to stop criminal activity. In both models, searching had a stronger effect than frisking in determining whether a person was arrested or not. The success of the random forest/bagging methods as compared to the simpler methods suggests that the relationship between whether a person is arrested or not and the various variables in the dataset is quite complex, and that it cannot be estimated as well using the simpler techniques.","tags":"Projects","url":"https://seanammirati.github.io/category/projects/nypd_stop_frisk_full.html","loc":"https://seanammirati.github.io/category/projects/nypd_stop_frisk_full.html"}]};