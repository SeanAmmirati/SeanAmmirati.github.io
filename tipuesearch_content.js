var tipuesearch = {"pages":[{"title":"About This Website","text":"Welcome to Stats Works! This website is dedicated to statistical approaches to various problems. Statistics and the analysis of data are becoming more and more relevant in the world today. With the introduction of machine learning techniques, statistics and computer science have become essential to the ability of business and governments' success. Unleashing the potential of information is more crucial than ever, and is having a remarkable impact on the world. This website is dedicated to personal projects surrounding both statistics and machine learning and shows how we can leverage the wealth of data in the world today to make actionable insights to further improve the world we live in. Parts of this website are also dedicated to explaining some basic statistical concepts to those who may have no background in statistics, or have a background in STEM but need a refresher on these topics. The author, Sean Ammirati, is a Data Scientist with a deep love and passion for statistics, programming and machine learning. The intention of creating this website is to demonstrate practical implementations of theoretical statistics as well as demystify some of the more complex topics in the dicipline. Sean has a Master's degree in Statistics from Hunter College.","tags":"About","url":"https://seanammirati.github.io/pages/about/","loc":"https://seanammirati.github.io/pages/about/"},{"title":"Survivor: Outwit, Outplay, Out...analyze? (Part 4)","text":"Analyzing the Game of Survivor -- Inferential Model of Sentiment for Reddit Posts (4) Welcome back, to the fourth installment of the Survivor analysis series! My last post began to investigate the [sentiment of Reddit comments for particular contestants and episodes]. The next step is to build a linear model to predict this sentiment, using the contestant episode combinations and other variables as independent variables. This will be inferrential in nature -- to show a statistically significant preference (or dislike) towards particular players. In thisinstallment of the series, I will continue to digging into some of the Reddit data that I had collected via the Pushift.io API . For more information on how this data was collected, please check out the [first article in this series], where I describe the ETL process for the Pushift (as well as other!) data. Also, we will be looking at the sentiment calculated in the [previous article in this series]. Without further ado, let's jump in! Jumping In -- Calculating Sentiment and Fitting a Model The first step is to catch up to where we were in the last installment! To do this, we will run the sentiment feature creating functions. For our purposes, we will start with predicting the sentence_sentiment . In [1]: import os from sqlalchemy import create_engine import pandas as pd import numpy as np import statsmodels.api as sm from IPython.display import HTML , display from plotly.express import scatter from textblob import TextBlob import statsmodels.api as sm pd . options . display . max_columns = 300 In [2]: pg_un , pg_pw , pg_ip , pg_port = [ os . getenv ( x ) for x in [ 'PG_UN' , 'PG_PW' , 'PG_IP' , 'PG_PORT' ]] In [3]: def pg_uri ( un , pw , ip , port ): return f 'postgresql:// { un } : { pw } @ { ip } : { port } ' In [4]: eng = create_engine ( pg_uri ( pg_un , pg_pw , pg_ip , pg_port )) In [5]: sql = ''' WITH contestants_to_seasons AS ( SELECT c.contestant_id, c.first_name, c.last_name, cs.contestant_season_id, c.sex, cs.season_id, occupation, location, age, placement, days_lasted, votes_against, med_evac, quit, individual_wins, attempt_number, tribe_0, tribe_1, tribe_2, tribe_3, alliance_0, alliance_1, alliance_2, challenge_wins, challenge_appearances, sitout, voted_for_bootee, votes_against_player, character_id, r.role, r.description, total_number_of_votes_in_episode, tribal_council_appearances, votes_at_council, number_of_jury_votes, total_number_of_jury_votes, number_of_days_spent_in_episode, days_in_exile, individual_reward_challenge_appearances, individual_reward_challenge_wins, individual_immunity_challenge_appearances, individual_immunity_challenge_wins, tribal_reward_challenge_appearances, tribal_reward_challenge_wins, tribal_immunity_challenge_appearances, tribal_immunity_challenge_wins, tribal_reward_challenge_second_of_three_place, tribal_immunity_challenge_second_of_three_place, fire_immunity_challenge, tribal_immunity_challenge_third_place, episode_id FROM survivor.contestant c RIGHT JOIN survivor.contestant_season cs ON c.contestant_id = cs.contestant_id JOIN survivor.episode_performance_stats eps ON eps.contestant_id = cs.contestant_season_id JOIN survivor.role r ON cs.character_id = r.role_id ), matched_exact AS ( SELECT reddit.*, c.* FROM survivor.reddit_comments reddit JOIN contestants_to_seasons c ON (POSITION(c.first_name IN reddit.body) > 0 OR POSITION(c.last_name IN reddit.body) > 0) AND c.season_id = reddit.within_season AND c.episode_id = reddit.most_recent_episode WHERE within_season IS NOT NULL ) SELECT * FROM matched_exact m ''' In [6]: reddit_df = pd . read_sql ( sql , eng ) In [7]: ep_df = pd . read_sql ( 'SELECT * FROM survivor.episode' , eng ) In [8]: season_to_name = pd . read_sql ( 'SELECT season_id, name AS season_name FROM survivor.season' , eng ) In [9]: reddit_df = reddit_df . merge ( season_to_name , on = 'season_id' ) In [10]: reddit_df . rename ( columns = { 'name' : 'season_name' }, inplace = True ) In [11]: reddit_df = reddit_df . merge ( ep_df . drop ( columns = [ 'season_id' ]), on = 'episode_id' ) In [12]: reddit_df [ 'created_dt' ] = pd . to_datetime ( reddit_df [ 'created_dt' ]) In [13]: pd . options . display . max_columns = 100 In [29]: def extract_overall_and_sentence_sentiment ( row ): body = row [ 'body' ] first = row [ 'first_name' ] last = row [ 'last_name' ] b = TextBlob ( body ) overall_sentiment = b . sentiment overall_polarity = overall_sentiment . polarity overall_subj = overall_sentiment . subjectivity rel_sentences_sentiment = [ x . sentiment for x in b . sentences if ( first in x ) or ( last in x )] sentence_polarity = np . mean ([ y . polarity for y in rel_sentences_sentiment ]) sentence_subj = np . mean ([ y . subjectivity for y in rel_sentences_sentiment ]) metrics = [ overall_polarity , overall_subj , sentence_polarity , sentence_subj ] metric_names = [ 'overall_polarity' , 'overall_subj' , 'sentence_polarity' , 'sentence_subj' ] return pd . Series ( metrics , index = metric_names ) def add_sentiment_cols ( df ): sentiment_df = df . apply ( extract_overall_and_sentence_sentiment , axis = 1 ) return pd . concat ([ df , sentiment_df ], axis = 1 ) In [30]: reddit_df = add_sentiment_cols ( reddit_df ) Looking at a linear model for sentiment The first step in this process will be building the features for the model that we will be using. For this iteration, I have looked at a few things as features including: Demographics: Age of Contestant Sex of the Contestant Role of the Contestant (according to Caunce Survivor Archetypes ) Overall Contestant (Season) Statistics: Whether the Contestant was the Sole Survivor (Won) Whether the Contestant placed 2nd or 3rd (Runnerups) Number of times the Contestant sit out of challenges Number of days the Contestant spent in Exile Island Number of individual immunity challenge wins Number of individual reward challenge wins Number of tribal immunity challenge wins Number of tribal reward challenge wins Total number of days lasted in the season Tribe of the contestant discussed Alliance of the contestant discussed Episode Contestant Statistics: Whether the Contestant sat out the most recent episode Whether the Contestant won a challenge that episode Number of days the Contestant spent in Exile Island in the most recent episode Number of individual immunity challenge wins in the most recent episode Number of individual reward challenge wins in the most recent episode Number of tribal immunity challenge wins in the most recent episode Number of tribal reward challenge wins in the most recent episode Number of votes against contestant in the most recent episode Other: Whether the episode had a high rating on TV relative to other shows on at that time An indicator for the contestant, regardless of season An indicator for the contestant in a particular episode An indicator for the season An indicator for the episode within the season In [49]: tribe_df = pd . read_sql ( 'SELECT tribe_id, name FROM survivor.tribe' , con = eng ) alliance_df = pd . read_sql ( 'SELECT alliance_id, name FROM survivor.alliance' , con = eng ) In [50]: tribe_map = tribe_df . set_index ( 'tribe_id' )[ 'name' ] alliance_map = alliance_df . set_index ( 'alliance_id' )[ 'name' ] In [51]: reddit_df [ 'contestant_episode' ] = reddit_df [ 'first_name' ] + ' ' + reddit_df [ 'last_name' ] + ' ' + reddit_df [ 'episode_name' ] In [110]: def grab_first_if_exists ( l ): try : return l [ 0 ] except IndexError : pass In [120]: def preprocess_df ( df ): df = df . copy () df . loc [ df [ 'age' ] . str . contains ( '[&#94;\\d]' ), 'age' ] = None drop = df [ 'sentence_polarity' ] . isnull () | df [ 'age' ] . isnull () | df [ 'sex' ] . isnull () df = df [ ~ drop ] . reset_index ( drop = True ) df [ 'age' ] = df [ 'age' ] . astype ( float ) df [ 'won' ] = ( df [ 'placement' ] == 1 ) . astype ( int ) df [ 'runnerup' ] = ( df [ 'placement' ] . isin ([ 2 , 3 ])) . astype ( int ) df [ 'pct_overall_slot' ] = df [ 'survivor_rating' ] / df [ 'overall_slot_rating' ] df [ 'pct_overall_slot' ] . fillna ( df [ 'pct_overall_slot' ] . mean (), inplace = True ) merge_on = [ 'contestant_season_id' , 'episode_id' ] merger_df = df . drop_duplicates ( merge_on ) cols_sum = [ 'sitout' , 'days_in_exile' , 'individual_reward_challenge_wins' , 'individual_immunity_challenge_wins' , 'tribal_reward_challenge_wins' , 'tribal_immunity_challenge_wins' ] for col in cols_sum : merger_df [ 'total_' + col ] = merger_df . groupby ( 'contestant_season_id' )[ col ] . transform ( lambda x : x . sum ()) add_cols = [ 'total_' + c for c in cols_sum ] + merge_on df = df . merge ( merger_df [ add_cols ], on = merge_on ) return df def create_binned_df ( df , col , bins , labels ): df [ col + '_bins' ] = pd . cut ( df [ col ], bins = bins , labels = labels ) return pd . get_dummies ( df [ col + '_bins' ], prefix = col , drop_first = True ) def create_binned_age_df ( df ): bins = [ 16 , 25 , 35 , 45 , 65 , 150 ] labels = [ '16-25' , '26-35' , '36-45' , '46-65' , '65+' ] col = 'age' return create_binned_df ( df , col , bins , labels ) def create_binned_days_lasted ( df ): bins = [ 0 , 10 , 20 , 30 , 40 ] labels = [ '0-10 days' , '10-20 days' , '20-30 days' , '30+ days' ] col = 'days_lasted' return create_binned_df ( df , col , bins , labels ) def extract_features ( df ): df = df . copy () single_cols = [ 'days_lasted' , 'age' , 'votes_against' , 'individual_wins' , 'season_episode_number' , 'won' , 'runnerup' , 'pct_overall_slot' ] agg_cols = [ 'sitout' , 'days_in_exile' , 'individual_reward_challenge_wins' , 'individual_immunity_challenge_wins' , 'tribal_reward_challenge_wins' , 'tribal_immunity_challenge_wins' ] agg_cols_tot = [ 'total_' + x for x in agg_cols ] sex_df = pd . get_dummies ( df [ 'sex' ], prefix = 'sex' , drop_first = True ) contestant_df = pd . get_dummies ( df [ 'first_name' ] + ' ' + df [ 'last_name' ], prefix = 'contestant' , drop_first = True ) role_df = pd . get_dummies ( df [ 'role' ] . astype ( str ), prefix = 'role' , drop_first = True ) season_id_df = pd . get_dummies ( df [ 'season_name' ], prefix = 'season' , drop_first = True ) contestant_episode_df = pd . get_dummies ( df [ 'contestant_episode' ], prefix = 'contestant_episode' , drop_first = True ) contestant_episode_df = contestant_episode_df . loc [:, contestant_episode_df . sum () > 25 ] # episode_df = pd.get_dummies(df['episode_name'], prefix='episode', drop_first=True) tribe_df = pd . concat ([ pd . get_dummies ( df [ c ] . map ( tribe_map ), prefix = 'tribe' , drop_first = False ) for c in [ 'tribe_0' , 'tribe_1' , 'tribe_2' ]], axis = 1 , join = 'outer' ) alliance_df = pd . concat ([ pd . get_dummies ( df [ c ] . map ( alliance_map ), prefix = 'alliance' , drop_first = False ) for c in [ 'alliance_0' , 'alliance_1' , 'alliance_2' ]], axis = 1 , join = 'outer' ) tribe_df = tribe_df . loc [: , ~ tribe_df . columns . duplicated ( keep = 'first' )] . iloc [:, 1 :] alliance_df = alliance_df . loc [:, ~ alliance_df . columns . duplicated ( keep = 'first' )] . iloc [:, 1 :] age_df = create_binned_age_df ( df ) days_lasted_df = create_binned_days_lasted ( df ) ret_df = pd . concat ([ df [ single_cols ], df [ agg_cols ], df [ agg_cols_tot ], age_df , sex_df , contestant_df , role_df , days_lasted_df , season_id_df , tribe_df , alliance_df , contestant_episode_df ], axis = 1 ) contestant_cols = contestant_df . columns . tolist () remove_cols = ret_df . groupby ( contestant_cols ) . apply ( lambda x : grab_first_if_exists ( x . columns [ x . columns . str . contains ( 'contestant_episode' ) & ( x > 0 ) . any ()] . tolist ())) remove_cols = remove_cols . dropna () . values . tolist () ret_df . drop ( columns = remove_cols , inplace = True ) return ret_df def extract_y_X ( df , y_name = 'sentence_polarity' ): df = preprocess_df ( df ) X = extract_features ( df ) X = sm . add_constant ( X ) y = extract_dependent ( df ) return y , X def extract_dependent ( df , col_name = 'sentence_polarity' ): return df [ col_name ] The functions above extract all of the relevant features for us. Looking at the dependent and independent random variable: In [252]: y , X = extract_y_X ( reddit_df ) <ipython-input-120-11a7d58c5e6b>:21: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Reference class: sex is M Reference class: first_name is Ryan Reference class: role is Know It All Reference class: season_name is Heroes vs. Healers vs. Hustlers Reference class: age_bins is 16-25 Reference class: days_lasted_bins is 0-10 days In [253]: y . head () Out[253]: 0 -0.041667 1 0.400000 2 0.033333 3 0.206250 4 0.316667 Name: sentence_polarity, dtype: float64 In [254]: X . head () Out[254]: const days_lasted age votes_against individual_wins season_episode_number won runnerup pct_overall_slot sitout days_in_exile individual_reward_challenge_wins individual_immunity_challenge_wins tribal_reward_challenge_wins tribal_immunity_challenge_wins total_sitout total_days_in_exile total_individual_reward_challenge_wins total_individual_immunity_challenge_wins total_tribal_reward_challenge_wins total_tribal_immunity_challenge_wins age_26-35 age_36-45 age_46-65 age_65+ sex_M contestant_Adam Klein contestant_Alan Ball contestant_Albert Destrade contestant_Alec Christy contestant_Alecia Holden contestant_Alexis Maxwell contestant_Ali Elliott contestant_Alicia Rosa contestant_Allie Pohevitz contestant_Andrea Boehlke contestant_Andrew Savage contestant_Angie Layton contestant_Anna Khait contestant_Aras Baskauskas contestant_Artis Silvester contestant_Ashley Nolan contestant_Ashley Underwood contestant_Aubry Bracco contestant_Baylor Wilson contestant_Ben Driebergen contestant_Bill Posley contestant_Brad Culpepper contestant_Brandon Hantz contestant_Brenda Lowe contestant_Bret LaBelle contestant_Brice Johnston contestant_Caleb Bankston contestant_Caleb Reynolds contestant_Candice Woodcock contestant_Carolyn Rivera contestant_Carter Williams contestant_CeCe Taylor contestant_Chelsea Meissner contestant_Chris Hammons contestant_Chrissy Hofbeck contestant_Christina Cha contestant_Christine Shields-Markoski contestant_Ciera Eastin contestant_Cirie Fields contestant_Cliff Robinson contestant_Coach Wade contestant_Cole Medders contestant_Colton Cumbie contestant_Corinne Kaplan contestant_Cydney Gillon contestant_Dale Wentworth contestant_Dan Foley contestant_Dana Lambert contestant_Darnell Hamilton contestant_David Murphy contestant_David Samson contestant_David Wright contestant_Dawn Meehan contestant_Debbie Wanner contestant_Denise Stapley contestant_Desi Williams contestant_Devon Pinto contestant_Drew Christy contestant_Eddie Fox contestant_Edna Ma contestant_Elyse Umemoto contestant_Erik Reichenbach contestant_Figgy Figueroa contestant_Francesca Hogi contestant_Garrett Adelstein contestant_Gervase Peterson contestant_Grant Mattos contestant_Hali Ford contestant_Hannah Shapiro contestant_Hayden Moss contestant_Hope Driskill contestant_J'Tia Taylor contestant_J.T. Thomas contestant_JP Hilsabeck contestant_Jaclyn Schultz contestant_Jay Byars contestant_Jay Starrett contestant_Jeff Kent contestant_Jeff Varner contestant_Jefra Bland contestant_Jenn Brown contestant_Jennifer Lanzetti contestant_Jeremiah Wood contestant_Jeremy Collins contestant_Jessica Johnston contestant_Jessica Lewis contestant_Jim Rice contestant_Joaquin Souberbielle contestant_Joe Anglim contestant_Joe Mena contestant_Joe del Campo contestant_John Cochran contestant_John Cody contestant_John Rocker contestant_Jon Misch contestant_Jonas Otsuji contestant_Jonathan Penner contestant_Josh Canfield contestant_Julia Landauer contestant_Julia Sokolowski contestant_Julie McGee contestant_Julie Wolfe contestant_Kass McQuillen contestant_Kat Edorsson contestant_Katie Collins contestant_Katie Hanson contestant_Katrina Radke contestant_Keith Nale contestant_Keith Tollefson contestant_Kelley Wentworth contestant_Kelly Remington contestant_Kelly Wiglesworth contestant_Ken McNickle contestant_Kim Spradlin-Wolfe contestant_Kimmi Kappenberg contestant_Kourtney Moon contestant_Kristina Kell contestant_Kyle Jason contestant_LJ McKanas contestant_Laura Alexander contestant_Laura Boneham contestant_Laura Morett contestant_Lauren Rimmer contestant_Leif Manson ... contestant_episode_Taylor Stocker Still Throwin' Punches contestant_episode_Taylor Stocker The Truth Works Well contestant_episode_Taylor Stocker Who's the Sucker at the Table%3F contestant_episode_Taylor Stocker Your Job Is Recon contestant_episode_Terry Deitz Survivor MacGyver contestant_episode_Terry Deitz We Got a Rat contestant_episode_Terry Deitz What's the Beef%3F contestant_episode_Tina Wesson Blood Is Thicker than Anything contestant_episode_Tina Wesson Gloves Come Off contestant_episode_Tina Wesson My Brother's Keeper contestant_episode_Tina Wesson One-Man Wrecking Ball contestant_episode_Tina Wesson Out on a Limb (episode) contestant_episode_Tina Wesson Rustle Feathers contestant_episode_Tina Wesson Skin of My Teeth contestant_episode_Tina Wesson Swoop In for the Kill contestant_episode_Tina Wesson The Dead Can Still Talk contestant_episode_Tony Vlachos Chaos Is My Friend contestant_episode_Tony Vlachos Cops-R-Us (episode) contestant_episode_Tony Vlachos Havoc to Wreak contestant_episode_Tony Vlachos Head of the Snake contestant_episode_Tony Vlachos Hot Girl with a Grudge contestant_episode_Tony Vlachos Mad Treasure Hunt contestant_episode_Tony Vlachos Odd One Out contestant_episode_Tony Vlachos Our Time to Shine contestant_episode_Tony Vlachos Sitting in My Spy Shack contestant_episode_Tony Vlachos Straw That Broke the Camel's Back contestant_episode_Tony Vlachos The Stakes Have Been Raised contestant_episode_Tony Vlachos We Found Our Zombies contestant_episode_Trish Hegarty Chaos Is My Friend contestant_episode_Trish Hegarty Havoc to Wreak contestant_episode_Trish Hegarty Head of the Snake contestant_episode_Trish Hegarty Hot Girl with a Grudge contestant_episode_Trish Hegarty Mad Treasure Hunt contestant_episode_Trish Hegarty Odd One Out contestant_episode_Trish Hegarty Our Time to Shine contestant_episode_Trish Hegarty Sitting in My Spy Shack contestant_episode_Trish Hegarty Straw That Broke the Camel's Back contestant_episode_Trish Hegarty We Found Our Zombies contestant_episode_Troyzan Robertson Dirty Deed contestant_episode_Troyzan Robertson Go Out with a Bang contestant_episode_Troyzan Robertson I'm No Dummy contestant_episode_Troyzan Robertson It Is Not a High Without a Low contestant_episode_Troyzan Robertson Never Say Die contestant_episode_Troyzan Robertson Parting Is Such Sweet Sorrow contestant_episode_Troyzan Robertson Reinventing How This Game Is Played contestant_episode_Troyzan Robertson Survivor Jackpot contestant_episode_Troyzan Robertson The Beauty in a Merge contestant_episode_Troyzan Robertson The Stakes Have Been Raised contestant_episode_Troyzan Robertson The Tables Have Turned contestant_episode_Troyzan Robertson There's a New Sheriff in Town contestant_episode_Troyzan Robertson Vote Early, Vote Often contestant_episode_Troyzan Robertson What Happened on Exile, Stays on Exile contestant_episode_Tyler Fredrickson Crazy Is as Crazy Does contestant_episode_Tyler Fredrickson Holding On for Dear Life contestant_episode_Tyler Fredrickson It Will Be My Revenge contestant_episode_Tyler Fredrickson It's Survivor Warfare contestant_episode_Tyler Fredrickson Keep It Real contestant_episode_Tyler Fredrickson Livin' on the Edge contestant_episode_Tyler Fredrickson Survivor Russian Roulette contestant_episode_Tyler Fredrickson The Line Will Be Drawn Tonight contestant_episode_Tyler Fredrickson Winner Winner, Chicken Dinner contestant_episode_Tyson Apostol Blood Is Thicker than Anything contestant_episode_Tyson Apostol Gloves Come Off contestant_episode_Tyson Apostol My Brother's Keeper contestant_episode_Tyson Apostol One Armed Dude and Three Moms contestant_episode_Tyson Apostol One-Man Wrecking Ball contestant_episode_Tyson Apostol Opening Pandora's Box contestant_episode_Tyson Apostol Out on a Limb (episode) contestant_episode_Tyson Apostol Rule in Chaos contestant_episode_Tyson Apostol Rustle Feathers contestant_episode_Tyson Apostol Skin of My Teeth contestant_episode_Tyson Apostol Swoop In for the Kill contestant_episode_Tyson Apostol The Dead Can Still Talk contestant_episode_Val Collins Suck It Up and Survive contestant_episode_Vince Sly It's Survivor Warfare contestant_episode_Vytas Baskauskas Blood Is Thicker than Anything contestant_episode_Vytas Baskauskas Gloves Come Off contestant_episode_Vytas Baskauskas My Brother's Keeper contestant_episode_Vytas Baskauskas One Armed Dude and Three Moms contestant_episode_Vytas Baskauskas One-Man Wrecking Ball contestant_episode_Vytas Baskauskas Opening Pandora's Box contestant_episode_Vytas Baskauskas Skin of My Teeth contestant_episode_Vytas Baskauskas Swoop In for the Kill contestant_episode_Vytas Baskauskas The Dead Can Still Talk contestant_episode_Wes Nale Blood Is Blood contestant_episode_Wes Nale Gettin' to Crunch Time contestant_episode_Wes Nale Make Some Magic Happen contestant_episode_Wes Nale Method to This Madness contestant_episode_Wes Nale Million Dollar Decision contestant_episode_Wes Nale Suck It Up and Survive contestant_episode_Wes Nale This Is Where We Build Trust contestant_episode_Wes Nale We're a Hot Mess contestant_episode_Wes Nale Wrinkle in the Plan contestant_episode_Will Sims II Crazy Is as Crazy Does contestant_episode_Will Sims II Holding On for Dear Life contestant_episode_Will Sims II It Will Be My Revenge contestant_episode_Will Sims II It's Survivor Warfare contestant_episode_Will Sims II Keep It Real contestant_episode_Will Sims II Livin' on the Edge contestant_episode_Will Sims II My Word Is My Bond contestant_episode_Will Sims II Survivor Russian Roulette contestant_episode_Will Sims II The Line Will Be Drawn Tonight contestant_episode_Will Sims II Winner Winner, Chicken Dinner contestant_episode_Will Wahl I Will Destroy You contestant_episode_Will Wahl I'm the Kingpin contestant_episode_Will Wahl Idol Search Party contestant_episode_Will Wahl Love Goggles contestant_episode_Will Wahl May the Best Generation Win contestant_episode_Will Wahl Million Dollar Gamble contestant_episode_Will Wahl Slayed the Survivor Dragon contestant_episode_Will Wahl Still Throwin' Punches contestant_episode_Will Wahl The Truth Works Well contestant_episode_Will Wahl Who's the Sucker at the Table%3F contestant_episode_Will Wahl Your Job Is Recon contestant_episode_Woo Hwang Bag of Tricks contestant_episode_Woo Hwang Bunking with the Devil contestant_episode_Woo Hwang Chaos Is My Friend contestant_episode_Woo Hwang Cops-R-Us (episode) contestant_episode_Woo Hwang Havoc to Wreak contestant_episode_Woo Hwang Head of the Snake contestant_episode_Woo Hwang Hot Girl with a Grudge contestant_episode_Woo Hwang Mad Treasure Hunt contestant_episode_Woo Hwang Odd One Out contestant_episode_Woo Hwang Our Time to Shine contestant_episode_Woo Hwang Sitting in My Spy Shack contestant_episode_Woo Hwang Straw That Broke the Camel's Back contestant_episode_Woo Hwang Survivor MacGyver contestant_episode_Woo Hwang We Found Our Zombies contestant_episode_Woo Hwang We Got a Rat contestant_episode_Woo Hwang What's the Beef%3F contestant_episode_Zeke Smith About to Have a Rumble contestant_episode_Zeke Smith Dirty Deed contestant_episode_Zeke Smith I Will Destroy You contestant_episode_Zeke Smith I'm the Kingpin contestant_episode_Zeke Smith Idol Search Party contestant_episode_Zeke Smith Love Goggles contestant_episode_Zeke Smith May the Best Generation Win contestant_episode_Zeke Smith Million Dollar Gamble contestant_episode_Zeke Smith Reinventing How This Game Is Played contestant_episode_Zeke Smith Slayed the Survivor Dragon contestant_episode_Zeke Smith Still Throwin' Punches contestant_episode_Zeke Smith Survivor Jackpot contestant_episode_Zeke Smith The Stakes Have Been Raised contestant_episode_Zeke Smith The Tables Have Turned contestant_episode_Zeke Smith The Truth Works Well contestant_episode_Zeke Smith There's a New Sheriff in Town contestant_episode_Zeke Smith Vote Early, Vote Often contestant_episode_Zeke Smith What Happened on Exile, Stays on Exile contestant_episode_Zeke Smith Who's the Sucker at the Table%3F contestant_episode_Zeke Smith Your Job Is Recon 0 1.0 1.0 18.0 0.0 0.0 6.0 0 1 0.266667 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 2.0 0.0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1.0 1.0 18.0 0.0 0.0 6.0 0 1 0.266667 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 2.0 0.0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1.0 1.0 18.0 0.0 0.0 6.0 0 1 0.266667 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 2.0 0.0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 1.0 1.0 18.0 0.0 0.0 6.0 0 1 0.266667 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 2.0 0.0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1.0 1.0 18.0 0.0 0.0 6.0 0 1 0.266667 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 2.0 0.0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 rows × 1773 columns Our random variable in this case is the sentiment of the sentence(s) containing the contestants' name. The predictors are a combination of all of the features mentioned above. There are quite a lot of features (2000+!) Many of these are comprised of the contestant_episode features, since there is a combination for every contestant and episode. If this were a predictive model, these features would be essentially useless -- since we would not see them in the future data. However, as a point of inference , we can use these to see what combinations that contestants had liked, holding constant some of the other variables we had considered. As such, I am also not attempting to use a cross-validation strategy, as my goal is simply to interpret the data we have now, not to predict the sentiment of a comment. If a contestant variable is significant after a selection process, we can say that this contestant had been shown to be statistically significantly more or less liked as a whole. If it is only for a contestant/episode combination, it may be that they were only shown to be liked/disliked more than everyone else during that particular episode. In this way, looking at the statistically significant variables we can understand differences in sentiment that are unlikely to have been caused by random chance alone . That is, people really disliked or liked these people. Another thing you'll notice is that this model will be based only on the content of the comment and, more specifically, the contestant/episode/season it is referring to. This means our predictive accuracy is pretty low -- but this is also because we are not looking at the users and their individual histories for instance. As an example, Redditors have a flair associated with their account. In r/survivor it is common place for Redditors to use this flair to indicate who they want to win after the first episode. As such, it wouldn't be surprising that this flair could be predictive of that users' sentiment toward that contestant. More generally, certain users may be more or less likely to use negative or positive language, or to have strong opinions, etc. While this is surely predictive, it doesn't exactly help us to explain why that contestant was liked or disliked. For this reason, we intentionally hinder our model a bit in order to only focus on the content. Secondly, in terms of prediction this model will be poor, because it is not accounting for anything about what is generating the comments (the users) but only the content of which they are commenting (the contestants). So, when you see an absymal $R&#94;2$ of < .001, don't cry foul! The intention is not to predict well -- instead, it is to find variables which are statistically significantly contributing to differences in the sentiment. Fitting the Model In [122]: full_fit = sm . OLS ( y , X ) . fit () In [123]: full_fit . summary () Out[123]: OLS Regression Results Dep. Variable: sentence_polarity R-squared: 0.011 Model: OLS Adj. R-squared: 0.008 Method: Least Squares F-statistic: 3.846 Date: Sat, 01 Aug 2020 Prob (F-statistic): 0.00 Time: 21:12:42 Log-Likelihood: -65261. No. Observations: 590728 AIC: 1.338e+05 Df Residuals: 589083 BIC: 1.524e+05 Df Model: 1644 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0394 0.016 2.477 0.013 0.008 0.071 days_lasted 0.0020 0.001 1.739 0.082 -0.000 0.004 age 0.0008 0.001 1.449 0.147 -0.000 0.002 votes_against -0.0027 0.001 -1.811 0.070 -0.006 0.000 individual_wins 0.0053 0.005 1.001 0.317 -0.005 0.016 season_episode_number -0.0035 0.002 -2.289 0.022 -0.007 -0.001 won -0.0037 0.018 -0.207 0.836 -0.039 0.032 runnerup -0.0133 0.013 -1.007 0.314 -0.039 0.013 pct_overall_slot -0.0099 0.009 -1.124 0.261 -0.027 0.007 sitout -0.0194 0.016 -1.207 0.227 -0.051 0.012 days_in_exile 8.203e-05 0.011 0.007 0.994 -0.022 0.022 individual_reward_challenge_wins 0.1553 0.139 1.115 0.265 -0.118 0.428 individual_immunity_challenge_wins 0.0123 0.015 0.791 0.429 -0.018 0.043 tribal_reward_challenge_wins 0.0264 0.012 2.245 0.025 0.003 0.049 tribal_immunity_challenge_wins -0.0017 0.010 -0.168 0.866 -0.022 0.018 total_sitout -0.0011 0.006 -0.175 0.861 -0.014 0.012 total_days_in_exile -0.0031 0.003 -1.163 0.245 -0.008 0.002 total_individual_reward_challenge_wins -0.0226 0.016 -1.399 0.162 -0.054 0.009 total_individual_immunity_challenge_wins 0.0030 0.008 0.386 0.700 -0.012 0.018 total_tribal_reward_challenge_wins -0.0038 0.005 -0.779 0.436 -0.013 0.006 total_tribal_immunity_challenge_wins 0.0054 0.005 1.169 0.242 -0.004 0.015 age_26-35 -0.0073 0.014 -0.506 0.613 -0.035 0.021 age_36-45 -0.0063 0.016 -0.396 0.692 -0.037 0.025 age_46-65 0.0118 0.021 0.571 0.568 -0.029 0.052 age_65+ 0.0089 0.018 0.498 0.618 -0.026 0.044 sex_M -0.0041 0.009 -0.474 0.635 -0.021 0.013 contestant_Adam Klein 0.0068 0.016 0.415 0.678 -0.025 0.039 contestant_Alan Ball 0.0017 0.017 0.103 0.918 -0.031 0.035 contestant_Albert Destrade 0.0048 0.038 0.125 0.901 -0.070 0.080 contestant_Alec Christy -0.0199 0.028 -0.708 0.479 -0.075 0.035 contestant_Alecia Holden -0.0235 0.024 -0.991 0.322 -0.070 0.023 contestant_Alexis Maxwell 0.0502 0.035 1.439 0.150 -0.018 0.119 contestant_Ali Elliott 0.0356 0.021 1.700 0.089 -0.005 0.077 contestant_Alicia Rosa 0.0033 0.030 0.111 0.912 -0.055 0.062 contestant_Allie Pohevitz -0.0122 0.033 -0.371 0.711 -0.076 0.052 contestant_Andrea Boehlke 0.0064 0.028 0.228 0.820 -0.049 0.062 contestant_Andrew Savage -0.0002 0.026 -0.006 0.995 -0.052 0.052 contestant_Angie Layton -0.0075 0.042 -0.176 0.860 -0.090 0.076 contestant_Anna Khait 0.0527 0.022 2.342 0.019 0.009 0.097 contestant_Aras Baskauskas 0.0200 0.038 0.531 0.596 -0.054 0.094 contestant_Artis Silvester 0.0353 0.037 0.964 0.335 -0.036 0.107 contestant_Ashley Nolan 0.0117 0.031 0.373 0.709 -0.050 0.073 contestant_Ashley Underwood -0.0742 0.083 -0.892 0.372 -0.237 0.089 contestant_Aubry Bracco 0.0498 0.019 2.679 0.007 0.013 0.086 contestant_Baylor Wilson 0.0151 0.022 0.689 0.491 -0.028 0.058 contestant_Ben Driebergen 0.0004 0.021 0.018 0.986 -0.041 0.042 contestant_Bill Posley 0.0160 0.026 0.605 0.545 -0.036 0.068 contestant_Brad Culpepper 0.0091 0.032 0.287 0.774 -0.053 0.071 contestant_Brandon Hantz -0.0390 0.028 -1.391 0.164 -0.094 0.016 contestant_Brenda Lowe 0.0006 0.036 0.016 0.988 -0.070 0.071 contestant_Bret LaBelle -0.0195 0.016 -1.232 0.218 -0.051 0.012 contestant_Brice Johnston 0.0058 0.026 0.221 0.825 -0.046 0.057 contestant_Caleb Bankston -0.0408 0.036 -1.134 0.257 -0.111 0.030 contestant_Caleb Reynolds 0.0166 0.015 1.108 0.268 -0.013 0.046 contestant_Candice Woodcock -0.0797 0.040 -1.971 0.049 -0.159 -0.000 contestant_Carolyn Rivera -0.0273 0.018 -1.478 0.139 -0.064 0.009 contestant_Carter Williams 0.0026 0.025 0.106 0.915 -0.046 0.051 contestant_CeCe Taylor -0.0007 0.018 -0.039 0.969 -0.036 0.035 contestant_Chelsea Meissner 0.0239 0.031 0.783 0.434 -0.036 0.084 contestant_Chris Hammons -0.0063 0.019 -0.331 0.740 -0.043 0.031 contestant_Chrissy Hofbeck 0.0298 0.016 1.848 0.065 -0.002 0.062 contestant_Christina Cha -0.0432 0.028 -1.546 0.122 -0.098 0.012 contestant_Christine Shields-Markoski 0.0463 0.049 0.937 0.349 -0.051 0.143 contestant_Ciera Eastin -0.0178 0.019 -0.911 0.362 -0.056 0.020 contestant_Cirie Fields 0.0444 0.020 2.232 0.026 0.005 0.083 contestant_Cliff Robinson 0.0015 0.033 0.044 0.965 -0.064 0.067 contestant_Coach Wade -0.0196 0.033 -0.590 0.555 -0.085 0.045 contestant_Cole Medders 0.0003 0.020 0.014 0.989 -0.038 0.039 contestant_Colton Cumbie -0.0411 0.028 -1.484 0.138 -0.095 0.013 contestant_Corinne Kaplan -0.0183 0.032 -0.565 0.572 -0.082 0.045 contestant_Cydney Gillon 0.0308 0.016 1.949 0.051 -0.000 0.062 contestant_Dale Wentworth 0.0148 0.024 0.615 0.538 -0.032 0.062 contestant_Dan Foley -0.0113 0.020 -0.563 0.573 -0.051 0.028 contestant_Dana Lambert -0.0373 0.088 -0.424 0.672 -0.210 0.135 contestant_Darnell Hamilton -0.0246 0.015 -1.587 0.112 -0.055 0.006 contestant_David Murphy 0.0523 0.069 0.753 0.452 -0.084 0.188 contestant_David Samson -0.0349 0.031 -1.122 0.262 -0.096 0.026 contestant_David Wright 0.0569 0.018 3.130 0.002 0.021 0.093 contestant_Dawn Meehan -0.0425 0.024 -1.797 0.072 -0.089 0.004 contestant_Debbie Wanner -0.0448 0.027 -1.663 0.096 -0.098 0.008 contestant_Denise Stapley 0.0440 0.052 0.847 0.397 -0.058 0.146 contestant_Desi Williams 0.0157 0.016 0.990 0.322 -0.015 0.047 contestant_Devon Pinto 0.0298 0.023 1.306 0.192 -0.015 0.075 contestant_Drew Christy -0.0250 0.025 -0.981 0.327 -0.075 0.025 contestant_Eddie Fox 0.0120 0.029 0.410 0.682 -0.045 0.069 contestant_Edna Ma 0.0332 0.035 0.951 0.341 -0.035 0.102 contestant_Elyse Umemoto -0.0674 0.081 -0.831 0.406 -0.227 0.092 contestant_Erik Reichenbach 0.0318 0.031 1.025 0.305 -0.029 0.093 contestant_Figgy Figueroa -0.0217 0.025 -0.883 0.377 -0.070 0.026 contestant_Francesca Hogi -0.0353 0.043 -0.828 0.408 -0.119 0.048 contestant_Garrett Adelstein -0.0456 0.019 -2.385 0.017 -0.083 -0.008 contestant_Gervase Peterson -0.0275 0.037 -0.745 0.456 -0.100 0.045 contestant_Grant Mattos -0.0220 0.048 -0.464 0.643 -0.115 0.071 contestant_Hali Ford 0.0586 0.024 2.418 0.016 0.011 0.106 contestant_Hannah Shapiro 0.0074 0.014 0.526 0.599 -0.020 0.035 contestant_Hayden Moss -0.0014 0.039 -0.036 0.971 -0.079 0.076 contestant_Hope Driskill 0.0293 0.031 0.933 0.351 -0.032 0.091 contestant_J'Tia Taylor 0.0028 0.032 0.086 0.931 -0.061 0.066 contestant_J.T. Thomas -0.0021 0.026 -0.083 0.934 -0.052 0.048 contestant_JP Hilsabeck -0.0017 0.014 -0.119 0.905 -0.030 0.026 contestant_Jaclyn Schultz -0.0293 0.023 -1.271 0.204 -0.075 0.016 contestant_Jay Byars -0.0297 0.028 -1.054 0.292 -0.085 0.026 contestant_Jay Starrett 0.0409 0.027 1.544 0.123 -0.011 0.093 contestant_Jeff Kent 0.0011 0.021 0.053 0.958 -0.039 0.042 contestant_Jeff Varner -0.0108 0.018 -0.601 0.548 -0.046 0.024 contestant_Jefra Bland 0.0007 0.024 0.028 0.978 -0.046 0.047 contestant_Jenn Brown 0.0084 0.028 0.303 0.762 -0.046 0.062 contestant_Jennifer Lanzetti 0.0024 0.027 0.088 0.930 -0.051 0.056 contestant_Jeremiah Wood -0.0015 0.022 -0.066 0.948 -0.045 0.042 contestant_Jeremy Collins 0.0159 0.014 1.134 0.257 -0.012 0.043 contestant_Jessica Johnston -0.0510 0.020 -2.551 0.011 -0.090 -0.012 contestant_Jessica Lewis 0.0062 0.018 0.336 0.737 -0.030 0.042 contestant_Jim Rice 0.0176 0.037 0.478 0.633 -0.055 0.090 contestant_Joaquin Souberbielle -0.0288 0.025 -1.171 0.242 -0.077 0.019 contestant_Joe Anglim -0.0048 0.024 -0.205 0.838 -0.051 0.042 contestant_Joe Mena 0.0235 0.016 1.490 0.136 -0.007 0.054 contestant_Joe del Campo 0.0089 0.018 0.498 0.618 -0.026 0.044 contestant_John Cochran 0.0022 0.022 0.105 0.917 -0.040 0.044 contestant_John Cody 0.0312 0.044 0.714 0.475 -0.054 0.117 contestant_John Rocker -0.0111 0.020 -0.557 0.578 -0.050 0.028 contestant_Jon Misch 0.0368 0.023 1.616 0.106 -0.008 0.081 contestant_Jonas Otsuji 0.0283 0.025 1.129 0.259 -0.021 0.078 contestant_Jonathan Penner 0.0192 0.022 0.856 0.392 -0.025 0.063 contestant_Josh Canfield -0.0031 0.022 -0.143 0.887 -0.046 0.039 contestant_Julia Landauer -0.0522 0.026 -1.982 0.047 -0.104 -0.001 contestant_Julia Sokolowski -0.0884 0.050 -1.782 0.075 -0.186 0.009 contestant_Julie McGee -0.0030 0.022 -0.138 0.890 -0.046 0.040 contestant_Julie Wolfe 0.0385 0.077 0.499 0.618 -0.113 0.190 contestant_Kass McQuillen 0.0241 0.026 0.945 0.345 -0.026 0.074 contestant_Kat Edorsson 0.0069 0.029 0.239 0.811 -0.050 0.064 contestant_Katie Collins -0.0325 0.028 -1.142 0.253 -0.088 0.023 contestant_Katie Hanson 0.0014 0.039 0.035 0.972 -0.075 0.077 contestant_Katrina Radke -0.0231 0.016 -1.410 0.159 -0.055 0.009 contestant_Keith Nale 0.0186 0.018 1.042 0.297 -0.016 0.054 contestant_Keith Tollefson 0.0616 0.042 1.464 0.143 -0.021 0.144 contestant_Kelley Wentworth 0.0106 0.021 0.501 0.616 -0.031 0.052 contestant_Kelly Remington 0.0250 0.029 0.869 0.385 -0.031 0.081 contestant_Kelly Wiglesworth 0.0012 0.038 0.033 0.974 -0.073 0.076 contestant_Ken McNickle -0.0330 0.015 -2.175 0.030 -0.063 -0.003 contestant_Kim Spradlin-Wolfe 0.0486 0.047 1.034 0.301 -0.043 0.141 contestant_Kimmi Kappenberg -0.0395 0.028 -1.435 0.151 -0.094 0.014 contestant_Kourtney Moon 0.1659 0.089 1.871 0.061 -0.008 0.340 contestant_Kristina Kell 0.0449 0.127 0.354 0.724 -0.204 0.294 contestant_Kyle Jason -0.0103 0.020 -0.527 0.598 -0.049 0.028 contestant_LJ McKanas 0.0139 0.019 0.738 0.460 -0.023 0.051 contestant_Laura Alexander 0.0241 0.038 0.630 0.529 -0.051 0.099 contestant_Laura Boneham 0.0181 0.025 0.735 0.463 -0.030 0.066 contestant_Laura Morett -0.0409 0.039 -1.039 0.299 -0.118 0.036 contestant_Lauren Rimmer -0.0490 0.062 -0.794 0.427 -0.170 0.072 contestant_Leif Manson -0.0294 0.026 -1.114 0.265 -0.081 0.022 contestant_Lindsey Cascaddan 0.0197 0.020 0.974 0.330 -0.020 0.059 contestant_Lindsey Ogle -0.0183 0.027 -0.670 0.503 -0.072 0.035 contestant_Lisa Whelchel 0.0073 0.032 0.227 0.821 -0.056 0.070 contestant_Liz Markham 0.0437 0.020 2.221 0.026 0.005 0.082 contestant_Lucy Huang 0.0024 0.027 0.091 0.928 -0.050 0.055 contestant_Malcolm Freberg 0.0101 0.024 0.418 0.676 -0.037 0.057 contestant_Mari Takahashi -0.0160 0.021 -0.768 0.442 -0.057 0.025 contestant_Marissa Peterson 0.0157 0.033 0.473 0.636 -0.049 0.081 contestant_Matt Bischoff 0.0047 0.028 0.166 0.868 -0.051 0.060 contestant_Matt Quinlan 0.0152 0.043 0.354 0.724 -0.069 0.100 contestant_Max Dawson -0.0007 0.022 -0.032 0.975 -0.045 0.043 contestant_Michael Jefferson -0.0027 0.038 -0.071 0.944 -0.078 0.072 contestant_Michael Skupin 0.0263 0.033 0.792 0.428 -0.039 0.091 contestant_Michael Snow 0.0238 0.026 0.909 0.364 -0.028 0.075 contestant_Michaela Bradshaw 0.0263 0.015 1.750 0.080 -0.003 0.056 contestant_Michele Fitzgerald -0.0543 0.040 -1.354 0.176 -0.133 0.024 contestant_Michelle Schubert -0.0109 0.018 -0.623 0.533 -0.045 0.023 contestant_Mikayla Wingle -0.0052 0.044 -0.118 0.906 -0.091 0.081 contestant_Mike Chiesl -0.1280 0.084 -1.517 0.129 -0.293 0.037 contestant_Mike Holloway -0.0023 0.026 -0.089 0.929 -0.054 0.049 contestant_Mike Zahalsky 0.0066 0.019 0.341 0.733 -0.031 0.045 contestant_Missy Payne 0.0369 0.028 1.302 0.193 -0.019 0.093 contestant_Monica Culpepper -0.0375 0.038 -0.986 0.324 -0.112 0.037 contestant_Monica Padilla 0.0194 0.019 1.041 0.298 -0.017 0.056 contestant_Morgan McLeod -0.0428 0.030 -1.430 0.153 -0.102 0.016 contestant_Nadiya Anderson 0.0227 0.023 0.967 0.334 -0.023 0.069 contestant_Natalie Anderson -0.0337 0.026 -1.295 0.195 -0.085 0.017 contestant_Natalie Tenerelli 0.1247 0.094 1.333 0.182 -0.059 0.308 contestant_Neal Gottlieb -0.0248 0.023 -1.095 0.274 -0.069 0.020 contestant_Nick Maiorano -0.0380 0.026 -1.441 0.150 -0.090 0.014 contestant_Nina Acosta -0.1009 0.089 -1.134 0.257 -0.275 0.074 contestant_Nina Poersch -0.0271 0.016 -1.644 0.100 -0.059 0.005 contestant_Ozzy Lusth 0.0269 0.027 0.983 0.325 -0.027 0.080 contestant_Patrick Bolton -0.0271 0.020 -1.356 0.175 -0.066 0.012 contestant_Paul Wachter 0.0136 0.020 0.693 0.488 -0.025 0.052 contestant_Peih-Gee Law -0.0141 0.031 -0.449 0.653 -0.075 0.047 contestant_Pete Yurkowski 0.0331 0.036 0.924 0.355 -0.037 0.103 contestant_Peter Baggenstos -0.0120 0.023 -0.512 0.609 -0.058 0.034 contestant_Phillip Sheppard -0.0337 0.031 -1.098 0.272 -0.094 0.026 contestant_R.C. Saint-Amour -0.0198 0.079 -0.251 0.802 -0.175 0.135 contestant_Rachel Ako -0.0010 0.017 -0.059 0.953 -0.034 0.032 contestant_Rachel Foulger -0.0140 0.031 -0.449 0.654 -0.075 0.047 contestant_Ralph Kiser -0.0238 0.054 -0.437 0.662 -0.130 0.083 contestant_Reed Kelly 0.0153 0.025 0.621 0.535 -0.033 0.064 contestant_Reynold Toepfer -0.0156 0.029 -0.536 0.592 -0.073 0.042 contestant_Rick Nelson -0.0125 0.035 -0.357 0.721 -0.081 0.056 contestant_Roark Luskin 0.0097 0.013 0.755 0.450 -0.015 0.035 contestant_Rob Mariano -0.0323 0.039 -0.823 0.411 -0.109 0.045 contestant_Rodney Lavoie Jr. 0.0215 0.022 0.997 0.319 -0.021 0.064 contestant_Roxanne Morris 0.0406 0.086 0.475 0.635 -0.127 0.208 contestant_Rupert Boneham 0.0621 0.034 1.831 0.067 -0.004 0.129 contestant_Russell Hantz 0.0371 0.047 0.784 0.433 -0.056 0.130 contestant_Russell Swan -0.0213 0.035 -0.615 0.538 -0.089 0.047 contestant_Ryan Ulrich -0.0021 0.015 -0.145 0.885 -0.031 0.027 contestant_Sabrina Thompson 0.0320 0.032 1.008 0.313 -0.030 0.094 contestant_Sandra Diaz-Twine 0.0139 0.015 0.896 0.370 -0.016 0.044 contestant_Sarah Dawson -0.0050 0.047 -0.106 0.915 -0.098 0.088 contestant_Sarah Lacina 0.0572 0.027 2.108 0.035 0.004 0.110 contestant_Sarita White 0.0675 0.130 0.519 0.604 -0.187 0.322 contestant_Scot Pollard 0.0273 0.020 1.371 0.170 -0.012 0.066 contestant_Semhar Tadesse 0.0077 0.038 0.202 0.840 -0.067 0.082 contestant_Shamar Thomas 0.0311 0.030 1.054 0.292 -0.027 0.089 contestant_Sherri Biethman -0.0359 0.035 -1.038 0.299 -0.104 0.032 contestant_Shirin Oskooi 0.0060 0.013 0.459 0.646 -0.020 0.032 contestant_Sierra Thomas 0.0170 0.020 0.861 0.389 -0.022 0.056 contestant_Simone Nguyen 0.0167 0.022 0.776 0.438 -0.026 0.059 contestant_So Kim -0.0096 0.017 -0.555 0.579 -0.043 0.024 contestant_Sophie Clarke -0.0250 0.039 -0.632 0.527 -0.102 0.052 contestant_Spencer Bledsoe -0.0002 0.016 -0.010 0.992 -0.032 0.031 contestant_Stacey Powell -0.0213 0.060 -0.353 0.724 -0.140 0.097 contestant_Stephanie Valencia -0.0289 0.069 -0.421 0.674 -0.163 0.106 contestant_Stephen Fishbach 0.0148 0.018 0.807 0.420 -0.021 0.051 contestant_Steve Wright -0.0214 0.074 -0.289 0.772 -0.166 0.124 contestant_Sunday Burquest -0.0153 0.017 -0.911 0.362 -0.048 0.018 contestant_Tai Trang -0.0311 0.027 -1.132 0.258 -0.085 0.023 contestant_Tarzan Smith -0.0050 0.031 -0.158 0.874 -0.066 0.056 contestant_Tasha Fox 0.0113 0.031 0.366 0.715 -0.049 0.072 contestant_Taylor Stocker -0.0418 0.018 -2.389 0.017 -0.076 -0.008 contestant_Terry Deitz -0.0157 0.027 -0.579 0.563 -0.069 0.037 contestant_Tina Wesson 0.0099 0.041 0.239 0.811 -0.071 0.091 contestant_Tony Vlachos 0.0157 0.019 0.823 0.411 -0.022 0.053 contestant_Trish Hegarty 0.0081 0.024 0.329 0.742 -0.040 0.056 contestant_Troyzan Robertson -0.0416 0.029 -1.439 0.150 -0.098 0.015 contestant_Tyler Fredrickson 0.0244 0.013 1.892 0.058 -0.001 0.050 contestant_Tyson Apostol -0.0129 0.034 -0.382 0.702 -0.079 0.053 contestant_Val Collins -0.0129 0.019 -0.661 0.508 -0.051 0.025 contestant_Vince Sly 0.0313 0.015 2.116 0.034 0.002 0.060 contestant_Vytas Baskauskas 0.0473 0.041 1.147 0.252 -0.034 0.128 contestant_Wes Nale 0.0213 0.026 0.813 0.416 -0.030 0.073 contestant_Whitney Duncan -0.0361 0.043 -0.833 0.405 -0.121 0.049 contestant_Will Sims II -0.0449 0.019 -2.417 0.016 -0.081 -0.008 contestant_Will Wahl -0.0158 0.017 -0.921 0.357 -0.049 0.018 contestant_Woo Hwang 0.0316 0.016 1.948 0.051 -0.000 0.063 contestant_Zane Knight -0.0253 0.045 -0.567 0.571 -0.113 0.062 contestant_Zeke Smith -0.0127 0.022 -0.585 0.559 -0.055 0.030 role_Chelsea Handler -0.0170 0.013 -1.354 0.176 -0.042 0.008 role_Crazy Cat Lady -0.0050 0.013 -0.371 0.710 -0.031 0.021 role_Culturally Awkward Girl 0.0166 0.026 0.637 0.524 -0.034 0.068 role_Erin Brokovich 0.0032 0.020 0.161 0.872 -0.036 0.042 role_G.I. Jane 0.0041 0.012 0.355 0.722 -0.019 0.027 role_Good Ol' Boy -0.0150 0.013 -1.154 0.249 -0.040 0.010 role_Heisenberg 0.0051 0.015 0.352 0.725 -0.023 0.034 role_I Can See Your Periscope -0.0134 0.026 -0.512 0.608 -0.065 0.038 role_John McClane 0.0162 0.013 1.293 0.196 -0.008 0.041 role_Know It All 0.0264 0.011 2.492 0.013 0.006 0.047 role_Lady Gaga 0.0292 0.025 1.177 0.239 -0.019 0.078 role_Little Red Riding Hood 0.0177 0.015 1.189 0.235 -0.011 0.047 role_Meredith Grey 0.0252 0.013 1.912 0.056 -0.001 0.051 role_Mommy Dearest 0.0104 0.011 0.923 0.356 -0.012 0.032 role_Mr. Miagi -0.0001 0.012 -0.009 0.993 -0.024 0.024 role_Oh No You Didn't! -0.0190 0.015 -1.297 0.195 -0.048 0.010 role_Ponyboy -0.0068 0.014 -0.487 0.626 -0.034 0.021 role_Secretly Smart Bikini Babe -0.0028 0.020 -0.140 0.889 -0.042 0.036 role_Seduce and Destroy 0.0218 0.012 1.786 0.074 -0.002 0.046 role_Siren -0.0001 0.011 -0.012 0.990 -0.022 0.022 role_Surfer Dude 0.0255 0.012 2.204 0.028 0.003 0.048 role_The Social Butterfly 0.0169 0.014 1.246 0.213 -0.010 0.043 role_The Specialist 0.0097 0.011 0.893 0.372 -0.012 0.031 role_Tough Old Broad 0.0009 0.019 0.046 0.963 -0.037 0.039 role_True Grit -0.0075 0.014 -0.516 0.606 -0.036 0.021 days_lasted_10-20 days -0.0075 0.011 -0.653 0.514 -0.030 0.015 days_lasted_20-30 days -0.0059 0.014 -0.414 0.679 -0.034 0.022 days_lasted_30+ days -0.0283 0.030 -0.942 0.346 -0.087 0.031 season_Cagayan -0.0112 0.013 -0.853 0.394 -0.037 0.015 season_Cambodia 0.0065 0.012 0.534 0.594 -0.017 0.030 season_Caramoan -0.0074 0.017 -0.428 0.668 -0.041 0.026 season_Game Changers 0.0026 0.011 0.228 0.820 -0.019 0.025 season_Heroes vs. Healers vs. Hustlers 0.0275 0.011 2.588 0.010 0.007 0.048 season_Kaôh Rōng 0.0141 0.007 2.087 0.037 0.001 0.027 season_Millennials vs. Gen X 0.0125 0.007 1.785 0.074 -0.001 0.026 season_One World -0.0106 0.015 -0.695 0.487 -0.040 0.019 season_Philippines -0.0174 0.017 -1.053 0.292 -0.050 0.015 season_Redemption Island 0.0167 0.057 0.295 0.768 -0.094 0.128 season_San Juan del Sur -0.0079 0.008 -0.943 0.346 -0.024 0.009 season_South Pacific -0.0036 0.017 -0.207 0.836 -0.038 0.031 season_Worlds Apart -0.0128 0.006 -1.979 0.048 -0.025 -0.000 tribe_Bayon -0.0125 0.013 -0.962 0.336 -0.038 0.013 tribe_Bikal -0.0163 0.025 -0.660 0.510 -0.065 0.032 tribe_Chan Loh 0.0128 0.011 1.215 0.224 -0.008 0.034 tribe_Coyopa -0.0023 0.007 -0.315 0.752 -0.017 0.012 tribe_Escameca 0.0039 0.011 0.362 0.717 -0.017 0.025 tribe_Galang 0.0007 0.018 0.037 0.971 -0.034 0.036 tribe_Gondol -0.0009 0.013 -0.069 0.945 -0.027 0.026 tribe_Gota 0.0089 0.011 0.813 0.416 -0.013 0.030 tribe_Hunahpu -0.0056 0.011 -0.528 0.597 -0.026 0.015 tribe_Kalabaw -0.0180 0.023 -0.773 0.439 -0.064 0.028 tribe_Levu 0.0189 0.012 1.626 0.104 -0.004 0.042 tribe_Luzon -0.0051 0.014 -0.358 0.720 -0.033 0.023 tribe_Mana 0.0148 0.010 1.527 0.127 -0.004 0.034 tribe_Manono -0.0084 0.013 -0.627 0.531 -0.035 0.018 tribe_Masaya 0.0043 0.011 0.392 0.695 -0.017 0.026 tribe_Matsing 0.0239 0.023 1.041 0.298 -0.021 0.069 tribe_Nagarote -0.0210 0.010 -2.085 0.037 -0.041 -0.001 tribe_Nuku -0.0122 0.015 -0.837 0.402 -0.041 0.016 tribe_Ometepe 0.0234 0.059 0.396 0.692 -0.092 0.139 tribe_Salani -0.0021 0.023 -0.094 0.925 -0.046 0.042 tribe_Savaii 0.0014 0.020 0.070 0.944 -0.037 0.040 tribe_Soko 0.0047 0.011 0.441 0.659 -0.016 0.025 tribe_Solana 0.0263 0.016 1.625 0.104 -0.005 0.058 tribe_Ta Keo 0.0190 0.012 1.626 0.104 -0.004 0.042 tribe_Tadhana 0.0297 0.013 2.232 0.026 0.004 0.056 tribe_Takali 0.0034 0.008 0.420 0.674 -0.012 0.019 tribe_Tandang -0.0233 0.012 -1.954 0.051 -0.047 7.22e-05 tribe_To Tang 0.0021 0.009 0.248 0.805 -0.015 0.019 tribe_Upolu -0.0050 0.018 -0.283 0.777 -0.039 0.030 tribe_Vanua 0.0092 0.011 0.838 0.402 -0.012 0.031 tribe_Yawa 0.0039 0.010 0.380 0.704 -0.016 0.024 tribe_Zapatera -0.0067 0.020 -0.329 0.742 -0.047 0.033 tribe_Angkor -0.0168 0.020 -0.823 0.410 -0.057 0.023 tribe_Dangrayne -0.0004 0.017 -0.022 0.982 -0.034 0.034 tribe_Dara -0.0025 0.016 -0.156 0.876 -0.034 0.029 tribe_Enil Edam 0.0102 0.040 0.258 0.797 -0.068 0.088 tribe_Huyopa -0.0505 0.013 -3.861 0.000 -0.076 -0.025 tribe_Ikabula 0.0266 0.016 1.681 0.093 -0.004 0.058 tribe_Kasama 0.0172 0.026 0.662 0.508 -0.034 0.068 tribe_Maku Maku -0.0274 0.018 -1.535 0.125 -0.062 0.008 tribe_Merica 0.0085 0.014 0.584 0.559 -0.020 0.037 tribe_Murlonio -0.0055 0.122 -0.045 0.964 -0.245 0.234 tribe_Orkun 0.0061 0.021 0.297 0.767 -0.034 0.046 tribe_Solarrion -0.0162 0.015 -1.064 0.288 -0.046 0.014 tribe_Solewa -0.0373 0.035 -1.062 0.288 -0.106 0.032 tribe_Tavua -0.0113 0.021 -0.540 0.589 -0.052 0.030 tribe_Te Tuna 0.0363 0.040 0.920 0.358 -0.041 0.114 tribe_Tikiano 0.0252 0.021 1.214 0.225 -0.015 0.066 tribe_Vinaka -0.0063 0.012 -0.547 0.584 -0.029 0.016 alliance_Bayon Alliance -0.0023 0.024 -0.094 0.925 -0.050 0.045 alliance_Cool Kids Alliance 0.0134 0.014 0.944 0.345 -0.014 0.041 alliance_Cops-R-Us 0.0094 0.014 0.663 0.507 -0.018 0.037 alliance_Coyopa Guys Alliance 0.0018 0.014 0.127 0.899 -0.025 0.029 alliance_Dara Women's Alliance -0.0175 0.018 -0.986 0.324 -0.052 0.017 alliance_David's Vinaka Alliance 0.0381 0.012 3.157 0.002 0.014 0.062 alliance_Escameca Alliance -0.0188 0.014 -1.363 0.173 -0.046 0.008 alliance_Fab Five 0.0109 0.015 0.729 0.466 -0.018 0.040 alliance_Five Guys Alliance -0.0268 0.036 -0.753 0.451 -0.097 0.043 alliance_Galang Alliance -0.0220 0.027 -0.826 0.409 -0.074 0.030 alliance_Gondol Alliance 0.0049 0.014 0.357 0.721 -0.022 0.032 alliance_Gota Alliance -0.0045 0.013 -0.355 0.723 -0.029 0.020 alliance_Kalabaw Alliance 0.0230 0.017 1.349 0.177 -0.010 0.056 alliance_Matsing Alliance 0.0299 0.036 0.830 0.406 -0.041 0.101 alliance_Misfit Alliance -0.0073 0.014 -0.506 0.613 -0.036 0.021 alliance_Muscle Alliance -0.0011 0.017 -0.069 0.945 -0.033 0.031 alliance_Nagarote Alliance 0.0212 0.013 1.622 0.105 -0.004 0.047 alliance_Power Six -0.0105 0.024 -0.438 0.661 -0.057 0.036 alliance_Russell's Zapatera Alliance 0.0082 0.047 0.175 0.861 -0.084 0.100 alliance_Salani Alliance -0.0087 0.032 -0.272 0.785 -0.071 0.054 alliance_Savaii Alliance -0.0063 0.032 -0.200 0.841 -0.068 0.056 alliance_Singles Alliance 0.0285 0.025 1.144 0.252 -0.020 0.077 alliance_Soko Alliance -0.0050 0.013 -0.391 0.696 -0.030 0.020 alliance_Solana Alliance -0.0104 0.014 -0.728 0.466 -0.038 0.018 alliance_Stealth R Us 0.0308 0.031 0.983 0.326 -0.031 0.092 alliance_Takali Alliance -0.0189 0.013 -1.502 0.133 -0.043 0.006 alliance_Tandang Alliance -0.0233 0.012 -1.954 0.051 -0.047 7.22e-05 alliance_Tavua Alliance -0.0028 0.021 -0.134 0.894 -0.043 0.038 alliance_The Family -0.0248 0.034 -0.722 0.470 -0.092 0.042 alliance_The Round Table 0.0190 0.016 1.208 0.227 -0.012 0.050 alliance_Triforce -0.0204 0.012 -1.678 0.093 -0.044 0.003 alliance_Witches Coven 0.0350 0.023 1.552 0.121 -0.009 0.079 alliance_Zapatera Six -0.0149 0.047 -0.317 0.751 -0.107 0.077 alliance_Zeke's Vinaka Alliance 0.0314 0.020 1.592 0.111 -0.007 0.070 alliance_Final Four Alliance 0.0231 0.012 1.848 0.065 -0.001 0.048 alliance_Fulcrum Alliance 0.0138 0.037 0.370 0.711 -0.059 0.087 contestant_episode_Abi-Maria Gomes My Wheels Are Spinning 0.0459 0.046 1.002 0.316 -0.044 0.136 contestant_episode_Abi-Maria Gomes Play to Win -0.0301 0.048 -0.626 0.531 -0.124 0.064 contestant_episode_Abi-Maria Gomes Survivor MacGyver -0.0338 0.040 -0.844 0.399 -0.112 0.045 contestant_episode_Abi-Maria Gomes Tiny Little Shanks to the Heart 0.0279 0.046 0.605 0.545 -0.062 0.118 contestant_episode_Abi-Maria Gomes Villains Have More Fun 0.0646 0.047 1.368 0.171 -0.028 0.157 contestant_episode_Abi-Maria Gomes We Got a Rat -0.0116 0.041 -0.280 0.780 -0.093 0.070 contestant_episode_Abi-Maria Gomes What's the Beef%3F -0.0578 0.043 -1.358 0.174 -0.141 0.026 contestant_episode_Abi-Maria Gomes Witches Coven (episode) 0.0332 0.055 0.599 0.549 -0.075 0.142 contestant_episode_Abi-Maria Gomes You Call, We'll Haul -0.0901 0.048 -1.861 0.063 -0.185 0.005 contestant_episode_Adam Klein I Will Destroy You -0.0526 0.018 -2.942 0.003 -0.088 -0.018 contestant_episode_Adam Klein I'm the Kingpin -0.0670 0.014 -4.880 0.000 -0.094 -0.040 contestant_episode_Adam Klein Idol Search Party -0.0070 0.019 -0.362 0.718 -0.045 0.031 contestant_episode_Adam Klein Love Goggles -0.0813 0.026 -3.129 0.002 -0.132 -0.030 contestant_episode_Adam Klein May the Best Generation Win 9.249e-05 0.025 0.004 0.997 -0.048 0.049 contestant_episode_Adam Klein Million Dollar Gamble -0.0764 0.017 -4.577 0.000 -0.109 -0.044 contestant_episode_Adam Klein Slayed the Survivor Dragon -0.0104 0.019 -0.562 0.574 -0.047 0.026 contestant_episode_Adam Klein Still Throwin' Punches -0.0862 0.012 -6.910 0.000 -0.111 -0.062 contestant_episode_Adam Klein The Truth Works Well -0.0275 0.022 -1.243 0.214 -0.071 0.016 contestant_episode_Adam Klein Who's the Sucker at the Table%3F -0.0223 0.026 -0.851 0.395 -0.074 0.029 contestant_episode_Adam Klein Your Job Is Recon -0.0148 0.024 -0.613 0.540 -0.062 0.032 contestant_episode_Alan Ball I'm Not Crazy, I'm Confident -0.0167 0.014 -1.161 0.246 -0.045 0.011 contestant_episode_Alan Ball I'm a Wild Banshee -0.0325 0.024 -1.381 0.167 -0.079 0.014 contestant_episode_Alan Ball My Kisses Are Very Private -0.0014 0.023 -0.059 0.953 -0.047 0.044 contestant_episode_Alec Christy Blood Is Blood 0.0570 0.044 1.297 0.195 -0.029 0.143 contestant_episode_Alec Christy Gettin' to Crunch Time -0.0274 0.037 -0.742 0.458 -0.100 0.045 contestant_episode_Alec Christy Make Some Magic Happen 0.0277 0.047 0.583 0.560 -0.065 0.121 contestant_episode_Alec Christy Method to This Madness -0.0108 0.046 -0.234 0.815 -0.102 0.080 contestant_episode_Alec Christy Million Dollar Decision 0.0162 0.039 0.418 0.676 -0.060 0.092 contestant_episode_Alec Christy Still Holdin' On 0.0304 0.039 0.788 0.431 -0.045 0.106 contestant_episode_Alec Christy Suck It Up and Survive -0.0105 0.050 -0.210 0.834 -0.108 0.087 contestant_episode_Alec Christy This Is Where We Build Trust 0.0103 0.038 0.268 0.788 -0.065 0.085 contestant_episode_Alec Christy We're a Hot Mess 0.0047 0.037 0.127 0.899 -0.068 0.077 contestant_episode_Alec Christy Wrinkle in the Plan -0.0040 0.036 -0.112 0.911 -0.074 0.066 contestant_episode_Alecia Holden Kindergarten Camp 0.0368 0.014 2.697 0.007 0.010 0.063 contestant_episode_Alecia Holden Signed, Sealed and Delivered -0.0198 0.014 -1.461 0.144 -0.046 0.007 contestant_episode_Alecia Holden The Circle of Life 0.0324 0.017 1.883 0.060 -0.001 0.066 contestant_episode_Alexis Maxwell Hot Girl with a Grudge 0.0329 0.065 0.508 0.612 -0.094 0.160 contestant_episode_Alexis Maxwell Odd One Out 0.0625 0.055 1.136 0.256 -0.045 0.170 contestant_episode_Alexis Maxwell Our Time to Shine -0.0249 0.059 -0.422 0.673 -0.141 0.091 contestant_episode_Alexis Maxwell We Found Our Zombies -0.0354 0.049 -0.729 0.466 -0.131 0.060 contestant_episode_Ali Elliott I'm Not Crazy, I'm Confident -0.0450 0.023 -1.929 0.054 -0.091 0.001 contestant_episode_Ali Elliott I'm a Wild Banshee 0.0027 0.021 0.128 0.898 -0.038 0.043 contestant_episode_Ali Elliott My Kisses Are Very Private -0.0239 0.020 -1.182 0.237 -0.063 0.016 contestant_episode_Ali Elliott The Past Will Eat You Alive -0.0667 0.018 -3.666 0.000 -0.102 -0.031 contestant_episode_Ali Elliott This Is Why You Play Survivor -0.0972 0.021 -4.637 0.000 -0.138 -0.056 contestant_episode_Alicia Rosa Go Out with a Bang 0.0166 0.057 0.291 0.771 -0.095 0.128 contestant_episode_Alicia Rosa I'm No Dummy 0.0097 0.050 0.193 0.847 -0.089 0.108 contestant_episode_Alicia Rosa It's Gonna Be Chaos 0.0574 0.056 1.017 0.309 -0.053 0.168 contestant_episode_Alicia Rosa It's Human Nature 0.0694 0.049 1.422 0.155 -0.026 0.165 contestant_episode_Alicia Rosa Never Say Die -0.0164 0.046 -0.359 0.720 -0.106 0.073 contestant_episode_Alicia Rosa Thanks for the Souvenir 0.0218 0.047 0.463 0.643 -0.070 0.114 contestant_episode_Alicia Rosa Two Tribes, One Camp, No Rules -0.0167 0.060 -0.279 0.780 -0.134 0.101 contestant_episode_Andrea Boehlke Blindside Time -0.0136 0.051 -0.266 0.791 -0.114 0.087 contestant_episode_Andrea Boehlke Come Over to the Dark Side 0.0642 0.044 1.466 0.143 -0.022 0.150 contestant_episode_Andrea Boehlke Cut Off the Head of the Snake 0.0385 0.044 0.875 0.382 -0.048 0.125 contestant_episode_Andrea Boehlke Dirty Deed 0.0469 0.024 1.928 0.054 -0.001 0.095 contestant_episode_Andrea Boehlke It Is Not a High Without a Low 0.0628 0.019 3.252 0.001 0.025 0.101 contestant_episode_Andrea Boehlke Kill or Be Killed -0.0635 0.066 -0.965 0.335 -0.193 0.066 contestant_episode_Andrea Boehlke Parting Is Such Sweet Sorrow 0.0874 0.029 3.029 0.002 0.031 0.144 contestant_episode_Andrea Boehlke Persona Non Grata -0.0554 0.064 -0.869 0.385 -0.180 0.070 contestant_episode_Andrea Boehlke Reinventing How This Game Is Played 0.0518 0.025 2.110 0.035 0.004 0.100 contestant_episode_Andrea Boehlke She Annoys Me Greatly -0.0553 0.052 -1.057 0.291 -0.158 0.047 contestant_episode_Andrea Boehlke Survivor Jackpot 0.0650 0.031 2.131 0.033 0.005 0.125 contestant_episode_Andrea Boehlke The Beginning of the End 0.0474 0.045 1.044 0.296 -0.042 0.136 contestant_episode_Andrea Boehlke The Stakes Have Been Raised 0.0660 0.035 1.883 0.060 -0.003 0.135 contestant_episode_Andrea Boehlke The Tables Have Turned 0.0930 0.033 2.789 0.005 0.028 0.158 contestant_episode_Andrea Boehlke There's a New Sheriff in Town 0.0718 0.032 2.228 0.026 0.009 0.135 contestant_episode_Andrea Boehlke Vote Early, Vote Often 0.0670 0.030 2.270 0.023 0.009 0.125 contestant_episode_Andrea Boehlke What Happened on Exile, Stays on Exile 0.0201 0.021 0.965 0.335 -0.021 0.061 contestant_episode_Andrea Boehlke Zipping Over the Cuckoo's Nest 0.0577 0.045 1.294 0.196 -0.030 0.145 contestant_episode_Andrew Savage Bunking with the Devil -0.0225 0.025 -0.912 0.362 -0.071 0.026 contestant_episode_Andrew Savage Play to Win -0.0044 0.020 -0.217 0.828 -0.044 0.035 contestant_episode_Andrew Savage Survivor MacGyver -0.0116 0.020 -0.580 0.562 -0.051 0.028 contestant_episode_Andrew Savage We Got a Rat -0.0131 0.023 -0.579 0.562 -0.058 0.031 contestant_episode_Andrew Savage What's the Beef%3F -0.0749 0.072 -1.036 0.300 -0.217 0.067 contestant_episode_Andrew Savage You Call, We'll Haul 0.0285 0.020 1.459 0.144 -0.010 0.067 contestant_episode_Angie Layton This Isn't a 'We' Game 0.1000 0.061 1.647 0.100 -0.019 0.219 contestant_episode_Anna Khait Kindergarten Camp 0.0138 0.031 0.443 0.658 -0.047 0.075 contestant_episode_Anna Khait Signed, Sealed and Delivered -0.0042 0.032 -0.133 0.894 -0.067 0.059 contestant_episode_Anna Khait The Circle of Life 0.0336 0.032 1.052 0.293 -0.029 0.096 contestant_episode_Anna Khait The Devils We Know -0.0328 0.021 -1.555 0.120 -0.074 0.009 contestant_episode_Aras Baskauskas Blood Is Thicker than Anything 0.0072 0.057 0.127 0.899 -0.104 0.119 contestant_episode_Aras Baskauskas One Armed Dude and Three Moms 0.0330 0.065 0.511 0.609 -0.094 0.160 contestant_episode_Aras Baskauskas One-Man Wrecking Ball -0.0074 0.050 -0.148 0.883 -0.105 0.091 contestant_episode_Aras Baskauskas Opening Pandora's Box -0.0112 0.054 -0.206 0.837 -0.117 0.095 contestant_episode_Aras Baskauskas Rule in Chaos 0.0181 0.058 0.312 0.755 -0.096 0.132 contestant_episode_Aras Baskauskas Skin of My Teeth -0.0173 0.046 -0.377 0.706 -0.107 0.073 contestant_episode_Aras Baskauskas Swoop In for the Kill -0.0134 0.048 -0.281 0.779 -0.107 0.080 contestant_episode_Aras Baskauskas The Dead Can Still Talk -0.0114 0.055 -0.205 0.838 -0.120 0.097 contestant_episode_Artis Silvester Little Miss Perfect -0.0106 0.045 -0.236 0.814 -0.099 0.078 contestant_episode_Artis Silvester Not the Only Actor on This Island -0.0224 0.061 -0.370 0.711 -0.141 0.096 contestant_episode_Ashley Nolan Fear of the Unknown -0.0030 0.024 -0.126 0.900 -0.050 0.044 contestant_episode_Ashley Nolan Get to Gettin' -0.0086 0.025 -0.341 0.733 -0.058 0.041 contestant_episode_Ashley Nolan I Don't Like Having Snakes Around -0.0165 0.023 -0.726 0.468 -0.061 0.028 contestant_episode_Ashley Nolan I'm Not Crazy, I'm Confident -0.0098 0.025 -0.385 0.700 -0.060 0.040 contestant_episode_Ashley Nolan I'm a Wild Banshee -0.0452 0.036 -1.244 0.213 -0.116 0.026 contestant_episode_Ashley Nolan My Kisses Are Very Private -0.0098 0.029 -0.341 0.733 -0.066 0.047 contestant_episode_Ashley Nolan Not Going to Roll Over and Die -0.0126 0.025 -0.507 0.612 -0.062 0.036 contestant_episode_Ashley Nolan Playing with the Devil -0.0161 0.024 -0.676 0.499 -0.063 0.031 contestant_episode_Ashley Nolan The Past Will Eat You Alive -0.0558 0.030 -1.858 0.063 -0.115 0.003 contestant_episode_Ashley Nolan The Survivor Devil -0.0272 0.014 -1.907 0.056 -0.055 0.001 contestant_episode_Ashley Nolan This Is Why You Play Survivor 0.0034 0.024 0.142 0.887 -0.043 0.050 contestant_episode_Aubry Bracco Dirty Deed -0.0209 0.019 -1.088 0.277 -0.059 0.017 contestant_episode_Aubry Bracco I'm Not Here to Make Good Friends 0.0395 0.010 3.901 0.000 0.020 0.059 contestant_episode_Aubry Bracco I'm a Mental Giant -0.0034 0.020 -0.168 0.866 -0.043 0.036 contestant_episode_Aubry Bracco It Is Not a High Without a Low -0.0272 0.015 -1.787 0.074 -0.057 0.003 contestant_episode_Aubry Bracco It's Merge Time -0.0149 0.013 -1.120 0.263 -0.041 0.011 contestant_episode_Aubry Bracco It's Psychological Warfare 0.0149 0.011 1.313 0.189 -0.007 0.037 contestant_episode_Aubry Bracco It's a 'Me' Game, Not a 'We' Game 0.0138 0.012 1.134 0.257 -0.010 0.038 contestant_episode_Aubry Bracco Kindergarten Camp -0.0486 0.025 -1.911 0.056 -0.099 0.001 contestant_episode_Aubry Bracco Now's the Time to Start Scheming 0.0407 0.012 3.346 0.001 0.017 0.065 contestant_episode_Aubry Bracco Parting Is Such Sweet Sorrow -0.0182 0.024 -0.766 0.444 -0.065 0.028 contestant_episode_Aubry Bracco Play or Go Home -0.0198 0.018 -1.095 0.274 -0.055 0.016 contestant_episode_Aubry Bracco Reinventing How This Game Is Played -0.0092 0.015 -0.626 0.531 -0.038 0.020 contestant_episode_Aubry Bracco Signed, Sealed and Delivered -0.0671 0.029 -2.351 0.019 -0.123 -0.011 contestant_episode_Aubry Bracco Survivor Jackpot -0.0314 0.023 -1.355 0.175 -0.077 0.014 contestant_episode_Aubry Bracco The Circle of Life -0.0028 0.027 -0.103 0.918 -0.056 0.051 contestant_episode_Aubry Bracco The Devils We Know -0.0074 0.020 -0.381 0.704 -0.046 0.031 contestant_episode_Aubry Bracco The Jocks vs. the Pretty People 0.0108 0.014 0.780 0.436 -0.016 0.038 contestant_episode_Aubry Bracco The Stakes Have Been Raised -0.0302 0.020 -1.480 0.139 -0.070 0.010 contestant_episode_Aubry Bracco The Tables Have Turned -0.0066 0.020 -0.323 0.747 -0.047 0.034 contestant_episode_Aubry Bracco There's a New Sheriff in Town -0.0122 0.018 -0.680 0.497 -0.047 0.023 contestant_episode_Aubry Bracco Vote Early, Vote Often -0.0008 0.021 -0.039 0.969 -0.042 0.040 contestant_episode_Aubry Bracco What Happened on Exile, Stays on Exile 0.0387 0.022 1.766 0.077 -0.004 0.082 contestant_episode_Aubry Bracco With Me or Not with Me 0.0451 0.014 3.319 0.001 0.018 0.072 contestant_episode_Baylor Wilson Blood Is Blood 0.0444 0.024 1.850 0.064 -0.003 0.091 contestant_episode_Baylor Wilson Gettin' to Crunch Time 0.0265 0.030 0.870 0.384 -0.033 0.086 contestant_episode_Baylor Wilson Let's Make a Move 0.1153 0.026 4.452 0.000 0.065 0.166 contestant_episode_Baylor Wilson Make Some Magic Happen 0.0819 0.029 2.846 0.004 0.025 0.138 contestant_episode_Baylor Wilson Method to This Madness 0.0615 0.027 2.252 0.024 0.008 0.115 contestant_episode_Baylor Wilson Million Dollar Decision 0.0728 0.028 2.571 0.010 0.017 0.128 contestant_episode_Baylor Wilson Still Holdin' On 0.0713 0.025 2.836 0.005 0.022 0.121 contestant_episode_Baylor Wilson Suck It Up and Survive 0.0393 0.027 1.440 0.150 -0.014 0.093 contestant_episode_Baylor Wilson This Is Where We Build Trust 0.0982 0.028 3.451 0.001 0.042 0.154 contestant_episode_Baylor Wilson We're a Hot Mess 0.0821 0.031 2.690 0.007 0.022 0.142 contestant_episode_Baylor Wilson Wrinkle in the Plan 0.0570 0.024 2.344 0.019 0.009 0.105 contestant_episode_Ben Driebergen Fear of the Unknown -0.0233 0.020 -1.166 0.244 -0.063 0.016 contestant_episode_Ben Driebergen Get to Gettin' -0.0187 0.021 -0.880 0.379 -0.060 0.023 contestant_episode_Ben Driebergen I Don't Like Having Snakes Around -0.0642 0.028 -2.310 0.021 -0.119 -0.010 contestant_episode_Ben Driebergen I'm Not Crazy, I'm Confident 0.0193 0.028 0.690 0.490 -0.035 0.074 contestant_episode_Ben Driebergen I'm a Wild Banshee -0.0065 0.028 -0.229 0.819 -0.062 0.049 contestant_episode_Ben Driebergen My Kisses Are Very Private -0.0233 0.027 -0.863 0.388 -0.076 0.030 contestant_episode_Ben Driebergen Not Going to Roll Over and Die -0.0514 0.022 -2.318 0.020 -0.095 -0.008 contestant_episode_Ben Driebergen Playing with the Devil -0.0422 0.020 -2.133 0.033 -0.081 -0.003 contestant_episode_Ben Driebergen The Past Will Eat You Alive 0.0023 0.024 0.097 0.923 -0.045 0.050 contestant_episode_Ben Driebergen The Survivor Devil 0.0099 0.019 0.532 0.595 -0.027 0.046 contestant_episode_Ben Driebergen This Is Why You Play Survivor 0.0143 0.023 0.627 0.531 -0.030 0.059 contestant_episode_Brad Culpepper Blood Is Thicker than Anything 0.0093 0.070 0.132 0.895 -0.129 0.147 contestant_episode_Brad Culpepper Dirty Deed 0.0012 0.021 0.058 0.954 -0.039 0.041 contestant_episode_Brad Culpepper It Is Not a High Without a Low 0.0220 0.021 1.023 0.306 -0.020 0.064 contestant_episode_Brad Culpepper One Armed Dude and Three Moms 0.0004 0.065 0.006 0.995 -0.128 0.128 contestant_episode_Brad Culpepper One-Man Wrecking Ball 0.0627 0.073 0.856 0.392 -0.081 0.206 contestant_episode_Brad Culpepper Opening Pandora's Box 0.0132 0.068 0.195 0.845 -0.119 0.146 contestant_episode_Brad Culpepper Parting Is Such Sweet Sorrow 0.0413 0.025 1.666 0.096 -0.007 0.090 contestant_episode_Brad Culpepper Reinventing How This Game Is Played 0.0181 0.016 1.112 0.266 -0.014 0.050 contestant_episode_Brad Culpepper Survivor Jackpot 0.0774 0.020 3.896 0.000 0.038 0.116 contestant_episode_Brad Culpepper The Dead Can Still Talk 0.0115 0.078 0.148 0.883 -0.141 0.164 contestant_episode_Brad Culpepper The Stakes Have Been Raised -0.0012 0.028 -0.043 0.966 -0.056 0.054 contestant_episode_Brad Culpepper The Tables Have Turned -0.0278 0.039 -0.716 0.474 -0.104 0.048 contestant_episode_Brad Culpepper There's a New Sheriff in Town 0.0401 0.018 2.217 0.027 0.005 0.076 contestant_episode_Brad Culpepper Vote Early, Vote Often 0.0349 0.022 1.623 0.105 -0.007 0.077 contestant_episode_Brad Culpepper What Happened on Exile, Stays on Exile 0.0697 0.021 3.261 0.001 0.028 0.112 contestant_episode_Brandon Hantz Kill or Be Killed -0.0157 0.042 -0.374 0.708 -0.098 0.066 contestant_episode_Brandon Hantz Persona Non Grata 0.0334 0.036 0.937 0.349 -0.036 0.103 contestant_episode_Brandon Hantz She Annoys Me Greatly 0.0144 0.043 0.336 0.737 -0.069 0.098 contestant_episode_Brandon Hantz Then There Were Five 0.1346 0.044 3.056 0.002 0.048 0.221 contestant_episode_Brandon Hantz There's Gonna Be Hell to Pay 0.0287 0.050 0.568 0.570 -0.070 0.128 contestant_episode_Brenda Lowe Come Over to the Dark Side 0.0173 0.035 0.500 0.617 -0.051 0.085 contestant_episode_Brenda Lowe Cut Off the Head of the Snake 0.0402 0.036 1.104 0.270 -0.031 0.112 contestant_episode_Brenda Lowe Don't Say Anything About My Mom -0.1460 0.143 -1.023 0.306 -0.426 0.134 contestant_episode_Brenda Lowe Operation Thunder Dome -0.0042 0.054 -0.077 0.939 -0.110 0.102 contestant_episode_Brenda Lowe Persona Non Grata 0.0088 0.054 0.162 0.871 -0.097 0.115 contestant_episode_Brenda Lowe She Annoys Me Greatly 0.0281 0.046 0.615 0.539 -0.062 0.118 contestant_episode_Brenda Lowe The Beginning of the End 0.0405 0.032 1.252 0.210 -0.023 0.104 contestant_episode_Brenda Lowe Tubby Lunchbox 0.1607 0.050 3.182 0.001 0.062 0.260 contestant_episode_Brenda Lowe Zipping Over the Cuckoo's Nest 0.0426 0.031 1.375 0.169 -0.018 0.103 contestant_episode_Bret LaBelle I Will Destroy You 0.0285 0.027 1.061 0.289 -0.024 0.081 contestant_episode_Bret LaBelle I'm the Kingpin -0.0001 0.025 -0.005 0.996 -0.050 0.049 contestant_episode_Bret LaBelle Idol Search Party -0.0150 0.029 -0.512 0.609 -0.072 0.042 contestant_episode_Bret LaBelle Love Goggles -0.0095 0.039 -0.243 0.808 -0.086 0.067 contestant_episode_Bret LaBelle May the Best Generation Win 0.0464 0.030 1.526 0.127 -0.013 0.106 contestant_episode_Bret LaBelle Million Dollar Gamble -0.0269 0.016 -1.713 0.087 -0.058 0.004 contestant_episode_Bret LaBelle Slayed the Survivor Dragon 0.0583 0.021 2.834 0.005 0.018 0.099 contestant_episode_Bret LaBelle Still Throwin' Punches 0.0250 0.020 1.236 0.217 -0.015 0.065 contestant_episode_Bret LaBelle The Truth Works Well 0.0428 0.032 1.334 0.182 -0.020 0.106 contestant_episode_Bret LaBelle Who's the Sucker at the Table%3F -0.0511 0.026 -1.928 0.054 -0.103 0.001 contestant_episode_Bret LaBelle Your Job Is Recon -0.0170 0.033 -0.523 0.601 -0.081 0.047 contestant_episode_Brice Johnston Hot Girl with a Grudge 0.0511 0.045 1.131 0.258 -0.037 0.140 contestant_episode_Caleb Bankston Gloves Come Off -0.0001 0.041 -0.003 0.998 -0.081 0.081 contestant_episode_Caleb Bankston My Brother's Keeper 0.0613 0.050 1.237 0.216 -0.036 0.158 contestant_episode_Caleb Bankston One Armed Dude and Three Moms 0.0100 0.043 0.234 0.815 -0.074 0.094 contestant_episode_Caleb Bankston Opening Pandora's Box 0.0486 0.048 1.007 0.314 -0.046 0.143 contestant_episode_Caleb Bankston Rustle Feathers -0.0056 0.040 -0.139 0.889 -0.084 0.073 contestant_episode_Caleb Bankston Skin of My Teeth -0.0260 0.058 -0.448 0.654 -0.140 0.088 contestant_episode_Caleb Bankston Swoop In for the Kill 0.1388 0.059 2.359 0.018 0.023 0.254 contestant_episode_Caleb Bankston The Dead Can Still Talk 0.0366 0.059 0.616 0.538 -0.080 0.153 contestant_episode_Caleb Reynolds Kindergarten Camp 0.0504 0.025 2.056 0.040 0.002 0.098 contestant_episode_Caleb Reynolds Signed, Sealed and Delivered -0.0229 0.019 -1.236 0.216 -0.059 0.013 contestant_episode_Caleb Reynolds Survivor Jackpot 0.0125 0.014 0.866 0.386 -0.016 0.041 contestant_episode_Caleb Reynolds The Circle of Life 0.0186 0.026 0.729 0.466 -0.031 0.069 contestant_episode_Caleb Reynolds The Stakes Have Been Raised -0.0087 0.015 -0.587 0.557 -0.038 0.020 contestant_episode_Candice Woodcock Opening Pandora's Box 0.0799 0.053 1.502 0.133 -0.024 0.184 contestant_episode_Candice Woodcock The Dead Can Still Talk 0.1145 0.063 1.831 0.067 -0.008 0.237 contestant_episode_Carolyn Rivera Crazy Is as Crazy Does 0.0152 0.033 0.454 0.650 -0.050 0.081 contestant_episode_Carolyn Rivera Holding On for Dear Life 0.0334 0.026 1.309 0.190 -0.017 0.083 contestant_episode_Carolyn Rivera It Will Be My Revenge 0.0354 0.040 0.880 0.379 -0.043 0.114 contestant_episode_Carolyn Rivera It's Survivor Warfare 0.0620 0.026 2.389 0.017 0.011 0.113 contestant_episode_Carolyn Rivera Keep It Real 0.0266 0.020 1.338 0.181 -0.012 0.066 contestant_episode_Carolyn Rivera Livin' on the Edge -0.0243 0.023 -1.037 0.300 -0.070 0.022 contestant_episode_Carolyn Rivera My Word Is My Bond 0.0504 0.020 2.499 0.012 0.011 0.090 contestant_episode_Carolyn Rivera Survivor Russian Roulette 0.0063 0.026 0.242 0.809 -0.045 0.057 contestant_episode_Carolyn Rivera The Line Will Be Drawn Tonight 0.0285 0.023 1.257 0.209 -0.016 0.073 contestant_episode_Carolyn Rivera Winner Winner, Chicken Dinner 0.0249 0.020 1.234 0.217 -0.015 0.064 contestant_episode_Carter Williams Down and Dirty -0.0612 0.058 -1.054 0.292 -0.175 0.053 contestant_episode_Carter Williams Hell Hath Frozen Over -0.0304 0.045 -0.673 0.501 -0.119 0.058 contestant_episode_Carter Williams Little Miss Perfect 0.0334 0.055 0.610 0.542 -0.074 0.141 contestant_episode_Carter Williams Shot into Smithereens 0.0305 0.047 0.643 0.520 -0.062 0.123 contestant_episode_Carter Williams Whiners Are Wieners 0.0344 0.055 0.626 0.532 -0.073 0.142 contestant_episode_CeCe Taylor Love Goggles 0.0086 0.022 0.397 0.691 -0.034 0.051 contestant_episode_CeCe Taylor May the Best Generation Win 0.0107 0.020 0.538 0.590 -0.028 0.050 contestant_episode_CeCe Taylor Who's the Sucker at the Table%3F -0.0107 0.025 -0.429 0.668 -0.060 0.038 contestant_episode_CeCe Taylor Your Job Is Recon -0.0083 0.021 -0.392 0.695 -0.050 0.033 contestant_episode_Chelsea Meissner It's Gonna Be Chaos 0.0980 0.059 1.666 0.096 -0.017 0.213 contestant_episode_Chelsea Meissner It's Human Nature -0.1024 0.148 -0.692 0.489 -0.393 0.188 contestant_episode_Chelsea Meissner Never Say Die 0.0334 0.047 0.714 0.475 -0.058 0.125 contestant_episode_Chris Hammons I'm the Kingpin 0.0549 0.026 2.073 0.038 0.003 0.107 contestant_episode_Chris Hammons Idol Search Party 0.0413 0.023 1.775 0.076 -0.004 0.087 contestant_episode_Chris Hammons Love Goggles 0.0379 0.031 1.239 0.215 -0.022 0.098 contestant_episode_Chris Hammons May the Best Generation Win 0.0602 0.027 2.265 0.024 0.008 0.112 contestant_episode_Chris Hammons Million Dollar Gamble 0.0777 0.023 3.344 0.001 0.032 0.123 contestant_episode_Chris Hammons Still Throwin' Punches 0.0347 0.021 1.640 0.101 -0.007 0.076 contestant_episode_Chris Hammons The Truth Works Well 0.0463 0.025 1.862 0.063 -0.002 0.095 contestant_episode_Chris Hammons Who's the Sucker at the Table%3F 0.0099 0.023 0.434 0.664 -0.035 0.055 contestant_episode_Chris Hammons Your Job Is Recon 0.0313 0.029 1.070 0.285 -0.026 0.089 contestant_episode_Chrissy Hofbeck Fear of the Unknown -0.0328 0.018 -1.836 0.066 -0.068 0.002 contestant_episode_Chrissy Hofbeck Get to Gettin' -0.0067 0.012 -0.576 0.565 -0.030 0.016 contestant_episode_Chrissy Hofbeck I Don't Like Having Snakes Around 0.0311 0.017 1.833 0.067 -0.002 0.064 contestant_episode_Chrissy Hofbeck I'm Not Crazy, I'm Confident -0.0035 0.019 -0.185 0.853 -0.041 0.034 contestant_episode_Chrissy Hofbeck I'm a Wild Banshee 0.0366 0.021 1.736 0.083 -0.005 0.078 contestant_episode_Chrissy Hofbeck My Kisses Are Very Private -0.0145 0.025 -0.588 0.556 -0.063 0.034 contestant_episode_Chrissy Hofbeck Not Going to Roll Over and Die -0.0074 0.018 -0.407 0.684 -0.043 0.028 contestant_episode_Chrissy Hofbeck Playing with the Devil -0.0435 0.016 -2.728 0.006 -0.075 -0.012 contestant_episode_Chrissy Hofbeck The Past Will Eat You Alive -0.0154 0.020 -0.778 0.436 -0.054 0.023 contestant_episode_Chrissy Hofbeck The Survivor Devil -0.1506 0.140 -1.076 0.282 -0.425 0.124 contestant_episode_Chrissy Hofbeck This Is Why You Play Survivor 0.0029 0.018 0.160 0.873 -0.032 0.038 contestant_episode_Christina Cha I'm No Dummy 0.0067 0.046 0.145 0.885 -0.084 0.097 contestant_episode_Christina Cha It's Gonna Be Chaos 0.1036 0.050 2.077 0.038 0.006 0.201 contestant_episode_Christina Cha It's Human Nature 0.1487 0.048 3.121 0.002 0.055 0.242 contestant_episode_Christina Cha Never Say Die 0.0568 0.041 1.376 0.169 -0.024 0.138 contestant_episode_Christina Cha Thanks for the Souvenir -0.0239 0.050 -0.481 0.631 -0.122 0.074 contestant_episode_Ciera Eastin Big Bad Wolf -0.0604 0.041 -1.462 0.144 -0.141 0.021 contestant_episode_Ciera Eastin Bunking with the Devil 0.0227 0.028 0.826 0.409 -0.031 0.077 contestant_episode_Ciera Eastin Gloves Come Off -0.0595 0.041 -1.464 0.143 -0.139 0.020 contestant_episode_Ciera Eastin My Brother's Keeper -0.0566 0.053 -1.064 0.287 -0.161 0.048 contestant_episode_Ciera Eastin One Armed Dude and Three Moms -0.0013 0.060 -0.023 0.982 -0.118 0.115 contestant_episode_Ciera Eastin Out on a Limb (episode) 0.0402 0.041 0.988 0.323 -0.040 0.120 contestant_episode_Ciera Eastin Play to Win 0.0145 0.029 0.495 0.621 -0.043 0.072 contestant_episode_Ciera Eastin Rustle Feathers -0.0005 0.037 -0.014 0.989 -0.074 0.073 contestant_episode_Ciera Eastin Skin of My Teeth -0.0302 0.049 -0.610 0.542 -0.127 0.067 contestant_episode_Ciera Eastin Survivor MacGyver 0.0095 0.030 0.312 0.755 -0.050 0.069 contestant_episode_Ciera Eastin Swoop In for the Kill 0.0091 0.057 0.161 0.872 -0.102 0.120 contestant_episode_Ciera Eastin The Stakes Have Been Raised 0.0352 0.023 1.538 0.124 -0.010 0.080 contestant_episode_Ciera Eastin We Got a Rat 0.0302 0.037 0.818 0.413 -0.042 0.103 contestant_episode_Ciera Eastin What's the Beef%3F 0.0526 0.032 1.666 0.096 -0.009 0.115 contestant_episode_Ciera Eastin Witches Coven (episode) 0.0249 0.029 0.859 0.390 -0.032 0.082 contestant_episode_Ciera Eastin You Call, We'll Haul 0.0297 0.027 1.110 0.267 -0.023 0.082 contestant_episode_Cirie Fields Dirty Deed -0.0260 0.026 -1.014 0.311 -0.076 0.024 contestant_episode_Cirie Fields It Is Not a High Without a Low 0.0267 0.013 2.049 0.040 0.001 0.052 contestant_episode_Cirie Fields Parting Is Such Sweet Sorrow -0.0200 0.011 -1.856 0.064 -0.041 0.001 contestant_episode_Cirie Fields Reinventing How This Game Is Played 0.0222 0.012 1.890 0.059 -0.001 0.045 contestant_episode_Cirie Fields Survivor Jackpot -0.0408 0.017 -2.425 0.015 -0.074 -0.008 contestant_episode_Cirie Fields The Stakes Have Been Raised -0.0106 0.023 -0.469 0.639 -0.055 0.034 contestant_episode_Cirie Fields The Tables Have Turned -0.0355 0.018 -1.943 0.052 -0.071 0.000 contestant_episode_Cirie Fields There's a New Sheriff in Town -0.0088 0.010 -0.881 0.378 -0.028 0.011 contestant_episode_Cirie Fields Vote Early, Vote Often -0.0321 0.016 -1.981 0.048 -0.064 -0.000 contestant_episode_Cirie Fields What Happened on Exile, Stays on Exile -0.0130 0.017 -0.777 0.437 -0.046 0.020 contestant_episode_Cliff Robinson Hot Girl with a Grudge 0.0600 0.051 1.176 0.239 -0.040 0.160 contestant_episode_Cliff Robinson Odd One Out 0.0295 0.047 0.629 0.530 -0.063 0.122 contestant_episode_Cliff Robinson Our Time to Shine 0.0596 0.052 1.144 0.252 -0.042 0.162 contestant_episode_Cliff Robinson We Found Our Zombies -0.0620 0.060 -1.040 0.299 -0.179 0.055 contestant_episode_Coach Wade Then There Were Five 0.1005 0.045 2.257 0.024 0.013 0.188 contestant_episode_Cole Medders Get to Gettin' 0.0002 0.018 0.010 0.992 -0.036 0.036 contestant_episode_Cole Medders I Don't Like Having Snakes Around -0.1139 0.018 -6.447 0.000 -0.149 -0.079 contestant_episode_Cole Medders I'm Not Crazy, I'm Confident -0.0165 0.024 -0.697 0.486 -0.063 0.030 contestant_episode_Cole Medders I'm a Wild Banshee 0.0087 0.025 0.344 0.731 -0.041 0.058 contestant_episode_Cole Medders My Kisses Are Very Private 0.0075 0.024 0.317 0.751 -0.039 0.054 contestant_episode_Cole Medders Playing with the Devil -0.0191 0.021 -0.909 0.363 -0.060 0.022 contestant_episode_Cole Medders The Past Will Eat You Alive -0.0105 0.022 -0.467 0.641 -0.054 0.033 contestant_episode_Cole Medders This Is Why You Play Survivor 0.0175 0.021 0.841 0.401 -0.023 0.058 contestant_episode_Colton Cumbie Blood Is Thicker than Anything -0.0504 0.027 -1.874 0.061 -0.103 0.002 contestant_episode_Colton Cumbie Bum-Puzzled -0.0241 0.032 -0.747 0.455 -0.087 0.039 contestant_episode_Colton Cumbie Opening Pandora's Box 0.0114 0.023 0.487 0.626 -0.034 0.057 contestant_episode_Colton Cumbie Rule in Chaos 0.0074 0.028 0.264 0.791 -0.047 0.062 contestant_episode_Colton Cumbie Thanks for the Souvenir -0.0110 0.036 -0.308 0.758 -0.081 0.059 contestant_episode_Colton Cumbie Total Dysfunction -0.0608 0.059 -1.026 0.305 -0.177 0.055 contestant_episode_Colton Cumbie Two Tribes, One Camp, No Rules 0.0717 0.055 1.299 0.194 -0.036 0.180 contestant_episode_Corinne Kaplan Operation Thunder Dome 0.0209 0.038 0.554 0.580 -0.053 0.095 contestant_episode_Corinne Kaplan She Annoys Me Greatly -0.0130 0.051 -0.254 0.799 -0.113 0.087 contestant_episode_Corinne Kaplan Tubby Lunchbox 0.0664 0.049 1.349 0.177 -0.030 0.163 contestant_episode_Cydney Gillon I'm a Mental Giant -0.0266 0.027 -0.971 0.332 -0.080 0.027 contestant_episode_Cydney Gillon It's Merge Time -0.0015 0.023 -0.066 0.948 -0.046 0.043 contestant_episode_Cydney Gillon It's Psychological Warfare 0.0116 0.016 0.726 0.468 -0.020 0.043 contestant_episode_Cydney Gillon It's a 'Me' Game, Not a 'We' Game -0.0238 0.021 -1.135 0.257 -0.065 0.017 contestant_episode_Cydney Gillon Kindergarten Camp -0.0419 0.026 -1.634 0.102 -0.092 0.008 contestant_episode_Cydney Gillon Now's the Time to Start Scheming 0.0020 0.022 0.090 0.928 -0.041 0.045 contestant_episode_Cydney Gillon Play or Go Home 0.0296 0.024 1.249 0.212 -0.017 0.076 contestant_episode_Cydney Gillon Signed, Sealed and Delivered -0.0403 0.020 -2.031 0.042 -0.079 -0.001 contestant_episode_Cydney Gillon The Circle of Life -0.0493 0.025 -1.945 0.052 -0.099 0.000 contestant_episode_Cydney Gillon The Devils We Know 0.0237 0.022 1.070 0.285 -0.020 0.067 contestant_episode_Cydney Gillon The Jocks vs. the Pretty People 0.0272 0.018 1.540 0.123 -0.007 0.062 contestant_episode_Cydney Gillon With Me or Not with Me 0.0299 0.017 1.745 0.081 -0.004 0.063 contestant_episode_Dale Wentworth Blood Is Blood -0.0308 0.034 -0.913 0.361 -0.097 0.035 contestant_episode_Dale Wentworth Make Some Magic Happen -0.0497 0.038 -1.322 0.186 -0.123 0.024 contestant_episode_Dale Wentworth Method to This Madness -0.0358 0.047 -0.764 0.445 -0.128 0.056 contestant_episode_Dale Wentworth Suck It Up and Survive -0.0397 0.036 -1.098 0.272 -0.111 0.031 contestant_episode_Dale Wentworth We're a Hot Mess -0.0137 0.042 -0.328 0.743 -0.096 0.068 contestant_episode_Dan Foley Crazy Is as Crazy Does -0.0243 0.022 -1.087 0.277 -0.068 0.019 contestant_episode_Dan Foley Holding On for Dear Life 0.0166 0.013 1.251 0.211 -0.009 0.043 contestant_episode_Dan Foley It Will Be My Revenge -0.0198 0.023 -0.872 0.383 -0.064 0.025 contestant_episode_Dan Foley It's Survivor Warfare 0.0227 0.022 1.029 0.304 -0.021 0.066 contestant_episode_Dan Foley Keep It Real 0.0002 0.011 0.021 0.983 -0.021 0.021 contestant_episode_Dan Foley Livin' on the Edge -0.0259 0.018 -1.444 0.149 -0.061 0.009 contestant_episode_Dan Foley My Word Is My Bond 0.0153 0.011 1.371 0.170 -0.007 0.037 contestant_episode_Dan Foley Survivor Russian Roulette -0.0255 0.016 -1.629 0.103 -0.056 0.005 contestant_episode_Dan Foley The Line Will Be Drawn Tonight 0.0295 0.018 1.616 0.106 -0.006 0.065 contestant_episode_Dan Foley Winner Winner, Chicken Dinner -0.0021 0.022 -0.096 0.924 -0.045 0.041 contestant_episode_David Wright I Will Destroy You -0.1030 0.027 -3.757 0.000 -0.157 -0.049 contestant_episode_David Wright I'm the Kingpin -0.0688 0.024 -2.863 0.004 -0.116 -0.022 contestant_episode_David Wright Idol Search Party -0.1048 0.024 -4.409 0.000 -0.151 -0.058 contestant_episode_David Wright Love Goggles -0.1035 0.027 -3.767 0.000 -0.157 -0.050 contestant_episode_David Wright May the Best Generation Win -0.1018 0.028 -3.689 0.000 -0.156 -0.048 contestant_episode_David Wright Million Dollar Gamble -0.0865 0.025 -3.487 0.000 -0.135 -0.038 contestant_episode_David Wright Slayed the Survivor Dragon -0.0225 0.020 -1.126 0.260 -0.062 0.017 contestant_episode_David Wright Still Throwin' Punches -0.0828 0.027 -3.049 0.002 -0.136 -0.030 contestant_episode_David Wright The Truth Works Well -0.1374 0.028 -4.927 0.000 -0.192 -0.083 contestant_episode_David Wright Who's the Sucker at the Table%3F -0.1067 0.028 -3.806 0.000 -0.162 -0.052 contestant_episode_David Wright Your Job Is Recon -0.1222 0.029 -4.262 0.000 -0.178 -0.066 contestant_episode_Dawn Meehan Come Over to the Dark Side 0.0247 0.030 0.819 0.413 -0.034 0.084 contestant_episode_Dawn Meehan Cut Off the Head of the Snake -0.0007 0.027 -0.027 0.978 -0.054 0.053 contestant_episode_Dawn Meehan Don't Say Anything About My Mom -0.0261 0.027 -0.983 0.326 -0.078 0.026 contestant_episode_Dawn Meehan Honey Badger 0.0028 0.050 0.055 0.956 -0.096 0.101 contestant_episode_Dawn Meehan Operation Thunder Dome 0.0339 0.056 0.602 0.547 -0.077 0.144 contestant_episode_Dawn Meehan She Annoys Me Greatly -0.0647 0.043 -1.520 0.128 -0.148 0.019 contestant_episode_Dawn Meehan The Beginning of the End 0.0048 0.030 0.159 0.874 -0.054 0.064 contestant_episode_Dawn Meehan Tubby Lunchbox 0.0215 0.047 0.460 0.646 -0.070 0.113 contestant_episode_Dawn Meehan Zipping Over the Cuckoo's Nest -0.0544 0.029 -1.897 0.058 -0.111 0.002 contestant_episode_Debbie Wanner Dirty Deed 0.0228 0.027 0.836 0.403 -0.031 0.076 contestant_episode_Debbie Wanner I'm a Mental Giant -0.0155 0.016 -0.951 0.342 -0.047 0.016 contestant_episode_Debbie Wanner It's Merge Time 0.0059 0.015 0.384 0.701 -0.024 0.036 contestant_episode_Debbie Wanner It's Psychological Warfare -0.0079 0.013 -0.633 0.526 -0.033 0.017 contestant_episode_Debbie Wanner Kindergarten Camp -0.0148 0.021 -0.693 0.488 -0.057 0.027 contestant_episode_Debbie Wanner Play or Go Home 0.0354 0.015 2.294 0.022 0.005 0.066 contestant_episode_Debbie Wanner Signed, Sealed and Delivered -0.0342 0.017 -2.019 0.044 -0.067 -0.001 contestant_episode_Debbie Wanner Survivor Jackpot 0.0405 0.029 1.375 0.169 -0.017 0.098 contestant_episode_Debbie Wanner The Circle of Life 0.0178 0.015 1.157 0.247 -0.012 0.048 contestant_episode_Debbie Wanner The Devils We Know -0.0100 0.016 -0.634 0.526 -0.041 0.021 contestant_episode_Debbie Wanner The Jocks vs. the Pretty People 0.0196 0.017 1.137 0.256 -0.014 0.054 contestant_episode_Debbie Wanner The Stakes Have Been Raised 0.0389 0.033 1.163 0.245 -0.027 0.104 contestant_episode_Debbie Wanner The Tables Have Turned 0.0265 0.028 0.947 0.343 -0.028 0.081 contestant_episode_Debbie Wanner There's a New Sheriff in Town 0.0355 0.026 1.376 0.169 -0.015 0.086 contestant_episode_Debbie Wanner What Happened on Exile, Stays on Exile 0.0099 0.019 0.532 0.594 -0.027 0.046 contestant_episode_Denise Stapley Don't Be Blinded by the Headlights -0.1922 0.059 -3.273 0.001 -0.307 -0.077 contestant_episode_Denise Stapley Gouge My Eyes Out -0.0452 0.037 -1.213 0.225 -0.118 0.028 contestant_episode_Denise Stapley Hell Hath Frozen Over -0.0494 0.042 -1.184 0.237 -0.131 0.032 contestant_episode_Denise Stapley Little Miss Perfect -0.0397 0.049 -0.811 0.418 -0.136 0.056 contestant_episode_Denise Stapley Not the Only Actor on This Island -0.0668 0.054 -1.232 0.218 -0.173 0.039 contestant_episode_Denise Stapley Shot into Smithereens -0.0206 0.040 -0.514 0.607 -0.099 0.058 contestant_episode_Denise Stapley This Isn't a 'We' Game -0.0430 0.058 -0.745 0.456 -0.156 0.070 contestant_episode_Denise Stapley Whiners Are Wieners -0.0156 0.045 -0.347 0.728 -0.104 0.072 contestant_episode_Desi Williams I Don't Like Having Snakes Around -0.0050 0.019 -0.262 0.793 -0.042 0.032 contestant_episode_Desi Williams I'm Not Crazy, I'm Confident 0.0118 0.034 0.345 0.730 -0.055 0.079 contestant_episode_Desi Williams I'm a Wild Banshee 0.0125 0.033 0.373 0.709 -0.053 0.078 contestant_episode_Desi Williams My Kisses Are Very Private -0.0069 0.026 -0.263 0.792 -0.058 0.044 contestant_episode_Desi Williams Playing with the Devil -0.0164 0.017 -0.963 0.336 -0.050 0.017 contestant_episode_Desi Williams The Past Will Eat You Alive 0.0078 0.026 0.300 0.764 -0.043 0.059 contestant_episode_Desi Williams This Is Why You Play Survivor 0.0253 0.022 1.167 0.243 -0.017 0.068 contestant_episode_Devon Pinto Fear of the Unknown -0.0042 0.022 -0.192 0.848 -0.047 0.038 contestant_episode_Devon Pinto Get to Gettin' -0.0328 0.023 -1.426 0.154 -0.078 0.012 contestant_episode_Devon Pinto I Don't Like Having Snakes Around -0.0676 0.023 -2.975 0.003 -0.112 -0.023 contestant_episode_Devon Pinto I'm Not Crazy, I'm Confident -0.0306 0.030 -1.022 0.307 -0.089 0.028 contestant_episode_Devon Pinto I'm a Wild Banshee -0.0409 0.031 -1.300 0.194 -0.103 0.021 contestant_episode_Devon Pinto My Kisses Are Very Private -0.0479 0.029 -1.638 0.101 -0.105 0.009 contestant_episode_Devon Pinto Not Going to Roll Over and Die -0.0438 0.023 -1.899 0.058 -0.089 0.001 contestant_episode_Devon Pinto Playing with the Devil -0.0637 0.025 -2.518 0.012 -0.113 -0.014 contestant_episode_Devon Pinto The Past Will Eat You Alive -0.0974 0.030 -3.264 0.001 -0.156 -0.039 contestant_episode_Devon Pinto The Survivor Devil -0.0210 0.019 -1.086 0.278 -0.059 0.017 contestant_episode_Devon Pinto This Is Why You Play Survivor -0.0075 0.024 -0.316 0.752 -0.054 0.039 contestant_episode_Drew Christy Method to This Madness -0.0025 0.040 -0.063 0.949 -0.081 0.076 contestant_episode_Drew Christy Suck It Up and Survive -0.0237 0.042 -0.564 0.572 -0.106 0.059 contestant_episode_Drew Christy We're a Hot Mess 0.0183 0.030 0.616 0.538 -0.040 0.077 contestant_episode_Eddie Fox Come Over to the Dark Side 0.0125 0.035 0.353 0.724 -0.057 0.082 contestant_episode_Eddie Fox Cut Off the Head of the Snake -0.0416 0.037 -1.126 0.260 -0.114 0.031 contestant_episode_Eddie Fox Don't Say Anything About My Mom 0.0269 0.031 0.857 0.391 -0.035 0.088 contestant_episode_Eddie Fox Kill or Be Killed -0.0221 0.049 -0.451 0.652 -0.118 0.074 contestant_episode_Eddie Fox Operation Thunder Dome 0.0349 0.059 0.588 0.556 -0.081 0.151 contestant_episode_Eddie Fox The Beginning of the End -0.0198 0.033 -0.595 0.552 -0.085 0.045 contestant_episode_Eddie Fox There's Gonna Be Hell to Pay -0.0131 0.048 -0.274 0.784 -0.107 0.081 contestant_episode_Eddie Fox Zipping Over the Cuckoo's Nest 0.0139 0.034 0.409 0.682 -0.052 0.080 contestant_episode_Erik Reichenbach Come Over to the Dark Side 0.0162 0.034 0.481 0.631 -0.050 0.082 contestant_episode_Erik Reichenbach Cut Off the Head of the Snake -0.0326 0.035 -0.927 0.354 -0.101 0.036 contestant_episode_Erik Reichenbach Don't Say Anything About My Mom 0.0183 0.030 0.607 0.544 -0.041 0.077 contestant_episode_Erik Reichenbach Kill or Be Killed -0.0369 0.054 -0.683 0.495 -0.143 0.069 contestant_episode_Erik Reichenbach Operation Thunder Dome 0.0103 0.052 0.198 0.843 -0.092 0.113 contestant_episode_Erik Reichenbach Persona Non Grata -0.0440 0.047 -0.939 0.348 -0.136 0.048 contestant_episode_Erik Reichenbach She Annoys Me Greatly -0.0002 0.044 -0.004 0.997 -0.086 0.086 contestant_episode_Erik Reichenbach The Beginning of the End 0.0391 0.034 1.137 0.256 -0.028 0.107 contestant_episode_Erik Reichenbach Tubby Lunchbox -0.0094 0.052 -0.181 0.857 -0.111 0.092 contestant_episode_Erik Reichenbach Zipping Over the Cuckoo's Nest -0.0323 0.031 -1.030 0.303 -0.094 0.029 contestant_episode_Figgy Figueroa Love Goggles -0.0125 0.020 -0.638 0.524 -0.051 0.026 contestant_episode_Figgy Figueroa May the Best Generation Win 0.0033 0.024 0.135 0.892 -0.044 0.050 contestant_episode_Figgy Figueroa The Truth Works Well -0.0171 0.018 -0.969 0.333 -0.052 0.017 contestant_episode_Figgy Figueroa Who's the Sucker at the Table%3F -0.0347 0.029 -1.206 0.228 -0.091 0.022 contestant_episode_Figgy Figueroa Your Job Is Recon -0.0259 0.028 -0.930 0.352 -0.080 0.029 contestant_episode_Gervase Peterson Blood Is Thicker than Anything 0.0096 0.051 0.188 0.851 -0.090 0.110 contestant_episode_Gervase Peterson Gloves Come Off 0.0686 0.046 1.478 0.139 -0.022 0.160 contestant_episode_Gervase Peterson My Brother's Keeper 0.0711 0.050 1.421 0.155 -0.027 0.169 contestant_episode_Gervase Peterson One Armed Dude and Three Moms 0.0787 0.059 1.338 0.181 -0.037 0.194 contestant_episode_Gervase Peterson One-Man Wrecking Ball 0.1125 0.060 1.888 0.059 -0.004 0.229 contestant_episode_Gervase Peterson Opening Pandora's Box 0.0612 0.066 0.925 0.355 -0.068 0.191 contestant_episode_Gervase Peterson Out on a Limb (episode) 0.0828 0.044 1.895 0.058 -0.003 0.168 contestant_episode_Gervase Peterson Rule in Chaos 0.0249 0.063 0.393 0.695 -0.099 0.149 contestant_episode_Gervase Peterson Rustle Feathers 0.0355 0.046 0.779 0.436 -0.054 0.125 contestant_episode_Gervase Peterson Skin of My Teeth 0.0497 0.053 0.942 0.346 -0.054 0.153 contestant_episode_Gervase Peterson Swoop In for the Kill 0.1626 0.056 2.917 0.004 0.053 0.272 contestant_episode_Gervase Peterson The Dead Can Still Talk -0.0347 0.062 -0.565 0.572 -0.155 0.086 contestant_episode_Hali Ford Dirty Deed -0.0154 0.013 -1.182 0.237 -0.041 0.010 contestant_episode_Hali Ford It Will Be My Revenge -0.0530 0.030 -1.760 0.078 -0.112 0.006 contestant_episode_Hali Ford It's Survivor Warfare -0.0502 0.041 -1.235 0.217 -0.130 0.030 contestant_episode_Hali Ford Keep It Real -0.0263 0.026 -0.998 0.318 -0.078 0.025 contestant_episode_Hali Ford Survivor Jackpot -0.0160 0.012 -1.357 0.175 -0.039 0.007 contestant_episode_Hali Ford The Line Will Be Drawn Tonight -0.0121 0.028 -0.438 0.661 -0.066 0.042 contestant_episode_Hali Ford The Stakes Have Been Raised -0.0188 0.014 -1.360 0.174 -0.046 0.008 contestant_episode_Hali Ford The Tables Have Turned -0.0070 0.010 -0.687 0.492 -0.027 0.013 contestant_episode_Hali Ford There's a New Sheriff in Town -0.0025 0.011 -0.231 0.817 -0.023 0.018 contestant_episode_Hali Ford Vote Early, Vote Often 0.0175 0.016 1.073 0.283 -0.014 0.049 contestant_episode_Hali Ford What Happened on Exile, Stays on Exile 0.0147 0.015 0.967 0.333 -0.015 0.045 contestant_episode_Hali Ford Winner Winner, Chicken Dinner -0.0318 0.031 -1.036 0.300 -0.092 0.028 contestant_episode_Hannah Shapiro I Will Destroy You -0.0304 0.023 -1.329 0.184 -0.075 0.014 contestant_episode_Hannah Shapiro I'm the Kingpin -0.0328 0.018 -1.841 0.066 -0.068 0.002 contestant_episode_Hannah Shapiro Idol Search Party -0.1056 0.028 -3.775 0.000 -0.160 -0.051 contestant_episode_Hannah Shapiro Love Goggles -0.0375 0.026 -1.453 0.146 -0.088 0.013 contestant_episode_Hannah Shapiro May the Best Generation Win -0.0312 0.025 -1.230 0.219 -0.081 0.018 contestant_episode_Hannah Shapiro Million Dollar Gamble -0.0681 0.018 -3.891 0.000 -0.102 -0.034 contestant_episode_Hannah Shapiro Slayed the Survivor Dragon -0.0005 0.013 -0.040 0.968 -0.026 0.025 contestant_episode_Hannah Shapiro Still Throwin' Punches -0.0058 0.017 -0.338 0.735 -0.039 0.028 contestant_episode_Hannah Shapiro The Truth Works Well -0.0473 0.026 -1.811 0.070 -0.099 0.004 contestant_episode_Hannah Shapiro Who's the Sucker at the Table%3F -0.0364 0.028 -1.317 0.188 -0.091 0.018 contestant_episode_Hannah Shapiro Your Job Is Recon -0.1129 0.025 -4.530 0.000 -0.162 -0.064 contestant_episode_Hayden Moss Gloves Come Off 0.0117 0.035 0.337 0.736 -0.056 0.080 contestant_episode_Hayden Moss My Brother's Keeper 0.0626 0.044 1.410 0.158 -0.024 0.150 contestant_episode_Hayden Moss One Armed Dude and Three Moms -0.0060 0.043 -0.142 0.887 -0.089 0.077 contestant_episode_Hayden Moss One-Man Wrecking Ball 0.0438 0.059 0.749 0.454 -0.071 0.158 contestant_episode_Hayden Moss Opening Pandora's Box 0.0128 0.059 0.217 0.829 -0.103 0.129 contestant_episode_Hayden Moss Out on a Limb (episode) 0.0444 0.033 1.363 0.173 -0.019 0.108 contestant_episode_Hayden Moss Rustle Feathers 0.0633 0.030 2.116 0.034 0.005 0.122 contestant_episode_Hayden Moss Skin of My Teeth 0.0234 0.048 0.490 0.624 -0.070 0.117 contestant_episode_Hayden Moss Swoop In for the Kill -0.0168 0.042 -0.402 0.687 -0.099 0.065 contestant_episode_J'Tia Taylor Hot Girl with a Grudge 0.0307 0.037 0.837 0.402 -0.041 0.103 contestant_episode_J'Tia Taylor Odd One Out 0.0341 0.062 0.553 0.580 -0.087 0.155 contestant_episode_J'Tia Taylor Our Time to Shine -0.0237 0.040 -0.596 0.551 -0.102 0.054 contestant_episode_J.T. Thomas Survivor Jackpot 0.0683 0.034 2.034 0.042 0.002 0.134 contestant_episode_J.T. Thomas The Stakes Have Been Raised 0.0377 0.036 1.043 0.297 -0.033 0.109 contestant_episode_J.T. Thomas The Tables Have Turned -0.0837 0.075 -1.122 0.262 -0.230 0.063 contestant_episode_JP Hilsabeck Fear of the Unknown 0.0160 0.020 0.810 0.418 -0.023 0.055 contestant_episode_JP Hilsabeck Get to Gettin' 0.0066 0.017 0.389 0.698 -0.027 0.040 contestant_episode_JP Hilsabeck I Don't Like Having Snakes Around 0.0292 0.022 1.350 0.177 -0.013 0.072 contestant_episode_JP Hilsabeck I'm Not Crazy, I'm Confident -0.0109 0.021 -0.521 0.602 -0.052 0.030 contestant_episode_JP Hilsabeck I'm a Wild Banshee -0.0199 0.026 -0.781 0.435 -0.070 0.030 contestant_episode_JP Hilsabeck My Kisses Are Very Private 0.0329 0.023 1.436 0.151 -0.012 0.078 contestant_episode_JP Hilsabeck Playing with the Devil -0.0256 0.019 -1.340 0.180 -0.063 0.012 contestant_episode_JP Hilsabeck The Past Will Eat You Alive -0.0057 0.017 -0.346 0.729 -0.038 0.027 contestant_episode_JP Hilsabeck This Is Why You Play Survivor -0.0331 0.018 -1.813 0.070 -0.069 0.003 contestant_episode_Jaclyn Schultz Blood Is Blood 0.1281 0.031 4.140 0.000 0.067 0.189 contestant_episode_Jaclyn Schultz Gettin' to Crunch Time 0.0943 0.028 3.382 0.001 0.040 0.149 contestant_episode_Jaclyn Schultz Let's Make a Move 0.1281 0.028 4.544 0.000 0.073 0.183 contestant_episode_Jaclyn Schultz Make Some Magic Happen 0.0856 0.034 2.503 0.012 0.019 0.153 contestant_episode_Jaclyn Schultz Method to This Madness 0.0113 0.035 0.320 0.749 -0.058 0.080 contestant_episode_Jaclyn Schultz Million Dollar Decision 0.0613 0.033 1.850 0.064 -0.004 0.126 contestant_episode_Jaclyn Schultz Still Holdin' On 0.0916 0.027 3.351 0.001 0.038 0.145 contestant_episode_Jaclyn Schultz Suck It Up and Survive 0.0269 0.039 0.683 0.494 -0.050 0.104 contestant_episode_Jaclyn Schultz This Is Where We Build Trust 0.0812 0.033 2.494 0.013 0.017 0.145 contestant_episode_Jaclyn Schultz We're a Hot Mess 0.1337 0.039 3.415 0.001 0.057 0.211 contestant_episode_Jaclyn Schultz Wrinkle in the Plan 0.1009 0.028 3.598 0.000 0.046 0.156 contestant_episode_Jay Starrett I Will Destroy You -0.0616 0.020 -3.142 0.002 -0.100 -0.023 contestant_episode_Jay Starrett I'm the Kingpin -0.0759 0.022 -3.488 0.000 -0.119 -0.033 contestant_episode_Jay Starrett Idol Search Party -0.0360 0.026 -1.391 0.164 -0.087 0.015 contestant_episode_Jay Starrett Love Goggles -0.0234 0.025 -0.936 0.349 -0.072 0.026 contestant_episode_Jay Starrett May the Best Generation Win -0.0728 0.027 -2.717 0.007 -0.125 -0.020 contestant_episode_Jay Starrett Million Dollar Gamble 2.715e-06 0.025 0.000 1.000 -0.049 0.049 contestant_episode_Jay Starrett Slayed the Survivor Dragon -0.1440 0.141 -1.022 0.307 -0.420 0.132 contestant_episode_Jay Starrett Still Throwin' Punches -0.0700 0.020 -3.483 0.000 -0.109 -0.031 contestant_episode_Jay Starrett The Truth Works Well -0.0476 0.024 -2.023 0.043 -0.094 -0.001 contestant_episode_Jay Starrett Who's the Sucker at the Table%3F -0.0582 0.030 -1.924 0.054 -0.118 0.001 contestant_episode_Jay Starrett Your Job Is Recon -0.0796 0.034 -2.369 0.018 -0.146 -0.014 contestant_episode_Jeff Kent Don't Be Blinded by the Headlights -0.0494 0.053 -0.939 0.347 -0.152 0.054 contestant_episode_Jeff Kent Down and Dirty 0.0144 0.038 0.381 0.703 -0.060 0.089 contestant_episode_Jeff Kent Got My Swag Back -0.0170 0.048 -0.357 0.721 -0.111 0.077 contestant_episode_Jeff Kent Not the Only Actor on This Island 0.0764 0.041 1.848 0.065 -0.005 0.157 contestant_episode_Jeff Varner Survivor Jackpot -0.0183 0.017 -1.046 0.296 -0.052 0.016 contestant_episode_Jeff Varner Survivor MacGyver -0.0015 0.015 -0.102 0.919 -0.031 0.028 contestant_episode_Jeff Varner The Stakes Have Been Raised -0.0391 0.014 -2.837 0.005 -0.066 -0.012 contestant_episode_Jeff Varner The Tables Have Turned -0.0505 0.014 -3.693 0.000 -0.077 -0.024 contestant_episode_Jeff Varner Vote Early, Vote Often -0.0083 0.013 -0.631 0.528 -0.034 0.017 contestant_episode_Jeff Varner We Got a Rat -0.0265 0.015 -1.793 0.073 -0.055 0.002 contestant_episode_Jeff Varner What Happened on Exile, Stays on Exile -0.1181 0.016 -7.585 0.000 -0.149 -0.088 contestant_episode_Jeff Varner What's the Beef%3F -0.0122 0.015 -0.820 0.412 -0.041 0.017 contestant_episode_Jefra Bland Chaos Is My Friend -0.0506 0.032 -1.591 0.112 -0.113 0.012 contestant_episode_Jefra Bland Cops-R-Us (episode) -0.0581 0.056 -1.032 0.302 -0.168 0.052 contestant_episode_Jefra Bland Havoc to Wreak 0.0056 0.042 0.134 0.894 -0.076 0.087 contestant_episode_Jefra Bland Head of the Snake 0.0091 0.033 0.278 0.781 -0.055 0.073 contestant_episode_Jefra Bland Hot Girl with a Grudge 0.0015 0.054 0.028 0.977 -0.105 0.108 contestant_episode_Jefra Bland Mad Treasure Hunt 0.0768 0.038 2.045 0.041 0.003 0.150 contestant_episode_Jefra Bland Odd One Out 0.0219 0.045 0.489 0.625 -0.066 0.110 contestant_episode_Jefra Bland Our Time to Shine -0.0207 0.058 -0.359 0.719 -0.134 0.092 contestant_episode_Jefra Bland Sitting in My Spy Shack 0.0456 0.034 1.359 0.174 -0.020 0.111 contestant_episode_Jefra Bland We Found Our Zombies -0.0229 0.048 -0.478 0.633 -0.117 0.071 contestant_episode_Jenn Brown Crazy Is as Crazy Does -0.0401 0.025 -1.613 0.107 -0.089 0.009 contestant_episode_Jenn Brown It Will Be My Revenge -0.0153 0.021 -0.720 0.471 -0.057 0.026 contestant_episode_Jenn Brown It's Survivor Warfare 0.0315 0.023 1.352 0.176 -0.014 0.077 contestant_episode_Jenn Brown Keep It Real 0.0217 0.014 1.566 0.117 -0.005 0.049 contestant_episode_Jenn Brown Livin' on the Edge 0.0188 0.013 1.435 0.151 -0.007 0.045 contestant_episode_Jenn Brown The Line Will Be Drawn Tonight 0.0414 0.015 2.681 0.007 0.011 0.072 contestant_episode_Jenn Brown Winner Winner, Chicken Dinner 0.0098 0.021 0.463 0.643 -0.032 0.051 contestant_episode_Jennifer Lanzetti Kindergarten Camp -0.0178 0.032 -0.559 0.576 -0.080 0.045 contestant_episode_Jeremiah Wood Chaos Is My Friend -0.0955 0.040 -2.391 0.017 -0.174 -0.017 contestant_episode_Jeremiah Wood Cops-R-Us (episode) -0.0404 0.052 -0.785 0.433 -0.141 0.061 contestant_episode_Jeremiah Wood Head of the Snake -0.0568 0.043 -1.333 0.183 -0.140 0.027 contestant_episode_Jeremiah Wood Hot Girl with a Grudge 0.0508 0.053 0.958 0.338 -0.053 0.155 contestant_episode_Jeremiah Wood Mad Treasure Hunt 0.1023 0.046 2.245 0.025 0.013 0.192 contestant_episode_Jeremiah Wood Odd One Out 0.0686 0.049 1.390 0.165 -0.028 0.165 contestant_episode_Jeremiah Wood Our Time to Shine 0.0801 0.049 1.622 0.105 -0.017 0.177 contestant_episode_Jeremiah Wood Sitting in My Spy Shack 0.0028 0.035 0.082 0.934 -0.065 0.071 contestant_episode_Jeremiah Wood We Found Our Zombies 0.0319 0.040 0.798 0.425 -0.047 0.110 contestant_episode_Jeremy Collins Actions vs. Accusations -0.0435 0.018 -2.406 0.016 -0.079 -0.008 contestant_episode_Jeremy Collins Blood Is Blood 0.0237 0.022 1.077 0.281 -0.019 0.067 contestant_episode_Jeremy Collins Bunking with the Devil 0.0050 0.026 0.193 0.847 -0.046 0.056 contestant_episode_Jeremy Collins Gettin' to Crunch Time -0.0005 0.025 -0.019 0.985 -0.049 0.048 contestant_episode_Jeremy Collins Make Some Magic Happen 0.0137 0.021 0.639 0.523 -0.028 0.056 contestant_episode_Jeremy Collins Method to This Madness -0.0039 0.024 -0.163 0.871 -0.051 0.043 contestant_episode_Jeremy Collins Million Dollar Decision -0.0083 0.018 -0.463 0.643 -0.043 0.027 contestant_episode_Jeremy Collins My Wheels Are Spinning -0.0110 0.023 -0.481 0.630 -0.056 0.034 contestant_episode_Jeremy Collins Play to Win -0.0464 0.024 -1.898 0.058 -0.094 0.002 contestant_episode_Jeremy Collins Suck It Up and Survive -0.0052 0.022 -0.234 0.815 -0.049 0.038 contestant_episode_Jeremy Collins Survivor MacGyver -0.0560 0.027 -2.039 0.041 -0.110 -0.002 contestant_episode_Jeremy Collins Tiny Little Shanks to the Heart 0.0034 0.024 0.141 0.888 -0.043 0.050 contestant_episode_Jeremy Collins Villains Have More Fun 0.0274 0.024 1.128 0.259 -0.020 0.075 contestant_episode_Jeremy Collins We Got a Rat -0.0289 0.026 -1.117 0.264 -0.080 0.022 contestant_episode_Jeremy Collins We're a Hot Mess 0.0258 0.023 1.133 0.257 -0.019 0.070 contestant_episode_Jeremy Collins What's the Beef%3F -0.0406 0.027 -1.482 0.138 -0.094 0.013 contestant_episode_Jeremy Collins Witches Coven (episode) -0.0060 0.022 -0.268 0.789 -0.050 0.038 contestant_episode_Jeremy Collins Wrinkle in the Plan -0.0167 0.022 -0.764 0.445 -0.060 0.026 contestant_episode_Jeremy Collins You Call, We'll Haul -0.0071 0.023 -0.314 0.753 -0.051 0.037 contestant_episode_Jessica Johnston I Don't Like Having Snakes Around -0.0117 0.026 -0.451 0.652 -0.062 0.039 contestant_episode_Jessica Johnston I'm Not Crazy, I'm Confident 0.0445 0.031 1.418 0.156 -0.017 0.106 contestant_episode_Jessica Johnston I'm a Wild Banshee 0.0581 0.030 1.965 0.049 0.000 0.116 contestant_episode_Jessica Johnston My Kisses Are Very Private 0.0530 0.024 2.229 0.026 0.006 0.100 contestant_episode_Jessica Johnston The Past Will Eat You Alive 0.0868 0.025 3.490 0.000 0.038 0.136 contestant_episode_Jessica Johnston This Is Why You Play Survivor 0.0692 0.024 2.897 0.004 0.022 0.116 contestant_episode_Jessica Lewis I Will Destroy You 0.0267 0.028 0.969 0.333 -0.027 0.081 contestant_episode_Jessica Lewis I'm the Kingpin 0.0248 0.029 0.849 0.396 -0.033 0.082 contestant_episode_Jessica Lewis Idol Search Party -0.0229 0.029 -0.789 0.430 -0.080 0.034 contestant_episode_Jessica Lewis Love Goggles -0.0847 0.045 -1.895 0.058 -0.172 0.003 contestant_episode_Jessica Lewis May the Best Generation Win 0.0128 0.032 0.406 0.685 -0.049 0.075 contestant_episode_Jessica Lewis Million Dollar Gamble -0.0205 0.022 -0.919 0.358 -0.064 0.023 contestant_episode_Jessica Lewis Still Throwin' Punches -0.0258 0.027 -0.941 0.347 -0.080 0.028 contestant_episode_Jessica Lewis The Truth Works Well 0.0377 0.033 1.134 0.257 -0.027 0.103 contestant_episode_Jessica Lewis Who's the Sucker at the Table%3F -0.0895 0.028 -3.156 0.002 -0.145 -0.034 contestant_episode_Jessica Lewis Your Job Is Recon -0.0198 0.033 -0.593 0.553 -0.085 0.046 contestant_episode_Joaquin Souberbielle It Will Be My Revenge 0.0338 0.039 0.867 0.386 -0.043 0.110 contestant_episode_Joaquin Souberbielle It's Survivor Warfare -0.0396 0.035 -1.124 0.261 -0.109 0.029 contestant_episode_Joaquin Souberbielle Winner Winner, Chicken Dinner 0.0387 0.032 1.193 0.233 -0.025 0.102 contestant_episode_Joe Anglim Bunking with the Devil 0.0498 0.026 1.927 0.054 -0.001 0.100 contestant_episode_Joe Anglim Crazy Is as Crazy Does 0.0318 0.027 1.167 0.243 -0.022 0.085 contestant_episode_Joe Anglim It Will Be My Revenge -0.0063 0.026 -0.240 0.810 -0.057 0.045 contestant_episode_Joe Anglim It's Survivor Warfare 0.0359 0.026 1.360 0.174 -0.016 0.088 contestant_episode_Joe Anglim Keep It Real -0.1439 0.123 -1.171 0.242 -0.385 0.097 contestant_episode_Joe Anglim Livin' on the Edge -0.0045 0.022 -0.206 0.837 -0.048 0.039 contestant_episode_Joe Anglim My Wheels Are Spinning 0.0396 0.023 1.739 0.082 -0.005 0.084 contestant_episode_Joe Anglim Play to Win 0.0181 0.027 0.663 0.507 -0.035 0.071 contestant_episode_Joe Anglim Survivor MacGyver 0.0057 0.026 0.214 0.830 -0.046 0.058 contestant_episode_Joe Anglim The Line Will Be Drawn Tonight 0.0344 0.026 1.318 0.187 -0.017 0.086 contestant_episode_Joe Anglim Tiny Little Shanks to the Heart 0.0320 0.024 1.355 0.175 -0.014 0.078 contestant_episode_Joe Anglim We Got a Rat 0.0503 0.026 1.915 0.056 -0.001 0.102 contestant_episode_Joe Anglim What's the Beef%3F 0.0224 0.024 0.924 0.356 -0.025 0.070 contestant_episode_Joe Anglim Winner Winner, Chicken Dinner -0.0219 0.025 -0.886 0.376 -0.070 0.026 contestant_episode_Joe Anglim Witches Coven (episode) 0.0234 0.024 0.995 0.320 -0.023 0.070 contestant_episode_Joe Anglim You Call, We'll Haul -0.0074 0.023 -0.324 0.746 -0.052 0.037 contestant_episode_Joe Mena Fear of the Unknown -0.0222 0.017 -1.296 0.195 -0.056 0.011 contestant_episode_Joe Mena Get to Gettin' -0.0363 0.014 -2.641 0.008 -0.063 -0.009 contestant_episode_Joe Mena I Don't Like Having Snakes Around -0.0319 0.015 -2.186 0.029 -0.060 -0.003 contestant_episode_Joe Mena I'm Not Crazy, I'm Confident -0.0632 0.024 -2.678 0.007 -0.110 -0.017 contestant_episode_Joe Mena I'm a Wild Banshee -0.0316 0.026 -1.203 0.229 -0.083 0.020 contestant_episode_Joe Mena My Kisses Are Very Private -0.0401 0.019 -2.066 0.039 -0.078 -0.002 contestant_episode_Joe Mena Not Going to Roll Over and Die -0.0477 0.020 -2.432 0.015 -0.086 -0.009 contestant_episode_Joe Mena Playing with the Devil -0.0111 0.020 -0.566 0.572 -0.049 0.027 contestant_episode_Joe Mena The Past Will Eat You Alive -0.0309 0.022 -1.423 0.155 -0.074 0.012 contestant_episode_Joe Mena This Is Why You Play Survivor 0.0098 0.016 0.613 0.540 -0.022 0.041 contestant_episode_Joe del Campo I'm a Mental Giant -0.0054 0.023 -0.233 0.816 -0.051 0.040 contestant_episode_Joe del Campo It's Merge Time -0.0264 0.019 -1.376 0.169 -0.064 0.011 contestant_episode_Joe del Campo It's Psychological Warfare -0.0235 0.022 -1.047 0.295 -0.068 0.020 contestant_episode_Joe del Campo It's a 'Me' Game, Not a 'We' Game -0.0048 0.015 -0.318 0.750 -0.034 0.025 contestant_episode_Joe del Campo Kindergarten Camp -0.0535 0.024 -2.245 0.025 -0.100 -0.007 contestant_episode_Joe del Campo Now's the Time to Start Scheming 0.0037 0.014 0.274 0.784 -0.023 0.030 contestant_episode_Joe del Campo Play or Go Home -0.0706 0.022 -3.156 0.002 -0.114 -0.027 contestant_episode_Joe del Campo Signed, Sealed and Delivered -0.0349 0.029 -1.194 0.233 -0.092 0.022 contestant_episode_Joe del Campo The Circle of Life -0.0344 0.027 -1.281 0.200 -0.087 0.018 contestant_episode_Joe del Campo The Devils We Know -0.0489 0.021 -2.341 0.019 -0.090 -0.008 contestant_episode_Joe del Campo The Jocks vs. the Pretty People -0.0253 0.018 -1.418 0.156 -0.060 0.010 contestant_episode_Joe del Campo With Me or Not with Me -0.1426 0.140 -1.015 0.310 -0.418 0.133 contestant_episode_John Cochran Come Over to the Dark Side 0.0348 0.024 1.435 0.151 -0.013 0.082 contestant_episode_John Cochran Cult Like 0.0608 0.054 1.135 0.256 -0.044 0.166 contestant_episode_John Cochran Cut Off the Head of the Snake -0.0012 0.034 -0.037 0.971 -0.067 0.065 contestant_episode_John Cochran Cut Throat 0.0181 0.050 0.360 0.719 -0.080 0.117 contestant_episode_John Cochran Don't Say Anything About My Mom 0.0101 0.027 0.374 0.709 -0.043 0.063 contestant_episode_John Cochran Double Agent -0.0363 0.042 -0.860 0.390 -0.119 0.046 contestant_episode_John Cochran Honey Badger -0.0244 0.048 -0.511 0.610 -0.118 0.069 contestant_episode_John Cochran I Need Redemption -0.0626 0.061 -1.021 0.307 -0.183 0.058 contestant_episode_John Cochran Operation Thunder Dome -0.0074 0.045 -0.163 0.870 -0.096 0.081 contestant_episode_John Cochran Persona Non Grata -0.0421 0.058 -0.724 0.469 -0.156 0.072 contestant_episode_John Cochran Running the Show -0.0329 0.052 -0.627 0.531 -0.136 0.070 contestant_episode_John Cochran She Annoys Me Greatly -0.0608 0.038 -1.606 0.108 -0.135 0.013 contestant_episode_John Cochran The Beginning of the End -0.0108 0.029 -0.375 0.708 -0.067 0.046 contestant_episode_John Cochran Tubby Lunchbox 0.0491 0.035 1.387 0.165 -0.020 0.119 contestant_episode_John Cochran Zipping Over the Cuckoo's Nest -0.0254 0.033 -0.759 0.448 -0.091 0.040 contestant_episode_John Cody One-Man Wrecking Ball 0.0792 0.059 1.342 0.180 -0.036 0.195 contestant_episode_John Cody Opening Pandora's Box -0.0377 0.047 -0.806 0.420 -0.129 0.054 contestant_episode_John Cody Rule in Chaos 0.0107 0.062 0.172 0.863 -0.111 0.132 contestant_episode_John Cody Skin of My Teeth 0.0010 0.053 0.019 0.985 -0.103 0.106 contestant_episode_John Cody Swoop In for the Kill -0.0066 0.048 -0.137 0.891 -0.102 0.088 contestant_episode_John Cody The Dead Can Still Talk -0.0506 0.060 -0.838 0.402 -0.169 0.068 contestant_episode_John Rocker Method to This Madness 0.0173 0.021 0.843 0.399 -0.023 0.058 contestant_episode_John Rocker Suck It Up and Survive 0.0483 0.018 2.714 0.007 0.013 0.083 contestant_episode_Jon Misch Blood Is Blood 0.0301 0.033 0.908 0.364 -0.035 0.095 contestant_episode_Jon Misch Gettin' to Crunch Time 0.0098 0.031 0.315 0.753 -0.051 0.071 contestant_episode_Jon Misch Let's Make a Move -0.0008 0.034 -0.023 0.982 -0.067 0.065 contestant_episode_Jon Misch Make Some Magic Happen -0.0406 0.034 -1.206 0.228 -0.106 0.025 contestant_episode_Jon Misch Method to This Madness -0.0613 0.039 -1.576 0.115 -0.138 0.015 contestant_episode_Jon Misch Million Dollar Decision 0.0007 0.035 0.020 0.984 -0.067 0.069 contestant_episode_Jon Misch Still Holdin' On 0.0049 0.033 0.150 0.881 -0.060 0.070 contestant_episode_Jon Misch Suck It Up and Survive -0.0026 0.037 -0.071 0.944 -0.075 0.069 contestant_episode_Jon Misch This Is Where We Build Trust -0.0038 0.034 -0.112 0.911 -0.070 0.062 contestant_episode_Jon Misch We're a Hot Mess 0.0076 0.038 0.198 0.843 -0.067 0.083 contestant_episode_Jon Misch Wrinkle in the Plan 0.0288 0.038 0.758 0.449 -0.046 0.103 contestant_episode_Jonathan Penner Down and Dirty -0.0156 0.046 -0.338 0.736 -0.106 0.075 contestant_episode_Jonathan Penner Hell Hath Frozen Over 0.0470 0.034 1.393 0.164 -0.019 0.113 contestant_episode_Jonathan Penner Little Miss Perfect 0.0720 0.033 2.168 0.030 0.007 0.137 contestant_episode_Jonathan Penner Not the Only Actor on This Island -0.0233 0.042 -0.549 0.583 -0.106 0.060 contestant_episode_Jonathan Penner Whiners Are Wieners -0.0158 0.043 -0.368 0.713 -0.100 0.068 contestant_episode_Josh Canfield Blood Is Blood 0.0406 0.030 1.361 0.174 -0.018 0.099 contestant_episode_Josh Canfield Make Some Magic Happen 0.0112 0.035 0.317 0.751 -0.058 0.081 contestant_episode_Josh Canfield Method to This Madness 0.0126 0.029 0.441 0.659 -0.043 0.068 contestant_episode_Josh Canfield Million Dollar Decision 0.0276 0.027 1.016 0.309 -0.026 0.081 contestant_episode_Josh Canfield Suck It Up and Survive -0.0199 0.027 -0.748 0.454 -0.072 0.032 contestant_episode_Josh Canfield We're a Hot Mess 0.0405 0.031 1.316 0.188 -0.020 0.101 contestant_episode_Josh Canfield Wrinkle in the Plan 0.0352 0.027 1.324 0.185 -0.017 0.087 contestant_episode_Julia Landauer There's Gonna Be Hell to Pay 0.0267 0.060 0.449 0.653 -0.090 0.144 contestant_episode_Julia Landauer Tubby Lunchbox 0.0389 0.051 0.767 0.443 -0.060 0.138 contestant_episode_Julia Sokolowski I'm a Mental Giant 0.1022 0.077 1.327 0.185 -0.049 0.253 contestant_episode_Julia Sokolowski It's Merge Time 0.0657 0.074 0.892 0.372 -0.079 0.210 contestant_episode_Julia Sokolowski It's Psychological Warfare 0.0718 0.074 0.972 0.331 -0.073 0.217 contestant_episode_Julia Sokolowski It's a 'Me' Game, Not a 'We' Game 0.0914 0.071 1.287 0.198 -0.048 0.230 contestant_episode_Julia Sokolowski Kindergarten Camp 0.0847 0.079 1.070 0.285 -0.071 0.240 contestant_episode_Julia Sokolowski Play or Go Home 0.0667 0.074 0.903 0.367 -0.078 0.211 contestant_episode_Julia Sokolowski Signed, Sealed and Delivered 0.1338 0.078 1.716 0.086 -0.019 0.287 contestant_episode_Julia Sokolowski The Circle of Life 0.1044 0.077 1.361 0.173 -0.046 0.255 contestant_episode_Julia Sokolowski The Jocks vs. the Pretty People 0.0557 0.074 0.754 0.451 -0.089 0.200 contestant_episode_Julie McGee Blood Is Blood 0.0848 0.044 1.924 0.054 -0.002 0.171 contestant_episode_Julie McGee Make Some Magic Happen 0.0764 0.038 1.992 0.046 0.001 0.151 contestant_episode_Julie McGee Method to This Madness 0.1006 0.042 2.423 0.015 0.019 0.182 contestant_episode_Julie McGee Million Dollar Decision 0.0301 0.030 1.002 0.316 -0.029 0.089 contestant_episode_Julie McGee Suck It Up and Survive 0.0429 0.041 1.045 0.296 -0.038 0.123 contestant_episode_Julie McGee We're a Hot Mess 0.0789 0.034 2.331 0.020 0.013 0.145 contestant_episode_Kass McQuillen Bag of Tricks 0.0128 0.019 0.670 0.503 -0.025 0.050 contestant_episode_Kass McQuillen Bunking with the Devil -0.0519 0.026 -1.995 0.046 -0.103 -0.001 contestant_episode_Kass McQuillen Chaos Is My Friend -0.0006 0.016 -0.037 0.970 -0.031 0.030 contestant_episode_Kass McQuillen Cops-R-Us (episode) -0.0698 0.041 -1.690 0.091 -0.151 0.011 contestant_episode_Kass McQuillen Havoc to Wreak 0.0064 0.014 0.458 0.647 -0.021 0.034 contestant_episode_Kass McQuillen Head of the Snake -0.0343 0.019 -1.796 0.072 -0.072 0.003 contestant_episode_Kass McQuillen Hot Girl with a Grudge -0.0494 0.019 -2.582 0.010 -0.087 -0.012 contestant_episode_Kass McQuillen Mad Treasure Hunt -0.0133 0.018 -0.753 0.451 -0.048 0.021 contestant_episode_Kass McQuillen Odd One Out 0.0493 0.047 1.054 0.292 -0.042 0.141 contestant_episode_Kass McQuillen Our Time to Shine -0.0330 0.030 -1.107 0.269 -0.091 0.025 contestant_episode_Kass McQuillen Play to Win -0.0385 0.022 -1.716 0.086 -0.082 0.005 contestant_episode_Kass McQuillen Sitting in My Spy Shack 0.0426 0.019 2.281 0.023 0.006 0.079 contestant_episode_Kass McQuillen Straw That Broke the Camel's Back 0.0006 0.018 0.036 0.971 -0.034 0.035 contestant_episode_Kass McQuillen Survivor MacGyver -0.0229 0.026 -0.885 0.376 -0.074 0.028 contestant_episode_Kass McQuillen We Found Our Zombies 0.0366 0.035 1.058 0.290 -0.031 0.104 contestant_episode_Kass McQuillen We Got a Rat -0.0033 0.027 -0.123 0.902 -0.056 0.050 contestant_episode_Kass McQuillen What's the Beef%3F -0.0006 0.025 -0.023 0.982 -0.049 0.048 contestant_episode_Kat Edorsson It's Gonna Be Chaos -0.1423 0.146 -0.976 0.329 -0.428 0.143 contestant_episode_Kat Edorsson Never Say Die -0.0370 0.042 -0.873 0.383 -0.120 0.046 contestant_episode_Kat Edorsson One Armed Dude and Three Moms 0.0021 0.056 0.037 0.970 -0.107 0.111 contestant_episode_Kat Edorsson One-Man Wrecking Ball 0.0389 0.040 0.966 0.334 -0.040 0.118 contestant_episode_Kat Edorsson Opening Pandora's Box -0.0229 0.057 -0.398 0.691 -0.136 0.090 contestant_episode_Kat Edorsson Swoop In for the Kill -0.0339 0.042 -0.817 0.414 -0.115 0.047 contestant_episode_Kat Edorsson The Dead Can Still Talk 0.0056 0.048 0.119 0.905 -0.087 0.099 contestant_episode_Katie Collins Blood Is Thicker than Anything 0.0179 0.056 0.322 0.748 -0.091 0.127 contestant_episode_Katie Collins Gloves Come Off 0.0133 0.036 0.369 0.712 -0.057 0.084 contestant_episode_Katie Collins My Brother's Keeper 0.0211 0.040 0.522 0.601 -0.058 0.100 contestant_episode_Katie Collins One-Man Wrecking Ball 0.0416 0.045 0.931 0.352 -0.046 0.129 contestant_episode_Katie Collins Out on a Limb (episode) 0.0955 0.036 2.624 0.009 0.024 0.167 contestant_episode_Katie Collins Rustle Feathers 0.0362 0.031 1.177 0.239 -0.024 0.097 contestant_episode_Katie Collins Skin of My Teeth 0.0215 0.048 0.443 0.658 -0.073 0.116 contestant_episode_Katie Collins Swoop In for the Kill 0.0287 0.042 0.680 0.497 -0.054 0.111 contestant_episode_Keith Nale Actions vs. Accusations 0.0261 0.031 0.856 0.392 -0.034 0.086 contestant_episode_Keith Nale Blood Is Blood 0.0117 0.018 0.633 0.527 -0.024 0.048 contestant_episode_Keith Nale Bunking with the Devil 0.0333 0.027 1.240 0.215 -0.019 0.086 contestant_episode_Keith Nale Gettin' to Crunch Time 0.0111 0.016 0.697 0.486 -0.020 0.042 contestant_episode_Keith Nale Let's Make a Move 0.0312 0.017 1.805 0.071 -0.003 0.065 contestant_episode_Keith Nale Make Some Magic Happen -0.0452 0.022 -2.008 0.045 -0.089 -0.001 contestant_episode_Keith Nale Method to This Madness -0.0173 0.034 -0.507 0.612 -0.084 0.050 contestant_episode_Keith Nale Million Dollar Decision -0.0100 0.024 -0.416 0.677 -0.057 0.037 contestant_episode_Keith Nale My Wheels Are Spinning 0.0480 0.023 2.045 0.041 0.002 0.094 contestant_episode_Keith Nale Play to Win 0.0147 0.023 0.641 0.521 -0.030 0.060 contestant_episode_Keith Nale Still Holdin' On 0.0369 0.015 2.462 0.014 0.008 0.066 contestant_episode_Keith Nale Suck It Up and Survive 0.0114 0.032 0.352 0.724 -0.052 0.074 contestant_episode_Keith Nale Survivor MacGyver -0.0205 0.030 -0.682 0.495 -0.079 0.038 contestant_episode_Keith Nale This Is Where We Build Trust -0.0286 0.021 -1.342 0.180 -0.070 0.013 contestant_episode_Keith Nale Tiny Little Shanks to the Heart 0.0302 0.028 1.079 0.281 -0.025 0.085 contestant_episode_Keith Nale Villains Have More Fun -0.1154 0.143 -0.808 0.419 -0.395 0.165 contestant_episode_Keith Nale We Got a Rat 0.0163 0.027 0.606 0.545 -0.036 0.069 contestant_episode_Keith Nale We're a Hot Mess -0.0151 0.018 -0.834 0.404 -0.051 0.020 contestant_episode_Keith Nale What's the Beef%3F -0.0160 0.026 -0.612 0.541 -0.067 0.035 contestant_episode_Keith Nale Witches Coven (episode) 0.0281 0.024 1.160 0.246 -0.019 0.076 contestant_episode_Keith Nale Wrinkle in the Plan -0.0278 0.020 -1.423 0.155 -0.066 0.010 contestant_episode_Keith Nale You Call, We'll Haul 0.0243 0.018 1.317 0.188 -0.012 0.061 contestant_episode_Kelley Wentworth Actions vs. Accusations -0.0520 0.036 -1.429 0.153 -0.123 0.019 contestant_episode_Kelley Wentworth Blood Is Blood 0.0150 0.022 0.680 0.496 -0.028 0.058 contestant_episode_Kelley Wentworth Bunking with the Devil 0.0063 0.032 0.196 0.844 -0.057 0.069 contestant_episode_Kelley Wentworth Method to This Madness -0.0399 0.033 -1.205 0.228 -0.105 0.025 contestant_episode_Kelley Wentworth My Wheels Are Spinning 0.0209 0.029 0.733 0.463 -0.035 0.077 contestant_episode_Kelley Wentworth Play to Win -0.0132 0.029 -0.452 0.651 -0.070 0.044 contestant_episode_Kelley Wentworth Suck It Up and Survive -0.0034 0.032 -0.106 0.916 -0.066 0.059 contestant_episode_Kelley Wentworth Survivor MacGyver -0.0171 0.029 -0.592 0.554 -0.074 0.040 contestant_episode_Kelley Wentworth Tiny Little Shanks to the Heart -0.1339 0.144 -0.932 0.352 -0.415 0.148 contestant_episode_Kelley Wentworth Villains Have More Fun 0.0452 0.030 1.529 0.126 -0.013 0.103 contestant_episode_Kelley Wentworth We Got a Rat -0.0227 0.032 -0.714 0.475 -0.085 0.040 contestant_episode_Kelley Wentworth We're a Hot Mess 0.0779 0.027 2.911 0.004 0.025 0.130 contestant_episode_Kelley Wentworth What's the Beef%3F 0.0153 0.031 0.488 0.626 -0.046 0.077 contestant_episode_Kelley Wentworth Witches Coven (episode) 0.0009 0.029 0.031 0.975 -0.056 0.058 contestant_episode_Kelley Wentworth You Call, We'll Haul 0.0067 0.024 0.276 0.783 -0.041 0.054 contestant_episode_Kelly Remington It Will Be My Revenge -0.0010 0.040 -0.025 0.980 -0.080 0.078 contestant_episode_Kelly Remington It's Survivor Warfare 0.0625 0.041 1.541 0.123 -0.017 0.142 contestant_episode_Kelly Remington The Line Will Be Drawn Tonight 0.0130 0.028 0.467 0.641 -0.042 0.068 contestant_episode_Kelly Remington Winner Winner, Chicken Dinner 0.0140 0.026 0.535 0.593 -0.037 0.065 contestant_episode_Kelly Wiglesworth Bunking with the Devil -0.0635 0.029 -2.206 0.027 -0.120 -0.007 contestant_episode_Kelly Wiglesworth Play to Win -0.0341 0.026 -1.296 0.195 -0.086 0.017 contestant_episode_Kelly Wiglesworth Survivor MacGyver -0.0555 0.029 -1.899 0.058 -0.113 0.002 contestant_episode_Kelly Wiglesworth We Got a Rat -0.0405 0.029 -1.400 0.162 -0.097 0.016 contestant_episode_Kelly Wiglesworth What's the Beef%3F -0.0162 0.030 -0.539 0.590 -0.075 0.043 contestant_episode_Kelly Wiglesworth Witches Coven (episode) -0.0012 0.024 -0.052 0.958 -0.048 0.045 contestant_episode_Kelly Wiglesworth You Call, We'll Haul -0.0241 0.028 -0.865 0.387 -0.079 0.031 contestant_episode_Ken McNickle I Will Destroy You 0.0350 0.017 2.047 0.041 0.001 0.069 contestant_episode_Ken McNickle I'm the Kingpin 0.0100 0.017 0.586 0.558 -0.023 0.043 contestant_episode_Ken McNickle Idol Search Party 0.0216 0.018 1.223 0.221 -0.013 0.056 contestant_episode_Ken McNickle Love Goggles 0.0366 0.020 1.823 0.068 -0.003 0.076 contestant_episode_Ken McNickle May the Best Generation Win 0.0680 0.023 2.937 0.003 0.023 0.113 contestant_episode_Ken McNickle Million Dollar Gamble 0.0360 0.012 3.079 0.002 0.013 0.059 contestant_episode_Ken McNickle Slayed the Survivor Dragon 0.0667 0.012 5.416 0.000 0.043 0.091 contestant_episode_Ken McNickle Still Throwin' Punches 0.0063 0.025 0.253 0.800 -0.042 0.055 contestant_episode_Ken McNickle The Truth Works Well 0.0440 0.016 2.704 0.007 0.012 0.076 contestant_episode_Ken McNickle Who's the Sucker at the Table%3F -0.0308 0.021 -1.434 0.152 -0.073 0.011 contestant_episode_Ken McNickle Your Job Is Recon 0.0188 0.019 0.981 0.326 -0.019 0.056 contestant_episode_Kim Spradlin-Wolfe I'm No Dummy -0.0728 0.039 -1.850 0.064 -0.150 0.004 contestant_episode_Kim Spradlin-Wolfe It's Gonna Be Chaos 0.0019 0.040 0.048 0.962 -0.076 0.080 contestant_episode_Kim Spradlin-Wolfe It's Human Nature -0.0187 0.039 -0.480 0.631 -0.095 0.058 contestant_episode_Kim Spradlin-Wolfe Just Annihilate Them -0.1065 0.059 -1.797 0.072 -0.223 0.010 contestant_episode_Kim Spradlin-Wolfe Never Say Die -0.1571 0.145 -1.084 0.278 -0.441 0.127 contestant_episode_Kim Spradlin-Wolfe The Beauty in a Merge 0.0453 0.057 0.794 0.427 -0.067 0.157 contestant_episode_Kimmi Kappenberg Bunking with the Devil 0.0038 0.023 0.167 0.867 -0.040 0.048 contestant_episode_Kimmi Kappenberg My Wheels Are Spinning 0.0386 0.019 2.040 0.041 0.002 0.076 contestant_episode_Kimmi Kappenberg Play to Win 0.0243 0.020 1.233 0.218 -0.014 0.063 contestant_episode_Kimmi Kappenberg Survivor MacGyver 0.0049 0.024 0.201 0.840 -0.042 0.052 contestant_episode_Kimmi Kappenberg Tiny Little Shanks to the Heart 0.0555 0.019 2.919 0.004 0.018 0.093 contestant_episode_Kimmi Kappenberg Villains Have More Fun 0.0687 0.020 3.478 0.001 0.030 0.107 contestant_episode_Kimmi Kappenberg We Got a Rat 0.0127 0.022 0.568 0.570 -0.031 0.056 contestant_episode_Kimmi Kappenberg What's the Beef%3F -0.0281 0.022 -1.276 0.202 -0.071 0.015 contestant_episode_Kimmi Kappenberg Witches Coven (episode) 0.0070 0.020 0.358 0.720 -0.031 0.045 contestant_episode_Kimmi Kappenberg You Call, We'll Haul 0.0259 0.023 1.145 0.252 -0.018 0.070 contestant_episode_Kyle Jason I'm a Mental Giant -0.0218 0.024 -0.899 0.369 -0.069 0.026 contestant_episode_Kyle Jason It's Merge Time -0.0144 0.022 -0.664 0.507 -0.057 0.028 contestant_episode_Kyle Jason It's Psychological Warfare -0.0210 0.023 -0.926 0.355 -0.066 0.023 contestant_episode_Kyle Jason It's a 'Me' Game, Not a 'We' Game 0.0415 0.026 1.629 0.103 -0.008 0.092 contestant_episode_Kyle Jason Kindergarten Camp -0.0717 0.022 -3.262 0.001 -0.115 -0.029 contestant_episode_Kyle Jason Now's the Time to Start Scheming -0.0109 0.024 -0.460 0.645 -0.057 0.035 contestant_episode_Kyle Jason Play or Go Home -0.0169 0.024 -0.698 0.485 -0.064 0.031 contestant_episode_Kyle Jason Signed, Sealed and Delivered -0.0681 0.020 -3.403 0.001 -0.107 -0.029 contestant_episode_Kyle Jason The Circle of Life -0.0542 0.023 -2.339 0.019 -0.100 -0.009 contestant_episode_Kyle Jason The Devils We Know -0.0064 0.023 -0.284 0.777 -0.051 0.038 contestant_episode_Kyle Jason The Jocks vs. the Pretty People -0.0140 0.020 -0.702 0.483 -0.053 0.025 contestant_episode_LJ McKanas Cops-R-Us (episode) 0.0461 0.053 0.876 0.381 -0.057 0.149 contestant_episode_LJ McKanas Head of the Snake 0.0194 0.028 0.702 0.483 -0.035 0.073 contestant_episode_LJ McKanas Hot Girl with a Grudge 0.0252 0.043 0.583 0.560 -0.060 0.110 contestant_episode_LJ McKanas Mad Treasure Hunt 0.0790 0.035 2.287 0.022 0.011 0.147 contestant_episode_LJ McKanas Odd One Out -0.0311 0.030 -1.034 0.301 -0.090 0.028 contestant_episode_LJ McKanas Our Time to Shine -0.0225 0.040 -0.567 0.570 -0.100 0.055 contestant_episode_LJ McKanas Sitting in My Spy Shack 0.0075 0.038 0.198 0.843 -0.067 0.082 contestant_episode_LJ McKanas We Found Our Zombies 0.0451 0.031 1.438 0.151 -0.016 0.107 contestant_episode_Laura Alexander Kill or Be Killed 0.0239 0.064 0.372 0.710 -0.102 0.150 contestant_episode_Laura Alexander There's Gonna Be Hell to Pay -0.0563 0.065 -0.864 0.388 -0.184 0.071 contestant_episode_Laura Boneham One Armed Dude and Three Moms -0.0689 0.056 -1.230 0.219 -0.179 0.041 contestant_episode_Laura Boneham One-Man Wrecking Ball 0.0161 0.041 0.395 0.693 -0.064 0.096 contestant_episode_Laura Boneham Opening Pandora's Box -0.0128 0.057 -0.226 0.821 -0.124 0.098 contestant_episode_Laura Boneham Skin of My Teeth -0.0199 0.040 -0.503 0.615 -0.097 0.058 contestant_episode_Laura Boneham Swoop In for the Kill -0.0141 0.035 -0.402 0.688 -0.083 0.055 contestant_episode_Laura Boneham The Dead Can Still Talk -0.0466 0.047 -0.992 0.321 -0.139 0.045 contestant_episode_Laura Morett Blood Is Thicker than Anything 0.1024 0.043 2.408 0.016 0.019 0.186 contestant_episode_Laura Morett Gloves Come Off 0.1048 0.052 2.033 0.042 0.004 0.206 contestant_episode_Laura Morett My Brother's Keeper 0.0142 0.046 0.312 0.755 -0.075 0.104 contestant_episode_Laura Morett One Armed Dude and Three Moms 0.0182 0.056 0.324 0.746 -0.092 0.128 contestant_episode_Laura Morett One-Man Wrecking Ball 0.1029 0.052 1.976 0.048 0.001 0.205 contestant_episode_Laura Morett Opening Pandora's Box 0.0743 0.057 1.306 0.192 -0.037 0.186 contestant_episode_Laura Morett Out on a Limb (episode) 0.1421 0.048 2.975 0.003 0.048 0.236 contestant_episode_Laura Morett Rustle Feathers 0.0788 0.049 1.596 0.110 -0.018 0.175 contestant_episode_Laura Morett Skin of My Teeth 0.0630 0.038 1.638 0.101 -0.012 0.138 contestant_episode_Laura Morett Swoop In for the Kill 0.0738 0.040 1.826 0.068 -0.005 0.153 contestant_episode_Laura Morett The Dead Can Still Talk 0.0412 0.046 0.896 0.370 -0.049 0.131 contestant_episode_Lauren Rimmer Fear of the Unknown 0.1200 0.142 0.844 0.399 -0.159 0.399 contestant_episode_Lauren Rimmer Get to Gettin' 0.1115 0.142 0.786 0.432 -0.166 0.389 contestant_episode_Lauren Rimmer I Don't Like Having Snakes Around 0.0613 0.142 0.431 0.667 -0.218 0.340 contestant_episode_Lauren Rimmer I'm Not Crazy, I'm Confident 0.0725 0.145 0.500 0.617 -0.212 0.357 contestant_episode_Lauren Rimmer I'm a Wild Banshee 0.0882 0.142 0.619 0.536 -0.191 0.367 contestant_episode_Lauren Rimmer My Kisses Are Very Private 0.0950 0.143 0.666 0.505 -0.185 0.375 contestant_episode_Lauren Rimmer Not Going to Roll Over and Die 0.0767 0.141 0.546 0.585 -0.199 0.352 contestant_episode_Lauren Rimmer Playing with the Devil 0.1066 0.141 0.754 0.451 -0.170 0.384 contestant_episode_Lauren Rimmer The Past Will Eat You Alive 0.1629 0.141 1.159 0.246 -0.113 0.439 contestant_episode_Lauren Rimmer The Survivor Devil 0.1224 0.141 0.868 0.385 -0.154 0.399 contestant_episode_Lauren Rimmer This Is Why You Play Survivor 0.1553 0.141 1.104 0.269 -0.120 0.431 contestant_episode_Leif Manson I'm No Dummy -0.0701 0.052 -1.338 0.181 -0.173 0.033 contestant_episode_Leif Manson Thanks for the Souvenir 0.0499 0.058 0.868 0.385 -0.063 0.163 contestant_episode_Lindsey Cascaddan It Will Be My Revenge -0.0113 0.039 -0.286 0.775 -0.089 0.066 contestant_episode_Lindsey Cascaddan It's Survivor Warfare 0.0171 0.040 0.428 0.668 -0.061 0.096 contestant_episode_Lindsey Cascaddan Winner Winner, Chicken Dinner 0.0214 0.026 0.825 0.409 -0.029 0.072 contestant_episode_Lindsey Ogle Odd One Out 0.0083 0.044 0.188 0.851 -0.078 0.095 contestant_episode_Lindsey Ogle We Found Our Zombies -0.0800 0.051 -1.553 0.120 -0.181 0.021 contestant_episode_Lisa Whelchel Gouge My Eyes Out 0.0209 0.037 0.559 0.576 -0.052 0.094 contestant_episode_Lisa Whelchel Hell Hath Frozen Over -0.0014 0.039 -0.037 0.971 -0.078 0.075 contestant_episode_Lisa Whelchel Little Miss Perfect -0.0105 0.033 -0.316 0.752 -0.075 0.054 contestant_episode_Lisa Whelchel Shot into Smithereens 0.0068 0.038 0.181 0.856 -0.067 0.080 contestant_episode_Lisa Whelchel Whiners Are Wieners -0.0013 0.041 -0.032 0.974 -0.081 0.079 contestant_episode_Liz Markham Kindergarten Camp -0.0964 0.027 -3.618 0.000 -0.149 -0.044 contestant_episode_Liz Markham The Circle of Life -0.0516 0.022 -2.299 0.022 -0.096 -0.008 contestant_episode_Lucy Huang May the Best Generation Win 0.0172 0.031 0.552 0.581 -0.044 0.078 contestant_episode_Lucy Huang Who's the Sucker at the Table%3F -0.0316 0.028 -1.130 0.258 -0.086 0.023 contestant_episode_Lucy Huang Your Job Is Recon 0.0064 0.027 0.239 0.811 -0.046 0.059 contestant_episode_Malcolm Freberg Come Over to the Dark Side 0.0263 0.026 1.017 0.309 -0.024 0.077 contestant_episode_Malcolm Freberg Cut Off the Head of the Snake -0.0320 0.025 -1.266 0.205 -0.081 0.018 contestant_episode_Malcolm Freberg Dead Man Walking 0.0284 0.047 0.604 0.546 -0.064 0.121 contestant_episode_Malcolm Freberg Don't Be Blinded by the Headlights -0.0561 0.061 -0.921 0.357 -0.175 0.063 contestant_episode_Malcolm Freberg Gouge My Eyes Out 0.0611 0.045 1.359 0.174 -0.027 0.149 contestant_episode_Malcolm Freberg Hell Hath Frozen Over 0.0277 0.047 0.595 0.552 -0.063 0.119 contestant_episode_Malcolm Freberg Honey Badger 0.0137 0.047 0.290 0.772 -0.079 0.106 contestant_episode_Malcolm Freberg Little Miss Perfect -0.0180 0.052 -0.349 0.727 -0.119 0.083 contestant_episode_Malcolm Freberg Not the Only Actor on This Island -0.0077 0.059 -0.130 0.896 -0.124 0.108 contestant_episode_Malcolm Freberg Operation Thunder Dome 0.0109 0.039 0.282 0.778 -0.065 0.086 contestant_episode_Malcolm Freberg Persona Non Grata -0.1042 0.050 -2.065 0.039 -0.203 -0.005 contestant_episode_Malcolm Freberg She Annoys Me Greatly -0.0089 0.036 -0.244 0.808 -0.080 0.063 contestant_episode_Malcolm Freberg Shot into Smithereens -0.1151 0.146 -0.787 0.431 -0.402 0.172 contestant_episode_Malcolm Freberg Survivor Jackpot 0.0633 0.026 2.417 0.016 0.012 0.115 contestant_episode_Malcolm Freberg Survivor Smacked Me in the Chops 0.0191 0.061 0.313 0.754 -0.101 0.139 contestant_episode_Malcolm Freberg The Stakes Have Been Raised 0.0286 0.024 1.175 0.240 -0.019 0.076 contestant_episode_Malcolm Freberg The Tables Have Turned -0.1046 0.061 -1.719 0.086 -0.224 0.015 contestant_episode_Malcolm Freberg There's Gonna Be Hell to Pay 0.0829 0.049 1.700 0.089 -0.013 0.178 contestant_episode_Malcolm Freberg This Isn't a 'We' Game -0.0310 0.060 -0.517 0.605 -0.149 0.087 contestant_episode_Malcolm Freberg Tubby Lunchbox -0.0171 0.045 -0.378 0.705 -0.105 0.071 contestant_episode_Malcolm Freberg Whiners Are Wieners -0.0634 0.051 -1.241 0.215 -0.164 0.037 contestant_episode_Malcolm Freberg Zipping Over the Cuckoo's Nest 0.0162 0.026 0.622 0.534 -0.035 0.067 contestant_episode_Mari Takahashi May the Best Generation Win 0.0775 0.021 3.617 0.000 0.035 0.119 contestant_episode_Matt Bischoff Kill or Be Killed 0.0576 0.058 0.991 0.322 -0.056 0.172 contestant_episode_Matt Bischoff Operation Thunder Dome 0.1059 0.051 2.092 0.036 0.007 0.205 contestant_episode_Matt Bischoff She Annoys Me Greatly 0.0104 0.050 0.210 0.834 -0.087 0.108 contestant_episode_Max Dawson It Will Be My Revenge 0.0042 0.034 0.124 0.902 -0.063 0.071 contestant_episode_Max Dawson It's Survivor Warfare 0.0291 0.029 0.988 0.323 -0.029 0.087 contestant_episode_Max Dawson Winner Winner, Chicken Dinner -0.0262 0.026 -0.992 0.321 -0.078 0.026 contestant_episode_Michael Skupin Gouge My Eyes Out -0.0963 0.145 -0.664 0.507 -0.380 0.188 contestant_episode_Michael Skupin Hell Hath Frozen Over 0.0440 0.043 1.024 0.306 -0.040 0.128 contestant_episode_Michael Skupin Little Miss Perfect -0.0027 0.039 -0.069 0.945 -0.079 0.073 contestant_episode_Michael Skupin Not the Only Actor on This Island -0.1168 0.057 -2.045 0.041 -0.229 -0.005 contestant_episode_Michael Skupin Shot into Smithereens 0.1019 0.041 2.485 0.013 0.022 0.182 contestant_episode_Michael Skupin Survivor Smacked Me in the Chops 0.0412 0.048 0.852 0.394 -0.054 0.136 contestant_episode_Michael Skupin Whiners Are Wieners -0.0296 0.042 -0.699 0.484 -0.112 0.053 contestant_episode_Michael Snow Cut Off the Head of the Snake 0.0062 0.043 0.146 0.884 -0.077 0.090 contestant_episode_Michael Snow Honey Badger -0.0781 0.060 -1.305 0.192 -0.195 0.039 contestant_episode_Michael Snow Operation Thunder Dome 0.0068 0.052 0.132 0.895 -0.094 0.108 contestant_episode_Michael Snow Tubby Lunchbox 0.0065 0.058 0.112 0.911 -0.107 0.120 contestant_episode_Michaela Bradshaw Dirty Deed -0.0295 0.021 -1.394 0.163 -0.071 0.012 contestant_episode_Michaela Bradshaw I Will Destroy You 0.0063 0.012 0.518 0.605 -0.018 0.030 contestant_episode_Michaela Bradshaw Idol Search Party 0.0037 0.015 0.240 0.811 -0.026 0.034 contestant_episode_Michaela Bradshaw It Is Not a High Without a Low 0.0228 0.021 1.087 0.277 -0.018 0.064 contestant_episode_Michaela Bradshaw Love Goggles -0.0355 0.017 -2.152 0.031 -0.068 -0.003 contestant_episode_Michaela Bradshaw May the Best Generation Win 0.0380 0.021 1.818 0.069 -0.003 0.079 contestant_episode_Michaela Bradshaw Parting Is Such Sweet Sorrow 0.0020 0.021 0.096 0.923 -0.039 0.042 contestant_episode_Michaela Bradshaw Reinventing How This Game Is Played -0.0318 0.021 -1.508 0.131 -0.073 0.010 contestant_episode_Michaela Bradshaw Survivor Jackpot -0.0011 0.028 -0.040 0.968 -0.056 0.054 contestant_episode_Michaela Bradshaw The Stakes Have Been Raised -0.0264 0.024 -1.081 0.280 -0.074 0.021 contestant_episode_Michaela Bradshaw The Tables Have Turned -0.0040 0.025 -0.157 0.875 -0.053 0.045 contestant_episode_Michaela Bradshaw The Truth Works Well 0.0167 0.014 1.191 0.234 -0.011 0.044 contestant_episode_Michaela Bradshaw There's a New Sheriff in Town 0.0235 0.020 1.148 0.251 -0.017 0.064 contestant_episode_Michaela Bradshaw Vote Early, Vote Often -0.0339 0.024 -1.387 0.165 -0.082 0.014 contestant_episode_Michaela Bradshaw What Happened on Exile, Stays on Exile -0.0180 0.025 -0.712 0.477 -0.068 0.032 contestant_episode_Michaela Bradshaw Who's the Sucker at the Table%3F 0.0015 0.021 0.071 0.943 -0.039 0.042 contestant_episode_Michaela Bradshaw Your Job Is Recon -0.0018 0.025 -0.073 0.942 -0.050 0.047 contestant_episode_Michele Fitzgerald I'm a Mental Giant 0.0601 0.076 0.786 0.432 -0.090 0.210 contestant_episode_Michele Fitzgerald It's Merge Time 0.0361 0.073 0.494 0.621 -0.107 0.179 contestant_episode_Michele Fitzgerald It's Psychological Warfare 0.0535 0.072 0.744 0.457 -0.087 0.194 contestant_episode_Michele Fitzgerald It's a 'Me' Game, Not a 'We' Game 0.0426 0.074 0.576 0.564 -0.102 0.187 contestant_episode_Michele Fitzgerald Kindergarten Camp 0.0673 0.077 0.878 0.380 -0.083 0.217 contestant_episode_Michele Fitzgerald Now's the Time to Start Scheming 0.0646 0.072 0.899 0.369 -0.076 0.205 contestant_episode_Michele Fitzgerald Play or Go Home 0.0787 0.072 1.087 0.277 -0.063 0.221 contestant_episode_Michele Fitzgerald Signed, Sealed and Delivered 0.0796 0.079 1.013 0.311 -0.075 0.234 contestant_episode_Michele Fitzgerald The Circle of Life 0.0863 0.080 1.078 0.281 -0.071 0.243 contestant_episode_Michele Fitzgerald The Devils We Know 0.0518 0.073 0.707 0.479 -0.092 0.196 contestant_episode_Michele Fitzgerald The Jocks vs. the Pretty People 0.0339 0.073 0.466 0.641 -0.109 0.176 contestant_episode_Michele Fitzgerald With Me or Not with Me 0.0976 0.071 1.374 0.170 -0.042 0.237 contestant_episode_Michelle Schubert I'm the Kingpin 0.0544 0.023 2.414 0.016 0.010 0.099 contestant_episode_Michelle Schubert Idol Search Party 0.0171 0.022 0.791 0.429 -0.025 0.060 contestant_episode_Michelle Schubert Love Goggles 0.0493 0.022 2.269 0.023 0.007 0.092 contestant_episode_Michelle Schubert May the Best Generation Win 0.0759 0.028 2.745 0.006 0.022 0.130 contestant_episode_Michelle Schubert The Truth Works Well 0.0198 0.019 1.017 0.309 -0.018 0.058 contestant_episode_Michelle Schubert Who's the Sucker at the Table%3F 0.0489 0.035 1.408 0.159 -0.019 0.117 contestant_episode_Michelle Schubert Your Job Is Recon 0.0590 0.028 2.070 0.038 0.003 0.115 contestant_episode_Mike Holloway Crazy Is as Crazy Does -0.0292 0.025 -1.170 0.242 -0.078 0.020 contestant_episode_Mike Holloway Holding On for Dear Life 0.0343 0.023 1.475 0.140 -0.011 0.080 contestant_episode_Mike Holloway It Will Be My Revenge -0.0439 0.029 -1.499 0.134 -0.101 0.013 contestant_episode_Mike Holloway It's Survivor Warfare 0.0088 0.029 0.305 0.761 -0.048 0.065 contestant_episode_Mike Holloway Keep It Real 0.0161 0.019 0.850 0.395 -0.021 0.053 contestant_episode_Mike Holloway Livin' on the Edge -0.0129 0.023 -0.549 0.583 -0.059 0.033 contestant_episode_Mike Holloway My Word Is My Bond 0.0261 0.016 1.659 0.097 -0.005 0.057 contestant_episode_Mike Holloway Survivor Russian Roulette 0.0341 0.010 3.419 0.001 0.015 0.054 contestant_episode_Mike Holloway The Line Will Be Drawn Tonight -0.0008 0.022 -0.036 0.971 -0.044 0.042 contestant_episode_Mike Holloway Winner Winner, Chicken Dinner 0.0115 0.025 0.462 0.644 -0.037 0.060 contestant_episode_Mike Zahalsky Fear of the Unknown -0.0430 0.018 -2.412 0.016 -0.078 -0.008 contestant_episode_Mike Zahalsky Get to Gettin' -0.0077 0.016 -0.471 0.638 -0.040 0.024 contestant_episode_Mike Zahalsky I Don't Like Having Snakes Around -0.0556 0.023 -2.407 0.016 -0.101 -0.010 contestant_episode_Mike Zahalsky I'm Not Crazy, I'm Confident -0.0553 0.025 -2.242 0.025 -0.104 -0.007 contestant_episode_Mike Zahalsky I'm a Wild Banshee -0.0075 0.026 -0.287 0.774 -0.059 0.044 contestant_episode_Mike Zahalsky My Kisses Are Very Private -0.0228 0.029 -0.786 0.432 -0.080 0.034 contestant_episode_Mike Zahalsky Not Going to Roll Over and Die -0.0042 0.012 -0.358 0.721 -0.027 0.019 contestant_episode_Mike Zahalsky Playing with the Devil -0.0166 0.015 -1.118 0.263 -0.046 0.013 contestant_episode_Mike Zahalsky The Past Will Eat You Alive -0.0055 0.025 -0.218 0.828 -0.055 0.044 contestant_episode_Mike Zahalsky The Survivor Devil 0.0145 0.013 1.133 0.257 -0.011 0.040 contestant_episode_Mike Zahalsky This Is Why You Play Survivor 0.0433 0.024 1.824 0.068 -0.003 0.090 contestant_episode_Missy Payne Blood Is Blood -0.1045 0.040 -2.597 0.009 -0.183 -0.026 contestant_episode_Missy Payne Gettin' to Crunch Time -0.0383 0.040 -0.960 0.337 -0.117 0.040 contestant_episode_Missy Payne Let's Make a Move -0.0477 0.044 -1.083 0.279 -0.134 0.039 contestant_episode_Missy Payne Make Some Magic Happen -0.1064 0.042 -2.543 0.011 -0.188 -0.024 contestant_episode_Missy Payne Method to This Madness -0.1003 0.054 -1.852 0.064 -0.206 0.006 contestant_episode_Missy Payne Million Dollar Decision -0.0610 0.043 -1.421 0.155 -0.145 0.023 contestant_episode_Missy Payne Still Holdin' On -0.0591 0.039 -1.506 0.132 -0.136 0.018 contestant_episode_Missy Payne Suck It Up and Survive -0.1192 0.062 -1.934 0.053 -0.240 0.002 contestant_episode_Missy Payne This Is Where We Build Trust -0.0298 0.042 -0.704 0.482 -0.113 0.053 contestant_episode_Missy Payne We're a Hot Mess -0.0445 0.047 -0.950 0.342 -0.136 0.047 contestant_episode_Missy Payne Wrinkle in the Plan -0.0391 0.044 -0.894 0.372 -0.125 0.047 contestant_episode_Monica Culpepper Blood Is Thicker than Anything 0.0620 0.054 1.144 0.252 -0.044 0.168 contestant_episode_Monica Culpepper Gloves Come Off 0.0482 0.037 1.314 0.189 -0.024 0.120 contestant_episode_Monica Culpepper My Brother's Keeper 0.0237 0.038 0.623 0.533 -0.051 0.098 contestant_episode_Monica Culpepper One Armed Dude and Three Moms -0.0291 0.045 -0.653 0.514 -0.116 0.058 contestant_episode_Monica Culpepper One-Man Wrecking Ball 0.0166 0.044 0.377 0.706 -0.070 0.103 contestant_episode_Monica Culpepper Opening Pandora's Box 0.0567 0.052 1.099 0.272 -0.044 0.158 contestant_episode_Monica Culpepper Out on a Limb (episode) 0.0903 0.035 2.597 0.009 0.022 0.159 contestant_episode_Monica Culpepper Rustle Feathers 0.0777 0.035 2.192 0.028 0.008 0.147 contestant_episode_Monica Culpepper Skin of My Teeth 0.0325 0.042 0.767 0.443 -0.051 0.116 contestant_episode_Monica Culpepper Swoop In for the Kill 0.0469 0.043 1.080 0.280 -0.038 0.132 contestant_episode_Monica Culpepper The Dead Can Still Talk -0.0195 0.048 -0.406 0.685 -0.114 0.075 contestant_episode_Monica Padilla Survivor MacGyver -0.0425 0.025 -1.694 0.090 -0.092 0.007 contestant_episode_Monica Padilla We Got a Rat -0.0482 0.025 -1.938 0.053 -0.097 0.001 contestant_episode_Monica Padilla What's the Beef%3F -0.0112 0.017 -0.647 0.517 -0.045 0.023 contestant_episode_Morgan McLeod Cops-R-Us (episode) 0.0442 0.053 0.841 0.400 -0.059 0.147 contestant_episode_Morgan McLeod Head of the Snake 0.0832 0.045 1.856 0.063 -0.005 0.171 contestant_episode_Morgan McLeod Hot Girl with a Grudge 0.0687 0.046 1.506 0.132 -0.021 0.158 contestant_episode_Morgan McLeod Mad Treasure Hunt 0.0742 0.039 1.883 0.060 -0.003 0.151 contestant_episode_Morgan McLeod Odd One Out 0.0948 0.050 1.882 0.060 -0.004 0.193 contestant_episode_Morgan McLeod Our Time to Shine 0.1259 0.048 2.613 0.009 0.031 0.220 contestant_episode_Morgan McLeod We Found Our Zombies 0.0985 0.050 1.952 0.051 -0.000 0.197 contestant_episode_Natalie Anderson Blood Is Blood 0.1542 0.026 5.993 0.000 0.104 0.205 contestant_episode_Natalie Anderson Gettin' to Crunch Time 0.1161 0.025 4.569 0.000 0.066 0.166 contestant_episode_Natalie Anderson Let's Make a Move 0.1609 0.031 5.115 0.000 0.099 0.223 contestant_episode_Natalie Anderson Make Some Magic Happen 0.1352 0.030 4.498 0.000 0.076 0.194 contestant_episode_Natalie Anderson Method to This Madness 0.0329 0.034 0.957 0.339 -0.034 0.100 contestant_episode_Natalie Anderson Million Dollar Decision 0.0952 0.029 3.279 0.001 0.038 0.152 contestant_episode_Natalie Anderson Still Holdin' On -0.0360 0.141 -0.254 0.799 -0.313 0.241 contestant_episode_Natalie Anderson Suck It Up and Survive 0.1144 0.032 3.571 0.000 0.052 0.177 contestant_episode_Natalie Anderson This Is Where We Build Trust 0.1615 0.023 7.084 0.000 0.117 0.206 contestant_episode_Natalie Anderson We're a Hot Mess 0.1106 0.027 4.136 0.000 0.058 0.163 contestant_episode_Natalie Anderson Wrinkle in the Plan 0.1049 0.030 3.543 0.000 0.047 0.163 contestant_episode_Neal Gottlieb It's Merge Time -0.0401 0.026 -1.537 0.124 -0.091 0.011 contestant_episode_Neal Gottlieb Kindergarten Camp 0.0055 0.032 0.171 0.864 -0.058 0.069 contestant_episode_Neal Gottlieb Play or Go Home 0.0297 0.033 0.915 0.360 -0.034 0.093 contestant_episode_Neal Gottlieb Signed, Sealed and Delivered 0.0133 0.037 0.362 0.717 -0.059 0.085 contestant_episode_Neal Gottlieb The Circle of Life 0.0064 0.030 0.216 0.829 -0.052 0.064 contestant_episode_Neal Gottlieb The Devils We Know -0.0049 0.028 -0.175 0.861 -0.059 0.050 contestant_episode_Nick Maiorano It's Merge Time 0.0624 0.028 2.221 0.026 0.007 0.117 contestant_episode_Nick Maiorano Kindergarten Camp 0.0244 0.032 0.773 0.439 -0.037 0.086 contestant_episode_Nick Maiorano Play or Go Home 0.0395 0.029 1.370 0.171 -0.017 0.096 contestant_episode_Nick Maiorano Signed, Sealed and Delivered -0.0055 0.030 -0.184 0.854 -0.064 0.053 contestant_episode_Nick Maiorano The Circle of Life -0.0334 0.029 -1.136 0.256 -0.091 0.024 contestant_episode_Nick Maiorano The Devils We Know 0.0279 0.031 0.909 0.364 -0.032 0.088 contestant_episode_Nick Maiorano The Jocks vs. the Pretty People 0.0242 0.027 0.892 0.372 -0.029 0.078 contestant_episode_Nina Poersch It Will Be My Revenge -0.0109 0.026 -0.417 0.676 -0.062 0.040 contestant_episode_Nina Poersch It's Survivor Warfare 0.0160 0.033 0.487 0.626 -0.048 0.080 contestant_episode_Ozzy Lusth Cult Like 0.0213 0.052 0.407 0.684 -0.081 0.124 contestant_episode_Ozzy Lusth Dirty Deed 0.0121 0.019 0.650 0.516 -0.024 0.048 contestant_episode_Ozzy Lusth Double Agent -0.1161 0.054 -2.145 0.032 -0.222 -0.010 contestant_episode_Ozzy Lusth Running the Show 0.0507 0.060 0.842 0.400 -0.067 0.169 contestant_episode_Ozzy Lusth Survivor Jackpot 0.0301 0.023 1.309 0.191 -0.015 0.075 contestant_episode_Ozzy Lusth The Stakes Have Been Raised 0.0159 0.028 0.576 0.564 -0.038 0.070 contestant_episode_Ozzy Lusth The Tables Have Turned 0.0297 0.023 1.288 0.198 -0.015 0.075 contestant_episode_Ozzy Lusth Then There Were Five 0.1287 0.054 2.405 0.016 0.024 0.234 contestant_episode_Ozzy Lusth There's a New Sheriff in Town 0.0112 0.019 0.589 0.556 -0.026 0.048 contestant_episode_Ozzy Lusth Trojan Horse 0.0184 0.056 0.328 0.743 -0.092 0.128 contestant_episode_Ozzy Lusth Vote Early, Vote Often 0.0037 0.020 0.181 0.856 -0.036 0.043 contestant_episode_Ozzy Lusth What Happened on Exile, Stays on Exile -0.0274 0.016 -1.678 0.093 -0.059 0.005 contestant_episode_Patrick Bolton I'm a Wild Banshee 0.0222 0.023 0.958 0.338 -0.023 0.067 contestant_episode_Patrick Bolton My Kisses Are Very Private -0.0235 0.022 -1.062 0.288 -0.067 0.020 contestant_episode_Paul Wachter May the Best Generation Win 0.0211 0.031 0.675 0.500 -0.040 0.082 contestant_episode_Paul Wachter Your Job Is Recon -0.0243 0.028 -0.880 0.379 -0.078 0.030 contestant_episode_Peih-Gee Law We Got a Rat -0.0374 0.028 -1.333 0.182 -0.092 0.018 contestant_episode_Pete Yurkowski Little Miss Perfect 0.0759 0.037 2.066 0.039 0.004 0.148 contestant_episode_Pete Yurkowski Not the Only Actor on This Island 0.0365 0.046 0.793 0.428 -0.054 0.127 contestant_episode_Pete Yurkowski Whiners Are Wieners -0.0361 0.039 -0.927 0.354 -0.112 0.040 contestant_episode_Peter Baggenstos Kindergarten Camp -0.0089 0.035 -0.258 0.796 -0.077 0.059 contestant_episode_Peter Baggenstos Play or Go Home -0.0399 0.030 -1.331 0.183 -0.099 0.019 contestant_episode_Peter Baggenstos Signed, Sealed and Delivered -0.0687 0.034 -2.015 0.044 -0.136 -0.002 contestant_episode_Peter Baggenstos The Circle of Life -0.0071 0.027 -0.263 0.793 -0.060 0.046 contestant_episode_Peter Baggenstos The Devils We Know -0.0147 0.028 -0.530 0.596 -0.069 0.040 contestant_episode_Phillip Sheppard Cut Off the Head of the Snake -0.0408 0.028 -1.460 0.144 -0.096 0.014 contestant_episode_Phillip Sheppard Honey Badger -0.0404 0.038 -1.058 0.290 -0.115 0.034 contestant_episode_Phillip Sheppard Kill or Be Killed -0.0569 0.042 -1.351 0.177 -0.140 0.026 contestant_episode_Phillip Sheppard Operation Thunder Dome -0.0740 0.035 -2.118 0.034 -0.142 -0.006 contestant_episode_Phillip Sheppard Persona Non Grata -0.0794 0.030 -2.668 0.008 -0.138 -0.021 contestant_episode_Phillip Sheppard She Annoys Me Greatly -0.0808 0.038 -2.115 0.034 -0.156 -0.006 contestant_episode_Phillip Sheppard There's Gonna Be Hell to Pay -0.0183 0.046 -0.398 0.690 -0.108 0.072 contestant_episode_Phillip Sheppard Tubby Lunchbox -0.0519 0.035 -1.502 0.133 -0.120 0.016 contestant_episode_Phillip Sheppard Zipping Over the Cuckoo's Nest -0.0393 0.031 -1.250 0.211 -0.101 0.022 contestant_episode_Reed Kelly Blood Is Blood 0.0100 0.037 0.271 0.786 -0.062 0.082 contestant_episode_Reed Kelly Gettin' to Crunch Time -0.0012 0.037 -0.032 0.974 -0.073 0.070 contestant_episode_Reed Kelly Make Some Magic Happen 0.0237 0.040 0.599 0.549 -0.054 0.101 contestant_episode_Reed Kelly Method to This Madness 0.0199 0.045 0.446 0.655 -0.068 0.107 contestant_episode_Reed Kelly Million Dollar Decision 0.0299 0.041 0.724 0.469 -0.051 0.111 contestant_episode_Reed Kelly Suck It Up and Survive 0.0118 0.048 0.246 0.805 -0.082 0.106 contestant_episode_Reed Kelly This Is Where We Build Trust 0.0341 0.039 0.880 0.379 -0.042 0.110 contestant_episode_Reed Kelly We're a Hot Mess 0.0208 0.037 0.563 0.574 -0.052 0.093 contestant_episode_Reed Kelly Wrinkle in the Plan -6.799e-05 0.039 -0.002 0.999 -0.076 0.076 contestant_episode_Reynold Toepfer Come Over to the Dark Side 0.0207 0.036 0.583 0.560 -0.049 0.090 contestant_episode_Reynold Toepfer Cut Off the Head of the Snake -0.0256 0.035 -0.732 0.464 -0.094 0.043 contestant_episode_Reynold Toepfer Honey Badger 0.0033 0.045 0.073 0.941 -0.084 0.091 contestant_episode_Reynold Toepfer Kill or Be Killed -0.0291 0.045 -0.645 0.519 -0.118 0.059 contestant_episode_Reynold Toepfer Operation Thunder Dome -0.0164 0.046 -0.356 0.722 -0.106 0.074 contestant_episode_Reynold Toepfer Persona Non Grata 0.0480 0.045 1.060 0.289 -0.041 0.137 contestant_episode_Reynold Toepfer The Beginning of the End 0.0291 0.038 0.767 0.443 -0.045 0.104 contestant_episode_Reynold Toepfer There's Gonna Be Hell to Pay -0.1169 0.044 -2.644 0.008 -0.204 -0.030 contestant_episode_Reynold Toepfer Tubby Lunchbox -0.0388 0.054 -0.719 0.472 -0.144 0.067 contestant_episode_Reynold Toepfer Zipping Over the Cuckoo's Nest -0.0025 0.040 -0.062 0.950 -0.082 0.077 contestant_episode_Roark Luskin I'm Not Crazy, I'm Confident 0.0089 0.030 0.297 0.767 -0.050 0.068 contestant_episode_Roark Luskin I'm a Wild Banshee 0.0125 0.031 0.405 0.686 -0.048 0.073 contestant_episode_Roark Luskin My Kisses Are Very Private -0.0175 0.026 -0.678 0.498 -0.068 0.033 contestant_episode_Roark Luskin The Past Will Eat You Alive 0.0007 0.019 0.037 0.970 -0.036 0.037 contestant_episode_Rodney Lavoie Jr. Crazy Is as Crazy Does -0.0470 0.025 -1.887 0.059 -0.096 0.002 contestant_episode_Rodney Lavoie Jr. Holding On for Dear Life 0.0266 0.015 1.832 0.067 -0.002 0.055 contestant_episode_Rodney Lavoie Jr. It Will Be My Revenge -0.0466 0.033 -1.395 0.163 -0.112 0.019 contestant_episode_Rodney Lavoie Jr. It's Survivor Warfare -0.0173 0.031 -0.559 0.576 -0.078 0.043 contestant_episode_Rodney Lavoie Jr. Keep It Real -0.0180 0.014 -1.305 0.192 -0.045 0.009 contestant_episode_Rodney Lavoie Jr. Livin' on the Edge -0.0037 0.015 -0.243 0.808 -0.033 0.026 contestant_episode_Rodney Lavoie Jr. My Word Is My Bond 0.0444 0.014 3.284 0.001 0.018 0.071 contestant_episode_Rodney Lavoie Jr. Survivor Russian Roulette 5.444e-05 0.014 0.004 0.997 -0.027 0.027 contestant_episode_Rodney Lavoie Jr. The Line Will Be Drawn Tonight -0.0207 0.018 -1.169 0.242 -0.055 0.014 contestant_episode_Rodney Lavoie Jr. Winner Winner, Chicken Dinner -0.0313 0.022 -1.414 0.157 -0.075 0.012 contestant_episode_Russell Swan Survivor Smacked Me in the Chops 0.0203 0.049 0.415 0.678 -0.075 0.116 contestant_episode_Russell Swan This Isn't a 'We' Game 0.0147 0.062 0.238 0.812 -0.106 0.136 contestant_episode_Ryan Ulrich Fear of the Unknown 0.0147 0.014 1.060 0.289 -0.012 0.042 contestant_episode_Ryan Ulrich Get to Gettin' 0.0080 0.013 0.606 0.545 -0.018 0.034 contestant_episode_Ryan Ulrich I Don't Like Having Snakes Around 0.0184 0.018 1.042 0.298 -0.016 0.053 contestant_episode_Ryan Ulrich I'm Not Crazy, I'm Confident 0.0147 0.019 0.756 0.450 -0.023 0.053 contestant_episode_Ryan Ulrich I'm a Wild Banshee -0.0090 0.020 -0.445 0.656 -0.048 0.031 contestant_episode_Ryan Ulrich My Kisses Are Very Private -0.0279 0.018 -1.585 0.113 -0.062 0.007 contestant_episode_Ryan Ulrich Not Going to Roll Over and Die 0.0217 0.012 1.769 0.077 -0.002 0.046 contestant_episode_Ryan Ulrich Playing with the Devil -0.0298 0.017 -1.763 0.078 -0.063 0.003 contestant_episode_Ryan Ulrich The Past Will Eat You Alive -0.0116 0.021 -0.564 0.573 -0.052 0.029 contestant_episode_Ryan Ulrich The Survivor Devil 0.0124 0.012 1.034 0.301 -0.011 0.036 contestant_episode_Ryan Ulrich This Is Why You Play Survivor -0.0559 0.017 -3.322 0.001 -0.089 -0.023 contestant_episode_Sabrina Thompson It's Human Nature -0.0697 0.047 -1.477 0.140 -0.162 0.023 contestant_episode_Sabrina Thompson Never Say Die -0.0220 0.057 -0.389 0.697 -0.133 0.089 contestant_episode_Sandra Diaz-Twine Survivor Jackpot -0.0371 0.013 -2.805 0.005 -0.063 -0.011 contestant_episode_Sandra Diaz-Twine The Stakes Have Been Raised -0.0255 0.009 -2.801 0.005 -0.043 -0.008 contestant_episode_Sandra Diaz-Twine The Tables Have Turned -0.0033 0.018 -0.184 0.854 -0.039 0.032 contestant_episode_Sandra Diaz-Twine Vote Early, Vote Often -0.0084 0.008 -1.018 0.309 -0.024 0.008 contestant_episode_Sarah Lacina Cops-R-Us (episode) 0.0022 0.029 0.075 0.940 -0.054 0.059 contestant_episode_Sarah Lacina Dirty Deed -0.0557 0.022 -2.542 0.011 -0.099 -0.013 contestant_episode_Sarah Lacina Head of the Snake -0.0786 0.017 -4.716 0.000 -0.111 -0.046 contestant_episode_Sarah Lacina Hot Girl with a Grudge 0.0358 0.033 1.070 0.285 -0.030 0.101 contestant_episode_Sarah Lacina It Is Not a High Without a Low 0.0199 0.012 1.717 0.086 -0.003 0.043 contestant_episode_Sarah Lacina Mad Treasure Hunt -0.0163 0.032 -0.512 0.609 -0.079 0.046 contestant_episode_Sarah Lacina Odd One Out 0.0588 0.035 1.681 0.093 -0.010 0.127 contestant_episode_Sarah Lacina Our Time to Shine -0.0390 0.028 -1.390 0.165 -0.094 0.016 contestant_episode_Sarah Lacina Parting Is Such Sweet Sorrow 0.0008 0.011 0.070 0.944 -0.021 0.022 contestant_episode_Sarah Lacina Reinventing How This Game Is Played -0.0167 0.017 -0.992 0.321 -0.050 0.016 contestant_episode_Sarah Lacina Survivor Jackpot -0.0143 0.022 -0.637 0.524 -0.058 0.030 contestant_episode_Sarah Lacina The Stakes Have Been Raised 0.0049 0.025 0.193 0.847 -0.045 0.055 contestant_episode_Sarah Lacina The Tables Have Turned 0.0010 0.023 0.044 0.965 -0.044 0.046 contestant_episode_Sarah Lacina There's a New Sheriff in Town -0.0187 0.013 -1.466 0.143 -0.044 0.006 contestant_episode_Sarah Lacina Vote Early, Vote Often -0.0010 0.017 -0.061 0.951 -0.034 0.032 contestant_episode_Sarah Lacina We Found Our Zombies 0.0065 0.028 0.228 0.820 -0.049 0.062 contestant_episode_Sarah Lacina What Happened on Exile, Stays on Exile -0.0619 0.018 -3.372 0.001 -0.098 -0.026 contestant_episode_Scot Pollard I'm a Mental Giant -0.0459 0.022 -2.091 0.037 -0.089 -0.003 contestant_episode_Scot Pollard It's Merge Time 0.0138 0.017 0.836 0.403 -0.019 0.046 contestant_episode_Scot Pollard It's Psychological Warfare -0.0243 0.016 -1.514 0.130 -0.056 0.007 contestant_episode_Scot Pollard Kindergarten Camp -0.0458 0.022 -2.100 0.036 -0.088 -0.003 contestant_episode_Scot Pollard Play or Go Home -0.0160 0.021 -0.766 0.443 -0.057 0.025 contestant_episode_Scot Pollard Signed, Sealed and Delivered -0.0945 0.015 -6.404 0.000 -0.123 -0.066 contestant_episode_Scot Pollard The Circle of Life -0.0399 0.025 -1.592 0.111 -0.089 0.009 contestant_episode_Scot Pollard The Devils We Know -0.0063 0.016 -0.402 0.688 -0.037 0.025 contestant_episode_Scot Pollard The Jocks vs. the Pretty People -0.0115 0.020 -0.585 0.559 -0.050 0.027 contestant_episode_Shamar Thomas Kill or Be Killed 0.0821 0.045 1.812 0.070 -0.007 0.171 contestant_episode_Shamar Thomas She Annoys Me Greatly -0.0110 0.055 -0.200 0.841 -0.118 0.097 contestant_episode_Shamar Thomas There's Gonna Be Hell to Pay -0.0419 0.037 -1.134 0.257 -0.114 0.030 contestant_episode_Sherri Biethman Come Over to the Dark Side 0.0758 0.036 2.088 0.037 0.005 0.147 contestant_episode_Sherri Biethman Cut Off the Head of the Snake 0.0375 0.041 0.918 0.358 -0.043 0.118 contestant_episode_Sherri Biethman Don't Say Anything About My Mom 0.0742 0.031 2.370 0.018 0.013 0.136 contestant_episode_Sherri Biethman Honey Badger 0.0922 0.048 1.913 0.056 -0.002 0.187 contestant_episode_Sherri Biethman Kill or Be Killed 0.0066 0.057 0.117 0.907 -0.104 0.117 contestant_episode_Sherri Biethman The Beginning of the End 0.0542 0.036 1.506 0.132 -0.016 0.125 contestant_episode_Sherri Biethman There's Gonna Be Hell to Pay 0.0409 0.048 0.845 0.398 -0.054 0.136 contestant_episode_Sherri Biethman Zipping Over the Cuckoo's Nest 0.0802 0.034 2.366 0.018 0.014 0.147 contestant_episode_Shirin Oskooi Crazy Is as Crazy Does -0.0377 0.024 -1.573 0.116 -0.085 0.009 contestant_episode_Shirin Oskooi It Will Be My Revenge -0.0079 0.027 -0.296 0.767 -0.060 0.045 contestant_episode_Shirin Oskooi It's Survivor Warfare 0.0258 0.024 1.094 0.274 -0.020 0.072 contestant_episode_Shirin Oskooi Keep It Real 0.0166 0.011 1.475 0.140 -0.005 0.039 contestant_episode_Shirin Oskooi Livin' on the Edge -0.0105 0.017 -0.605 0.545 -0.045 0.024 contestant_episode_Shirin Oskooi Survivor MacGyver -0.0402 0.019 -2.155 0.031 -0.077 -0.004 contestant_episode_Shirin Oskooi Survivor Russian Roulette 0.0018 0.010 0.177 0.860 -0.019 0.022 contestant_episode_Shirin Oskooi The Line Will Be Drawn Tonight 0.0488 0.018 2.750 0.006 0.014 0.084 contestant_episode_Shirin Oskooi Winner Winner, Chicken Dinner -0.0116 0.021 -0.547 0.585 -0.053 0.030 contestant_episode_Sierra Thomas Bring the Popcorn 0.0001 0.015 0.010 0.992 -0.029 0.029 contestant_episode_Sierra Thomas Crazy Is as Crazy Does -0.0441 0.029 -1.536 0.125 -0.100 0.012 contestant_episode_Sierra Thomas Dirty Deed 0.0200 0.024 0.846 0.398 -0.026 0.067 contestant_episode_Sierra Thomas Holding On for Dear Life -0.0021 0.020 -0.107 0.915 -0.041 0.037 contestant_episode_Sierra Thomas It Is Not a High Without a Low -0.0145 0.019 -0.767 0.443 -0.051 0.022 contestant_episode_Sierra Thomas It Will Be My Revenge -0.0147 0.030 -0.490 0.624 -0.073 0.044 contestant_episode_Sierra Thomas It's Survivor Warfare -0.0432 0.031 -1.386 0.166 -0.104 0.018 contestant_episode_Sierra Thomas Keep It Real 0.0202 0.015 1.320 0.187 -0.010 0.050 contestant_episode_Sierra Thomas Livin' on the Edge -0.0203 0.021 -0.990 0.322 -0.061 0.020 contestant_episode_Sierra Thomas My Word Is My Bond 0.0408 0.015 2.752 0.006 0.012 0.070 contestant_episode_Sierra Thomas Reinventing How This Game Is Played 0.0087 0.020 0.444 0.657 -0.030 0.047 contestant_episode_Sierra Thomas Survivor Jackpot 0.0012 0.022 0.055 0.957 -0.042 0.044 contestant_episode_Sierra Thomas Survivor Russian Roulette 0.0005 0.014 0.039 0.969 -0.027 0.028 contestant_episode_Sierra Thomas The Line Will Be Drawn Tonight 0.0054 0.024 0.223 0.824 -0.042 0.053 contestant_episode_Sierra Thomas The Stakes Have Been Raised -0.0046 0.026 -0.176 0.861 -0.057 0.047 contestant_episode_Sierra Thomas The Tables Have Turned -0.0094 0.019 -0.508 0.612 -0.046 0.027 contestant_episode_Sierra Thomas There's a New Sheriff in Town 0.0141 0.017 0.813 0.416 -0.020 0.048 contestant_episode_Sierra Thomas Vote Early, Vote Often 0.0191 0.023 0.820 0.412 -0.027 0.065 contestant_episode_Sierra Thomas What Happened on Exile, Stays on Exile 0.0378 0.024 1.584 0.113 -0.009 0.085 contestant_episode_Sierra Thomas Winner Winner, Chicken Dinner 0.0086 0.014 0.597 0.550 -0.020 0.037 contestant_episode_Simone Nguyen I'm a Wild Banshee -0.0384 0.021 -1.810 0.070 -0.080 0.003 contestant_episode_So Kim Keep It Real 0.0267 0.018 1.519 0.129 -0.008 0.061 contestant_episode_Spencer Bledsoe Bag of Tricks -0.0178 0.023 -0.784 0.433 -0.062 0.027 contestant_episode_Spencer Bledsoe Bunking with the Devil -0.0455 0.018 -2.581 0.010 -0.080 -0.011 contestant_episode_Spencer Bledsoe Chaos Is My Friend -0.0320 0.017 -1.933 0.053 -0.064 0.000 contestant_episode_Spencer Bledsoe Cops-R-Us (episode) -0.0180 0.032 -0.559 0.576 -0.081 0.045 contestant_episode_Spencer Bledsoe Havoc to Wreak 0.0473 0.016 2.956 0.003 0.016 0.079 contestant_episode_Spencer Bledsoe Head of the Snake -0.0275 0.017 -1.571 0.116 -0.062 0.007 contestant_episode_Spencer Bledsoe Hot Girl with a Grudge -0.0760 0.019 -4.071 0.000 -0.113 -0.039 contestant_episode_Spencer Bledsoe Mad Treasure Hunt 0.0024 0.017 0.143 0.886 -0.031 0.036 contestant_episode_Spencer Bledsoe My Wheels Are Spinning -0.0048 0.021 -0.230 0.818 -0.045 0.036 contestant_episode_Spencer Bledsoe Odd One Out 0.0892 0.033 2.686 0.007 0.024 0.154 contestant_episode_Spencer Bledsoe Our Time to Shine -0.0551 0.021 -2.638 0.008 -0.096 -0.014 contestant_episode_Spencer Bledsoe Play to Win -0.0020 0.013 -0.152 0.879 -0.028 0.024 contestant_episode_Spencer Bledsoe Sitting in My Spy Shack -0.0091 0.018 -0.516 0.606 -0.044 0.026 contestant_episode_Spencer Bledsoe Straw That Broke the Camel's Back 0.0295 0.019 1.519 0.129 -0.009 0.068 contestant_episode_Spencer Bledsoe Survivor MacGyver -0.0347 0.014 -2.519 0.012 -0.062 -0.008 contestant_episode_Spencer Bledsoe Tiny Little Shanks to the Heart 0.0269 0.017 1.594 0.111 -0.006 0.060 contestant_episode_Spencer Bledsoe Villains Have More Fun 0.0202 0.022 0.927 0.354 -0.022 0.063 contestant_episode_Spencer Bledsoe We Found Our Zombies 0.0849 0.027 3.183 0.001 0.033 0.137 contestant_episode_Spencer Bledsoe We Got a Rat -0.0174 0.015 -1.123 0.261 -0.048 0.013 contestant_episode_Spencer Bledsoe What's the Beef%3F -0.0169 0.019 -0.894 0.371 -0.054 0.020 contestant_episode_Spencer Bledsoe Witches Coven (episode) -0.0176 0.019 -0.914 0.361 -0.055 0.020 contestant_episode_Spencer Bledsoe You Call, We'll Haul 0.0164 0.017 0.992 0.321 -0.016 0.049 contestant_episode_Stephen Fishbach Bunking with the Devil -0.0214 0.025 -0.845 0.398 -0.071 0.028 contestant_episode_Stephen Fishbach My Wheels Are Spinning -0.1546 0.142 -1.090 0.276 -0.433 0.123 contestant_episode_Stephen Fishbach Play to Win -0.0106 0.023 -0.450 0.653 -0.057 0.035 contestant_episode_Stephen Fishbach Survivor MacGyver -0.0258 0.025 -1.046 0.296 -0.074 0.023 contestant_episode_Stephen Fishbach We Got a Rat -0.0273 0.026 -1.055 0.291 -0.078 0.023 contestant_episode_Stephen Fishbach What's the Beef%3F -0.0357 0.028 -1.273 0.203 -0.091 0.019 contestant_episode_Stephen Fishbach Witches Coven (episode) -0.0208 0.025 -0.824 0.410 -0.070 0.029 contestant_episode_Stephen Fishbach You Call, We'll Haul -0.0215 0.022 -0.968 0.333 -0.065 0.022 contestant_episode_Sunday Burquest I Will Destroy You 0.0304 0.029 1.067 0.286 -0.025 0.086 contestant_episode_Sunday Burquest I'm the Kingpin -0.0110 0.028 -0.395 0.693 -0.065 0.043 contestant_episode_Sunday Burquest Idol Search Party -0.0413 0.030 -1.376 0.169 -0.100 0.018 contestant_episode_Sunday Burquest Love Goggles -0.0292 0.042 -0.696 0.486 -0.111 0.053 contestant_episode_Sunday Burquest May the Best Generation Win -0.0356 0.033 -1.082 0.279 -0.100 0.029 contestant_episode_Sunday Burquest Million Dollar Gamble 0.0002 0.017 0.012 0.991 -0.034 0.034 contestant_episode_Sunday Burquest Slayed the Survivor Dragon 0.0346 0.021 1.633 0.102 -0.007 0.076 contestant_episode_Sunday Burquest Still Throwin' Punches -0.0295 0.021 -1.433 0.152 -0.070 0.011 contestant_episode_Sunday Burquest The Truth Works Well 0.0097 0.035 0.281 0.779 -0.058 0.078 contestant_episode_Sunday Burquest Who's the Sucker at the Table%3F -0.0587 0.029 -2.019 0.044 -0.116 -0.002 contestant_episode_Sunday Burquest Your Job Is Recon -0.0092 0.038 -0.244 0.807 -0.083 0.065 contestant_episode_Tai Trang Dirty Deed 0.0406 0.027 1.529 0.126 -0.011 0.093 contestant_episode_Tai Trang I'm Not Here to Make Good Friends -0.0622 0.063 -0.989 0.323 -0.185 0.061 contestant_episode_Tai Trang I'm a Mental Giant 0.0390 0.018 2.165 0.030 0.004 0.074 contestant_episode_Tai Trang It Is Not a High Without a Low 0.0825 0.025 3.252 0.001 0.033 0.132 contestant_episode_Tai Trang It's Merge Time 0.0146 0.017 0.868 0.386 -0.018 0.048 contestant_episode_Tai Trang It's Psychological Warfare -0.0009 0.015 -0.062 0.951 -0.031 0.029 contestant_episode_Tai Trang It's a 'Me' Game, Not a 'We' Game 0.0310 0.015 2.096 0.036 0.002 0.060 contestant_episode_Tai Trang Kindergarten Camp 0.0213 0.018 1.157 0.247 -0.015 0.057 contestant_episode_Tai Trang Now's the Time to Start Scheming -0.0106 0.016 -0.661 0.509 -0.042 0.021 contestant_episode_Tai Trang Parting Is Such Sweet Sorrow 0.0516 0.025 2.077 0.038 0.003 0.100 contestant_episode_Tai Trang Play or Go Home 0.0111 0.020 0.562 0.574 -0.028 0.050 contestant_episode_Tai Trang Reinventing How This Game Is Played 0.0467 0.025 1.851 0.064 -0.003 0.096 contestant_episode_Tai Trang Signed, Sealed and Delivered 0.0020 0.024 0.085 0.932 -0.045 0.049 contestant_episode_Tai Trang Survivor Jackpot 0.0423 0.024 1.742 0.082 -0.005 0.090 contestant_episode_Tai Trang The Circle of Life -0.0083 0.018 -0.460 0.646 -0.044 0.027 contestant_episode_Tai Trang The Devils We Know 0.0488 0.015 3.297 0.001 0.020 0.078 contestant_episode_Tai Trang The Jocks vs. the Pretty People -0.0144 0.020 -0.719 0.472 -0.054 0.025 contestant_episode_Tai Trang The Stakes Have Been Raised 0.0176 0.029 0.609 0.543 -0.039 0.074 contestant_episode_Tai Trang The Tables Have Turned -0.0058 0.042 -0.138 0.890 -0.088 0.076 contestant_episode_Tai Trang There's a New Sheriff in Town 0.0632 0.024 2.670 0.008 0.017 0.110 contestant_episode_Tai Trang Vote Early, Vote Often 0.0279 0.023 1.212 0.226 -0.017 0.073 contestant_episode_Tai Trang What Happened on Exile, Stays on Exile 0.0306 0.022 1.370 0.171 -0.013 0.074 contestant_episode_Tai Trang With Me or Not with Me 0.0430 0.016 2.677 0.007 0.012 0.074 contestant_episode_Tarzan Smith Bum-Puzzled -0.0828 0.053 -1.575 0.115 -0.186 0.020 contestant_episode_Tarzan Smith Go Out with a Bang -0.1846 0.060 -3.090 0.002 -0.302 -0.068 contestant_episode_Tarzan Smith I'm No Dummy -0.0328 0.049 -0.672 0.502 -0.129 0.063 contestant_episode_Tarzan Smith It's Gonna Be Chaos 0.1053 0.053 1.974 0.048 0.001 0.210 contestant_episode_Tarzan Smith It's Human Nature 0.0482 0.057 0.845 0.398 -0.064 0.160 contestant_episode_Tarzan Smith Never Say Die -0.0474 0.053 -0.894 0.371 -0.151 0.057 contestant_episode_Tarzan Smith Thanks for the Souvenir -0.0497 0.062 -0.800 0.424 -0.172 0.072 contestant_episode_Tarzan Smith The Beauty in a Merge -0.1941 0.063 -3.085 0.002 -0.317 -0.071 contestant_episode_Tasha Fox Bag of Tricks 0.0362 0.024 1.509 0.131 -0.011 0.083 contestant_episode_Tasha Fox Bunking with the Devil 0.0177 0.022 0.788 0.431 -0.026 0.062 contestant_episode_Tasha Fox Chaos Is My Friend -0.0097 0.021 -0.451 0.652 -0.052 0.032 contestant_episode_Tasha Fox Cops-R-Us (episode) 0.0450 0.044 1.019 0.308 -0.042 0.132 contestant_episode_Tasha Fox Havoc to Wreak -0.0025 0.023 -0.108 0.914 -0.047 0.042 contestant_episode_Tasha Fox Head of the Snake 0.0034 0.028 0.123 0.902 -0.051 0.057 contestant_episode_Tasha Fox Hot Girl with a Grudge -0.0426 0.022 -1.963 0.050 -0.085 -7.49e-05 contestant_episode_Tasha Fox Mad Treasure Hunt 0.0500 0.032 1.565 0.118 -0.013 0.113 contestant_episode_Tasha Fox My Wheels Are Spinning 0.0446 0.022 2.051 0.040 0.002 0.087 contestant_episode_Tasha Fox Odd One Out 0.0214 0.043 0.494 0.621 -0.064 0.106 contestant_episode_Tasha Fox Our Time to Shine -0.0377 0.031 -1.204 0.229 -0.099 0.024 contestant_episode_Tasha Fox Play to Win -0.0128 0.020 -0.630 0.529 -0.052 0.027 contestant_episode_Tasha Fox Sitting in My Spy Shack 0.0277 0.024 1.177 0.239 -0.018 0.074 contestant_episode_Tasha Fox Straw That Broke the Camel's Back 0.0053 0.028 0.186 0.853 -0.050 0.061 contestant_episode_Tasha Fox Survivor MacGyver 0.0220 0.025 0.880 0.379 -0.027 0.071 contestant_episode_Tasha Fox Tiny Little Shanks to the Heart 0.0317 0.021 1.480 0.139 -0.010 0.074 contestant_episode_Tasha Fox Villains Have More Fun 0.0214 0.021 1.011 0.312 -0.020 0.063 contestant_episode_Tasha Fox We Found Our Zombies 0.0100 0.037 0.274 0.784 -0.062 0.082 contestant_episode_Tasha Fox We Got a Rat 0.0166 0.023 0.730 0.465 -0.028 0.061 contestant_episode_Tasha Fox What's the Beef%3F 0.0046 0.023 0.205 0.838 -0.040 0.049 contestant_episode_Tasha Fox Witches Coven (episode) -0.0077 0.026 -0.295 0.768 -0.059 0.044 contestant_episode_Tasha Fox You Call, We'll Haul -0.0028 0.023 -0.121 0.903 -0.048 0.042 contestant_episode_Taylor Stocker I'm the Kingpin 0.0120 0.020 0.599 0.550 -0.027 0.051 contestant_episode_Taylor Stocker Idol Search Party 0.0040 0.023 0.170 0.865 -0.042 0.049 contestant_episode_Taylor Stocker Love Goggles 0.0060 0.026 0.229 0.819 -0.045 0.057 contestant_episode_Taylor Stocker May the Best Generation Win 0.0259 0.023 1.127 0.260 -0.019 0.071 contestant_episode_Taylor Stocker Still Throwin' Punches -0.0141 0.022 -0.639 0.523 -0.057 0.029 contestant_episode_Taylor Stocker The Truth Works Well -0.0034 0.020 -0.175 0.861 -0.042 0.035 contestant_episode_Taylor Stocker Who's the Sucker at the Table%3F -0.0248 0.030 -0.828 0.408 -0.084 0.034 contestant_episode_Taylor Stocker Your Job Is Recon -0.0017 0.029 -0.061 0.952 -0.058 0.054 contestant_episode_Terry Deitz Survivor MacGyver -0.0242 0.029 -0.835 0.404 -0.081 0.033 contestant_episode_Terry Deitz We Got a Rat -0.0085 0.033 -0.259 0.796 -0.072 0.056 contestant_episode_Terry Deitz What's the Beef%3F -0.0727 0.047 -1.556 0.120 -0.164 0.019 contestant_episode_Tina Wesson Blood Is Thicker than Anything -0.0373 0.056 -0.666 0.505 -0.147 0.072 contestant_episode_Tina Wesson Gloves Come Off 0.0268 0.051 0.521 0.602 -0.074 0.128 contestant_episode_Tina Wesson My Brother's Keeper -0.0153 0.040 -0.385 0.700 -0.093 0.062 contestant_episode_Tina Wesson One-Man Wrecking Ball 0.0618 0.041 1.495 0.135 -0.019 0.143 contestant_episode_Tina Wesson Out on a Limb (episode) 0.0247 0.047 0.530 0.596 -0.067 0.116 contestant_episode_Tina Wesson Rustle Feathers 0.0074 0.047 0.158 0.874 -0.084 0.099 contestant_episode_Tina Wesson Skin of My Teeth -0.0478 0.040 -1.203 0.229 -0.126 0.030 contestant_episode_Tina Wesson Swoop In for the Kill 0.0160 0.040 0.405 0.685 -0.062 0.094 contestant_episode_Tina Wesson The Dead Can Still Talk 0.0127 0.050 0.254 0.800 -0.086 0.111 contestant_episode_Tony Vlachos Chaos Is My Friend -0.0096 0.016 -0.586 0.558 -0.042 0.023 contestant_episode_Tony Vlachos Cops-R-Us (episode) 0.0081 0.031 0.266 0.790 -0.052 0.068 contestant_episode_Tony Vlachos Havoc to Wreak 0.0203 0.017 1.185 0.236 -0.013 0.054 contestant_episode_Tony Vlachos Head of the Snake 0.0026 0.023 0.110 0.913 -0.043 0.049 contestant_episode_Tony Vlachos Hot Girl with a Grudge -0.0425 0.031 -1.370 0.171 -0.103 0.018 contestant_episode_Tony Vlachos Mad Treasure Hunt -0.0161 0.024 -0.664 0.507 -0.064 0.031 contestant_episode_Tony Vlachos Odd One Out -0.0003 0.032 -0.009 0.993 -0.062 0.062 contestant_episode_Tony Vlachos Our Time to Shine -0.0012 0.036 -0.034 0.973 -0.072 0.070 contestant_episode_Tony Vlachos Sitting in My Spy Shack -0.0163 0.020 -0.799 0.424 -0.056 0.024 contestant_episode_Tony Vlachos Straw That Broke the Camel's Back 0.0294 0.016 1.804 0.071 -0.003 0.061 contestant_episode_Tony Vlachos The Stakes Have Been Raised -0.0245 0.023 -1.064 0.288 -0.070 0.021 contestant_episode_Tony Vlachos We Found Our Zombies -0.0556 0.027 -2.053 0.040 -0.109 -0.003 contestant_episode_Trish Hegarty Chaos Is My Friend 0.0369 0.027 1.372 0.170 -0.016 0.090 contestant_episode_Trish Hegarty Havoc to Wreak 0.0301 0.026 1.159 0.246 -0.021 0.081 contestant_episode_Trish Hegarty Head of the Snake -0.0231 0.032 -0.731 0.465 -0.085 0.039 contestant_episode_Trish Hegarty Hot Girl with a Grudge 0.0338 0.049 0.692 0.489 -0.062 0.129 contestant_episode_Trish Hegarty Mad Treasure Hunt 0.0572 0.038 1.491 0.136 -0.018 0.132 contestant_episode_Trish Hegarty Odd One Out -0.0560 0.031 -1.827 0.068 -0.116 0.004 contestant_episode_Trish Hegarty Our Time to Shine -0.0289 0.053 -0.544 0.587 -0.133 0.075 contestant_episode_Trish Hegarty Sitting in My Spy Shack 0.0653 0.027 2.450 0.014 0.013 0.117 contestant_episode_Trish Hegarty Straw That Broke the Camel's Back 0.0254 0.024 1.039 0.299 -0.023 0.073 contestant_episode_Trish Hegarty We Found Our Zombies -0.0773 0.039 -1.991 0.046 -0.153 -0.001 contestant_episode_Troyzan Robertson Dirty Deed 0.0527 0.033 1.618 0.106 -0.011 0.117 contestant_episode_Troyzan Robertson Go Out with a Bang 0.0387 0.062 0.628 0.530 -0.082 0.159 contestant_episode_Troyzan Robertson I'm No Dummy 0.0092 0.058 0.159 0.874 -0.105 0.123 contestant_episode_Troyzan Robertson It Is Not a High Without a Low 0.0516 0.028 1.844 0.065 -0.003 0.106 contestant_episode_Troyzan Robertson Never Say Die 0.0908 0.058 1.561 0.119 -0.023 0.205 contestant_episode_Troyzan Robertson Parting Is Such Sweet Sorrow 0.0734 0.027 2.734 0.006 0.021 0.126 contestant_episode_Troyzan Robertson Reinventing How This Game Is Played 0.0808 0.029 2.833 0.005 0.025 0.137 contestant_episode_Troyzan Robertson Survivor Jackpot 0.0312 0.026 1.210 0.226 -0.019 0.082 contestant_episode_Troyzan Robertson The Beauty in a Merge 0.0111 0.068 0.165 0.869 -0.121 0.144 contestant_episode_Troyzan Robertson The Stakes Have Been Raised 0.0119 0.027 0.439 0.661 -0.041 0.065 contestant_episode_Troyzan Robertson The Tables Have Turned 0.0614 0.032 1.935 0.053 -0.001 0.124 contestant_episode_Troyzan Robertson There's a New Sheriff in Town 0.0197 0.027 0.727 0.467 -0.033 0.073 contestant_episode_Troyzan Robertson Vote Early, Vote Often 0.0536 0.029 1.834 0.067 -0.004 0.111 contestant_episode_Troyzan Robertson What Happened on Exile, Stays on Exile 0.0221 0.031 0.703 0.482 -0.039 0.084 contestant_episode_Tyler Fredrickson Crazy Is as Crazy Does -0.0025 0.027 -0.093 0.926 -0.055 0.050 contestant_episode_Tyler Fredrickson Holding On for Dear Life -0.0110 0.016 -0.679 0.497 -0.043 0.021 contestant_episode_Tyler Fredrickson It Will Be My Revenge -0.0539 0.031 -1.741 0.082 -0.115 0.007 contestant_episode_Tyler Fredrickson It's Survivor Warfare -0.0270 0.028 -0.961 0.336 -0.082 0.028 contestant_episode_Tyler Fredrickson Keep It Real -0.0403 0.016 -2.497 0.013 -0.072 -0.009 contestant_episode_Tyler Fredrickson Livin' on the Edge -0.0303 0.027 -1.130 0.259 -0.083 0.022 contestant_episode_Tyler Fredrickson Survivor Russian Roulette -0.0325 0.018 -1.814 0.070 -0.068 0.003 contestant_episode_Tyler Fredrickson The Line Will Be Drawn Tonight -0.0220 0.020 -1.110 0.267 -0.061 0.017 contestant_episode_Tyler Fredrickson Winner Winner, Chicken Dinner 0.0216 0.024 0.899 0.369 -0.025 0.069 contestant_episode_Tyson Apostol Blood Is Thicker than Anything -0.0878 0.046 -1.914 0.056 -0.178 0.002 contestant_episode_Tyson Apostol Gloves Come Off -0.0044 0.028 -0.159 0.874 -0.058 0.050 contestant_episode_Tyson Apostol My Brother's Keeper 0.0102 0.032 0.322 0.748 -0.052 0.072 contestant_episode_Tyson Apostol One Armed Dude and Three Moms -0.0153 0.045 -0.339 0.734 -0.104 0.073 contestant_episode_Tyson Apostol One-Man Wrecking Ball 0.0167 0.034 0.489 0.625 -0.050 0.083 contestant_episode_Tyson Apostol Opening Pandora's Box 0.0403 0.039 1.041 0.298 -0.036 0.116 contestant_episode_Tyson Apostol Out on a Limb (episode) 0.0162 0.026 0.611 0.542 -0.036 0.068 contestant_episode_Tyson Apostol Rule in Chaos 0.0398 0.039 1.020 0.308 -0.037 0.116 contestant_episode_Tyson Apostol Rustle Feathers 0.0235 0.024 0.963 0.336 -0.024 0.071 contestant_episode_Tyson Apostol Skin of My Teeth -0.0068 0.030 -0.230 0.818 -0.065 0.051 contestant_episode_Tyson Apostol Swoop In for the Kill 0.0616 0.035 1.762 0.078 -0.007 0.130 contestant_episode_Tyson Apostol The Dead Can Still Talk -0.0233 0.044 -0.524 0.600 -0.110 0.064 contestant_episode_Val Collins Suck It Up and Survive 0.0603 0.037 1.642 0.100 -0.012 0.132 contestant_episode_Vince Sly It's Survivor Warfare 0.0162 0.021 0.760 0.447 -0.026 0.058 contestant_episode_Vytas Baskauskas Blood Is Thicker than Anything 0.0532 0.052 1.024 0.306 -0.049 0.155 contestant_episode_Vytas Baskauskas Gloves Come Off 0.0154 0.045 0.344 0.731 -0.072 0.103 contestant_episode_Vytas Baskauskas My Brother's Keeper -0.0053 0.041 -0.129 0.898 -0.086 0.075 contestant_episode_Vytas Baskauskas One Armed Dude and Three Moms -0.0122 0.053 -0.228 0.819 -0.117 0.092 contestant_episode_Vytas Baskauskas One-Man Wrecking Ball 0.0087 0.041 0.213 0.831 -0.071 0.089 contestant_episode_Vytas Baskauskas Opening Pandora's Box -0.0395 0.051 -0.773 0.440 -0.140 0.061 contestant_episode_Vytas Baskauskas Skin of My Teeth -0.0544 0.045 -1.198 0.231 -0.143 0.035 contestant_episode_Vytas Baskauskas Swoop In for the Kill 0.0094 0.041 0.232 0.817 -0.070 0.089 contestant_episode_Vytas Baskauskas The Dead Can Still Talk -0.0578 0.055 -1.047 0.295 -0.166 0.050 contestant_episode_Wes Nale Blood Is Blood -0.0070 0.044 -0.159 0.874 -0.093 0.079 contestant_episode_Wes Nale Gettin' to Crunch Time -0.0075 0.033 -0.229 0.819 -0.072 0.057 contestant_episode_Wes Nale Make Some Magic Happen 0.0005 0.046 0.010 0.992 -0.089 0.090 contestant_episode_Wes Nale Method to This Madness -0.0164 0.047 -0.347 0.728 -0.109 0.076 contestant_episode_Wes Nale Million Dollar Decision 0.0056 0.040 0.142 0.887 -0.072 0.083 contestant_episode_Wes Nale Suck It Up and Survive -0.0071 0.037 -0.190 0.849 -0.080 0.066 contestant_episode_Wes Nale This Is Where We Build Trust -0.0077 0.034 -0.229 0.819 -0.074 0.058 contestant_episode_Wes Nale We're a Hot Mess -0.0086 0.040 -0.217 0.828 -0.086 0.069 contestant_episode_Wes Nale Wrinkle in the Plan 0.0162 0.035 0.458 0.647 -0.053 0.086 contestant_episode_Will Sims II Crazy Is as Crazy Does -0.0242 0.023 -1.055 0.291 -0.069 0.021 contestant_episode_Will Sims II Holding On for Dear Life 0.0857 0.022 3.908 0.000 0.043 0.129 contestant_episode_Will Sims II It Will Be My Revenge 0.0308 0.021 1.490 0.136 -0.010 0.071 contestant_episode_Will Sims II It's Survivor Warfare 0.0871 0.032 2.715 0.007 0.024 0.150 contestant_episode_Will Sims II Keep It Real 0.0290 0.015 1.949 0.051 -0.000 0.058 contestant_episode_Will Sims II Livin' on the Edge 0.0417 0.016 2.623 0.009 0.011 0.073 contestant_episode_Will Sims II My Word Is My Bond 0.0502 0.017 2.929 0.003 0.017 0.084 contestant_episode_Will Sims II Survivor Russian Roulette -0.0048 0.017 -0.291 0.771 -0.037 0.028 contestant_episode_Will Sims II The Line Will Be Drawn Tonight 0.0074 0.016 0.475 0.635 -0.023 0.038 contestant_episode_Will Sims II Winner Winner, Chicken Dinner -0.0099 0.021 -0.461 0.645 -0.052 0.032 contestant_episode_Will Wahl I Will Destroy You 0.0208 0.022 0.949 0.343 -0.022 0.064 contestant_episode_Will Wahl I'm the Kingpin -0.0145 0.023 -0.627 0.531 -0.060 0.031 contestant_episode_Will Wahl Idol Search Party -0.0065 0.027 -0.239 0.811 -0.060 0.047 contestant_episode_Will Wahl Love Goggles -0.0303 0.023 -1.346 0.178 -0.074 0.014 contestant_episode_Will Wahl May the Best Generation Win 0.0024 0.026 0.092 0.927 -0.048 0.053 contestant_episode_Will Wahl Million Dollar Gamble 0.0160 0.012 1.285 0.199 -0.008 0.040 contestant_episode_Will Wahl Slayed the Survivor Dragon 0.0102 0.013 0.785 0.432 -0.015 0.036 contestant_episode_Will Wahl Still Throwin' Punches 0.0194 0.021 0.915 0.360 -0.022 0.061 contestant_episode_Will Wahl The Truth Works Well -0.0151 0.026 -0.591 0.555 -0.065 0.035 contestant_episode_Will Wahl Who's the Sucker at the Table%3F -0.0257 0.032 -0.813 0.416 -0.088 0.036 contestant_episode_Will Wahl Your Job Is Recon -0.0048 0.030 -0.159 0.874 -0.064 0.054 contestant_episode_Woo Hwang Bag of Tricks -0.0081 0.021 -0.395 0.693 -0.048 0.032 contestant_episode_Woo Hwang Bunking with the Devil -0.0687 0.023 -2.938 0.003 -0.115 -0.023 contestant_episode_Woo Hwang Chaos Is My Friend 0.0378 0.018 2.052 0.040 0.002 0.074 contestant_episode_Woo Hwang Cops-R-Us (episode) -0.0111 0.038 -0.295 0.768 -0.085 0.063 contestant_episode_Woo Hwang Havoc to Wreak 0.0193 0.016 1.180 0.238 -0.013 0.051 contestant_episode_Woo Hwang Head of the Snake 0.0170 0.026 0.649 0.516 -0.034 0.068 contestant_episode_Woo Hwang Hot Girl with a Grudge 0.0081 0.028 0.290 0.772 -0.047 0.063 contestant_episode_Woo Hwang Mad Treasure Hunt -0.0538 0.026 -2.094 0.036 -0.104 -0.003 contestant_episode_Woo Hwang Odd One Out 0.0406 0.030 1.336 0.182 -0.019 0.100 contestant_episode_Woo Hwang Our Time to Shine -0.0162 0.033 -0.494 0.622 -0.081 0.048 contestant_episode_Woo Hwang Sitting in My Spy Shack -0.0249 0.022 -1.148 0.251 -0.067 0.018 contestant_episode_Woo Hwang Straw That Broke the Camel's Back -0.0139 0.017 -0.806 0.421 -0.048 0.020 contestant_episode_Woo Hwang Survivor MacGyver -0.0476 0.023 -2.092 0.036 -0.092 -0.003 contestant_episode_Woo Hwang We Found Our Zombies -0.0278 0.032 -0.871 0.384 -0.091 0.035 contestant_episode_Woo Hwang We Got a Rat -0.0652 0.021 -3.035 0.002 -0.107 -0.023 contestant_episode_Woo Hwang What's the Beef%3F -0.0312 0.021 -1.499 0.134 -0.072 0.010 contestant_episode_Zeke Smith About to Have a Rumble -0.0150 0.014 -1.103 0.270 -0.042 0.012 contestant_episode_Zeke Smith Dirty Deed 0.0181 0.023 0.783 0.434 -0.027 0.063 contestant_episode_Zeke Smith I Will Destroy You -0.0242 0.016 -1.547 0.122 -0.055 0.006 contestant_episode_Zeke Smith I'm the Kingpin 0.0075 0.015 0.512 0.609 -0.021 0.036 contestant_episode_Zeke Smith Idol Search Party 0.0090 0.015 0.585 0.559 -0.021 0.039 contestant_episode_Zeke Smith Love Goggles -0.0003 0.019 -0.014 0.989 -0.037 0.036 contestant_episode_Zeke Smith May the Best Generation Win 0.0484 0.017 2.869 0.004 0.015 0.081 contestant_episode_Zeke Smith Million Dollar Gamble -0.0292 0.012 -2.456 0.014 -0.052 -0.006 contestant_episode_Zeke Smith Reinventing How This Game Is Played 0.0208 0.013 1.642 0.101 -0.004 0.046 contestant_episode_Zeke Smith Slayed the Survivor Dragon 0.0134 0.017 0.791 0.429 -0.020 0.047 contestant_episode_Zeke Smith Still Throwin' Punches 0.0379 0.022 1.741 0.082 -0.005 0.081 contestant_episode_Zeke Smith Survivor Jackpot 0.0614 0.024 2.508 0.012 0.013 0.109 contestant_episode_Zeke Smith The Stakes Have Been Raised 0.0466 0.028 1.676 0.094 -0.008 0.101 contestant_episode_Zeke Smith The Tables Have Turned 0.0768 0.028 2.756 0.006 0.022 0.131 contestant_episode_Zeke Smith The Truth Works Well -0.0202 0.016 -1.248 0.212 -0.052 0.012 contestant_episode_Zeke Smith There's a New Sheriff in Town 0.0153 0.016 0.927 0.354 -0.017 0.048 contestant_episode_Zeke Smith Vote Early, Vote Often 0.0641 0.019 3.314 0.001 0.026 0.102 contestant_episode_Zeke Smith What Happened on Exile, Stays on Exile 0.0183 0.011 1.663 0.096 -0.003 0.040 contestant_episode_Zeke Smith Who's the Sucker at the Table%3F -0.0024 0.022 -0.111 0.912 -0.046 0.041 contestant_episode_Zeke Smith Your Job Is Recon 0.0064 0.021 0.309 0.758 -0.034 0.047 Omnibus: 32172.529 Durbin-Watson: 1.975 Prob(Omnibus): 0.000 Jarque-Bera (JB): 116899.849 Skew: 0.162 Prob(JB): 0.00 Kurtosis: 5.155 Cond. No. 1.10e+17 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 8.3e-26. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. Fitting the model with all of these features, we see there is a lot of insignificant variables. For our purposes, we will use the following quick_backwards_selection function. It by no means is the most robust -- it simply eliminates all variables which have non significant p-values until there are only significant variables remaining. It is however the fastest as compared to Forward or Backward selection. Additionally, both of these methods have their own issues . This doesn't guarantee that we will have all of the significant variables in our model, nor does it guarantee that this is the \"optimal\" model in terms of the number of significant variables, or any other metrics (like $R&#94;2$ , MSE or AIC , for instance). The variables which remain, though, are statistically significant at the 5% level. Many, by design, are mutually exclusive (or at least seem to be) -- allowing us to dig into the combinations and be certain they are uncorrelated with one another. In [54]: def quick_backwards_selection ( y , X ): use_cols = X . columns . tolist () fit = sm . OLS ( y , X ) . fit () while fit . pvalues [ fit . pvalues < . 05 ] . index . tolist () != use_cols : use_cols = fit . pvalues [ fit . pvalues < . 05 ] . index . tolist () fit = sm . OLS ( y , X [ use_cols ]) . fit () return fit In [255]: q_fit = quick_backwards_selection ( y , X ) In [251]: q_fit . summary () Out[251]: OLS Regression Results Dep. Variable: sentence_polarity R-squared: 0.004 Model: OLS Adj. R-squared: 0.004 Method: Least Squares F-statistic: 17.71 Date: Sun, 02 Aug 2020 Prob (F-statistic): 0.00 Time: 12:29:26 Log-Likelihood: -67239. No. Observations: 590728 AIC: 1.347e+05 Df Residuals: 590594 BIC: 1.363e+05 Df Model: 133 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 0.0714 0.001 79.590 0.000 0.070 0.073 season_episode_number 0.0008 0.000 7.652 0.000 0.001 0.001 tribal_reward_challenge_wins 0.0040 0.001 4.244 0.000 0.002 0.006 contestant_Anna Khait 0.0197 0.008 2.562 0.010 0.005 0.035 contestant_Aubry Bracco 0.0090 0.003 3.323 0.001 0.004 0.014 contestant_Candice Woodcock -0.0550 0.023 -2.418 0.016 -0.100 -0.010 contestant_Cirie Fields 0.0203 0.003 7.118 0.000 0.015 0.026 contestant_Garrett Adelstein -0.0653 0.014 -4.699 0.000 -0.093 -0.038 contestant_Hali Ford 0.0217 0.004 6.095 0.000 0.015 0.029 contestant_Jessica Johnston -0.0275 0.007 -3.745 0.000 -0.042 -0.013 contestant_Julia Landauer -0.0661 0.022 -3.061 0.002 -0.108 -0.024 contestant_Ken McNickle -0.0124 0.004 -2.843 0.004 -0.021 -0.004 contestant_Liz Markham 0.0744 0.017 4.348 0.000 0.041 0.108 contestant_Sarah Lacina 0.0078 0.003 2.455 0.014 0.002 0.014 contestant_Taylor Stocker -0.0344 0.005 -7.321 0.000 -0.044 -0.025 contestant_Will Sims II -0.0414 0.004 -9.840 0.000 -0.050 -0.033 role_Surfer Dude 0.0149 0.002 9.362 0.000 0.012 0.018 season_Heroes vs. Healers vs. Hustlers 0.0070 0.001 6.296 0.000 0.005 0.009 season_Kaôh Rōng 0.0044 0.001 3.528 0.000 0.002 0.007 season_Worlds Apart -0.0095 0.001 -7.426 0.000 -0.012 -0.007 alliance_David's Vinaka Alliance 0.0231 0.002 9.533 0.000 0.018 0.028 contestant_episode_Adam Klein I Will Destroy You -0.0311 0.014 -2.261 0.024 -0.058 -0.004 contestant_episode_Adam Klein I'm the Kingpin -0.0477 0.011 -4.400 0.000 -0.069 -0.026 contestant_episode_Adam Klein Million Dollar Gamble -0.0435 0.009 -4.719 0.000 -0.062 -0.025 contestant_episode_Adam Klein Still Throwin' Punches -0.0715 0.010 -7.295 0.000 -0.091 -0.052 contestant_episode_Ali Elliott The Past Will Eat You Alive -0.0318 0.010 -3.277 0.001 -0.051 -0.013 contestant_episode_Ali Elliott This Is Why You Play Survivor -0.0445 0.008 -5.519 0.000 -0.060 -0.029 contestant_episode_Andrea Boehlke The Tables Have Turned 0.0433 0.019 2.278 0.023 0.006 0.081 contestant_episode_Andrea Boehlke There's a New Sheriff in Town 0.0186 0.009 2.152 0.031 0.002 0.036 contestant_episode_Aubry Bracco I'm Not Here to Make Good Friends 0.0276 0.008 3.296 0.001 0.011 0.044 contestant_episode_Aubry Bracco Now's the Time to Start Scheming 0.0203 0.009 2.203 0.028 0.002 0.038 contestant_episode_Aubry Bracco With Me or Not with Me 0.0204 0.010 2.031 0.042 0.001 0.040 contestant_episode_Ben Driebergen Not Going to Roll Over and Die -0.0222 0.007 -3.382 0.001 -0.035 -0.009 contestant_episode_Ben Driebergen Playing with the Devil -0.0226 0.008 -2.778 0.005 -0.039 -0.007 contestant_episode_Brad Culpepper Survivor Jackpot 0.0677 0.009 7.523 0.000 0.050 0.085 contestant_episode_Brad Culpepper What Happened on Exile, Stays on Exile 0.0409 0.011 3.818 0.000 0.020 0.062 contestant_episode_Brenda Lowe Tubby Lunchbox 0.1225 0.041 2.963 0.003 0.041 0.204 contestant_episode_Bret LaBelle Slayed the Survivor Dragon 0.0320 0.010 3.268 0.001 0.013 0.051 contestant_episode_Caleb Bankston Swoop In for the Kill 0.1395 0.050 2.771 0.006 0.041 0.238 contestant_episode_Caleb Reynolds Kindergarten Camp 0.0558 0.017 3.347 0.001 0.023 0.088 contestant_episode_Carolyn Rivera It's Survivor Warfare 0.0772 0.018 4.407 0.000 0.043 0.112 contestant_episode_Carolyn Rivera My Word Is My Bond 0.0365 0.009 4.042 0.000 0.019 0.054 contestant_episode_Chris Hammons May the Best Generation Win 0.0428 0.017 2.543 0.011 0.010 0.076 contestant_episode_Chrissy Hofbeck Playing with the Devil -0.0148 0.008 -1.969 0.049 -0.030 -7.03e-05 contestant_episode_Cirie Fields It Is Not a High Without a Low 0.0276 0.011 2.626 0.009 0.007 0.048 contestant_episode_Coach Wade Then There Were Five 0.0791 0.039 2.021 0.043 0.002 0.156 contestant_episode_Cole Medders I Don't Like Having Snakes Around -0.1036 0.011 -9.492 0.000 -0.125 -0.082 contestant_episode_David Wright Idol Search Party -0.0347 0.011 -3.250 0.001 -0.056 -0.014 contestant_episode_David Wright Million Dollar Gamble -0.0261 0.007 -3.809 0.000 -0.040 -0.013 contestant_episode_David Wright The Truth Works Well -0.0492 0.014 -3.557 0.000 -0.076 -0.022 contestant_episode_David Wright Your Job Is Recon -0.0433 0.017 -2.512 0.012 -0.077 -0.010 contestant_episode_Gervase Peterson Swoop In for the Kill 0.1312 0.040 3.281 0.001 0.053 0.210 contestant_episode_Hannah Shapiro Idol Search Party -0.0756 0.024 -3.101 0.002 -0.123 -0.028 contestant_episode_Hannah Shapiro Million Dollar Gamble -0.0359 0.008 -4.270 0.000 -0.052 -0.019 contestant_episode_Hannah Shapiro Your Job Is Recon -0.0740 0.020 -3.723 0.000 -0.113 -0.035 contestant_episode_Hayden Moss Rustle Feathers 0.0580 0.014 4.243 0.000 0.031 0.085 contestant_episode_Jaclyn Schultz Blood Is Blood 0.0612 0.023 2.715 0.007 0.017 0.105 contestant_episode_Jaclyn Schultz Let's Make a Move 0.0266 0.011 2.454 0.014 0.005 0.048 contestant_episode_Jaclyn Schultz We're a Hot Mess 0.0697 0.029 2.370 0.018 0.012 0.127 contestant_episode_Jay Starrett Still Throwin' Punches -0.0210 0.010 -2.172 0.030 -0.040 -0.002 contestant_episode_Jeff Varner What Happened on Exile, Stays on Exile -0.0746 0.004 -18.744 0.000 -0.082 -0.067 contestant_episode_Jefra Bland Mad Treasure Hunt 0.0715 0.026 2.751 0.006 0.021 0.122 contestant_episode_Jenn Brown The Line Will Be Drawn Tonight 0.0428 0.012 3.724 0.000 0.020 0.065 contestant_episode_Jeremiah Wood Chaos Is My Friend -0.0944 0.033 -2.828 0.005 -0.160 -0.029 contestant_episode_Jeremiah Wood Mad Treasure Hunt 0.0938 0.038 2.446 0.014 0.019 0.169 contestant_episode_Jessica Johnston I'm a Wild Banshee 0.0526 0.024 2.216 0.027 0.006 0.099 contestant_episode_Jessica Johnston My Kisses Are Very Private 0.0451 0.016 2.746 0.006 0.013 0.077 contestant_episode_Jessica Johnston The Past Will Eat You Alive 0.0685 0.017 4.075 0.000 0.036 0.101 contestant_episode_Jessica Johnston This Is Why You Play Survivor 0.0463 0.015 3.138 0.002 0.017 0.075 contestant_episode_Jessica Lewis Who's the Sucker at the Table%3F -0.0474 0.014 -3.511 0.000 -0.074 -0.021 contestant_episode_Joe Mena Get to Gettin' -0.0286 0.010 -2.836 0.005 -0.048 -0.009 contestant_episode_Joe Mena Not Going to Roll Over and Die -0.0348 0.013 -2.582 0.010 -0.061 -0.008 contestant_episode_Jonathan Penner Little Miss Perfect 0.0932 0.025 3.746 0.000 0.044 0.142 contestant_episode_Keith Nale My Wheels Are Spinning 0.0376 0.010 3.910 0.000 0.019 0.056 contestant_episode_Keith Nale Still Holdin' On 0.0231 0.010 2.270 0.023 0.003 0.043 contestant_episode_Ken McNickle May the Best Generation Win 0.0764 0.016 4.844 0.000 0.045 0.107 contestant_episode_Ken McNickle Slayed the Survivor Dragon 0.0274 0.010 2.776 0.005 0.008 0.047 contestant_episode_Ken McNickle The Truth Works Well 0.0308 0.012 2.489 0.013 0.007 0.055 contestant_episode_Kimmi Kappenberg Villains Have More Fun 0.0248 0.011 2.220 0.026 0.003 0.047 contestant_episode_Kyle Jason Kindergarten Camp -0.0528 0.011 -4.973 0.000 -0.074 -0.032 contestant_episode_Kyle Jason Signed, Sealed and Delivered -0.0573 0.009 -6.598 0.000 -0.074 -0.040 contestant_episode_Kyle Jason The Circle of Life -0.0391 0.014 -2.831 0.005 -0.066 -0.012 contestant_episode_LJ McKanas Mad Treasure Hunt 0.0603 0.027 2.257 0.024 0.008 0.113 contestant_episode_Laura Morett Blood Is Thicker than Anything 0.0647 0.031 2.066 0.039 0.003 0.126 contestant_episode_Laura Morett Out on a Limb (episode) 0.0544 0.023 2.363 0.018 0.009 0.100 contestant_episode_Liz Markham Kindergarten Camp -0.0995 0.025 -3.991 0.000 -0.148 -0.051 contestant_episode_Liz Markham The Circle of Life -0.0586 0.021 -2.852 0.004 -0.099 -0.018 contestant_episode_Malcolm Freberg Survivor Jackpot 0.0713 0.012 5.782 0.000 0.047 0.095 contestant_episode_Mari Takahashi May the Best Generation Win 0.0584 0.015 3.838 0.000 0.029 0.088 contestant_episode_Matt Bischoff Operation Thunder Dome 0.0794 0.039 2.028 0.043 0.003 0.156 contestant_episode_Michael Skupin Not the Only Actor on This Island -0.1306 0.052 -2.503 0.012 -0.233 -0.028 contestant_episode_Michael Skupin Shot into Smithereens 0.0667 0.033 2.028 0.043 0.002 0.131 contestant_episode_Michelle Schubert Love Goggles 0.0423 0.011 3.749 0.000 0.020 0.064 contestant_episode_Michelle Schubert May the Best Generation Win 0.0713 0.019 3.715 0.000 0.034 0.109 contestant_episode_Michelle Schubert Your Job Is Recon 0.0460 0.021 2.229 0.026 0.006 0.086 contestant_episode_Mike Holloway Survivor Russian Roulette 0.0416 0.007 5.852 0.000 0.028 0.055 contestant_episode_Mike Zahalsky Fear of the Unknown -0.0248 0.010 -2.443 0.015 -0.045 -0.005 contestant_episode_Missy Payne Blood Is Blood -0.0328 0.017 -1.977 0.048 -0.065 -0.000 contestant_episode_Morgan McLeod Our Time to Shine 0.0919 0.033 2.753 0.006 0.026 0.157 contestant_episode_Natalie Anderson Blood Is Blood 0.0622 0.021 2.928 0.003 0.021 0.104 contestant_episode_Natalie Anderson Gettin' to Crunch Time 0.0310 0.015 2.131 0.033 0.002 0.060 contestant_episode_Natalie Anderson Let's Make a Move 0.0485 0.009 5.311 0.000 0.031 0.066 contestant_episode_Natalie Anderson This Is Where We Build Trust 0.0497 0.014 3.481 0.000 0.022 0.078 contestant_episode_Nick Maiorano It's Merge Time 0.0475 0.011 4.362 0.000 0.026 0.069 contestant_episode_Ozzy Lusth Double Agent -0.1364 0.045 -3.057 0.002 -0.224 -0.049 contestant_episode_Reynold Toepfer There's Gonna Be Hell to Pay -0.0882 0.033 -2.682 0.007 -0.153 -0.024 contestant_episode_Rodney Lavoie Jr. My Word Is My Bond 0.0339 0.008 4.012 0.000 0.017 0.050 contestant_episode_Ryan Ulrich This Is Why You Play Survivor -0.0356 0.007 -5.033 0.000 -0.049 -0.022 contestant_episode_Sandra Diaz-Twine The Stakes Have Been Raised 0.0121 0.005 2.302 0.021 0.002 0.022 contestant_episode_Sarah Lacina Head of the Snake -0.0377 0.014 -2.766 0.006 -0.064 -0.011 contestant_episode_Sarah Lacina What Happened on Exile, Stays on Exile -0.0333 0.012 -2.867 0.004 -0.056 -0.011 contestant_episode_Scot Pollard Signed, Sealed and Delivered -0.0695 0.009 -7.485 0.000 -0.088 -0.051 contestant_episode_Shirin Oskooi Survivor MacGyver -0.0504 0.008 -6.554 0.000 -0.066 -0.035 contestant_episode_Shirin Oskooi The Line Will Be Drawn Tonight 0.0513 0.016 3.221 0.001 0.020 0.082 contestant_episode_Spencer Bledsoe Havoc to Wreak 0.0438 0.013 3.371 0.001 0.018 0.069 contestant_episode_Spencer Bledsoe Hot Girl with a Grudge -0.0365 0.016 -2.272 0.023 -0.068 -0.005 contestant_episode_Spencer Bledsoe Odd One Out 0.1160 0.035 3.340 0.001 0.048 0.184 contestant_episode_Spencer Bledsoe We Found Our Zombies 0.1055 0.026 4.042 0.000 0.054 0.157 contestant_episode_Tai Trang I'm a Mental Giant 0.0458 0.011 4.034 0.000 0.024 0.068 contestant_episode_Tai Trang Parting Is Such Sweet Sorrow -0.0226 0.008 -2.745 0.006 -0.039 -0.006 contestant_episode_Tai Trang The Devils We Know 0.0383 0.010 3.702 0.000 0.018 0.059 contestant_episode_Tarzan Smith Go Out with a Bang -0.1569 0.046 -3.423 0.001 -0.247 -0.067 contestant_episode_Tarzan Smith It's Gonna Be Chaos 0.0977 0.038 2.597 0.009 0.024 0.171 contestant_episode_Tarzan Smith The Beauty in a Merge -0.1800 0.051 -3.512 0.000 -0.280 -0.080 contestant_episode_Trish Hegarty Sitting in My Spy Shack 0.0664 0.018 3.757 0.000 0.032 0.101 contestant_episode_Trish Hegarty We Found Our Zombies -0.0559 0.027 -2.089 0.037 -0.108 -0.003 contestant_episode_Will Sims II Holding On for Dear Life 0.0379 0.013 2.882 0.004 0.012 0.064 contestant_episode_Will Sims II It's Survivor Warfare 0.1041 0.029 3.584 0.000 0.047 0.161 contestant_episode_Will Sims II My Word Is My Bond 0.0398 0.011 3.706 0.000 0.019 0.061 contestant_episode_Woo Hwang Bunking with the Devil -0.0290 0.011 -2.743 0.006 -0.050 -0.008 contestant_episode_Woo Hwang Chaos Is My Friend 0.0446 0.017 2.574 0.010 0.011 0.079 contestant_episode_Woo Hwang We Got a Rat -0.0348 0.011 -3.107 0.002 -0.057 -0.013 contestant_episode_Zeke Smith May the Best Generation Win 0.0771 0.015 5.224 0.000 0.048 0.106 contestant_episode_Zeke Smith Million Dollar Gamble -0.0156 0.007 -2.193 0.028 -0.029 -0.002 Omnibus: 32376.276 Durbin-Watson: 1.962 Prob(Omnibus): 0.000 Jarque-Bera (JB): 119264.235 Skew: 0.157 Prob(JB): 0.00 Kurtosis: 5.179 Cond. No. 1.18e+03 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The condition number is large, 1.18e+03. This might indicate that there are strong multicollinearity or other numerical problems. We can see a lot of the more generalized variables drop out when considering the contestant_episode variables. In the case of using Forward or Backward selection, perhaps these variables would reemerge as some of the very specific and close to collectively exhaustive contestant_episode variables were dropped. However, for now, it is interesting to just look at the most significant of the contestant and contestant_episode variables. A few interesting things of note for the other variables: The intercept is .07 -- aligning with our observation that most of the comments skew slightly positive. Season Episode Number is significant with a somewhat small effect of 0.00008 . This indicates that as the season goes on, most contestants are viewed more favorably (albeit slightly). Perhaps this is due to the fact that most of the unbearably annoying contestants are voted out in the first few tribal councils. Tribal Reward challenge wins is significant with a coefficient of 0.004 -- an interesting result. Perhaps living a better lifestyle (from the rewards) makes people more likable? Perhaps winning rewards leads to more amenities and in turn leads to more immunity challenges, giving contestants less time to be unamicable and therefore disliked? It could be a few things, but regardless -- tribal reward challenge wins are correlated with higher sentiment. Surfer Dudes are viewed considerably more favorably ( .0149 which is large relative to other coefficients.) These players include the following: In [257]: reddit_df . loc [ reddit_df [ 'role' ] == 'Surfer Dude' , [ 'first_name' , 'last_name' ]] . drop_duplicates () Out[257]: first_name last_name 31 Devon Pinto 88649 Woo Hwang 113036 Alec Christy 147883 Joe Anglim 356456 Taylor Stocker 431162 Ozzy Lusth 549587 Grant Mattos 552658 Bill Posley 555622 Carter Williams 576495 Tyson Apostol 576500 Aras Baskauskas Contestants from the seasons Heroes vs. Healers vs. Hustlers and Kaôh Rōng are viewed more favorably than others, with a coefficient of 0.007 and 0.0044 respetively. Contestants from Worlds Apart are viewed less favorably than others, with a coefficient of -0.0095 . These are the only seasons that stand out as statistically significantly different than the others. The only tribe or alliance that was found to have a statistically significant like/dislike (besides the liking of the individual contestants, that is) is David's Vinaka Alliance . The members of this alliance tended to have a much larger sentiment -- 0.0231. This is an interesting one too! Contestant Episode and Contestant Columns The dominating variables that remained significant were those referring to the contestant and contestant_episode columns. These were generated for inference, to show contestants who had a statistically significant relationship with sentiment for comments in which their name appeared. These effects likely relate to either their actions within a particular episode (for contestant_episode variables) or their actions as a whole (for contestant variables). The plots and code below investigates the 15 largest magnitude coefficients for the significant variables for each. In [269]: top_contestant_ep_cols = [ x for x in q_fit . params . abs () . sort_values ( ascending = False ) . index if 'contestant_episode' in x ][ 0 : 15 ] In [270]: def bolden ( full_string , bolden ): boldened = '<b>' + bolden + '</b>' return full_string . replace ( bolden , boldened ) In [271]: def create_story_contestant_episode ( contestant_episode , init_text = '' ): example = reddit_df [ reddit_df [ 'contestant_episode' ] == contestant_episode ] . iloc [ 0 ] try : sentences = TextBlob ( example [ 'story' ]) . sentences except TypeError : return display ( HTML ( f 'Results from <i>Story</i> aspect of the Wiki for { contestant_episode } ...' )) relevant = [ s for s in sentences if example [ 'first_name' ] in s or example [ 'last_name' ] in s ] full_story = init_text + '</br></br></br>' if init_text else '' for r in relevant : story = str ( r ) story = story + '</br>' emphasized = bolden ( story , example [ 'first_name' ]) emphasized = bolden ( emphasized , example [ 'last_name' ]) full_story += emphasized full_story = force_breaks ( full_story ) return full_story In [272]: from copy import deepcopy def force_breaks ( string , every = 70 , split_on_words = False ): split_on_breaks = string . split ( '</br>' ) final_string = '' for s in split_on_breaks : temp = s while len ( temp ) > every : this_jump = deepcopy ( every ) while ( temp [ this_jump ] != ' ' ) and ( this_jump <= ( len ( temp ) - 2 )): this_jump += 1 final_string += ( temp [ 0 : this_jump ] + '</br>' ) temp = temp [ this_jump :] final_string += ( temp + '</br>' ) return final_string In [273]: def print_story_with_contestant ( first_name , last_name ): example = reddit_df [( reddit_df [ 'last_name' ] == last_name ) & ( reddit_df [ 'first_name' ] == first_name )] . drop_duplicates () full_story = '' for story in example [ 'story' ] . unique (): if story : full_story += story try : sentences = TextBlob ( full_story ) . sentences except TypeError : return display ( HTML ( f 'Results from <i>Story</i> aspect of the Wiki for { contestant_episode } ...' )) relevant = [ s for s in sentences if ( first_name in s ) or ( last_name in s )] for r in relevant : story = str ( r ) story = '<p hidden> ' + story + '</p>' emphasized = bolden ( story , first_name ) emphasized = bolden ( emphasized , last_name ) display ( HTML ( emphasized )) return rele In [73]: fig = go . Figure () buttons = [] l = top_contestant_ep_cols [ 0 : 15 ] for i , col in enumerate ( l ): temp_df = pd . concat ([ y , X [ col ]], axis = 1 ) temp_df = temp_df [ temp_df [ y . name ] != 0 ] contestant_episode = col . split ( '_' )[ - 1 ] temp_df [ col ] = temp_df [ col ] . map ({ 1 : contestant_episode , 0 : 'All Other Contestants' }) temp_df . rename ( columns = { col : col . replace ( 'contestant_episode_' , '' )}) coef = q_fit . params . loc [ col ] title = contestant_episode + f ' (Coefficient: { round ( coef , 3 ) } ) vs. All Other Comments' px_hist = histogram ( data_frame = temp_df , x = y . name , color = col , title = title , nbins = 25 , opacity =. 8 , histnorm = 'percent' ) hist_tup = tuple ( px_hist . data ) if i == 0 : fig . update_layout ({ 'legend' : px_hist . layout . legend , 'template' : px_hist . layout . template , 'xaxis' : px_hist . layout . xaxis , 'yaxis' : px_hist . layout . yaxis , 'title' : title , 'showlegend' : True }) for hist in hist_tup : hist . visible = ( i < 1 ) fig . add_trace ( hist ) m = len ( hist_tup ) n = len ( l ) vis = [ False ] * n * m vis [( m * i ): m * ( i + 1 )] = [ True ] * m text = f 'That is, Redditors viewed this contestant <b> { \"poorly\" if coef < 0 else \"favorably\" } .</b>' buttons . append ( dict ( label = contestant_episode , method = 'update' , args = [{ 'visible' : vis . copy ()}, { 'legend' : px_hist . layout . legend , 'template' : px_hist . layout . template , 'annotations' : [ dict ( x = 1.5 , y = 1.5 , text = text , showarrow = False , xref = 'paper' , yref = 'paper' , height = 400 )], 'xaxis' : px_hist . layout . xaxis , 'yaxis' : px_hist . layout . yaxis , 'title' : title , 'automargin' : True , 'showlegend' : True }])) fig . update_layout ( updatemenus = [ go . layout . Updatemenu ( active = 0 , buttons = buttons , # arg=dict(text='Contestant Episode Combination'), x = 1.5 , y = 1.5 )]) fig . update_layout ( dict ( barmode = 'overlay' , width = 1000 , height = 800 )) fig . show () In [83]: top_contestant_cols = [ x for x in q_fit . params . abs () . sort_values ( ascending = False ) . index if ( 'contestant' in x ) and ( 'episode' not in x )][ 0 : 10 ] In [84]: fig = go . Figure () buttons = [] l = top_contestant_cols [ 0 : 10 ] print ( l ) for i , col in enumerate ( l ): temp_df = pd . concat ([ y , X [ col ]], axis = 1 ) temp_df = temp_df [ temp_df [ y . name ] != 0 ] contestant = col . split ( '_' )[ - 1 ] temp_df [ col ] = temp_df [ col ] . map ({ 1 : contestant , 0 : 'All Other Contestants' }) temp_df . rename ( columns = { col : col . replace ( 'contestant_episode_' , '' )}) coef = q_fit . params . loc [ col ] title = contestant + f ' (Coefficient: { round ( coef , 3 ) } ) vs. All Other Comments' px_hist = histogram ( data_frame = temp_df , x = y . name , color = col , title = title , nbins = 25 , opacity =. 8 , histnorm = 'percent' ) hist_tup = tuple ( px_hist . data ) if i == 0 : fig . update_layout ({ 'legend' : px_hist . layout . legend , 'template' : px_hist . layout . template , 'xaxis' : px_hist . layout . xaxis , 'yaxis' : px_hist . layout . yaxis , 'title' : title , 'showlegend' : True }) for hist in hist_tup : hist . visible = ( i < 1 ) fig . add_trace ( hist ) m = len ( hist_tup ) n = len ( l ) vis = [ False ] * n * m vis [( m * i ): m * ( i + 1 )] = [ True ] * m text = f 'That is, Redditors viewed this contestant <b> { \"poorly\" if coef < 0 else \"favorably\" } .</b>' buttons . append ( dict ( label = contestant , method = 'update' , args = [{ 'visible' : vis . copy ()}, { 'legend' : px_hist . layout . legend , 'template' : px_hist . layout . template , 'annotations' : [ dict ( x = 1.5 , y = 1.5 , text = text , showarrow = False , xref = 'paper' , yref = 'paper' , height = 400 )], 'xaxis' : px_hist . layout . xaxis , 'yaxis' : px_hist . layout . yaxis , 'title' : title , 'automargin' : True , 'showlegend' : True }])) fig . update_layout ( updatemenus = [ go . layout . Updatemenu ( active = 0 , buttons = buttons , # arg=dict(text='Contestant Episode Combination'), x = 1.5 , y = 1.5 )]) fig . update_layout ( dict ( barmode = 'overlay' , width = 1000 , height = 800 )) fig . show () [] In [ ]:","tags":"Survivor","url":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-4.html","loc":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-4.html"},{"title":"Survivor: Outwit, Outplay, Out...analyze? (Part 3)","text":"Analyzing the Game of Survivor -- Analyzing Sentiment of Reddit Posts (3) Welcome back, to the third installment of the Survivor analysis series! My last post began to investigate the [relationship between Reddit mentions and particular contestants]. The next step is to dive a bit deeper into the Reddit comments themselves, using some basic NLP techniques. In this third installment of the series, I will continue to digging into some of the Reddit data that I had collected via the Pushift.io API . For more information on how this data was collected, please check out the [first article in this series], where I describe the ETL process for the Pushift (as well as other!) data. Introduction to NLP and Sentiment Analysis We will be using a similar query to the first post on this issue, so I will skip over any explanations here. A quick explanation of NLP -- and a disclaimer: I have done some work in this field, but am by no means an expert. The next few paragraphs gives a brief overview of some of the topics in this area. You are encouraged to dive deeper into this yourself. The following isn't necessary to understand this analysis, but I wanted to give a quick overview of some of the challenges in this field and some of the potential shortcoming of this analysis before I dive in. Sentiment analysis is a type of analysis in the realm of N atural L anguage P rocessing which investigates the sentiment of particular words and sentences. There is quite a lot of research on this topic, but essentially an annotated list of sentences is used to generate a model to predict the sentiment of the sentence using the attributes of the text itself. Extracting these attributes is an article in and to itself, and can be done in quite a few ways. The simplest, and most widely known, is using a bag of words approach. In this approach, each word is first tokenized to represent its meaning, and then across a variety of examples is vectorized based on the counts of each words. This assumes that essentially, everything interesting about a sentence can be broken down into its components (words). That is, the whole is the sum of the parts. However, we know this isn't the case. Words have contextual meaning as well, and there are correlations with other words in a sentence. In this space, there are some contextual encoders that try to handle this issue, like ELMO and BERT. That is a topic for another time, however. NLP is a rich field, and there are many topics of interest in the field. For this analysis, we will mostly be glossing over the most important portions of this to use pretrained models for sentiment analysis. Often times in these kinds of tasks, if the problem is generalizable enough, using pretrained models from a general corpus (like Wikipedia, or reviews across industries, for example) provides a sufficient model for your use-case. Of course, there may be domain specific meaning to particular words or phrases. For instance, a sentence: I hate Russell. is semantically very similar to: I hate Frosted Flakes. but Tony got the third immunity idol. has a Survivor -specific meaning in terms the word \"immunity\". In other contexts, like: I am immune to the Chicken Pox, since I've already had it. the word immunity has an essentially different meaning. In fact, in no cases (in general) will the word \"Immunity\" mean exactly the same thing as in a Survivor context. This is important to recognize, as it shows us the limitations of using a generalized model. For this reason, there is often a desire to fine-tune these generalized models to a specific use-case. However, while this is true, the word fine-tune is quite intentional. The generalized model handles the bulk of the work (usually a neural network in the case of embeddings) -- the fine-tuning has a smaller, incremental benefit as compared to the generalizable model. The general model gets you a lot of the way there. While the topics above ( contextual embeddings and fine-tuning models to domain-specific applications) are certainly of interest, and could potentially improve the models, I will leave this (and all model fitting) to a later date. For now, we will just use publically available, out-of-the-box solutions to get a quick view on the sentiment. Then, if the results seem worthwhile to dig deeper into, I will investigate in future articles building contextual models to gain more insight into the semantics and context of the sentences. In [1]: import os from sqlalchemy import create_engine import pandas as pd import numpy as np import statsmodels.api as sm from IPython.display import HTML , display from plotly.express import scatter In [2]: pg_un , pg_pw , pg_ip , pg_port = [ os . getenv ( x ) for x in [ 'PG_UN' , 'PG_PW' , 'PG_IP' , 'PG_PORT' ]] In [3]: def pg_uri ( un , pw , ip , port ): return f 'postgresql:// { un } : { pw } @ { ip } : { port } ' In [4]: eng = create_engine ( pg_uri ( pg_un , pg_pw , pg_ip , pg_port )) In [5]: sql = ''' WITH contestants_to_seasons AS ( SELECT c.contestant_id, c.first_name, c.last_name, cs.contestant_season_id, c.sex, cs.season_id, occupation, location, age, placement, days_lasted, votes_against, med_evac, quit, individual_wins, attempt_number, tribe_0, tribe_1, tribe_2, tribe_3, alliance_0, alliance_1, alliance_2, challenge_wins, challenge_appearances, sitout, voted_for_bootee, votes_against_player, character_id, r.role, r.description, total_number_of_votes_in_episode, tribal_council_appearances, votes_at_council, number_of_jury_votes, total_number_of_jury_votes, number_of_days_spent_in_episode, days_in_exile, individual_reward_challenge_appearances, individual_reward_challenge_wins, individual_immunity_challenge_appearances, individual_immunity_challenge_wins, tribal_reward_challenge_appearances, tribal_reward_challenge_wins, tribal_immunity_challenge_appearances, tribal_immunity_challenge_wins, tribal_reward_challenge_second_of_three_place, tribal_immunity_challenge_second_of_three_place, fire_immunity_challenge, tribal_immunity_challenge_third_place, episode_id FROM survivor.contestant c RIGHT JOIN survivor.contestant_season cs ON c.contestant_id = cs.contestant_id JOIN survivor.episode_performance_stats eps ON eps.contestant_id = cs.contestant_season_id JOIN survivor.role r ON cs.character_id = r.role_id ), matched_exact AS ( SELECT reddit.*, c.* FROM survivor.reddit_comments reddit JOIN contestants_to_seasons c ON (POSITION(c.first_name IN reddit.body) > 0 OR POSITION(c.last_name IN reddit.body) > 0) AND c.season_id = reddit.within_season AND c.episode_id = reddit.most_recent_episode WHERE within_season IS NOT NULL ) SELECT * FROM matched_exact m ''' In [6]: reddit_df = pd . read_sql ( sql , eng ) In [7]: ep_df = pd . read_sql ( 'SELECT * FROM survivor.episode' , eng ) In [8]: season_to_name = pd . read_sql ( 'SELECT season_id, name AS season_name FROM survivor.season' , eng ) In [9]: reddit_df = reddit_df . merge ( season_to_name , on = 'season_id' ) In [10]: reddit_df . rename ( columns = { 'name' : 'season_name' }, inplace = True ) In [11]: reddit_df = reddit_df . merge ( ep_df . drop ( columns = [ 'season_id' ]), on = 'episode_id' ) In [12]: reddit_df [ 'created_dt' ] = pd . to_datetime ( reddit_df [ 'created_dt' ]) In [13]: pd . options . display . max_columns = 100 TextBlob To analyze the sentiment of the comments, we will be using textblob , a NLP package that builds on top of NLTK and pattern , two very popular NLP libraries. They have a very simple API which will allow us to do alot of interesting analysis right out of the box! We will be using their pre-trained sentiment analysis to extract the sentiment from a word. To give a sense of what this looks like, let's look at a dummy example: In [14]: from textblob import TextBlob In [15]: text = 'I love Katy Perry.' blob = TextBlob ( text ) blob . sentiment Out[15]: Sentiment(polarity=0.5, subjectivity=0.6) In [16]: blob . noun_phrases Out[16]: WordList(['katy perry']) In each of these cases, we will get a polarity which represents the actual sentiment (best being 1, worst being -1) and subjectivity, which tries to analyze how subjective the sentence itself it (0 being \"objective\", 1 being subjective.) We will take a look at both in this analysis. Let's look at a particular comment and see what we will want to look at. In [242]: idx_two_example = reddit_df [ reddit_df [ 'body' ] . str . contains ( 'Tyler made the absolutely best strategic move he could have. Will... is an idiot.' )] . index [ 0 ] example_comment = reddit_df [ 'body' ] . iloc [ idx_two_example ] In [243]: example_comment Out[243]: 'Tyler made the absolutely best strategic move he could have. Will... is an idiot.' The first thing we can do is just look at the sentiment of this entire paragraph. That's straightforward enough: In [244]: b = TextBlob ( example_comment ) In [245]: TextBlob ( example_comment ) . sentiment Out[245]: Sentiment(polarity=0.09999999999999998, subjectivity=0.55) But there's a few thing with this. First off, we notices there are two different names in the above sentence, with different sentiments towards both. How do we associate the sentiment to the correct person? This is a hard problem. Essentially, we would have to find a way to match each noun to a verb or adjective. This isn't a trivial task, and assumes a lot -- that we can identify the Part of Speech (not a given at all), and then we can unambiguously match them. In short, this is not easy to do and, as far as I can see, there's no way to handle this out the box with TextBlob. There is , however, a somewhat \"close enough\" way to handle this problem. For now, since we're just looking at things on aggregate, this is what we will do. First, we can extract all of the relevant contestants from the string. Remember, each comment is replicated for each possible subject, if there are multiple names. In this case, we have: In [246]: example_subject = reddit_df [ 'first_name' ] . iloc [ idx_two_example ] In [247]: example_subject Out[247]: 'Will' It's Will. Looking at the above sentences, it looks like this person doesn't like Will too much. And that Tyson made the \"absolutely best strategic move\". So let's first extract only the sentences that have Will's name in it, and look at the average sentiment of these sentences. In [25]: [ x . sentiment for x in TextBlob ( example_comment ) . sentences ] Out[25]: [Sentiment(polarity=0.10000000000000003, subjectivity=0.65)] In [248]: np . mean ([ x . sentiment . polarity for x in TextBlob ( example_comment ) . sentences if example_subject in x ]) Out[248]: -0.8 Let's compare this with the other potential subject, on the next row: In [250]: example_subject = reddit_df [ 'first_name' ] . iloc [ idx_two_example + 1 ] print ( example_subject ) np . mean ([ x . sentiment . polarity for x in TextBlob ( example_comment ) . sentences if example_subject in x ]) Tyler Out[250]: 1.0 We see that this meets our expectations, rather than just using the overall polarity. Note that this method is far from perfect, but short of being able to associate each potential subject-adjective pair, it will perform well enough for our purposes. To give an example of how difficult this problem can be, take a look at this comment: In [222]: idx_many_example = reddit_df [ reddit_df [ 'body' ] . str . contains ( 'I am really wishing for a David win, partly because' )] . index [ 0 ] reddit_df [ 'body' ] . iloc [ idx_many_example ] Out[222]: \"I am really wishing for a David win, partly because I picked him as my flair and winner pick. (I just started joining this stuff this season.)\\n\\nDavid for the first episode was edited as super paranoid but even Tony, Mike, and Kristie were edited as super paranoid. David found two freaking idols! And probably will see the clue this merge episode. David was edited positively about showing his idol to Zeke kind of like Yul using the idol with Jonathan. David also saved Jessica which is kind of heroic. I can see the growth edit so much.\\n\\nThe only thing against David was that he was shown during the pre-season interviews and I don't remember last seasons where someone showed during those preseason clips and won. But Jay would be my next choice. Jeff Probst loved how a nerd like Cochran won survivor and he might like how David wins this one. Oh God, I was heartbroken when Aubry lost, please don't let another Michelle win. \" Look how many names we have there! By my count there is: David Tony Mike Kristie Zeke Yul Jonathan Jessica Jay Jeff (Probst!) Cochran Michelle Somehow, this person managed to jam that many names into so few words! (Impressive!) In this case, using the above method doesn't work so well. We'll stop here for now, but potentially there could be some interesting venues to go down associating subjects/objects with verbs and adjectives. In [29]: def extract_overall_and_sentence_sentiment ( row ): body = row [ 'body' ] first = row [ 'first_name' ] last = row [ 'last_name' ] b = TextBlob ( body ) overall_sentiment = b . sentiment overall_polarity = overall_sentiment . polarity overall_subj = overall_sentiment . subjectivity rel_sentences_sentiment = [ x . sentiment for x in b . sentences if ( first in x ) or ( last in x )] sentence_polarity = np . mean ([ y . polarity for y in rel_sentences_sentiment ]) sentence_subj = np . mean ([ y . subjectivity for y in rel_sentences_sentiment ]) metrics = [ overall_polarity , overall_subj , sentence_polarity , sentence_subj ] metric_names = [ 'overall_polarity' , 'overall_subj' , 'sentence_polarity' , 'sentence_subj' ] return pd . Series ( metrics , index = metric_names ) def add_sentiment_cols ( df ): sentiment_df = df . apply ( extract_overall_and_sentence_sentiment , axis = 1 ) return pd . concat ([ df , sentiment_df ], axis = 1 ) In [30]: reddit_df = add_sentiment_cols ( reddit_df ) Let's take a look at one of the comments to get a sense of what this looks like! In [31]: reddit_df . sample ( 1 )[[ 'body' , 'first_name' , 'last_name' , 'overall_subj' , 'sentence_subj' , 'overall_polarity' , 'sentence_polarity' ]] . to_dict () Out[31]: {'body': {157474: 'Sell Shirin, Max. Buy Will and Joaquin. Probably sell Mike and buy Sierra too.'}, 'first_name': {157474: 'Sierra'}, 'last_name': {157474: 'Thomas'}, 'overall_subj': {157474: 0.0}, 'sentence_subj': {157474: 0.0}, 'overall_polarity': {157474: 0.0}, 'sentence_polarity': {157474: 0.0}} It's important to remember that there seems to be a lot of discussion about which contestants could win or should win or whether they made the right decision or not. This is, of course, a separate question from sentiment -- there are plenty of people who make good strategic moves but are not the most well liked, by fans or by other contestants. Strong words in either direction (\"hate\", \"love\", etc.) Out of curiosity, let's take a look at the overall feelings of sentiment in these comments. In [32]: from plotly.express import bar , histogram , box In [33]: histogram ( data_frame = reddit_df , x = 'sentence_polarity' , nbins = 50 , title = 'Sentence Sentiment Polarity' ) In [34]: histogram ( data_frame = reddit_df , x = 'overall_polarity' , nbins = 50 , title = 'Overall Sentiment Polarity' ) The first thing that stands out is that there are a large number of 0 sentiments for both the sentence and overall polarity. In [35]: ( reddit_df [ 'sentence_polarity' ] == 0 ) . mean () Out[35]: 0.3642624016467816 In [36]: ( reddit_df [ 'overall_polarity' ] == 0 ) . mean () Out[36]: 0.20653668016413645 To only look at cases where there is some sentiment, let's look at a histogram of non-zero values: In [37]: histogram ( data_frame = reddit_df [ reddit_df [ 'sentence_polarity' ] != 0 ], x = 'sentence_polarity' , nbins = 50 , title = 'Non-Zero Sentence Sentiment Polarity' , ) In [38]: histogram ( data_frame = reddit_df [ reddit_df [ 'overall_polarity' ] != 0 ], x = 'overall_polarity' , nbins = 50 , title = 'Non-Zero Overall Sentiment Polarity' , ) We see that in both cases, it seems that most comments are positive (a slight skew to the left). This is interesting -- anyone who says redditors are inherently negative would probably disagree! To break this down a bit more, let's tae a look at the different seasons: In [39]: box ( data_frame = reddit_df [ reddit_df [ 'sentence_polarity' ] != 0 ], x = 'sentence_polarity' , color = 'season_name' , title = 'Non-Zero Sentence Sentiment Polarity by Season' , ) Box plots by each season don't immediately reveal too much -- it appears that the distributions are somewhat similar for all of the seasons. Next, we will look at some of the individual contestant episode combinations to see which have the highest (and lowest) skewing distributions. In [40]: reddit_df [ 'contestant_episode' ] = reddit_df [ 'first_name' ] + ' ' + reddit_df [ 'last_name' ] + ' ' + reddit_df [ 'episode_name' ] In [41]: def plot_grouped_sentiment_extremes ( df , extreme_group = 'contestant_episode' , plot_group = 'contestant_episode' , sentiment_col = 'sentence_polarity' , lower =. 001 , upper =. 999 , size_thresh = 10 , include_zeros = True , * args , ** kwargs ): if not include_zeros : df = df [ df [ sentiment_col ] != 0 ] . reset_index ( drop = True ) df = df [ df [ plot_group ] . notnull ()] . reset_index ( drop = True ) df = df [ df . groupby ( extreme_group )[ sentiment_col ] . transform ( len ) > size_thresh ] . reset_index ( drop = True ) lower_quart , upper_quart = df . groupby ( extreme_group )[ sentiment_col ] . mean () . quantile ([ lower , upper ]) def include_bool ( x ): return (( x . mean () <= lower_quart ) or ( x . mean () >= upper_quart )) best_worst_episodes_bool = df . groupby ( extreme_group )[ sentiment_col ] . transform ( include_bool ) df [ 'order_by' ] = df . groupby ( plot_group )[ sentiment_col ] . transform ( lambda x : x . median ()) plot_df = df [ best_worst_episodes_bool ] . sort_values ( 'order_by' ) . reset_index () bx = box ( data_frame = plot_df , x = sentiment_col , color = plot_group , category_orders = dict ( group = plot_df [ plot_group ] . unique () . tolist ()), * args , ** kwargs ) return bx In [165]: def create_story_contestant_episode ( contestant_episode , init_text = '' ): example = reddit_df [ reddit_df [ 'contestant_episode' ] == contestant_episode ] . iloc [ 0 ] try : sentences = TextBlob ( example [ 'story' ]) . sentences except TypeError : return display ( HTML ( f 'Results from <i>Story</i> aspect of the Wiki for { contestant_episode } ...' )) relevant = [ s for s in sentences if example [ 'first_name' ] in s or example [ 'last_name' ] in s ] full_story = init_text + '</br></br></br>' if init_text else '' for r in relevant : story = str ( r ) story = story + '</br>' emphasized = bolden ( story , example [ 'first_name' ]) emphasized = bolden ( emphasized , example [ 'last_name' ]) full_story += emphasized full_story = force_breaks ( full_story ) return full_story def display_with_breaks ( text , every = 120 , split_on_words = False ): display ( HTML ( force_breaks ( text , every = every , split_on_words = split_on_words ))) def bolden ( full_string , bolden ): boldened = '<b>' + bolden + '</b>' return full_string . replace ( bolden , boldened ) def highlight_comment ( row , sentiment_col = 'sentence_polarity' ): body = row [ 'body' ] sent = row [ sentiment_col ] for n in row [[ 'first_name' , 'last_name' ]]: body = bolden ( body , n ) ret_str = f '<b>Comment</b>: { body } </br></br/> <b>Sentiment</b>: { sent } </br></br>' return ret_str def get_example_comments ( contestant_episode , polarity_var = 'sentence_polarity' , n = 1 , polarity =- 1 ): subset = reddit_df [ reddit_df [ 'contestant_episode' ] == contestant_episode ] subset . sort_values ( by = polarity_var , ascending = ( polarity <= 0 ), inplace = True ) s_top = subset . iloc [ 0 : n ] s_top . apply ( lambda x : display_with_breaks ( highlight_comment ( x , polarity_var )), axis = 1 ) In [ ]: In [42]: plot_grouped_sentiment_extremes ( reddit_df , lower =. 005 , upper =. 995 , include_zeros = False ) Interestingly, we see some of the most and least popular players/episodes in this graph. It appears that the players who were liked (for instance, Malcolm in Kill or Be Killed and John Cody in Blood is Thicker than Anything ) seemed to have a smaller variance -- meaning they are more unanimously liked. While others who were disliked (like Colton Cumbie in One World Is Out the Window and Ozzy from Double Agent ) had a larger range of values for the 1st and 3rd quantile, indicating that the sentiment towards them are are more divided among Redditors. Interestingly, even some of the least \"liked\" contestant episode combinations (like Sophie Clarke in Cult Like ) are above zero, which reflects the average positive sentiment for most of these comments. Disliked Player -- Examples \"Worst\" things people said about Ozzy: In [166]: get_example_comments ( 'Ozzy Lusth Double Agent' , n = 4 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : Christine was a trooper. Heartbreaking to see her get that close and to have idiot Ozzy ruin her chance... Sentiment : -0.8 Comment : I'm liking this season, so far. Survivor: Nicaragua bored me to death, and Redemption Island was more like Survivor: Boston Rob Beats the Shit Out of Everyone. Here's My Brief Analysis: * Sophie: Right now, in a pretty good position. Sophie seems very smart, and is in a good position with Coach and Cochran. * Cochran: Made a smart move, but may bite him in the ass later on. Hopefully, he can take the heat from the butthurt Savaii. * Albert: Very close with Coach, and seems to be a good player. However, he poses an athletic threat. * Whitney: An all-around threat to Cochran and Upolu. Athletic, and could definitely win. * Brandon: Strong with Coach and hopefully, Cochran. Does not provide much of a threat, and if he doesn't have another episode like in the past, can make it far. * Ozzy : Now without an idol, Ozzy is in a very vulnerable position. However, if he stays strong, and can reel a few Upolu in, can regain long-term strength. * Edna: Coach's \"little friend\". She'll probably be the first one Savaii will go to for help, as a crack in the alliance. * Dawn: Is not very strong with Savaii, but does not pose a threat to Upolu. In an awkward position. Cochran may be able to convince to come over to Upolu. * Rick: Has said approximately three words this entire season. He definitely poses a threat to Savaii, and will most likely stay loyal to Coach. *Jim: Probably will be the most butthurt of the Savaii members. He's made great moves in the past, but now will have to find a crack in Upolu for votes. * Coach: In a very good position. Has an alliance of many loyal members, has Cochran on his side, and plus, has an idol. Sentiment : -0.65 Comment : Man, Ozzy kicked fucking ass that season. While I admit his social skills may not have been super, I swear to god he one all of those individual immunity challenges save on or two. He was also great at camp, climbing trees for fruit and fishing and shit. He was truly a \"survivor\", Yul was just better socially. Sentiment : -0.6 Comment : Yea. Jeff looked like he was thinking, \"How do we make Ozzy not sound like an idiot in the editing room?\" Sentiment : -0.5 \"Best\" things people said about Ozzy In [171]: get_example_comments ( 'Ozzy Lusth Double Agent' , n = 4 , polarity = 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : I've always loved Ozzy , He's like Boston Rob for me. Picked him in his first season and have wanted him to win ever since- This year I'm not so sure, he has grown up a lot and become more of a jackass, which bothers me. I've been back and forth with Cochran- although after this week I think i'm done with him. I think I'll go for Keith (praying that he comes back from redemption) or Dawn. I don't like anyone on Coach's tribe. Sentiment : 0.7 Comment : As much as I enjoyed last night's move, Coch should've stayed with his alliance, and be the last 6... This will probably get him to the top 3 with Ozzy since he's not a physical threat to win immunity. Sentiment : 0.43333333333333335 Comment : Yeah, but I like Ozzy and think Cochran is a sniveling weasel who is too full of himself. Sentiment : 0.35 Comment : I really like the one that is two votes after this where they convince Erik- who has the necklace to give the necklace to Natile, and then they all vote him out. *Edit- back when Ozzy was cute and innocent. Sentiment : 0.3333333333333333 \"Worst\" Things People Said About Colton In [172]: get_example_comments ( 'Colton Cumbie One World Is Out the Window' , n = 4 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : While I agree that Colton can get quite ... annoying... I was actually more annoyed with the \"roosters\" than anything. Damn, they are idiots, both in challenge and out. HOW DOES IT TAKE SEVEN MINUTES TO SOLVE A 8/9 UNIQUE PIECE PUZZLE? That's almost a minute a piece. You could literally \"hard break it\" in less time (spam every slot fast; ala word search tactics). Also, since he was allowed assistance, why didn't he just stand them up and let each member of the team see them so they could solve it while he went at it. Sentiment : -0.8 Comment : I know! I can't stand Colton , he's seriously a bad person. Sentiment : -0.6999999999999998 Comment : How can these guys let Colton run the game. I mean seriously? Sentiment : -0.4 Comment : It was hilarious. When Colton first said he hated the way Bill talked, I was confused... But then Bill spoke and I understood. YO, BRO, TRIBAL COUNCIL IS SO INTENSE! LIKE, CRAZY MAN! I'M JUST SO EXCITED! It was bizarre! Sentiment : -0.35000000000000003 \"Best\" things people said about Colton In [173]: get_example_comments ( 'Colton Cumbie One World Is Out the Window' , n = 4 , polarity = 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : It seems like most seasons people are too scared to make this move. Its understandable, knowing giving Colton all that power could be your ticket to leaving way earlier then you may have otherwise should he wise up and vote against you that night. Sentiment : 0.35 Comment : Same here. The way he acted at tribal, I realized that Colton was not entirely wrong about him. Sentiment : 0.25 Comment : In the end, it's the men's tribe's decision whether they help them out or not. Women didn't promise them anything in return, so they don't owe them anything. Asking for help isn't dumb or wrong. When it got clear the men demanded the use of the boat, the women apparently stayed on their own side. I've been more annoyed by the men's tribe because of Matt and Colton to be honest. One down, one to go... Sentiment : 0.2333333333333333 Comment : Agreed. Bill called him out on this at tribal council too. Colton acted like he'd be an outcast because he's gay, but he only did that to himself. Sentiment : 0.20833333333333334 Liked Player -- Examples \"Best\" things people said about Malcolm: In [174]: get_example_comments ( 'Malcolm Freberg Kill or Be Killed' , n = 4 , polarity = 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : The winners rarely ever need the idol to win. Your example only shows one winner. It didn't help Malcolm or Russell win. Sentiment : 0.8 Comment : You are saying Russell would have made it just as far as he did without idols? Or Malcolm would have done just as good without it? Ridiculous. Sentiment : 0.7 Comment : Spoiler Alert :D 1. If there is a tribe switch they will be even uneven numbers because 5 people have gone home so far. And with the fans down in numbers 6 to 9 they will most likely be a minority in both tribes. 2. The fans will have to find cracks in the favorites relationships if they are going to survive a tribe switch. Strength doesn't matter because a fan will be voted off no matter who wins. 3. Nobody said that, and it is still early. There were previews of Brandon going crazy and dumping food so who knows what will happen. Eventually they will merge though, and when they do you will see the real players come out (Sherri, Malcolm , Cochran, Matt) One of those four will win in my opinion depending on how situations play out Sentiment : 0.5 Comment : Phillipino Gollem. Malcolm , I love you. Sentiment : 0.5 \"Worst\" things people said about Malcolm: In [175]: get_example_comments ( 'Malcolm Freberg Kill or Be Killed' , n = 4 , polarity =- 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : I think Pete got edited into much more of a villain than he was. Outside of the game he seems like a cool guy (although maybe a little douchey, but so is Malcolm ). At Ponderosa, RC repeatedly called him out yet he just took it all calmly. Sentiment : -0.05937500000000001 Comment : I wouldn't be surprised if that happens a lot on Survivor. If Tandang had have held together I bet they never would have suggested that RC was being bullied. She may have even been edited as fairly crazy. Similarly, if Denise and Malcolm go out soon after the merge Russell Swan is probably edited as a hero who didn't deserve to go home. Sentiment : 0.0 Comment : I think it is a pretty fair assumption both of them are on the jury. Seems unlikely they'll go out soon but also unlikely from what we've seen so far they'll be finalists. Corinne's name keeps popping up to go next and Malcolm hiding from his alliance the fact that he has an idol doesn't bode well. Sentiment : 0.0 Comment : I think Phillip is playing a great game, whether it's intentional or not. He created his own alliance and everyone's like \"You can't have a 9 person alliance, blah, blah, blah\", but I think he gets that and everyone knows that no matter what Erik, Brandon, and Brenda are on the bottom, but if the numbers don't go his way, who's going to try to vote him out? No one, they'll go for a bigger \"threat\" like Andrea or Malcolm . I think Eriks assessment of him was pretty spot on. Sentiment : 0.0 \"Best\" things people said about John Cody: In [179]: get_example_comments ( 'John Cody Blood Is Thicker than Anything' , n = 4 , polarity = 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : I hope one of the brothers win, or John . Sentiment : 0.8 Comment : Top - 1) Aras 2) Tina 3) John C. Bottom - 1) Katie 2) Laura B 3) Gervase I also like Caleb and Vytas and want to throw them in there are honorable mentions Sentiment : 0.5 Comment : Top 3 Hayden John Vytas Bottem 3 Laura b ciera katie Sentiment : 0.5 Comment : **Top 3:** * Vytas * John * Candice **Bottom 3:** * Gervase * Colton * Hayden Sentiment : 0.5 Worst things people said about John Cody: In [180]: get_example_comments ( 'John Cody Blood Is Thicker than Anything' , n = 4 , polarity =- 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : \"Where they will compete, John , against your wife.\" But not Laura's husband? What's up with that? Weird to call only John out. Sentiment : -0.125 Comment : If he hadn't told Jeff to call him Cochran (a true scholar of the game, the dude knew that he was scratching one of Probst's itches), he would have been John , and that would have been that. Sentiment : -0.025000000000000022 Comment : Judd Sergeant (11) Chris Daugherty (9) Andrew Savage (7) Shane Powers (12) Todd Herzog (15) Matty Whitmore (17) Stephen Fishbach (18) Mike Snow (26) Brett Clouser (19) Rafe Judkins (11) RC Saint-Amour (25) Taj John son-George (18) Kim Spradlin (24) Natalie Bolton (16) Sundra Oakley (13) Christina Cha (24) Yve Rojas (21) Peih-Gee Law (15) Erinn Lobdell (18) Sophie Clarke (23) Sentiment : 0.0 Comment : Extra plot twist: Naonka, Alicia and John ny FairPlay are already on the island. Sentiment : 0.0 In [43]: plot_grouped_sentiment_extremes ( reddit_df , sentiment_col = 'overall_polarity' , lower =. 005 , upper =. 995 , include_zeros = False ) For the overall sentiment, it seems like we are getting some leakage from the other potential contestants. This is still interesting in some ways -- for instance, if you compare the distribution of Colton Cumbie in One World is Out The Window between the two graphs, you can see that Colton had a larger distribution with the sentence polarity as compared to the overall polarity. This seems to indicate that comments concerning Colton are more divided than comments mentioning Colton, which indicates his controversiality. Also, comments concerning Colton has a slightly lower median (-.125, -.0625). Additionally, and somewhat subjectively, we notice a few characters who were associated or allied with people who were liked (or disliked) more than they themselves were noticed or liked. For instance, Ken McNickle and Woo Hwang with David and Tony, respectively, from their seasons. You can see below that there are some relationships between the person who is speaking and general characters in the game. For instance, Caleb Bankston is mentioned often with Colton , because they are enggaged. However, Caleb appears to be generally well liked -- especially in his relation to other characters. In [181]: get_example_comments ( 'Caleb Bankston Blood Is Thicker than Anything' , polarity_var = 'overall_polarity' , n = 4 , polarity =- 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : Colton is a horrible person. It almost makes me want to root for Caleb less because we know if he wins, colton will still get some of that money.. Sentiment : -0.2888888888888889 Comment : Caleb is looking like Colton, but not a douche Sentiment : 0.0 Comment : Caleb is definitely going to turn on him as soon as he can. Sentiment : 0.0 Comment : I'm hoping Vytas and Caleb team up against him. Sentiment : 0.0 In [182]: get_example_comments ( 'Caleb Bankston Blood Is Thicker than Anything' , polarity_var = 'overall_polarity' , n = 4 , polarity = 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : Though fiancée Caleb seems like a nice guy Sentiment : 0.6 Comment : Top - 1) Aras 2) Tina 3) John C. Bottom - 1) Katie 2) Laura B 3) Gervase I also like Caleb and Vytas and want to throw them in there are honorable mentions Sentiment : 0.5 Comment : I am already in love with Caleb . Sentiment : 0.5 Comment : Without Caleb getting much airtime, this actually shows what kind of person he is the best. If this is how Colton acts under pressure, Caleb must have either have infinite patience or be a complete push-over. Sentiment : 0.38 In [184]: get_example_comments ( 'Kat Edorsson Thanks for the Souvenir' , polarity_var = 'overall_polarity' , n = 4 , polarity =- 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : I cannot imagine a worse final 3 than Kat , Alicia and Tarzan, haha. While I understand the strategy of it, I really hate that people bring awful players to the end with them. Sentiment : -0.4 Comment : Kat is not the sharpest pencil in the box, I don't think anyone can argue that. Either she is seriously DUMB, or very very naive. Sentiment : -0.3825 Comment : Kat number 2? Isn't she the complete idiot? I'm curious why you placed her there... Sentiment : -0.26666666666666666 Comment : Kat , as a person, is completely adorable. She is a horrible player. Sentiment : -0.25 In [185]: get_example_comments ( 'Kat Edorsson Thanks for the Souvenir' , polarity_var = 'overall_polarity' , n = 4 , polarity = 1 ) <ipython-input-165-1bea09ad0605>:44: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Comment : The Chelsea, Kim, Troyzan, and Jay alliance would be very good. As for bringing on the other 3 needed players, Leif is a risk since he might say something he shouldn't (although maybe he did that because he wanted to warn Bill about Colton's plan and try to save him, he still broke an alliance though). Jonas seems to want to cook up something of his own now (its about time he grows a pair, albeit after Colton leaves), although he is just strategizing and might not be very big threat in individual immunity. Sabrina would be good for the seven, but could be a threat for individual immunity, she had an alliance with Kim and Chelsea to begin with, so bringing her on will not be an issue. So I would have Chelsea, Kim, Jay, Troyzan, Jonas, Sabrina, and Kat . Kat because she probably will not be much of a threat in the end and will remain more loyal. In regards to Tarzan being in there somewhere, I think he is up to something more devious then we think and could be playing a very good and very quiet game, just bidding his time. Sentiment : 0.29488095238095235 Comment : Not the best player, but just the right person to take with you to the final 3. Kat on one side, Alicia on the other. Who wins? Sentiment : 0.29214285714285715 Comment : I'd like to see Jay and Sabrina at the end, they can bring Kat because she's far to dumb to win. Sentiment : 0.17500000000000002 Comment : Unfortunately, Survivor is more about big characters and personalities nowadays than people surviving in the wilderness. I pretty much watch the show to laugh and be entertained. Kat is very entertaining and also seems to be a genuinely nice girl. My ranking was purely \"How much I like them.\" If I was ranking on chances at winning or strategic prowess. Kat would likely finish at the bottom. She IS a complete idiot. Sentiment : 0.15816326530612246 In [44]: plot_grouped_sentiment_extremes ( reddit_df , sentiment_col = 'sentence_subj' , lower =. 005 , upper =. 995 , include_zeros = False ) Looking at the subjectivity of the sentences, we see some interesting results. In particular, there are some examples of popular, well-known male characters (Ozzy, Jonas, and John) recieving less subjective evaluations than some female players. We investigate this in the next graphs, and it appears there os a slight increase in subjectivity at the extremes (the players with the highest/lowest subjectivity scores) but not in general for female vs. male contestants. It is also interesting that Ozzy Lusth , Jonathan Penner and Russell Swan have the lower subjectivity scores. To me, this seems to indicate that they are being judged based on their merits in the game. All three of these players have some level of strategic gameplay (or in Ozzys case, great at challenges) that makes them seem like objectively good Survivor players. At the same time, they are controversial players and may not be the most socially well-liked. This isn't a thorough investigation, and we've seemed to gloss over subjectivity so far (and will not dive deeper), but it may be an interesting area for further analysis. Conversely, at the bottom of the list, you see players like Malcolm Freberg and Denise Stapley . I'm focusing on these two because they're the most immediately familiar to me, but they are characters that tend to be well liked, and were underdogs (their tribes lost a lot at the start of the game). Perhaps this has led them to being ranked higher here. (Also of note, of course, are the episodes that these occur in. For Malcolm, it was actually an episode on Caramoan , which was the returning season. Malcolm was well liked previously, and perhaps was one of the few returning \"favorites\" who was actually liked. Perhaps this made people less objective about him?) Of course, these are all hypotheses and would require some further research to investigate, but these are interesting results nevertheless. Breakdowns by Gender After the subjectivity analysis, as well as some data I have read concerning attitudes towards different genders of survivor players, I wanted to take a look at the subjectivity and polarity based on the contestants sex. The following graphs only consider the most (and least) subjective players for each sex. The results are inconclusive -- it does not appear, based on this information alone, that the distributions are substantially different from one another. This statement is supported by the simple linear regression model later, which investigates what is predictive of the sentence sentiment, where Sex is removed as an insignificant feature. In [45]: plot_grouped_sentiment_extremes ( reddit_df , plot_group = 'sex' , sentiment_col = 'sentence_subj' , lower =. 05 , upper =. 95 , include_zeros = False ) In [46]: plot_grouped_sentiment_extremes ( reddit_df , plot_group = 'sex' , sentiment_col = 'sentence_polarity' , lower =. 05 , upper =. 95 , include_zeros = False ) Now we've dug into the sentiment analysis a bit, the next step will be to build a linear model for sentiment and see which variables are significant. In the [next post], we will fit a linear model to the sentence_polarity variable and take a look at some of the coefficients to determine a statistically significant effect. This will be an inferential model, in the sense that we are not aiming for predictive accuracy but to explain changes in the dependent variable.","tags":"Survivor","url":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-3.html","loc":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-3.html"},{"title":"Survivor: Outwit, Outplay, Out...analyze? (Part 2)","text":"Analyzing the Game of Survivor -- Looking at Fan Favorites via Reddit (2) After reading through [my last post] on the ETL process, you may be interested in some of the data that I had collected. Fear not, my dear data science enthusiast, for I have come baring gifts of analysis! In this second installment of the series, I will begin the analysis by digging into some of the Reddit data that I had collected via the Pushift.io API . For more information on how this data was collected, please check out the [first article in this series], where I describe the ETL process for the Pushift (as well as other!) data. Looking at Contestants Mentioned in Comments The first query I will be using uses a few different tables. First, I use a CTE (Common Table Expression) which combines information from the contestant and episode performance stats tables. I also contain, in a separate dataframe, the episode table, which may come in handy later. Then, we look at instances where the first or last name is contained inside of the body of the comment for comments made within a particular season. While there are cases where this will not work (for instance, when a contestant is best known by their nickname or when a shortened, or misspelled, version is used), it should give us a sense of the comments pertaining to particular players. It's worth noting that the comments only go back to 2011 -- and the number of comments have greatly increased over time. We try a few ways of normalizing based on this information, but some players (particularly older players) will not be considered in this analysis.D In [1]: import os from sqlalchemy import create_engine import numpy as np import pandas as pd import plotly.graph_objects as go from copy import deepcopy from plotly.express import line , bar In [2]: pg_un , pg_pw , pg_ip , pg_port = [ os . getenv ( x ) for x in [ 'PG_UN' , 'PG_PW' , 'PG_IP' , 'PG_PORT' ]] In [3]: def pg_uri ( un , pw , ip , port ): return f 'postgresql:// { un } : { pw } @ { ip } : { port } ' In [4]: eng = create_engine ( pg_uri ( pg_un , pg_pw , pg_ip , pg_port )) In [5]: sql = ''' WITH contestants_to_seasons AS ( SELECT c.contestant_id, c.first_name, c.last_name, cs.contestant_season_id, cs.season_id, occupation, location, age, placement, days_lasted, votes_against, med_evac, quit, individual_wins, attempt_number, tribe_0, tribe_1, tribe_2, tribe_3, alliance_0, alliance_1, alliance_2, challenge_wins, challenge_appearances, sitout, voted_for_bootee, votes_against_player, total_number_of_votes_in_episode, tribal_council_appearances, votes_at_council, number_of_jury_votes, total_number_of_jury_votes, number_of_days_spent_in_episode, days_in_exile, individual_reward_challenge_appearances, individual_reward_challenge_wins, individual_immunity_challenge_appearances, individual_immunity_challenge_wins, tribal_reward_challenge_appearances, tribal_reward_challenge_wins, tribal_immunity_challenge_appearances, tribal_immunity_challenge_wins, tribal_reward_challenge_second_of_three_place, tribal_immunity_challenge_second_of_three_place, fire_immunity_challenge, tribal_immunity_challenge_third_place, episode_id FROM survivor.contestant c RIGHT JOIN survivor.contestant_season cs ON c.contestant_id = cs.contestant_id JOIN survivor.episode_performance_stats eps ON eps.contestant_id = cs.contestant_season_id ), matched_exact AS ( SELECT reddit.*, c.* FROM survivor.reddit_comments reddit JOIN contestants_to_seasons c ON (POSITION(c.first_name IN reddit.body) > 0 OR POSITION(c.last_name IN reddit.body) > 0) AND c.season_id = reddit.within_season AND c.episode_id = reddit.most_recent_episode WHERE within_season IS NOT NULL ) SELECT * FROM matched_exact m ''' In [6]: reddit_df = pd . read_sql ( sql , eng ) In [7]: ep_df = pd . read_sql ( 'SELECT * FROM survivor.episode' , eng ) In [8]: season_to_name = pd . read_sql ( 'SELECT season_id, name AS season_name FROM survivor.season' , eng ) In [9]: reddit_df = reddit_df . merge ( season_to_name , on = 'season_id' ) In [10]: reddit_df . rename ( columns = { 'name' : 'season_name' }, inplace = True ) In [11]: reddit_df = reddit_df . merge ( ep_df . drop ( columns = [ 'season_id' ]), on = 'episode_id' ) In [12]: reddit_df [ 'created_dt' ] = pd . to_datetime ( reddit_df [ 'created_dt' ]) In [13]: pd . options . display . max_columns = 100 Taking a Look At The Data In [14]: reddit_df . head () Out[14]: index_x author author_created_utc author_flair_css_class author_flair_text author_fullname body controversiality created_utc distinguished gilded id link_id nest_level parent_id reply_delay retrieved_on score score_hidden subreddit subreddit_id edited user_removed mod_removed stickied author_cakeday can_gild collapsed collapsed_reason is_submitter gildings permalink permalink_url updated_utc subreddit_type no_follow send_replies author_flair_template_id author_flair_background_color author_flair_richtext author_flair_text_color author_flair_type rte_mode subreddit_name_prefixed all_awardings associated_award author_patreon_flair author_premium awarders collapsed_because_crowd_control ... attempt_number tribe_0 tribe_1 tribe_2 tribe_3 alliance_0 alliance_1 alliance_2 challenge_wins challenge_appearances sitout voted_for_bootee votes_against_player total_number_of_votes_in_episode tribal_council_appearances votes_at_council number_of_jury_votes total_number_of_jury_votes number_of_days_spent_in_episode days_in_exile individual_reward_challenge_appearances individual_reward_challenge_wins individual_immunity_challenge_appearances individual_immunity_challenge_wins tribal_reward_challenge_appearances tribal_reward_challenge_wins tribal_immunity_challenge_appearances tribal_immunity_challenge_wins tribal_reward_challenge_second_of_three_place tribal_immunity_challenge_second_of_three_place fire_immunity_challenge tribal_immunity_challenge_third_place episode_id season_name index_y summary story challenges trivia image firstbroadcast viewership wiki_link season_episode_number overall_episode_number overall_slot_rating survivor_rating episode_name created_y updated_y 0 4540248 sampete1157 NaN None None t2_yvimwo1 Yul NaN 1585333566 None NaN flo8j0x t3_fpql0y None t1_flmiy69 NaN 1.585335e+09 1.0 None survivor t5_2qhu3 NaN None None false None None None None false {} /r/survivor/comments/fpql0y/yul/flo8j0x/ None NaN None true true None None [] None text None None [] None false false [] None ... 2.0 NaN NaN NaN NaN NaN NaN NaN 0.125 0.5 0.0 0.0 3.0 4.0 1.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 NaN NaN -2.0 NaN 695.0 Winners at War 695 We're in the Majors is the seventh episode of ... Getting voted out before the merge-- that's so... Challenge: (No Title)Two members of each tribe... * \"Boa Constrictor at Yara\" (Day 17): At Yara... https://vignette.wikia.nocookie.net/survivor/i... 2020-03-25 818000000.0 https://survivor.fandom.com/wiki/We%27re_in_th... 7.0 590.0 8.0 1.7 We%27re in the Majors 2020-07-11 01:03:00.566347+00:00 2020-07-19 00:48:27.943994+00:00 1 4538586 lvl4lapras NaN None None t2_54mpbhfb Yul NaN 1585316533 None NaN flne39u t3_fpql0y None t3_fpql0y NaN 1.585317e+09 1.0 None survivor t5_2qhu3 NaN None None false None None None None false {} /r/survivor/comments/fpql0y/yul/flne39u/ None NaN None true true None None [] None text None None [] None false false [] None ... 2.0 NaN NaN NaN NaN NaN NaN NaN 0.125 0.5 0.0 0.0 3.0 4.0 1.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 NaN NaN -2.0 NaN 695.0 Winners at War 695 We're in the Majors is the seventh episode of ... Getting voted out before the merge-- that's so... Challenge: (No Title)Two members of each tribe... * \"Boa Constrictor at Yara\" (Day 17): At Yara... https://vignette.wikia.nocookie.net/survivor/i... 2020-03-25 818000000.0 https://survivor.fandom.com/wiki/We%27re_in_th... 7.0 590.0 8.0 1.7 We%27re in the Majors 2020-07-11 01:03:00.566347+00:00 2020-07-19 00:48:27.943994+00:00 2 4539257 swells61 NaN 34Gold WS33W J.T. t2_fex1f Yul NaN 1585324546 None NaN flnrm96 t3_fpql0y None t1_flmiy69 NaN 1.585325e+09 1.0 None survivor t5_2qhu3 NaN None None false None None None None false {} /r/survivor/comments/fpql0y/yul/flnrm96/ None NaN None true true None None [{'e': 'text', 't': 'J.T.'}] dark richtext None None [] None false false [] None ... 2.0 NaN NaN NaN NaN NaN NaN NaN 0.125 0.5 0.0 0.0 3.0 4.0 1.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 NaN NaN -2.0 NaN 695.0 Winners at War 695 We're in the Majors is the seventh episode of ... Getting voted out before the merge-- that's so... Challenge: (No Title)Two members of each tribe... * \"Boa Constrictor at Yara\" (Day 17): At Yara... https://vignette.wikia.nocookie.net/survivor/i... 2020-03-25 818000000.0 https://survivor.fandom.com/wiki/We%27re_in_th... 7.0 590.0 8.0 1.7 We%27re in the Majors 2020-07-11 01:03:00.566347+00:00 2020-07-19 00:48:27.943994+00:00 3 4539655 ekwag NaN 40Gold WW Nick t2_fssfb Yul NaN 1585328171 None NaN flnyaj9 t3_fpql0y None t3_fpql0y NaN 1.585329e+09 1.0 None survivor t5_2qhu3 NaN None None false None None None None false {} /r/survivor/comments/fpql0y/yul/flnyaj9/ None NaN None true true None None [{'e': 'text', 't': 'Nick'}] dark richtext None None [] None false false [] None ... 2.0 NaN NaN NaN NaN NaN NaN NaN 0.125 0.5 0.0 0.0 3.0 4.0 1.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 NaN NaN -2.0 NaN 695.0 Winners at War 695 We're in the Majors is the seventh episode of ... Getting voted out before the merge-- that's so... Challenge: (No Title)Two members of each tribe... * \"Boa Constrictor at Yara\" (Day 17): At Yara... https://vignette.wikia.nocookie.net/survivor/i... 2020-03-25 818000000.0 https://survivor.fandom.com/wiki/We%27re_in_th... 7.0 590.0 8.0 1.7 We%27re in the Majors 2020-07-11 01:03:00.566347+00:00 2020-07-19 00:48:27.943994+00:00 4 4539815 Lunarmise NaN None None t2_1r7mqcjo Yul NaN 1585329676 None NaN flo13ml t3_fpql0y None t1_flo0u7q NaN 1.585330e+09 1.0 None survivor t5_2qhu3 NaN None None false None None None None false {} /r/survivor/comments/fpql0y/yul/flo13ml/ None NaN None true true None None [] None text None None [] None false false [] None ... 2.0 NaN NaN NaN NaN NaN NaN NaN 0.125 0.5 0.0 0.0 3.0 4.0 1.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 NaN NaN -2.0 NaN 695.0 Winners at War 695 We're in the Majors is the seventh episode of ... Getting voted out before the merge-- that's so... Challenge: (No Title)Two members of each tribe... * \"Boa Constrictor at Yara\" (Day 17): At Yara... https://vignette.wikia.nocookie.net/survivor/i... 2020-03-25 818000000.0 https://survivor.fandom.com/wiki/We%27re_in_th... 7.0 590.0 8.0 1.7 We%27re in the Majors 2020-07-11 01:03:00.566347+00:00 2020-07-19 00:48:27.943994+00:00 5 rows × 126 columns In [15]: reddit_df . shape Out[15]: (1149008, 126) There is a wealth of data here -- the actual content of the message, other Reddit information (like the user, upvotes, flairs, etc.) For this part of the analysis, we will just be looking at the occurances of the names in the body of the comment. In the next installment, we will look a bit deeper at some of the text inside the body and how that relates to the contestants. Additionally, we will take a look at some of the users and other information in later installments. One thing to note is what the above query did the heavy lifting for -- finding the first and last names in the bodies of texts. So this dataframe (at a short 1.1 M rows) represents only the comments that had either one of these. Comments can appear multiple times if they contain multiple names. Comments Per Season The first -- and most obvious -- question we can answer is -- how many comments are there each season? And, which seasons are represented by the subreddit? In [16]: from plotly.express import bar , line In [17]: def plot_season_comments ( df ): comments_per_season = df . groupby ( 'season_name' ) . size () . reset_index () comments_per_season . rename ( columns = { 'season_name' : 'Season' , 0 : 'Number of Comments (with names)' }, inplace = True ) return bar ( data_frame = comments_per_season . sort_values ( by = 'Number of Comments (with names)' ), x = 'Season' , y = 'Number of Comments (with names)' ) In [18]: plot_season_comments ( reddit_df ) We can see that the seasons have been those since 2011. We also see that certain seasons, in particular those that were more recent, have much more comments than other seasons. This makes sense intuitively, as there has been a good deal of increased use in Reddit over the years. Winners at War , the most recent season, has the most reddit comments, and also gained a lot of TV viewership as well, as it was an \"all-star\" type game. To see this increase a bit more clearly, we can look at this over time based on the broadcast date of the episodes: In [19]: def plot_season_comments_time ( df ): comments_per_season = df . sort_values ( by = 'firstbroadcast' ) . groupby ([ df [ 'firstbroadcast' ] . sort_values () . dt . year , 'season_name' ]) . size () . reset_index () comments_per_season [ 'Season, year' ] = comments_per_season [ 'season_name' ] + ', ' + comments_per_season [ 'firstbroadcast' ] . astype ( str ) comments_per_season . rename ( columns = { 0 : 'Number of Comments (with names)' }, inplace = True ) return line ( data_frame = comments_per_season , x = 'Season, year' , y = 'Number of Comments (with names)' , ) In [20]: plot_season_comments_time ( reddit_df ) Not too different from the above sorted chart, with a few exceptions of dips and peaks during certain years. Interestingly, two confounding factors exist here -- the increased popularity of Reddit over time, and the popularity of some seasons over others. Something we must keep in mind throughout this analysis! Comments Per Contestant Now, to jump into the meat of the reason for this query -- to take a look at the number of comments about particular contestants. First we look at the contestants that had the highest absolute count of comments on Reddit. Since different seasons may have more (or less) comments based on factors not related to the popularity of the season itself, this will not necessarily give us an unbiased answer. However, it still will be interesting to consider this in both absolute and relative terms. For the absolute chart, we look at the number of mentions of each contestant and plot a bar chart with the top 20 contestants. For the relative, we consider how many comments they got relative to the total number of mentions that season. In [21]: def plot_sorted_n_alltime ( df , total = True , n = 20 , top = True ): grper = [ 'contestant_season_id' , 'first_name' , 'last_name' ] measured = 'Number of Mentions' if total else 'Percent of Season Mentions' abs_rel = 'Absolute' if total else 'Relative' top_bot = 'Top' if top else 'Bottom' title = f ' { top_bot } { n } by { abs_rel } Number of Mentions' totals = df . groupby ( 'season_name' ) . apply ( lambda x : x . groupby ( grper ) . size () / ( x . shape [ 0 ] if not total else 1 )) . sort_values () totals_clipped = totals . tail ( n ) if top else totals . head ( n ) totals_clipped = totals_clipped . reset_index () if not total : totals_clipped [ 0 ] *= 100 totals_clipped [ 'full_name' ] = totals_clipped [ 'first_name' ] + \\ ' ' + totals_clipped [ 'last_name' ] + \\ ', ' + totals_clipped [ 'season_name' ] . astype ( str ) totals_clipped . rename ( columns = { 0 : measured , 'full_name' : 'Contestant' }, inplace = True ) return bar ( data_frame = totals_clipped , x = measured , y = 'Contestant' , title = title ) In [22]: plot_sorted_n_alltime ( reddit_df ) In [23]: plot_sorted_n_alltime ( reddit_df , False ) We see some interesting results. First, some of the big names are at the top of this list -- Tony is a very popular player, as well as a two time winner of the game. We see that the first chart mainly has members from Winners at War , which makes sense since this season has much more comments than the others. Even, Adam Klein makes this list, although he may have ben one of the least popular of the contestants on that season. The relative chart shows a bit more of a holistic view -- the top 8 contestants are immediately recognizable to me as interesting contestants from past seasons. Rob Mariano or \"Boston Rob\" is one of the most popular players the show has ever had. As is the nerd figure, John Cochran . We could look at this a few different ways of course -- if we looked at the bottom of this list, I'm sure we'd see a lot of people who we've never heard of who were voted out in the first episode. Out of curiosity let's check it out! In [24]: plot_sorted_n_alltime ( reddit_df , False , top = False ) Hm, the results here are somewhat interesting! One one end, there are some names who I have certainly never heard of. On the other, there are some that are quite popular -- J.T. for instance. In this case, I think it's a data error (a data error, unfortunately for JT, that I don't think is worth diving into in this analysis) that J.T. is probably not said much in the comments (maybe JT). Others have contestants thatwere popular overall, but probably unpopular or voted out quicky during their season (like Rupert). Then, you have people who are notoriously unlucky, like Francesca Hogi , who lost both of the first episodes she was on in Survivor. Hate to say it, but she's exactly who you'd hope would be on the bottom of this list! Season Breakdown While there were definitely some interesting takeaways looking at his on the aggregate, the next step is to drill down into individual seasons -- when did people get the most comments? Were some people popular (and then voted out?) The next plots look into this. In [25]: from plotly.express import bar In [26]: def create_count_from_episode_df ( ep_df , unique_idxs ): grper = [ 'season_id' , 'contestant_season_id' , 'first_name' , 'last_name' ] reindexed = ep_df . groupby ( grper ) . size () . reindex ( unique_idxs ) reindexed . name = 'count' reindexed . fillna ({ 'count' : 0 }, inplace = True ) reindexed . drop ( columns = [ 'episode_name' ], inplace = True ) reindexed = reindexed . reset_index () return reindexed def create_episode_counts ( df ): grper = [ 'season_id' , 'contestant_season_id' , 'first_name' , 'last_name' ] idx = df [ grper ] . drop_duplicates () ep_counts = df . groupby ([ 'episode_id' , 'episode_name' ]) . apply ( lambda x : create_count_from_episode_df ( x , idx )) ep_counts = ep_counts . reset_index () ep_counts [ 'cumulative_player_counts' ] = ep_counts . groupby ( 'contestant_season_id' )[ 'count' ] . transform ( lambda x : x . cumsum ()) ep_counts . sort_values ( by = [ 'episode_id' , 'cumulative_player_counts' ], inplace = True ) ep_counts [ 'total_counts' ] = ep_counts . groupby ( 'contestant_season_id' )[ 'count' ] . transform ( 'sum' ) return ep_counts def create_racetrack_by_episode ( df , * args , ** kwargs ): ep_counts = create_episode_counts ( df ) fig = bar ( y = 'first_name' , x = 'cumulative_player_counts' , data_frame = ep_counts , animation_frame = 'episode_name' , range_x = [ 0 , ep_counts [ 'cumulative_player_counts' ] . max () * 1.05 ], * args , ** kwargs ) fig . layout . updatemenus [ 0 ] . buttons [ 0 ] . args [ 1 ][ \"frame\" ][ \"duration\" ] = 2000 return fig def create_racetrack_by_episode_for_season ( df , season_id , * args , ** kwargs ): subset_df = df [ df [ 'season_id' ] == season_id ] season_name = subset_df [ 'season_name' ] . iloc [ 0 ] set_kwargs = dict ( title = f 'Cumulative Comment Counts for Season: <b> { season_name } </b>' , labels = { 'first_name' : 'First Name' , 'cumulative_player_counts' : 'Number of Comments' , 'episode_name' : 'Episode Name' }) set_kwargs . update ( kwargs ) return create_racetrack_by_episode ( subset_df , * args , ** set_kwargs ) These plots below are animated plots for each of the seasons considered in the reddit data. These are what I'm calling racecar plots, or really animated barplots where the categories (in this case the contestants) race against one another to get to the right side of the chart. In this case, to make things interesting, I only consider when the contestant is still getting comments in reddit -- once you are voted out of the game, your total goes down to zero. An interesting thing you may notice is that this correlates very strongly with the person who was voted out last. This makes sense -- for most players, once they are voted out they are rarely, if ever, mentioned again on Reddit for the episodes after that (or rather, the time after the subsequent epsiodes). People have short memories. Plotly Express makes this plot very easy to make, at the cost of some customization. The above mentioned aspect of the plot was actually completely unavoidable for this plot, without some big workarounds. Still, the animation capabilities of plotly express are worth mentioning. As you can see below, you can press on the play button to start the animation, and it will begin to play episode by episode until the season is \"over\" (the last episode of the season). After each episode, the remaining contestants are sorted. Keep an eye on the players, they can be a bit hard to track! In [1]: for season in reddit_df [ 'season_id' ] . unique (): fig = create_racetrack_by_episode_for_season ( reddit_df , season , height = 1000 , width = 1000 ) --------------------------------------- NameError Traceback (most recent call last) <ipython-input-1-9f991c8c17d1> in <module> ----> 1 for season in reddit_df [ 'season_id' ] . unique ( ) : 2 fig = create_racetrack_by_episode_for_season ( reddit_df , season , height = 1000 , width = 1000 ) NameError : name 'reddit_df' is not defined Episode by Episode Breakdown While the last plots did show us how the cumumlative comment counts grew over time for each contestant, we'd like to be able to visualize the difference between episodes a bit more clearly. Additionally, it would be nice to have the bars stick around even after that player no longer has additional comments. For this, we will use our own custom plotly animation function below to generate similar plots for each of these seasons. In [28]: def plot_episode_by_episode_breakdown_by_season ( df , season_id , * args , ** kwargs ): reduced = df [ df [ 'season_id' ] == season_id ] season_name = reduced [ 'season_name' ] . iloc [ 0 ] set_kwargs = dict ( title = f 'Cumulative Comment Counts by Episode for Season <b> { season_name } </b>' , xaxis = dict ( title = 'Number of Comments' , autorange = False ), yaxis = dict ( title = 'First Name' )) set_kwargs . update ( kwargs ) return plot_episode_by_episode_breakdown ( reduced , * args , ** set_kwargs ) In [29]: def plot_episode_by_episode_breakdown ( df , * args , ** kwargs ): ep_counts = create_episode_counts ( df ) ep_counts . sort_values ( by = 'total_counts' , inplace = True ) episodes = ep_counts [ 'episode_id' ] . unique () episodes . sort () empty = dict ( type = 'bar' , orientation = 'h' , y = ep_counts [ 'first_name' ] . unique (), x = [ None ] * ep_counts [ 'first_name' ] . nunique ()) traces = [ empty . copy () for i in range ( len ( episodes ))] frames = [] sliders_dict = { \"active\" : 0 , \"yanchor\" : \"top\" , \"xanchor\" : \"left\" , \"currentvalue\" : { \"font\" : { \"size\" : 20 }, \"prefix\" : \"Episode: \" , \"visible\" : True , \"xanchor\" : \"right\" }, \"transition\" : { \"duration\" : 300 , \"easing\" : \"cubic-in-out\" }, \"pad\" : { \"b\" : 10 , \"t\" : 50 }, \"len\" : 0.9 , \"x\" : 0.1 , \"y\" : 0 , \"steps\" : [] } for i , ep in enumerate ( episodes ): fr_dict = dict ( type = 'bar' , orientation = 'h' ) new_bool = ep_counts [ 'episode_id' ] == ep ep_name = ep_counts . loc [ new_bool , 'episode_name' ] . iloc [ 0 ] traces [ i ][ 'name' ] = ep_name fr_dict . update ( dict ( y = ep_counts . loc [ new_bool , 'first_name' ] . reset_index ( drop = True ), x = ep_counts . loc [ new_bool , 'count' ] . reset_index ( drop = True ))) if i > 0 : last_frame = deepcopy ( frames [ - 1 ]) last_frame [ 'data' ] . append ( fr_dict ) last_frame [ 'traces' ] += [ i ] else : last_frame = dict ( data = [ fr_dict ], traces = [ 0 ]) frames . append ( last_frame ) slider_step = { \"args\" : [ [ ep_name ], { \"frame\" : { \"duration\" : 300 , \"redraw\" : False }, \"mode\" : \"immediate\" , \"transition\" : { \"duration\" : 300 }} ], \"label\" : ep_name , \"method\" : \"animate\" } sliders_dict [ \"steps\" ] . append ( slider_step ) layout = go . Layout ( width = 1000 , height = 1000 , showlegend = True , hovermode = 'closest' ) layout [ \"sliders\" ] = [ sliders_dict ] layout [ \"updatemenus\" ] = [ { \"buttons\" : [ { \"args\" : [ None , { \"frame\" : { \"duration\" : 500 , \"redraw\" : False }, \"fromcurrent\" : True , \"transition\" : { \"duration\" : 300 , \"easing\" : \"quadratic-in-out\" }}], \"label\" : \"Play\" , \"method\" : \"animate\" }, { \"args\" : [[ None ], { \"frame\" : { \"duration\" : 0 , \"redraw\" : False }, \"mode\" : \"immediate\" , \"transition\" : { \"duration\" : 0 }}], \"label\" : \"Pause\" , \"method\" : \"animate\" } ], \"direction\" : \"left\" , \"pad\" : { \"r\" : 10 , \"t\" : 87 }, \"showactive\" : False , \"type\" : \"buttons\" , \"x\" : 0.1 , \"xanchor\" : \"right\" , \"y\" : 0 , \"yanchor\" : \"top\" } ] try : kwargs [ 'xaxis' ] . update ( range = [ 0 , ep_counts [ 'total_counts' ] . max () * 1.05 ]) except KeyError : kwargs [ 'xaxis' ] = dict ( range = [ 0 , ep_counts [ 'total_counts' ] . max () * 1.05 ]) layout . update ( barmode = 'stack' , * args , ** kwargs ) fig = go . Figure ( data = traces , frames = frames , layout = layout ) return fig In [30]: for season in reddit_df [ 'season_id' ] . unique (): plot_episode_by_episode_breakdown_by_season ( reddit_df , season ) . show () Conclusions This first jump into the analysis definitely gave us some interesting results! Some high-level takeaways: r/survivor has been around since 2011. Over the years, engagement (in terms of comments) has increased by quite a lot. Using absolute comment counts doesn't seem to be the way to go when comparing across seasons -- instead using relative comment counts to the total seemed to work best. The players who are the most popular according to Reddit comments pass the common sense check -- Tony and Rob Mariano top the list. Comments (or lack thereof) can be used as a proxy for in game events. We've only just started to see this (without looking at the body of the text), but we can already see that, in most cases a users' reddit comments will drop off to zero or near-zero after they are eliminated from the game. This could be interesting to dive a bit deeper into (especially if we want to eventually build a model!) Generally, as we can see above, players who last longer tend to dominate the number of comments. For instance, in only a few rare instances do contestants who get voted out earlier than those in the final 4 or 5 end up surpassing these contestants in reddit comments. This passes the sniff test -- we will remember contestants, even those we wouldn't really like normally, who last longer simply because they will be more \"in the running\" with less contestants. This has been an interesting look at the rich dataset of Reddit comments on r/survivor. But we aren't done yet! The next iteration of the Survivor analysis Series will use Sentiment analysis and other attributes of the body of the text to try to glean information about the contestants and reddit users' behavior. Stay tuned, you won't want to miss it!","tags":"Survivor","url":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-2.html","loc":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-2.html"},{"title":"Survivor: Outwit, Outplay, Out...analyze? (Part 1)","text":"Analyzing the Game of Survivor -- Collecting The Data (1) The Data of Survivor Recently, I have been interested in the CBS reality/game show Survivor. Part of this has been because of the extra time I have had due to the recent circumstances surrounding COVID 19, and some of this is because it is a show that greatly resembles other games that I enjoy playing (social deduction games, etc.) If you have never watched it, I would highly suggest it! I'm not a fan of reality TV, but this show is a great combination of all of the elements of reality TV and a game show that make it enthralling. After watching a few seasons, being a data scientist, I began to be interested in some of the results of the show. It seems that certain personalities are more or less likely to win, or to be voted out at any time in the game. Additionally, there seems to be a bit of a juggling act between the importance of challenges, strategy and social games. This is the beginning of a series of posts relating to Survivor, data that can be gathered from it, analysis we can make with some of the sources, and perhaps ending in a few machine learning or statistical learning applications (to try to estimate the season winner, or who will be voted out on which days, etc!) A quick primer on Survivor To give a brief overview of some of the terms and information I will be using throughout this post, I have written a brief summary here. From wikipedia : [ Survivor ] features a group of contestants deliberately marooned in an isolated location, where they must provide > food, water, fire, and shelter for themselves. The contestants compete in challenges for rewards and immunity from elimination. The contestants are progressively eliminated from the game as they are voted out by their fellow-contestants until only one remains to be awarded the grand prize and named the \"Sole Survivor\". In Survivor , there are a few things of note. First: Survivors must survive in the elements with one another as a tribe. Much of the game focuses on them being able to get basic necessities. However, it is hardly a game about survival in the wilderness, despite the name. The majority of the game is a social and challenge-based game. Every episode, there is one elimination from the game. Contestants vote players off by majority votes. In the beginning of the season, there are two or more tribes . These groups are separated from one another, and perform in challenges collectively. Reward challenges provide a chance for a contestant (called castaways by the show) to win luxury prizes, like food, survival gear, or other items of interest. As the game progresses, the rewards usually get more and more enticing. Immunity challenges provide a strategic advantage in the game. When there are two (or more) tribes, winning an immunity challenge allows the whole tribe to avoid elimination -- someone from the other tribe must leave. When there is one tribe, immunity challenges are individual, which prevents that person from being voted out. The game changes substantially when the two (or more) tribes are merged into one, usually around halfway through the game. This is known as the merge and from this point on, it is an individual game. Players tend to work in groups (or alliances ) to achieve goals in the game. They can act as a voting block. The game of Survivor is edited after the whole season is filmed -- meaning the people editing often know who the winner will be. This is important to keep in mind. Additionally, each episode there are some number of confessionals -- which are segments where contestants speak to the camera away from the other contestants. They may complain, for instance, about a particular contestant, or discuss other parts of the game. While there is more to the show than just this (and it has evolved quite a bit over time), for our purposes it is enough to understand these basic ideas of the game before jumping in. If you're curious, Season 7 (Pearl Islands) is one of my favorite seasons and is a great place to start! Detail of: The ETL Process The first step in this process, of course, is getting the actual data to use. To this end, I have actually gone quite far in collecting and creating a workflow to update data as the seasons start. This behind-the-scenes work is a bit more data engineering than data science, but is probably the most important part of any of the analysis I will be doing! ETL stands for E xtract T ransform L oad which are the three steps to building a data pipeline. This general structure can generally describe any process using data. Most of the time, we spend a lot of time as data scientists considering the Transform part of this equation. You may not realize it, but any data analysis work can usually be abstracted to fit into these broad three categories. In many of the toy examples, both on this site and elsewhere, the extract portion is limited to loading from an Excel or CSV file, and fitting the model. However, in real life (and in any job as a data scientist!) the method by which we gain access to the data, and which form it is in, is of crucial importance. Don't think this is part of Data Science -- think again! You may have heard the line that more than 80% of data scientists' time is spent cleaning the data. While the reliability of this result is somewhat questionable (check out this survey for more recent results from 2018) it is without a doubt one of the most time consuming and important parts of a data scientists' toolkit. Without good, clean, and easily accessible data, all of the analytical muscle and machine learning techniques you may use will be to no avail. As the old adage goes -- garbage in, garbage out . Without further ado, I want to detail some of the goals of this project: Find reliable data sources related to as many elements of the game as survivor as I can Create reproducible code to pull this information moving forward (... Extract ..) Connect different data sources together, generate metrics based on them, and clean the data from these sources (... Transform ...) Load this information into a relational database Update this information on a daily basis (as new seasons come out, etc.) To this end, I have written a series of scripts to generalize this process. Then, I use Apache Airflow and a postgres database to periodically run the code and store it in a database. This is all stored on my homeserver at home. The Airflow tasks are run asynchonically on 5 Raspberry Pis. In the future, I may detail this infrastructure work I have done (which is outside the realm of data science, but may be of interest none the less.) This whole process took me roughly 1 month. It was well worth it, thought -- the data I have collected is rich and from many different sources, including True Dorks data on statistics on challenges (Excel spreadsheets) This collection of confessional data from various seasons, with every confessional listed for each episode (Word documents) The Survivor Wiki for information about episodes, seasons, contestants, tribes and alliances. (HTML, webpages) Pushshift.io data for the reddit r/survivor subreddit, where people discuss all things related to survivor (API) Caunce Character Types for each of the different contestants on the show (Excel) As you can imagine, combining all of these data sources to tell a cohesive story was quite a challenge, but a worthwhile one all the same! The remainder of this post will be discussing the data that I had collected, how it has been saved, and what I plan to do with it! Querying the Database I have stored all of the results in a Postgres database, hosted on my homeserver. There is information here that should be kept a secret, like my password, of course, so I will access it using environment varaibles. This was actually saved using the conda environment I have specified for this project, survivor_scraping . In [6]: import os from sqlalchemy import create_engine import pandas as pd In [2]: pg_un , pg_pw , pg_ip , pg_port = [ os . getenv ( x ) for x in [ 'PG_UN' , 'PG_PW' , 'PG_IP' , 'PG_PORT' ]] In [3]: def pg_uri ( un , pw , ip , port ): return f 'postgresql:// { un } : { pw } @ { ip } : { port } ' In [4]: eng = create_engine ( pg_uri ( pg_un , pg_pw , pg_ip , pg_port )) I will now use this engine, as well as the handy read_sql function of pandas, to read in some of the tables and show some of the interesting data we have here! True Dorks Data For the True Dorks data, we have a few different tables that were extracted from here. The first is the episode_performance_stats table, which details the results from the challenges and other events over the course of the entire episode. In [12]: pd . options . display . max_columns = 100 In [13]: pd . read_sql ( 'SELECT * FROM survivor.episode_performance_stats LIMIT 100' , con = eng ) Out[13]: index challenge_wins challenge_appearances sitout voted_for_bootee votes_against_player total_number_of_votes_in_episode tribal_council_appearances votes_at_council number_of_jury_votes total_number_of_jury_votes number_of_days_spent_in_episode days_in_exile individual_reward_challenge_appearances individual_reward_challenge_wins individual_immunity_challenge_appearances individual_immunity_challenge_wins tribal_reward_challenge_appearances tribal_reward_challenge_wins tribal_immunity_challenge_appearances tribal_immunity_challenge_wins season_id tribal_reward_challenge_second_of_three_place tribal_immunity_challenge_second_of_three_place fire_immunity_challenge tribal_immunity_challenge_third_place contestant_id episode_id created updated 0 5803 0.111111 0.211111 0.0 1 1.0 10.0 1.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 1.0 40.0 None None 1.0 None 740 689.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 1 5804 0.000000 0.111111 0.0 1 0.0 9.0 1.0 1.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 40.0 None None 0.0 None 740 690.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 2 5805 0.000000 0.125000 0.0 0 3.0 8.0 1.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 40.0 None None 0.0 None 740 691.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 3 5806 0.142857 0.142857 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 40.0 None None 0.0 None 740 692.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 4 5807 0.000000 0.200000 0.0 1 0.0 5.0 1.0 1.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 40.0 None None 0.0 None 740 693.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 5898 0.000000 1.000000 0.0 0 0.0 0.0 0.0 0.0 0.0 16.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 40.0 None None 0.0 None 750 702.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 96 5899 0.100000 0.211111 0.0 1 3.0 10.0 1.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 1.0 40.0 None None 1.0 None 751 689.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 97 5900 0.111111 0.111111 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 40.0 None None 0.0 None 751 690.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 98 5901 0.125000 0.125000 0.0 0 0.0 0.0 0.0 0.0 0.0 0.0 3.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 40.0 None None 0.0 None 751 691.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 99 5902 0.000000 0.142857 0.0 1 1.0 9.0 1.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 40.0 None None 0.0 None 751 692.0 2020-07-19 00:58:07.828136+00:00 2020-07-19 00:58:07.828136+00:00 100 rows × 30 columns Here, you can see soem interesting information. Notice that these are based on contestant_id s, episode_id s and season_id s. These are foreign keys for the other three tables, contestant , episode and season . This information was pulled from the wiki. In order to map these to the correct values, there was a semi-automatic process to find the correct contestant, episode and season based on the names. This is why the Transform portion of the ETL process was necessary -- to make sure all of these tables were consistent. While a lot of work, it has made the data much richer than any of these individual sources alone. From here out, you can assume id columns required matching with other data sources to ensure correctness and to conform with relational database paradigms. Additionally, for each table you can see a created and updated section. This is present in all of the tables. These were updated most recently on July 19th. If the automated process (using Airflow and the Raspberry Pis) updates these tables, we will see the timestamp at which this was done in these tables. It uses an upsert method, so it should not overwrite the currently used values. You can see this table details information at an episode level. For instance, if there was a reward challenge and an immunity challenge, this will have the results from both challenges. Some episodes will have multiple of each (especially toward the end) so this is important to take note of. Challenge wins are fractional if they wored as part of a tribe -- for instance, if I am in a tribe of 4 and I win a challenge, I will get .25 challenge wins. Same goes for the challenge appearances. The remainder of the results here should be relatively self explanatory. More information is listed on the True Dorks site. Thanks, True Dorks! Additional tables form this source and the immunity_challenge , reward_challenge and vote tables. In [19]: pd . read_sql ( 'SELECT * FROM survivor.immunity_challenge LIMIT 100' , con = eng ) Out[19]: index team win_pct total_players_remaining episode_win_pct sitout tc_number season_id contestant_id episode_id win created updated 0 2452 10.0 0.100000 20.0 0.100000 NaN 1.0 0 278 633 1.0 2020-07-18 02:11:20.917866+00:00 2020-07-18 02:11:20.917866+00:00 1 2453 10.0 0.000000 20.0 0.333333 NaN 1.0 0 515 633 0.0 2020-07-18 02:11:20.917866+00:00 2020-07-18 02:11:20.917866+00:00 2 2454 10.0 0.000000 20.0 0.000000 NaN 1.0 0 283 633 0.0 2020-07-18 02:11:20.917866+00:00 2020-07-18 02:11:20.917866+00:00 3 2455 10.0 0.000000 20.0 0.000000 NaN 1.0 0 163 633 0.0 2020-07-18 02:11:20.917866+00:00 2020-07-18 02:11:20.917866+00:00 4 2456 10.0 0.000000 20.0 0.000000 NaN 1.0 0 362 633 0.0 2020-07-18 02:11:20.917866+00:00 2020-07-18 02:11:20.917866+00:00 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 108 12.0 0.000000 24.0 NaN NaN 1.0 36 674 136 0.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 96 109 12.0 0.000000 24.0 NaN NaN 1.0 36 673 136 0.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 97 110 12.0 0.000000 24.0 NaN NaN 1.0 36 672 136 0.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 98 111 12.0 0.000000 24.0 NaN NaN 1.0 36 671 136 0.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 99 112 12.0 0.083333 24.0 NaN NaN 1.0 36 664 136 1.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 100 rows × 13 columns In [21]: pd . read_sql ( 'SELECT * FROM survivor.reward_challenge LIMIT 100' , con = eng ) Out[21]: index total_players_remaining challenge_number tc_number season_id sitout contestant_id episode_id win win_pct team episode_win_pct created updated 0 57 24.0 1.0 1.0 36 None 676 136 1.0 0.083333 12.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 58 24.0 1.0 1.0 36 None 653 136 0.0 0.000000 12.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 59 24.0 1.0 1.0 36 None 654 136 0.0 0.000000 12.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 60 24.0 1.0 1.0 36 None 655 136 0.0 0.000000 12.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 61 24.0 1.0 1.0 36 None 656 136 0.0 0.000000 12.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 152 20.0 1.0 5.0 36 None 653 144 0.0 0.000000 9.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 96 153 20.0 1.0 5.0 36 None 664 144 0.0 0.000000 9.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 97 154 20.0 1.0 5.0 36 None 667 144 1.0 0.111111 9.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 98 155 20.0 1.0 5.0 36 None 668 144 1.0 0.111111 9.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 99 156 20.0 1.0 5.0 36 None 669 144 1.0 0.111111 9.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 100 rows × 14 columns The two tables above are pretty self explanatory -- they explain each contestants result in an episode for each particular challenge in that episode. In [22]: pd . read_sql ( 'SELECT * FROM survivor.vote LIMIT 100' , con = eng ) Out[22]: index total_players_remaining tc_number season_id contestant_id voted_for_id episode_id vote_number created updated 0 58 18 3 5 212 571.0 490 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 59 18 3 5 6 177.0 490 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 72 16 5 5 575 6.0 491 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 73 16 5 5 541 6.0 491 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 74 16 5 5 43 6.0 491 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 ... ... ... ... ... ... ... ... ... ... ... 95 17 20 1 11 628 236.0 562 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 96 18 20 1 11 616 31.0 562 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 97 19 20 1 11 445 31.0 562 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 98 29 19 2 11 285 445.0 563 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 99 30 19 2 11 236 52.0 563 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 100 rows × 10 columns The above table, also from TrueDorks, lists each vote in ech episode. Note that the contestant and voted_for_id both refer to values in the contestant_season table. Confessionals Data One of the more interesting data sources here, the confessionals data has the actual words that were said in confessionals by contestants in a select number of seasons. This information could be a very interesting analysis for some NLP work, which I plan to explore in future articles. This was sourced from the google drive here , and was scraped based on the names in the word documents. In some cases, as you'll notice below, there is not anything mentioned in the content field. In these cases, it was recorded that that particular contestant_id had spoken, but not what they had said. While this is a bit disappointing, the counts per episode or per contestant can surely be an interesting analysis. In [29]: pd . read_sql ( 'SELECT * FROM survivor.confessional LIMIT 1' , con = eng ) Out[29]: index content day n_from_player n_in_episode total_confessionals_in_episode contestant_id episode_id season_id created updated 0 5337 None 7 4 16 5 38.0 262 25 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 1 5338 None 7 2 17 4 592.0 262 25 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 2 5339 None 7 2 18 4 342.0 262 25 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 3 6310 Denise was like, \"Hey, guys, what are you thin... 34 5 33 6 756.0 701 40 2020-07-11 00:50:29.685756+00:00 2020-07-11 00:50:29.685756+00:00 4 6556 Last night at Tribal Council, I just said to J... 30 1 4 3 744.0 701 40 2020-07-11 00:50:29.685756+00:00 2020-07-11 00:50:29.685756+00:00 ... ... ... ... ... ... ... ... ... ... ... ... 95 4663 Yeah, this whole thing is just a game. Scout's... 22 2 11 6 442.0 238 26 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 96 4664 I've been fed up with Eliza since Day 2. I'm t... 22 1 12 2 120.0 238 26 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 97 5419 None 26 3 15 4 53.0 269 25 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 98 4710 Twila is just insecure. She's scared, and you ... 21 5 29 6 152.0 237 26 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 99 5420 None 26 3 16 4 342.0 269 25 2020-07-11 00:44:59.947552+00:00 2020-07-11 00:45:58.492724+00:00 100 rows × 11 columns Survivor Wikia The information scraped from the Survivor wiki is the following: Episodes Seasons Contestants Tribes Alliances Final Words Story Quotes Voting Confessionals Hopefully, this rich data collected by the people at that wiki will be able to be used to good use! In the process, I was also able to identify small edits which I pushed up to the wikia. Hopefully, I'll be able to find even more! Seasons , Episodes and Contestants are the building blocks of what makes Survivor survivor. Results pulled from the wiki tended to be of the \"more is better\" mentality, so data from summary and story sections were included as well. In [41]: pd . read_sql ( 'SELECT * FROM survivor.episode LIMIT 10' , con = eng ) Out[41]: index summary story challenges trivia image firstbroadcast viewership wiki_link season_episode_number overall_episode_number overall_slot_rating survivor_rating season_id episode_id episode_name created updated 0 488 She Annoys Me Greatly is the season premiere o... Across the islands of Caramoan in the eastern ... Challenge: Water SlaughterTwo members from eac... * A clip of Jeff Probst filming the \"39 days,... https://vignette.wikia.nocookie.net/survivor/i... 2013-02-13 894000000.0 https://survivor.fandom.com/wiki/She_Annoys_Me... 1.0 383.0 7.0 2.4 5.0 488.0 She Annoys Me Greatly 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 489 Honey Badger is the second episode of Survivor... Brandon is upset that Francesca got voted out ... Reward/Immunity Challenge: Plunge, Pull, PopFo... * This episode marks the first time that Malc... https://vignette.wikia.nocookie.net/survivor/i... 2013-02-20 932000000.0 https://survivor.fandom.com/wiki/Honey_Badger 2.0 384.0 7.0 2.4 5.0 489.0 Honey Badger 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 502 * \\n\\n\\tExplore Wikis\\n* \\n\\n\\tCommunity Centr... https://vignette.wikia.nocookie.net/survivor/i... 2013-05-12 813000000.0 https://survivor.fandom.com/wiki/Reunion_(Cara... 15.0 397.0 6.0 2.2 5.0 502.0 Reunion (Caramoan) 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 27 None None None * \\n\\n\\tExplore Wikis\\n* \\n\\n\\tCommunity Centr... https://vignette.wikia.nocookie.net/survivor/i... 2000-08-23 NaN https://survivor.fandom.com/wiki/Survivor:_The... 14.0 14.0 NaN NaN 6.0 27.0 Question of Trust 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 490 There's Gonna Be Hell to Pay is the third epis... Reward/Immunity Challenge: Cell Block SeaTribe... * This is the second episode where there is a... https://vignette.wikia.nocookie.net/survivor/i... 2013-02-27 917000000.0 https://survivor.fandom.com/wiki/There%27s_Gon... 3.0 385.0 8.0 2.6 5.0 490.0 There%27s Gonna Be Hell to Pay 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 491 Kill or Be Killed is the fourth episode of Sur... Challenge: Head and ShouldersTwo members of ea... * This is the ninth episode of Survivor to fe... https://vignette.wikia.nocookie.net/survivor/i... 2013-03-06 958000000.0 https://survivor.fandom.com/wiki/Kill_or_Be_Ki... 4.0 386.0 7.0 2.6 5.0 491.0 Kill or Be Killed 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 492 Persona Non Grata is the fifth episode of Surv... Tribal Council was held at the location of the... Challenge: Nut BucketTwo tribe members would e... * In an interview with Dalton Ross for Entert... https://vignette.wikia.nocookie.net/survivor/i... 2013-03-13 989000000.0 https://survivor.fandom.com/wiki/Persona_Non_G... 5.0 387.0 8.0 2.7 5.0 492.0 Persona Non Grata 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 500 Don't Say Anything About My Mom is the penulti... Day 35's Tree Mail was a Sprint HTC Evo 4G LTE... Challenge: Dizzy GillespieEach castaway must t... * This episode marks the first time in Surviv... https://vignette.wikia.nocookie.net/survivor/i... 2013-05-08 NaN https://survivor.fandom.com/wiki/Don%27t_Say_A... 13.0 395.0 NaN NaN 5.0 500.0 Don%27t Say Anything About My Mom 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 493 Operation Thunder Dome is the sixth episode of... On Day 14, when the tribes arrived at what loo... Immunity Challenge: Crate OutdoorsEach tribe s... https://vignette.wikia.nocookie.net/survivor/i... 2013-03-20 979000000.0 https://survivor.fandom.com/wiki/Operation_Thu... 6.0 388.0 8.0 2.6 5.0 493.0 Operation Thunder Dome 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 494 Tubby Lunchbox is the seventh episode of Survi... After Tribal Council, Phillip and Corinne stil... Challenge: Hot PursuitThe two tribes would rac... * The Reward Challenge was used in \"Dangerous... https://vignette.wikia.nocookie.net/survivor/i... 2013-03-27 943000000.0 https://survivor.fandom.com/wiki/Tubby_Lunchbox 7.0 389.0 7.0 2.5 5.0 494.0 Tubby Lunchbox 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 In [42]: pd . read_sql ( 'SELECT * FROM survivor.season LIMIT 10' , con = eng ) Out[42]: index season_id days n_episodes history location season_number summary n_survivors trivia twists version viewership name type runnerup_0_id runnerup_1_id winner_id filming_started filming_ended showing_started showing_ended viewership_in_millions created updated 0 0 0 39.0 14.0 The season was filmed in the summer of 2017, o... Mamanuca Islands, Fiji 36.0 The jury vote ended in a tie between two playe... 20 * This is the first season since Survivor: Sa... * Ghost Island:[4] The tribe that wins a Rewa... United States NaN Ghost Island Survivor 0 296.0 278 2017-06-05 2017-07-13 2018-02-28 2018-05-23 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 1 1 39.0 15.0 The season was filmed once again in the vicini... Upolu, Samoa 24.0 Relations between the men's tribe and the wome... 18 * The designs for this season's props, such a... * One World Format: The two competing tribes,... United States 1.163600e+09 One World Survivor 574 406.0 546 2011-08-01 2011-09-08 2012-02-15 2012-05-13 1.163600e+09 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 2 2 39.0 14.0 The season was filmed shortly after filming Su... Palaui Island, Santa Ana, Cagayan, Philippines 28.0 Cagayan is famed among Survivor seasons for it... 18 * The font used for Cagayan's logo is a modif... * Three Tribes: The castaways are divided int... United States NaN Cagayan Survivor 284 NaN 601 2013-07-10 2013-08-17 2014-02-26 2014-05-21 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 3 3 39.0 15.0 &#94;1 Matthew and Rob did not vote as they could ... Rio Negro, Amazonas, Brazil 6.0 The winner is Jenna Morasca, a 21-year-old swi... 16 * For the first time, the contestants were di... * Tribe Composition: The sixteen castaways we... United States 1.997000e+09 The Amazon Survivor 603 NaN 415 2002-11-04 2002-12-12 2003-02-13 2003-05-11 1.997000e+09 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 4 4 39.0 14.0 Filming for the season was scheduled to start ... Mamanuca Islands, Fiji 33.0 The season used a variant of the tribe divisio... 20 * The font used for this season is a variant ... * Millennials vs. Gen X: The 20 castaways will... United States NaN Millennials vs. Gen X Survivor 516 400.0 331 2016-04-04 2016-05-12 2016-09-21 2016-12-14 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 5 5 39.0 15.0 This season repeats the back-to-back filming s... Caramoan, Camarines Sur, Philippines 26.0 None 20 * The theme of the season is inspired from th... * Fans vs. Favorites: Similar to past season ... United States 1.081500e+09 Caramoan Survivor 405 43.0 123 2012-05-21 2012-06-28 2013-02-13 2013-05-12 1.081500e+09 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 6 6 39.0 14.0 Due to overwhelming interest in the show's for... Pulau Tiga, Sabah, Borneo, Malaysia 1.0 Among Survivor fans, the term \"Pagonging\" has ... 16 * This is the first season in which both trib... * Tribe Composition: The sixteen castaways we... United States 2.830000e+09 Borneo Survivor 627 NaN 257 2000-03-13 2000-04-20 2000-05-31 2000-08-23 2.830000e+09 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 7 7 39.0 15.0 Casting for this season was originally done as... Koror, Palau 16.0 Survivor: Micronesia, also known as Survivor: ... 20 * This is the first season to have an even-nu... * Fans vs. Favorites: One tribe, Airai​​, is ... United States 1.361000e+09 Micronesia Survivor 325 NaN 513 2007-10-29 2007-12-06 2008-02-07 2008-05-11 1.361000e+09 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 8 8 39.0 15.0 The theme and name of the season were announce... San Juan del Sur, Rivas, Nicaragua 29.0 Survivor: San Juan del Sur, also known as Surv... 18 * Based on the logo and the names of the trib... * Blood vs. Water: Nine pairs of castaways, ea... United States NaN San Juan del Sur Survivor 508 450.0 10 2014-06-01 2014-07-10 2014-09-24 2014-12-17 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 9 9 39.0 15.0 Applications were due June 16, 2006. Mellisa M... Macuata, Vanua Levu, Fiji 14.0 Survivor: Fiji is the fourteenth season of Uni... 19 * Due to Mellisa McNulty dropping out only ho... * Haves vs. Have Nots: The castaways (after th... United States 1.480000e+09 Fiji Survivor 215 265.0 126 2006-10-30 2006-12-07 2007-02-08 2007-05-13 1.480000e+09 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 Contestants were broken into two tables, to separate the idea of contestants in a particular season vs contestants on the whole . Some recurring characters act very differently season to season, so this information may be useful to separate. In [43]: pd . read_sql ( 'SELECT * FROM survivor.contestant LIMIT 10' , con = eng ) Out[43]: index wiki_survivor_text wiki_postsurvivor_text trivia birthdate other_profile hometown current_residence occupation_self_reported hobbies pet_peeves three_words claim_to_fame inspiration three_things most_similar_self_reported reason why_survive previous_season first_name last_name nickname twitter sex image wikia contestant_id created updated 0 0 A superfan of the show, AK started the game wi... None None 1987-08-03 Getting to that Final Tribal Council is AK's u... Adelaide, South Australia None Wedding DJState: South AustraliaTribe: Samatau None None None None None None None None None None AK Knight None None M https://vignette.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/AK_Knight 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 2 At the start of the game, Aaron was placed on ... None None 1975-04-25 None Venice, California None Surfing Instructor None None None None None None None None None None Aaron Reisberger None None M http://vignette2.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Aaron_Reisberger 3 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 3 Originally in the majority Sporty Seven allian... None None 1991-01-07 \"I was so close last time, purely by being mys... Melbourne, VictoriaPerth, Western Australia None AFL Premiership Winner None None None None None None None None None None Abbey Holmes None None F https://vignette.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Abbey_Holmes 4 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 4 Abi-Maria was part of the yellow Tandang tribe... None None 1979-10-21 Name: Abi-Maria Gomes\\nSeason 25\\nSurvivor: Ph... Los Angeles, CA[3] None Business Student Languages, hiking, dancing, surfing and skiing. Complainers. Driven, creative and charming. None Steve Jobs' words, \"Stay Hungry. Stay foolish.\" None Parvati – she is as charming as I am. The money! I bring social skills and team work. I am a mo... None Abi-Maria Gomes None None F http://vignette2.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Abi-Maria_Gomes 5 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 5 None None None 1980-09-03 None Naples, Florida None Jewelry Sales & Photographer None None None None None None None None None None Ace Gordon None None M http://vignette2.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Ace_Gordon 6 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 6 Adam started out on the Rarotonga tribe with P... None None 1978-08-21 None San Diego, California None Copier Sales None None None None None None None None None None Adam Gentry None None M http://vignette1.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Adam_Gentry 7 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 24 Amanda formed an early alliance with fellow Fe... None None 1984-08-03 Tribe: Heroes\\nCurrent Residence: Los Angeles,... Kalispell, Mont. None Hiking GuideAspiring Designer None None None None \"God.\" None None None None None Amanda Kimmel None None F http://vignette3.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Amanda_Kimmel 25 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 8 None None None None Retrieved from TVNZ.co.nz\\nName: Adam\\nWith a ... Auckland None Self Employed None None None None None None None None None None Adam O'Brien None None M https://vignette.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Adam_O'Brien 9 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 9 Alan started as a member of the Levu tribe whe... None None 1985-03-29 Current Residence: Houston, Texas\\n Detroit, Mich. None NFL Player Golf, scuba diving, and riding my motorcycle. I can't stand liars or people who feel entitle... Intelligent, athletic, and clever. Being a seventh round draft pick and grinding ... My parents. They are my best friends. I respec... Hot sauce because it makes everything better, ... I honestly don't think you will see gameplay l... Playing football has fed my drive to compete p... Everything I've accomplished I've had to scrat... None Alan Ball None None M https://vignette.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Alan_Ball 10 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 10 &#94;1 In \"Double Agent\", the vote ended with a 6... None None 1985-03-31 Tribe designation: Upolu\\nInspiration in life:... Plantation, Florida Plantation, Fla. Baseball/Dating Coach Poker, writing and exercise. I have little to no patience for ignorance. Pe... Versatile, dynamic and resourceful. I hit my first college home run off of a guy t... None None None None None None Albert Destrade None None M http://vignette2.wikia.nocookie.net/survivor/i... http://survivor.wikia.com/wiki/Albert_Destrade 11 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 In [44]: pd . read_sql ( 'SELECT * FROM survivor.contestant_season LIMIT 10' , con = eng ) Out[44]: index contestant_season_id occupation location age placement days_lasted votes_against med_evac quit individual_wins season_id attempt_number tribe_0 tribe_1 tribe_2 tribe_3 alliance_0 alliance_1 alliance_2 contestant_id character_id created updated 0 71 71 Aspiring Writer Los Angeles, CA 33 17.0 11.0 6.0 0.0 0.0 0.0 7 1.0 3.0 NaN NaN NaN NaN NaN None 474 9.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 73 73 Social media marketer Cambridge, Massachusetts 29 2.0 39.0 6.0 0.0 0.0 0.0 18 1.0 39.0 45.0 96.0 NaN 46.0 NaN None 49 23.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 74 74 Social media marketer Cambridge, Massachusetts 30 5.0 1.0 0.0 0.0 0.0 0.0 30 2.0 87.0 84.0 87.0 91.0 32.0 NaN None 49 23.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 75 75 Pharmaceutical Sales Saint Louis, MO 26 7.0 30.0 5.0 0.0 0.0 0.0 32 1.0 35.0 158.0 NaN NaN 28.0 57.0 None 660 14.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 76 76 Vacation Club Sales Fort Worth, Texas 18 14.0 20.0 4.0 0.0 0.0 0.0 4 1.0 6.0 144.0 NaN NaN 78.0 NaN None 458 14.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 77 77 Vacation Club Sales Fort Worth, Texas 26 7.0 1.0 0.0 0.0 0.0 0.0 30 2.0 87.0 84.0 87.0 91.0 32.0 NaN None 458 21.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 78 78 Dog trainer Jackson Springs, NC 56 6.0 36.0 11.0 0.0 0.0 3.0 23 1.0 27.0 4.0 86.0 NaN 72.0 52.0 None 277 12.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 79 79 Administrative Assistant Beaver, PA 22 6.0 33.0 6.0 0.0 0.0 0.0 28 1.0 101.0 161.0 NaN NaN 70.0 NaN None 27 16.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 80 80 Administrative Assistant Beaver, PA 25 1.0 39.0 6.0 0.0 0.0 0.0 13 2.0 110.0 24.0 NaN NaN 40.0 NaN None 27 16.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 81 81 Maintenance Supervisor Portland, OR 25 14.0 15.0 6.0 0.0 0.0 1.0 19 1.0 108.0 NaN NaN NaN 65.0 NaN None 222 11.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 Something else that may be interesting to look at is the tribes and the alliances . Who is alligned with whom? How strong are they? This could be interesting to look at historically and for future seasons In [45]: pd . read_sql ( 'SELECT * FROM survivor.tribe LIMIT 10' , con = eng ) Out[45]: index tribe_id summary tribal_history trivia name tribenameorigin tribetype dayformed status lowest_placing_member insigniaimage flagimage buffimage image opponent_0 opponent_1 opponent_2 season_id highest_placing_member created updated 0 139 140 Galang is a tribe from Survivor: Blood vs. Wat... \\n\\n\\n * Galang is the only tribe of Returning Playe... Galang Filipino word meaning \"respect\" Starting Tribe Day 1 Merged with Tadhana on Day 19 68.0 https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... 33.0 NaN None 16.0 18.0 2020-07-11 01:03:00.566347+00:00 2020-07-13 03:35:15.035735+00:00 1 1 2 Jaburu is a tribe from Survivor: The Amazon.\\n... None * Jaburu is the first all-female tribe in Sur... Jaburu The Jabiru stork Starting tribe Day 1 Merged with Tambaqui on Day 20 335.0 None https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... 116.0 NaN None 3.0 415.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 13 171 David None None None None NaN None None None None NaN NaN None NaN NaN 2020-07-18 02:59:05.938012+00:00 2020-07-18 02:59:05.938012+00:00 3 3 4 La Flor (known informally as the Younger Tribe... \\n\\nDuring the marooning on Day 1, the twenty ... * La Flor is the third tribe in Survivor hist... La Flor Spanish word for \"the flower\" Starting Tribe Day 1 Merged with Espada on Day 19 190.0 https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... 27.0 NaN None 23.0 47.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 137 138 Villains​ is a tribe from Survivor: Heroes vs.... The Villains.From the beginning, almost every ... * Russell Hantz was not known by the Heroes v... Villains \"Deception, Manipulation, and Duplicity\" Starting Tribe Day 1 Merged with Heroes on Day 25 23.0 https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... NaN NaN None 15.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-13 03:35:15.035735+00:00 5 38 39 Chan Loh (ចន្លុះ) is a tribe from Survivor: Ka... \\n\\nThe \"Brains\" tribe of the season, Chan Loh... * The Chan Loh beach eventually became Ta Keo... Chan Loh Koh Chanloh, an island off Sihanoukville, Camb... Starting Tribe Day 1 Merged with Gondol on Day 17 389.0 https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... 45.0 NaN None 18.0 73.0 2020-07-11 01:03:00.566347+00:00 2020-07-13 03:35:15.035735+00:00 6 7 8 Tandang is a tribe from Survivor: Philippines.... \\n\\nThe minute the Tandang tribe reached their... * Yellow is one of the three colors represent... Tandang Filipino word meaning, \"rooster\" Starting Tribe Day 1 Merged with Kalabaw on Day 17 500.0 https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... 77.0 66.0 None 35.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 8 9 Gitanos​ is the merged tribe of Casaya​​ and L... None * Gitanos merged the earliest (Day 16), later... Gitanos Spanish for \"gypsies\" Merged tribe Day 16 None 548.0 None https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... NaN NaN None 22.0 28.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 10 11 Kalo Kalo is the merged tribe of Mokuta and Va... None None Kalo Kalo Fijian word meaning \"star\" Merged tribe Day 29 None 825.0 None None None None NaN NaN None 44.0 821.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 126 127 Kasama is the merged tribe of Galang​ and Tadh... * Kasama is the first merged tribe that Monic... Kasama Filipino word meaning \"companion\" Merged tribe Day 19 None 29.0 None https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... https://vignette.wikia.nocookie.net/survivor/i... NaN NaN None 16.0 18.0 2020-07-11 01:03:00.566347+00:00 2020-07-13 03:35:15.035735+00:00 In [46]: pd . read_sql ( 'SELECT * FROM survivor.alliance LIMIT 10' , con = eng ) Out[46]: index summary history trivia name dayformed image alliance_id season_id founder_0 founder_1 founder_2 lowest_placing_member highest_placing_0 highest_placing_1 created updated 0 0 None On Day 6, after reading the clue to the Hidden... * Jill is the only member of the alliance to n... Espada Alliance Day 6 https://vignette.wikia.nocookie.net/survivor/i... 1 23.0 459.0 NaN None 35.0 356.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 1 None When Russell Hantz was revealed that he would ... * This alliance is the first to successfully v... Zapatera Six Day 7 https://vignette.wikia.nocookie.net/survivor/i... 2 29.0 329.0 NaN None 625.0 112.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 2 None After losing their first Immunity Challenge on... * Lindsey Cascaddan is the only member of the... Escameca Alliance Day 11 https://vignette.wikia.nocookie.net/survivor/i... 3 10.0 348.0 NaN None 479.0 279.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 7 After Nagarote lost its first challenge on Day... * Will is the only member of the alliance to m... Nagarote Alliance Day 5 https://vignette.wikia.nocookie.net/survivor/i... 8 10.0 12.0 NaN None 209.0 NaN NaN 2020-07-11 01:03:00.566347+00:00 2020-07-13 03:35:14.421798+00:00 4 4 None The Tandang Alliance was originally formed by ... * Every member of the Tandang tribe was a part... Tandang Alliance Day 1 https://vignette.wikia.nocookie.net/survivor/i... 5 35.0 235.0 500.0 None 500.0 623.0 529.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 10 Final Four Alliance None None 112 NaN NaN NaN None NaN NaN NaN 2020-07-13 03:35:14.421798+00:00 2020-07-13 03:35:14.421798+00:00 6 6 None On Day 13, the Zhan Hu tribe received a note f... * This was the first pre-merge 3-person allian... Zhan Hu Alliance Day 13 https://vignette.wikia.nocookie.net/survivor/i... 7 24.0 178.0 NaN None 178.0 349.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 40 The alliance of original Matsing tribe members... * This was the first alliance to be created by... Matsing Alliance Day 1 https://vignette.wikia.nocookie.net/survivor/i... 41 35.0 549.0 216.0 None 351.0 549.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-13 03:35:14.421798+00:00 8 30 None Although not created until after the eliminati... * Jaison Robinson is the only original member... Foa Foa Four Day 18 https://vignette.wikia.nocookie.net/survivor/i... 31 21.0 251.0 NaN None 494.0 624.0 NaN 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 23 Witches Coven None None 113 NaN NaN NaN None NaN NaN NaN 2020-07-13 03:35:14.421798+00:00 2020-07-13 03:35:14.421798+00:00 Finally, to compliment the confessionals from above, some episodes have a wealth of quotes from contestants in particular situations. Final words contains the final words spoken by contestants at the end of the episode: In [47]: pd . read_sql ( 'SELECT * FROM survivor.final_words LIMIT 10' , con = eng ) Out[47]: index contestant_id content season episode_id created updated 0 0 782 I came in claiming that I had the most knowled... 37 0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 1 113 Well, I think this has been, uh, an awesome ex... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 2 7 I'm a little sad to leave these people because... 6 3 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 3 784 I'm feeling real gutted to be voted out. I don... 37 4 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 4 551 They kicked off their bug-eating hero instead ... 6 5 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 5 242 I think the first two days that I was sick jus... 6 7 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 6 795 My Survivor journey's finally come to an end. ... 37 8 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 7 34 I guess I want to start by just thanking the L... 6 9 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 8 332 I wish I could've made it a little longer. I t... 6 10 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 9 785 I am so devastated to lose and to be going hom... 37 12 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 Voting Confessionals contain words spoken in the voting area each episode (often censored or redacted for dramatic suspense): In [48]: pd . read_sql ( 'SELECT * FROM survivor.voting_confessional LIMIT 10' , con = eng ) Out[48]: index voter_id season episode_id type_of_vote initial_or_changed for_or_against content recipient_id created updated 0 0 789 37 0 vote initial against Dee, you're an awesome girl, but, this is a tr... 782.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 1 259 6 1 vote initial against Got to remove the weakest link in the... crew. 113.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 2 318 6 1 vote initial against My vote really kills me because I love her, bu... 113.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 3 257 6 1 vote initial against Stacey. Um, tough call. (sighs) Subtle reasons... 551.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 4 551 6 1 vote initial against He's just... He's an ornery guy... doesn't rea... 58.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 5 58 6 1 vote initial against I'm picking her because I think she's the reas... 113.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 6 34 6 1 vote initial against Just physically it's too much. That's it. That... 113.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 7 113 6 1 vote initial against I'll miss his, uh, skills and, um, his speakin... 58.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 8 780 37 2 vote initial against Really sorry mate. But it's just what I need t... 795.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 9 793 37 2 vote initial against You're awesome, Tony. I'm sorry. 795.0 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 Finally, story quotes contain information from the entire episode. There may be overlap with the confessionals section, or it may be within the tribes (not considered a confessional). Additionally, when there are no contestants, it may represent voice overs or things spoken by people outside of the main contestants. In [49]: pd . read_sql ( 'SELECT * FROM survivor.story_quotes LIMIT 10' , con = eng ) Out[49]: index contestant_id content season episode_id created updated 0 0 NaN The sixteen contestants have been separated in... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 1 NaN The Tagi tribe, who will always wear orange, c... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 2 NaN The Pagong tribe, who will always wear yellow,... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 3 NaN Here, it's the impressions you make on the oth... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 4 NaN Throughout their time on the island, tribes wi... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 5 NaN This is Tribal Council, where each week one me... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 6 58.0 Paddling over, uh, we had two or three of thos... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 7 58.0 The hardest part is hanging around with all th... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 8 58.0 Up until, uh, probably last night, I never gav... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 9 627.0 He was yelling at everybody \"Let's lose the bo... 6 1 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 NOTE : You may notice that there is a wealth of information here from the wiki itself. While some of this hasn't been cleaned entirely, the text fields so far have been mostly untampered with. What has been cleaned are numeric, or datetime, columns, and relational foreign keys. Until a good use is found for some of the text data, it may not make sense to clean this yet, as I can see there being a lot of information in there that I don't want to slice just yet. Pushshift.io Pushift.io is a public api for requesting archived Reddit data. The nice thing is that it does not have a small limit to accessing the data, like Reddit's API does, and does not limit on total requests. This data is updated every day, and contains all of the posts since the inception of the r/survivor subreddit in 2011. r/survivor is actually a thriving community, and a rich data source for engagement, as well as predictions for the show. We have two tables here: Submissions contains data on the submissions as topics to the subreddit. Comments contains data on the comments in the subreddit. In [50]: pd . read_sql ( 'SELECT * FROM survivor.reddit_submissions LIMIT 10' , con = eng ) Out[50]: index author author_flair_css_class author_flair_text created_utc domain full_link id is_self media_embed num_comments over_18 permalink score selftext subreddit subreddit_id thumbnail title url author_created_utc author_fullname edited distinguished media link_flair_css_class link_flair_text gilded mod_reports retrieved_on secure_media_embed stickied user_reports secure_media post_hint preview locked banned_by contest_mode spoiler brand_safe suggested_sort author_cakeday thumbnail_height thumbnail_width is_video approved_at_utc banned_at_utc can_mod_post view_count ... link_flair_template_id author_flair_background_color author_flair_text_color send_replies no_follow subreddit_subscribers is_original_content previous_visits wls pwls media_only author_id is_meta all_awardings allow_live_comments awarders gildings is_robot_indexable total_awards_received treatment_tags upvote_ratio author_patreon_flair author_premium media_metadata author_flair_template_id removed_by_category event_end event_is_live event_start archived can_gild category content_categories hidden quarantine removal_reason subreddit_name_prefixed collections updated_utc steward_reports og_description og_title removed_by poll_data created_dt most_recent_season within_season most_recent_episode created updated 0 16384 MrUnderdawg S31Pregame Spencer 1430250605 self.survivor https://www.reddit.com/r/survivor/comments/346... 346vvp True {} 17.0 False /r/survivor/comments/346vvp/what_would_you_guy... 0.0 I am 14 and love Survivor and have seen most s... survivor t5_2qhu3 self What would you guys think of a teenage version... http://www.reddit.com/r/survivor/comments/346v... 1.376778e+09 t2_csnqs None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 19:50:05 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 16385 DantheManFoley S31Pregame Savage 1430253710 youtube.com https://www.reddit.com/r/survivor/comments/347... 34739a False {'content': '&lt;iframe class=\"embedly-embed\" ... 6.0 False /r/survivor/comments/34739a/cool_behind_the_sc... 0.0 None survivor t5_2qhu3 http://b.thumbs.redditmedia.com/pzXfFWQiveSigX... Cool behind the scenes Survivor Samoa https://www.youtube.com/watch?v=Z2r4V8TohTQ 1.428826e+09 t2_muy3e None None {'oembed': {'author_name': 'CBS', 'author_url'... None None 0.0 None 1.440766e+09 {'content': '&lt;iframe class=\"embedly-embed\" ... False None {'oembed': {'author_name': 'CBS', 'author_url'... rich:video {'images': [{'id': 'WberhPZBCTrBBVqeOcPSBlPvUw... None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 20:41:50 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 16386 AnghellicKarma S31Pregame Tasha 1430254375 self.survivor https://www.reddit.com/r/survivor/comments/347... 3474so True {} 21.0 False /r/survivor/comments/3474so/spoilers_what_are_... 0.0 Granted, anything can happen, and we know that... survivor t5_2qhu3 self (Spoilers) What are the likely paths to victor... http://www.reddit.com/r/survivor/comments/3474... 1.325176e+09 t2_6je30 None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 20:52:55 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 16387 LenoxTillman_is_ANTM Player Shirin 1430255552 self.survivor https://www.reddit.com/r/survivor/comments/347... 3477jd True {} 45.0 False /r/survivor/comments/3477jd/spoilers_i_think_i... 17.0 No matter what Sierra is going to be in the mi... survivor t5_2qhu3 self [SPOILERS] I think ____ is screwed http://www.reddit.com/r/survivor/comments/3477... 1.416627e+09 t2_jit7n None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 21:12:32 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 16388 numberonepassion S31Pregame Varner 1430257442 self.survivor https://www.reddit.com/r/survivor/comments/347... 347bzu True {} 205.0 False /r/survivor/comments/347bzu/least_deserving_pl... 7.0 Any of the players that have returned on any o... survivor t5_2qhu3 self Least deserving player to ever be brought back? http://www.reddit.com/r/survivor/comments/347b... 1.430160e+09 t2_n5ppp None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 21:44:02 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 16389 [deleted] None None 1430259006 proprofs.com https://www.reddit.com/r/survivor/comments/347... 347fhg False {} 0.0 False /r/survivor/comments/347fhg/puzzlleee/ 1.0 None survivor t5_2qhu3 default puzzlleee http://www.proprofs.com/games/puzzle/sliding/s... NaN None None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 22:10:06 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 16390 [deleted] None None 1430261862 imgur.com https://www.reddit.com/r/survivor/comments/347... 347ln3 False {} 0.0 False /r/survivor/comments/347ln3/the_closest_i_will... 1.0 None survivor t5_2qhu3 default the closest I will ever get to playing survivor. http://imgur.com/2F3kKWe NaN None None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 22:57:42 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 16391 lbblur S31Pregame Keith 1430263877 self.survivor https://www.reddit.com/r/survivor/comments/347... 347pqa True {} 0.0 False /r/survivor/comments/347pqa/how_different_woul... 1.0 None survivor t5_2qhu3 default How different would Survivor South Pacific hav... http://www.reddit.com/r/survivor/comments/347p... NaN None None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 23:31:17 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 16392 numberonepassion S31Pregame Varner 1430264587 self.survivor https://www.reddit.com/r/survivor/comments/347... 347r5o True {} 57.0 False /r/survivor/comments/347r5o/predictions_for_th... 6.0 What do you think will happen? What irrelevant... survivor t5_2qhu3 self Predictions for the reunion show? http://www.reddit.com/r/survivor/comments/347r... 1.430160e+09 t2_n5ppp None None None None None 0.0 None 1.440766e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-04-28 23:43:07 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 16845 [deleted] None None 1430868753 mobile.twitter.com https://www.reddit.com/r/survivor/comments/34z... 34zzhu False {} 3.0 False /r/survivor/comments/34zzhu/survivors_got_a_bi... 1.0 None survivor t5_2qhu3 default Survivor's Got a Big Announcement Tomorrow Eve... https://mobile.twitter.com/Survivor_Tweet/stat... NaN None None None None None None 0.0 None 1.440753e+09 {} False None None None None None None None None None None None None None None None None None None ... None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-05-05 23:32:33 None None None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 10 rows × 116 columns In [51]: pd . read_sql ( 'SELECT * FROM survivor.reddit_comments LIMIT 10' , con = eng ) Out[51]: index author author_created_utc author_flair_css_class author_flair_text author_fullname body controversiality created_utc distinguished gilded id link_id nest_level parent_id reply_delay retrieved_on score score_hidden subreddit subreddit_id edited user_removed mod_removed stickied author_cakeday can_gild collapsed collapsed_reason is_submitter gildings permalink permalink_url updated_utc subreddit_type no_follow send_replies author_flair_template_id author_flair_background_color author_flair_richtext author_flair_text_color author_flair_type rte_mode subreddit_name_prefixed all_awardings associated_award author_patreon_flair author_premium awarders collapsed_because_crowd_control locked total_awards_received treatment_tags steward_reports top_awarded_type created_dt most_recent_season within_season most_recent_episode submission_id created updated 0 647761 sseidl88 1.383535e+09 S31Pregame Spencer t2_drjpb She bout to lose it next episode 0.0 1443106261 None 0.0 cvclc5e t3_3m4uta 2.0 t1_cvc187u 50932.0 1.444555e+09 3.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:51:01 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 647762 jsreid 1.283268e+09 S31Pregame Fishbach t2_4arr8 Episode 1 spoiler or worse? 0.0 1443106288 None 0.0 cvclcso t3_3m58ai 5.0 t1_cvchlyq 6819.0 1.444555e+09 2.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:51:28 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 647763 SawRub 1.321492e+09 S31Pregame Spencer t2_69agp Maybe Spencer sees this as the good three-Brai... 0.0 1443106290 None 0.0 cvclcus t3_3m4uta 3.0 t1_cvc70r9 40938.0 1.444555e+09 5.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:51:30 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 647764 sseidl88 1.383535e+09 S31Pregame Spencer t2_drjpb He was trying so hard not to laugh 0.0 1443106349 None 0.0 cvcle88 t3_3m4uta 2.0 t1_cvc1r7c 50089.0 1.444555e+09 3.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:52:29 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 647765 InquisitorialSquad 1.406635e+09 S31Pregame Fishbach t2_hmisk I've been off this sub-reddit in a while... is... 0.0 1443106370 None 0.0 cvclepd t3_3m576l 3.0 t1_cvc38a0 47585.0 1.444555e+09 1.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:52:50 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 647766 [deleted] NaN None None None [deleted] 0.0 1443106372 None 0.0 cvcler6 t3_3m4uta 4.0 t1_cvc7u7g 39388.0 1.444555e+09 1.0 None survivor t5_2qhu3 None True None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:52:52 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 647767 lex_machine 1.402710e+09 S31Pregame Varner t2_gz19g He's got the full schemer edit:\\n\\n- Aras is t... 0.0 1443106406 None 0.0 cvclfk9 t3_3m6nsz 2.0 t1_cvckx52 796.0 1.444555e+09 1.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:53:26 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 647768 zallirog23 1.387086e+09 S31Pregame Wiglesworth t2_eczt1 She was talking to people, no one mentioned he... 0.0 1443106433 None 0.0 cvclg80 t3_3m70bk 2.0 t1_cvchrgp 6619.0 1.444555e+09 15.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:53:53 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 647769 zallirog23 1.387086e+09 S31Pregame Wiglesworth t2_eczt1 Got her bracelet back, it's a victory. 0.0 1443106467 None 0.0 cvclgzw t3_3m70bk 2.0 t1_cvchrgf 6653.0 1.444555e+09 25.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:54:27 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 647770 SkyborneScout NaN S31Pregame Monica None Didn't Kelly Goldsmith vote out Diane in Episo... 0.0 1443106484 None 0.0 cvclhek t3_3m59f8 3.0 t1_cvc6hs1 42115.0 1.444555e+09 11.0 None survivor t5_2qhu3 None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None 2015-09-24 14:54:44 11.0 11.0 562.0 None 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 There is a wealth of information that Pushift.io provides here. In terms of connecting to the other tables, we have a most_recent_season , within_season and most_recent_episode column which relates the date of the post (or comment) to the most recent season/episode in our tables. This will be the subject of our first analysis! Caunce Character Types Last, and certainly not least, are the Caunce Character types. These are character types assigned by Angie Caunce a reporter on the show on a popular podcast, Rob has a Podcast . I actually don't know too much about this, but it is talked about quite a bit on the subreddit and other forums. Each contestant is linked to the character type to which Angie has assinged them. These have ids, like the other tables, and look like this: In [53]: pd . read_sql ( 'SELECT * FROM survivor.role LIMIT 10' , con = eng ) Out[53]: index role_id role description created updated 0 0 0 Good Ol' Boy Southern accent, country roots, often a farmer... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 1 1 1 Know It All Superfan, highly intelligent, understands stra... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 2 2 2 Seduce and Destroy Young professional (marketing/sales), \"ladies ... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 3 3 3 The Social Butterfly Sometimes gay, super social, witty, extremely ... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 4 4 4 Alpha Male Control Freak CEO or doctor, rich and powerful, bossy, contr... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 5 5 5 The Specialist Eccentric, little crazy, full of himself, huge... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 6 6 6 True Grit Retired pro athlete or military guy, cop, fire... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 7 7 7 John McClane 25-35 regular Joe (blue collar job), aggressiv... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 8 8 8 Surfer Dude Long hair, easy going, very athletic, new agey... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 9 9 9 Mr. Miagi Kind, wise, intelligent, well spoken, not inte... 2020-07-11 01:03:00.566347+00:00 2020-07-11 01:03:00.566347+00:00 Okay, cool. So What? Totally understand you asking yourself this question, if you've made it this far. So what? Who cares about all of this data? Well, I wanted to make a post about this since, for the next few months (and possibly beyond...) I will be posting an analysis weekly on Survivor based on these tables. And since it can certainly be confusing, I figured the best place to start is where all analysis starts -- data collection and cleaning. And while the next few analyses may be the most fun to dig into, they will by far be much less time consuming, and ultimately much less important, than this step. One of the coolest parts about all of this is that the generation of these tables is now put into production in my little mini homeserver environment. Now, my Raspberry Pis will be put to great use scraping and otherwise collecting data from all of these sources every day. Even better, I don't even have to think about it anymore! As a data scientist, this is a beautiful situation -- I am able to be entirely self sufficient to do all of the cool data science stuff that I want! :) Also, I'm a huge nerd and hope you are too and hope that seeing all of this data makes you drool! If you would like for me to share this data with you, please feel free to reach out to me!","tags":"Survivor","url":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-1.html","loc":"https://seanammirati.github.io/category/survivor/survivor-outwit-outplay-outanalyze-part-1.html"},{"title":"Hidden Markov Models","text":"In my previous project, I developed the reasoning behind Hidden Markov Models (HMM) and developed a way of determining the most likely sequence of states that resulted in an observed sequence of emissions using the Viterbi algorithm. This project will explore extensions to the Hidden Markov Model, such as the Forwards-Backwards algorithm and the Baum-Welch algorithm. Consider a discrete time, discrete state-space Hidden Markov chain with: $\\pi_{t}$ : the state at time t (unknown in the HMM) of n possible states $\\pi_{t} = 1,2,\\ldots,n$ $x_{\\text{t}}$: the emission at time t of m possible emissions, $x_{\\text{t}} = 1,2,\\ldots,m$ P(0) : initial probabilities of being in the states. P : a nxn transition matrix of transitions between states, giving P($\\pi_{t} = i\\left| \\pi_{t - 1} = j \\right)$ for the ith row and jth column. E : a nxm matrix containing P($x_{\\text{t}} = j\\ \\left| \\ \\pi_{t} = i \\right)$ in the ith row, jth column, the emission probabilities. Note that the row sums must sum to 1. Given a set of observed emissions, $x_{1,\\ldots,\\ T}$, the Viterbi algorithm gives us the most likely sequence of states, $\\pi_{1,\\ldots,\\ T}$ that resulted in the observed emissions, given P and E. Although this decoding algorithm is very useful, there may be other aspects of the HMM we are interested in. One such problem is, given a set of observed emissions $x_{1,\\ldots,\\ T}$, what is the probability of the observed emissions? This problem can be solved using the law of total probability, i.e.: $$P\\left( x_{1},\\ \\ldots,x_{T} \\right) = \\ \\sum_{\\mathbf{\\pi}}&#94;{}{P(x_{1},\\ \\ldots,x_{T}}\\left| \\ \\mathbf{\\pi} \\right)P(\\mathbf{\\pi})\\ $$ where $\\mathbf{\\pi}$ refers to any vector of state sequences. This can be computationally intensive -- for many steps, this will involve summing over an exponentially large number of possible state sequences. Thus, an algorithmic approach will be useful to make this result practical to obtain for large T. This is a similar problem to the Viterbi algorithm, but not exactly the same. This can be solved using the forward algorithm , as described below. Note that we can also write the above as: $$P\\left( x_{1},\\ \\ldots,x_{T} \\right) = \\ \\sum_{\\mathbf{\\pi}}&#94;{}{P(x_{1},\\ \\ldots,x_{T}},\\mathbf{\\pi})\\ $$ Let's call $\\alpha_{t}(\\pi_{t})$ = $P\\left( x_{1},\\ \\ldots,x_{t},\\ \\pi_{t} \\right) = \\ \\sum_{\\pi_{t - 1}}&#94;{}{P(x_{1},\\ \\ldots,x_{T},\\ \\pi_{t},\\pi_{t - 1})}$ It follows that our desired probability of the sequence is equal to the sum over all states of the above quantity. $$\\alpha_{t}(\\pi_{t}) = \\ \\sum_{\\mathbf{\\pi}_{t - 1}}&#94;{}{P(x_{t}|x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t},\\ \\pi_{t - 1})P(\\pi_{t}|x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t - 1}})P(x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t - 1})$$ But $x_{t}$ is conditionally independent of everything but $\\pi_{t},\\ $and $\\pi_{t}$conditionally independent on everything but $\\pi_{t - 1}$. So we can write this simply as: $$\\alpha_{t}(\\pi_{t}) = \\ P(x_{t}|\\pi_{t})\\sum_{\\mathbf{\\pi}_{t - 1}}&#94;{}{P(\\pi_{t}|\\pi_{t - 1}})P(x_{1},\\ \\ldots,x_{t - 1},\\ \\pi_{t - 1})$$$$= \\ P(x_{t}|\\pi_{t})\\sum_{\\mathbf{\\pi}_{t - 1}}&#94;{}{P(\\pi_{t}|\\pi_{t - 1}})\\alpha_{t - 1}(\\pi_{t - 1})$$ It follows, then, that we can compute each $\\alpha_{t}$ recursively. This will eliminate the need to look at the probability of all states. This has a computation time of $O(\\text{nm}&#94;{2})$ which is far faster than doing it for each possible sequence, which is$\\ O(\\text{nm}&#94;{n})$. After computing this for each possible state at time T, we sum the probabilities to get the probability of the observed sequence. Note that $\\alpha_{1}\\left( \\pi_{1} \\right) = P(x_{1}|\\pi_{1})\\sum_{x_{0}}&#94;{}{P(\\pi_{1}|\\pi_{0}})P(\\pi_{0}) = \\ \\ P(x_{1}|\\pi_{1})P(\\pi_{1})$, where $P\\left( \\pi_{1} \\right)$ is to be considered the initial probabilities of entering the chain, predefined by the problem. Consider the following example of a fair and biased coin, where: Initial probabilities are equal, .5 for fair and .5 for biased $$\\mathbf{P = \\ }\\begin{matrix} .9 & .1 \\\\ .05 & .95 \\\\ \\end{matrix}$$$$\\mathbf{E = \\ }\\begin{matrix} .5 & .5 \\\\ .25 & .75 \\\\ \\end{matrix}$$ We observe the sequence HTHHTTHH and want to determine the probability of this sequence occurring given our HMM. First, $$\\alpha_{1}\\left( \\pi_{1} = F \\right) = \\ P\\left( x_{1} \\middle| \\pi_{1} = F \\right)P\\left( \\pi_{1} = F \\right) = \\left( 0.5 \\right)\\left( 0.5 \\right) = \\ .25$$$$\\alpha_{1}\\left( \\pi_{1} = B \\right) = P\\left( x_{1} \\middle| \\pi_{1} = B \\right)P\\left( \\pi_{1} = B \\right) = \\left( 0.25 \\right)\\left( 0.5 \\right) = \\ .135$$ So it follows that the probability of a head being observed, alone, is the sum of these partial probabilities, or .385. To determine the probability of the full sequence, we continue this recursively, considering the results of the function at the last time step. We can also simplify this using matrix multiplication. If we consider $\\mathbf{\\alpha}_{\\mathbf{t}}$ as a nx1 vector pertaining to each of the n states, we can calculate the following: $${\\mathbf{\\alpha}_{\\mathbf{t}}\\mathbf{= \\ E}}_{\\mathbf{.,j}} x \\mathbf{P'\\alpha}_{\\mathbf{t - 1}}$$ Where j refers to a row vector of the jth column pertaining to the observed value at time t . This can help to simplify the calculations. Here, x refers to component-wise multiplication. I have included a user-defined function to determine this probability recursively for the problem at hand: In [6]: forwardalgor <- function ( obs , trans , emissions , init ){ alphas <- list ( 1 , init * emissions [, obs [ 1 ]]) for ( i in 2 : ( length ( obs ) + 1 )){ alphasum <- as.numeric ( trans %*% alphas [[ i ]]) alphas [[ i + 1 ]] <- emissions [, obs [ i -1 ]] * alphasum } print ( \"Sequences:\" ) print ( alphas ) print ( \"Final Probability\" ) print ( sum ( alphas [[ length ( obs ) +1 ]])) return ( sum ( alphas [[ length ( obs ) +1 ]])) } forwardalgor ( c ( 1 , 2 , 1 , 1 , 2 , 2 , 1 , 1 ), rbind ( c ( . 9 , . 1 ), c ( . 95 , . 05 )), rbind ( c ( . 5 , . 5 ), c ( . 25 , . 75 )), c ( . 5 , . 5 )) [1] \"Sequences:\" [[1]] [1] 1 [[2]] [1] 0.250 0.125 [[3]] [1] 0.1187500 0.0609375 [[4]] [1] 0.05648438 0.08689453 [[5]] [1] 0.02976270 0.01450122 [[6]] [1] 0.014118274 0.007249905 [[7]] [1] 0.006715719 0.010331142 [[8]] [1] 0.003538630 0.005172367 [[9]] [1] 0.0018510021 0.0009050793 [[10]] [1] 0.0008782049 0.0004509265 [1] \"Final Probability\" [1] 0.002756081 0.00275608136978746 This gives us our result -- there is about a .2% chance of this observation being output by this particular HMM. This can be useful for numerous applications: consider if one was testing against n possible HMMs. We could then find which HMM is the more likely fit by considering $\\text{argmax}_{i}(P\\left( \\text{HMM}_{i} \\middle| x_{1},\\ \\ldots,x_{T} \\right))$. Bayes' Rule tells us that: $$P\\left( \\text{HMM}_{i} \\middle| x_{1},\\ \\ldots,x_{T} \\right) \\propto \\ P\\left( x_{1},\\ \\ldots,x_{T} \\middle| \\text{HMM}_{i} \\right)P(\\text{HMM}_{i})$$ If we consider our n models as equally likely, we can then determine which model is most probable given the observations simply by considering the likelihood in the fashion calculated above. That is, $\\operatorname{}\\left( P\\left( \\text{HMM}_{i} \\middle| x_{1},\\ \\ldots,x_{T} \\right) \\right) = \\text{argmax}_{i}(\\ P\\left( x_{1},\\ \\ldots,x_{T} \\middle| \\text{HMM}_{i} \\right))$. This relatively simple result and simple algorithm gives us a method of comparing and optimizing our fit. Also, since the probabilities here can become quite small, the fact that a logarithmic function is monotonically increasing means we can also compare the logarithms, in order to avoid machine zeros. The closely related, but not exactly equivalent, Forwards-Backwards algorithm attempts to find probability of a particular state of a particular time point given all of the observations, i.e. : $$P({\\pi_{k}|x}_{1},\\ \\ldots,x_{T})$$$$P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k,\\ }x_{k + 1},\\ldots,x_{T} \\right) = \\ \\frac{P\\left( x_{k + 1},\\ldots,x_{T}|{\\pi_{k},x}_{1},\\ \\ldots,x_{k} \\right)P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k} \\right)}{P\\left( x_{k + 1},\\ldots,x_{T} \\right)}$$ $$\\propto P\\left( x_{k + 1},\\ldots,x_{T}|\\pi_{k} \\right)P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k} \\right)$$ This is due to the fact that $x_{k + 1},\\ldots,x_{T}$ is conditionally independent of $x_{1},\\ \\ldots,x_{k}$ given $\\pi_{k}$ and that $P\\left( x_{k + 1},\\ldots,x_{T} \\right)$ is a multiplicative constant which does not change with a selection of $\\pi_{k}$. This means we can normalize this probability at each point to ensure that $\\sum_{}&#94;{}{P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k,\\ }x_{k + 1},\\ldots,x_{T} \\right) = 1}$. Notice that $P\\left( {\\pi_{k}|x}_{1},\\ \\ldots,x_{k} \\right)\\ \\propto \\ P\\left( x_{1},\\ \\ldots,x_{k},\\ \\pi_{k} \\right) = \\ \\alpha_{k}\\mathbf{(}\\pi_{k})$ as described earlier. Notice that $P\\left( x_{k + 1},\\ldots,x_{T}|\\pi_{k} \\right)$ is a similar proposition, but backwards. That is, we can call $$b_{k}(\\pi_{k})$$ $$= \\sum_{\\pi_{k + 1}}&#94;{}{P\\left( \\pi_{k + 1},x_{k + 1},\\ldots,x_{T}|\\pi_{k} \\right)}$$ $$= \\sum_{\\pi_{k + 1}}&#94;{}{P\\left( x_{k + 2},\\ldots,x_{T}|{x_{k + 1},\\pi}_{k + 1},\\pi_{k} \\right)P(x_{k + 1}|\\pi_{k + 1},\\pi_{k})P(\\pi_{k + 1}|\\pi_{k})}$$ $$= \\sum_{\\pi_{k + 1}}&#94;{}{P\\left( x_{k + 2},\\ldots,x_{T}|\\pi_{k + 1} \\right)P(x_{k + 1}|\\pi_{k + 1})P(\\pi_{k + 1}|\\pi_{k})}$$ This is obtained using the chain rule and the conditional independencies of the HMM. Notice now that $P\\left( x_{k + 2},\\ldots,x_{T}|\\pi_{k + 1} \\right) = \\ b_{k + 1}\\left( \\pi_{k + 1} \\right)$ and we can now see the recursive nature of this function. That is, $$b_{k}\\left( \\pi_{k} \\right) = \\ \\sum_{\\pi_{k + 1}}&#94;{}{b_{k + 1}\\left( \\pi_{k + 1} \\right)P(x_{k + 1}|\\pi_{k + 1})P(\\pi_{k + 1}|\\pi_{k})}$$ We take $b_{T}\\left( \\pi_{T} \\right) = 1$ for all states, as this can be considered the exit probability. Intuitively, this means that we are considering, for some intermediate time step, the probabilities of getting there from zero and the probabilities of \"back tracking\" from T. This gives us the total probability of an intermediate state based on our observations. The implementation is as follows: Compute the forward probabilities (by the previously stated algorithm) until time step k. For each step, normalize the results so that the probabilities sum to 1. Then, compute the backward probabilities from T to k. Normalize these at each step as well. Then, multiply the two results and normalize to get the probabilities of being in each state after k time steps. To calculate this, we may use the following formula: $${\\mathbf{b}_{\\mathbf{t}}\\mathbf{= \\ P(E}}_{\\mathbf{.,j}} x \\mathbf{b}_{\\mathbf{t + 1}}\\mathbf{)}$$ Notice that this is similar to the formula from before for the forwards portion, but the multiplications are \"flipped.\" Thus we can implement the Forward-Backwards algorithm as follows: In [7]: bkforwardalgor <- function ( obs , k , trans , emissions , init ) { alphas <- list ( 1 , init * emissions [, obs [ 1 ]] / sum ( init * emissions [, obs [ 1 ]])) betas <- list ( rep ( 1 , nrow ( trans ))) for ( i in 3 : ( k + 1 )) { alphasum <- as.numeric ( t ( trans ) %*% alphas [[ i - 1 ]]) alphas [[ i ]] <- emissions [, obs [ i - 1 ]] * alphasum alphas [[ i ]] <- alphas [[ i ]] / sum ( alphas [[ i ]]) } for ( i in 2 : ( length ( obs ) - k + 1 )) { betasum <- emissions [, obs [ length ( obs ) - i + 2 ]] * betas [[ i - 1 ]] betas [[ i ]] <- as.numeric ( trans %*% betasum ) betas [[ i ]] <- betas [[ i ]] / sum ( betas [[ i ]]) } pointalph <- alphas [[( k + 1 )]] pointbet <- betas [[( length ( obs ) - k + 1 )]] return ( list ( forward = alphas , backward = betas , probstates = pointalph * pointbet / sum ( pointalph * pointbet ))) } bkforwardalgor ( c ( 1 , 2 , 1 , 1 , 2 , 2 , 1 , 1 ), 3 , rbind ( c ( 0.9 , 0.1 ), c ( 0.95 , 0.05 )), rbind ( c ( 0.5 , 0.5 ), c ( 0.25 , 0.75 )), c ( 0.5 , 0.5 )) $forward 1 0.666666666666667 0.333333333333333 0.88 0.12 0.950682056663169 0.0493179433368311 $backward 1 1 0.493506493506494 0.506493506493506 0.493683851143735 0.506316148856265 0.50646857940524 0.49353142059476 0.505577910274165 0.494422089725835 0.493357500634704 0.506642499365296 $probstates 0.949421205954081 0.0505787940459189 So, at the third time step, we are close to 95% certain that we were in the first state (Heads), given all of the observations. This all leads us to the Baum-Welch algorithm, which is a way of estimating parameters of a HMM given a sequence of emissions. Let $\\gamma_{i}\\left( t \\right) = P\\left( \\pi_{t} \\middle| \\theta,x_{1},\\ \\ldots,x_{T} \\right)$ where $\\theta$ is the parameters of the distribution (the transition matrix, the emission matrix and the initial probabilities.) Note that we have already calculated this in the previous exercise using the forwards backwards algorithm. Further, let $$\\xi_{i,j}\\left( t \\right) = \\ P\\left( \\pi_{t}\\ ,\\pi_{t + 1}\\ \\middle| \\theta,x_{1},\\ \\ldots,x_{T} \\right) = \\frac{\\alpha_{t}\\left( \\pi_{t} = i \\right)\\mathbf{P}_{\\text{ij}}b_{t + 1}\\left( \\pi_{t + 1} = j \\right)\\mathbf{E}_{{j,x}_{t + 1}}}{\\sum_{}&#94;{}{\\alpha_{t}\\left( \\pi_{t} = i \\right)\\mathbf{P}_{\\text{ij}}b_{t + 1}\\left( \\pi_{t + 1} = j \\right)\\mathbf{E}_{{j,x}_{t + 1}}}}$$ This represents the probability of entering state i from the left, going from state i to j, exiting state j from the right, and emitting the emission at the t+1 time step from state j. All of this culminates in the joint probability of two consecutive states. Then, using an initial set of parameters we can calculate these values and update our parameters to find a local maxima. Our updates are as follows: Initial probabilities = $\\gamma_{i}\\left( 1 \\right)$. This is the expected frequency spent in state i at time 1. The new transition matrix, P' , will have entries: $$\\mathbf{P'}_{\\text{ij}} = \\frac{\\sum_{t = 1}&#94;{T - 1}{\\ \\xi_{i,j}\\left( t \\right)}\\ }{\\sum_{t = 1}&#94;{T - 1}{\\gamma_{i}\\left( t \\right)}}$$ This is the expected number of transitions from state i to j compared to the expected total number of transitions away from state i (including itself). The new emission matrix, E' , will have entries: $\\mathbf{E'}_{\\text{ij}} = \\frac{\\sum_{t = 1}&#94;{T}1_{x_{t} = j}\\gamma_{i}\\left( t \\right)}{\\sum_{t = 1}&#94;{T}{\\gamma_{i}\\left( t \\right)}}$ Where $1_{x_{t} = j}$ is an indicator function which is 1 if the emission was the jth emission and zero otherwise. This calculates the average amount of time an emission j was emitted in state i divided by the average amount of time it was in state i in general. We then iterate this process until we are within a threshold to determine the optimal parameters for the model given only an observed sequence. While I have not implemented this algorithm directly here, I have used the HMM package in R to determine this. Here is the output: In [8]: library ( HMM ) chk <- initHMM ( States = c ( \"F\" , \"B\" ), Symbols = c ( \"H\" , \"T\" ), startProbs = c ( . 5 , . 5 ), transProbs = rbind ( c ( . 9 , . 1 ), c ( . 95 , . 05 )), emissionProbs = rbind ( c ( . 5 , . 5 ), c ( . 25 , . 75 ))) baumWelch ( chk , c ( \"H\" , \"T\" , \"H\" , \"H\" , \"T\" , \"T\" , \"H\" , \"H\" )) $hmm $States 'F' 'B' $Symbols 'H' 'T' $startProbs F 0.5 B 0.5 $transProbs F B F 0.8356626 1.643374e-01 B 1.0000000 4.663436e-19 $emissionProbs H T F 0.530374 4.696260e-01 B 1.000000 8.442496e-29 $difference 0.508902752112227 0.240556572158092 0.175367150726267 0.125799629747644 0.0789866075573572 0.0430462501906644 0.0220242002384686 0.0120457248068965 0.00797054223724487 0.00648938284089056 0.00596923716449372 0.00573434947770014 0.00556884788402301 0.00541435766962771 0.00525534145586094 0.0050878669619669 0.00491154731418667 0.0047272548854208 0.00453636210810545 0.00434044024235019 0.00414111692008325 0.0039399973466488 0.00373861517021827 0.00353839967199841 0.00334065357605126 0.00314653876626508 0.00295706838232949 0.00277310422503745 0.00259535857201991 0.00242439957307494 0.00226065943795158 0.00210444467891964 0.00195594773481778 0.00181525938160172 0.00168238142175776 0.00155723923619973 0.00143969387155143 0.0013295534195225 0.00122658352013238 0.00113051688536663 0.00104106179381376 0.000957909549977724 0.000880740935187623 0.000809231701136347 0.000743057173680575 0.00068189604460203 0.000625433433944128 0.000573363306621804 0.000525390324947385 0.00048123121464055 0.000440615716505618 0.000403287189561262 0.000369002924884535 0.0003375342226381 0.000308666278318049 0.00028219791808662 0.000257941217349493 0.000235721031633014 0.000215374464079931 0.000196750289868947 0.000179708354205615 0.000164118957457245 0.000149862238244094 0.000136827563139715 0.000124912929580535 0.000114024387063381 0.000104075480274582 9.49867167999181e-05 8.66850610378905e-05 7.91034553396986e-05 7.21803686502851e-05 6.58593726685694e-05 6.00887450076778e-05 5.48210986672737e-05 5.00130368923791e-05 4.56248323759889e-05 4.16201296418831e-05 3.79656693828655e-05 3.46310335586279e-05 3.1588409960111e-05 2.88123750393403e-05 2.62796938294031e-05 2.39691357210852e-05 2.18613050690297e-05 1.99384854986017e-05 1.81844969456332e-05 1.65845644250734e-05 1.51251977163732e-05 1.37940810142869e-05 1.25799718653171e-05 1.14726085741031e-05 1.04626254579159e-05 9.54147526764379e-06 8.70135822614897e-06 7.93515712464892e-06 7.23637797002243e-06 6.59909573466095e-06 6.0179047717908e-06 5.48787350674592e-06 5.00450304905593e-06 Here, we see that the algorithm suggests changing the transition probabilities to almost never transition when in the Biased state, and to transition more frequently when in the fair state. It also suggests that the emission probabilities be heads nearly in all cases when it is biased. This is troublesome, since we knew (by our definition) that the emissions would be fair for the fair coin and biased otherwise. Still, this generalizes the problem and allows us to attempt to fit optimal HMMs to a given set of emissions. Using the Viterbi algorithm is known as \"decoding\" -- we are attempting to determine the most likely state sequence given a sequence of observed emissions. Determining the most likely state at a time step is known as \"filtering\" (when the step is the final step, T) or \"smoothing\" (for some intermediate step, k ). This is what the forward-backward algorithm was used for. Attempting to determine the best fitting parameters is known as \"training\". This is the motivation of the Baum-Welch algorithm. Together, these three algorithms provide the basis for many applications of the HMM model.","tags":"Markov Chains","url":"https://seanammirati.github.io/category/markov-chains/hidden-markov-models.html","loc":"https://seanammirati.github.io/category/markov-chains/hidden-markov-models.html"},{"title":"Heteroskedacity with Weighted Least Squares","text":"An Approach to Solving Heteroskedacity when the Variance is a Multiplicative Constant One of the most widely known assumptions of linear regression is homoskedacity , that is that the variance of the errors are constant for all of the values (they do not depend on the particular observation). Weighted least squares is a way of dealing with this problem. We will detail the intuition and reasoning behind why this is the case. Consider a scale that measures two objects. For each object, we have some randomized error. Consider the weights of two objects, first alone and then together. That is, consider the following model: $$ y_{1} = \\alpha_{1} + \\varepsilon_{1} $$$$ y_{2} = \\alpha_{2} + \\varepsilon_{2} $$$$ y_{3} = \\alpha_{1} + \\alpha_{2}\\ + \\ \\varepsilon_{3} $$ This is like a regression model, in that we can solve for the alphas (the true weights) by the method of least squares. $${\\mathbf{y = X\\alpha + \\ \\varepsilon} } $$$$\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{X} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\\\ \\end{bmatrix}$$$$ {(\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{X)}}&#94;{- 1} = \\frac{1}{3} \\begin{bmatrix} 2 & - 1 \\\\ -1 & 2 \\\\ \\end{bmatrix} $$$$ {(\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{X)})}&#94;{- 1}\\mathbf{X}&#94;{\\mathbf{T}}=\\frac{1}{3} \\begin{bmatrix} 2 & -1 & 1 \\\\ -1 & 2 & 1 \\\\ \\end{bmatrix} $$$$ \\widehat{\\mathbf{\\alpha}} = \\frac{1}{3}\\begin{bmatrix} 2y_{1} - y_{2} + y_{3} \\\\ -y_{1} + 2y_{2} + y_{3} \\\\ \\end{bmatrix} $$ Now consider that the random element is not equally variant for the two objects -- that is, that for both weights combined there is some multiplicative factor on the error $> 1$. That is, $$ \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3} \\\\ \\end{bmatrix} = \\mathbf{MVN}(0, \\Sigma) \\quad \\text{where } \\Sigma = \\begin{bmatrix} \\sigma&#94;{2} & 0 & 0 \\\\ 0 & \\sigma&#94;{2} & 0 \\\\ 0 & 0 & {k&#94;{2}\\sigma}&#94;{2} \\\\ \\end{bmatrix} $$ Note that this violates the least squares assumptions. One method to fix this issue is to transform the variables so they have equal variance, since $k$ is a numeric value $> 0$. Note that we can transform the third variable as follows: $$\\frac{y_{3}}{k} = \\frac{\\alpha_{1}}{k} + \\frac{\\alpha_{2}}{k}\\ + \\ \\frac{\\varepsilon_{3}}{k}$$ Now, $\\frac{\\varepsilon_{3}}{k}$ has variance $\\sigma&#94;{2}$. Let $$\\mathbf{y}&#94;{\\mathbf{*}} = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ y_{3}/k \\\\ \\end{bmatrix} $$ $$ \\mathbf{X}&#94;{\\mathbf{*}} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1/k & 1/k \\\\ \\end{bmatrix} $$$$ \\mathbf{\\alpha}&#94;{\\mathbf{*}} = \\begin{bmatrix} \\alpha_{1} \\\\ \\alpha_{2} \\\\ \\end{bmatrix} $$$$ \\mathbf{\\varepsilon}&#94;{\\mathbf{*}} = \\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3}/k \\\\ \\end{bmatrix} $$ Then we can solve as in the case of OLS: $${{\\widehat{\\mathbf{\\alpha}}}&#94;{*} = \\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1}{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{y}&#94;{\\mathbf{*}} }{{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}}\\mathbf{= \\ }\\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{1}{k&#94;{2}} \\\\ \\frac{1}{k&#94;{2}} & \\frac{k&#94;{2} + 1}{k&#94;{2}} \\\\ \\end{bmatrix}}$$$$\\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1} = \\frac{k&#94;{4}}{k&#94;{4} + 2k&#94;{2}}\\ \\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{- 1}{k&#94;{2}} \\\\ \\frac{- 1}{k&#94;{2}} & \\frac{k&#94;{2} + 1}{k&#94;{2}} \\\\ \\end{bmatrix}$$$$\\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1}{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{=}\\frac{k&#94;{4}}{k&#94;{4} + 2k&#94;{2}}\\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{- 1}{k&#94;{2}} & \\frac{1}{k} \\\\ \\frac{- 1}{k&#94;{2}} & \\frac{k&#94;{2} + 1}{k&#94;{2}} & \\frac{1}{k} \\\\ \\end{bmatrix}$$$${\\widehat{\\mathbf{\\alpha}}}&#94;{*}\\mathbf{=}\\frac{k&#94;{4}}{k&#94;{4} + 2k&#94;{2}}\\mathbf{\\ }\\begin{bmatrix} \\frac{k&#94;{2} + 1}{k&#94;{2}}y_{1} - \\frac{y_{2}}{k&#94;{2}} + \\frac{y_{3}}{k&#94;{2}} \\\\ \\frac{{- y}_{1}}{k&#94;{2}} + \\frac{k&#94;{2} + 1}{k&#94;{2}}y_{2} + \\frac{y_{3}}{k&#94;{2}} \\\\ \\end{bmatrix}$$$${\\widehat{\\mathbf{\\alpha}}}&#94;{*}\\mathbf{=}\\frac{k&#94;{2}}{k&#94;{4} + 2k&#94;{2}}\\mathbf{\\ }\\begin{bmatrix} \\mathbf{(}k&#94;{2} + 1)y_{1} - y_{2} + y_{3} \\\\ \\mathbf{-}y_{1}\\mathbf{+ (}k&#94;{2} + 1)y_{2} + y_{3} \\\\ \\end{bmatrix}$$ However, this is not a general solution. Consider instead the following: $$\\begin{bmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3} \\\\ \\end{bmatrix}\\mathbf{\\ \\sim\\ MVN(0, \\Sigma)\\ \\ } \\quad \\text{where } \\Sigma = \\begin{bmatrix} {a&#94;{2}\\sigma}&#94;{2} & 0 & 0 \\\\ 0 & b&#94;{2}\\sigma&#94;{2} & 0 \\\\ 0 & 0 & {c&#94;{2}\\sigma}&#94;{2} \\\\ \\end{bmatrix}$$ We could attempt to perform the same procedure, that is, divide $y_{1}$ by $a&#94;{2}$, $y_{2}$ by $b&#94;{2}$ and so on. However, this is arduous, and it would be convenient to use a matrix of weights to solve this. Let W be a matrix of the reciprocals of these constants. Using the same procedure as above, we can express $\\mathbf{y}&#94;{\\mathbf{*}}$ as $\\mathbf{W}&#94;{\\mathbf{1/2}}\\mathbf{\\ }\\mathbf{y}$ and $\\mathbf{X}&#94;{\\mathbf{*}}$ as $\\mathbf{W}&#94;{\\mathbf{1/2}}\\mathbf{X}$ Then our result is: $${\\widehat{\\mathbf{\\alpha}}}&#94;{*} = \\left( {\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{X}&#94;{\\mathbf{*}} \\right)&#94;{- 1}{\\mathbf{X}&#94;{\\mathbf{*}}}&#94;{\\mathbf{T}}\\mathbf{y}&#94;{\\mathbf{*}}$$$${\\widehat{\\mathbf{\\alpha}}}&#94;{*} = \\left( \\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{\\text{WX}} \\right)&#94;{- 1}\\mathbf{X}&#94;{\\mathbf{T}}\\mathbf{W}\\mathbf{y}&#94;{\\mathbf{*}}$$ We can see that if W = $\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & {1/k}&#94;{2} \\\\ \\end{bmatrix}\\mathbf{\\ }$ we get the same result as earlier. Code Implementation In [3]: ols_preds <- function ( X , y ) { return ( solve ( t ( X ) %*% X ) %*% t ( X ) %*% y ) } weighted_ols_preds <- function ( X , y , W ) { return ( solve ( t ( X ) %*% W %*% X ) %*% t ( X ) %*% W %*% y ) } In [6]: y <- c ( 41 , 53 , 97 ) X <- matrix ( c ( 1 , 0 , 1 , 0 , 1 , 1 ), nrow = 3 ) alphahat <- solve ( t ( X ) %*% X ) %*% t ( X ) alphahat %*% y 42 54 This calculates the predictions in the first case. In [7]: k <- 1.25 ystar <- y ystar [ 3 ] <- y [ 3 ] / k xstar <- matrix ( c ( 1 , 0 , 1 / k , 0 , 1 , 1 / k ), nrow = 3 ) alphahat <- solve ( t ( xstar ) %*% xstar ) %*% t ( xstar ) alphahat %*% ystar 41.84211 53.84211 This calculates the predictions in the second case Using the functions we get the same results: In [8]: ols_preds ( X , y ) W <- diag ( c ( rep ( 1 , 2 ), 1 / k &#94; 2 )) weighted_ols_preds ( X , y , W ) 42 54 41.84211 53.84211","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/heteroskedacity-with-weighted-least-squares.html","loc":"https://seanammirati.github.io/category/linear-models/heteroskedacity-with-weighted-least-squares.html"},{"title":"Constrained Least Squares","text":"Consider the following: We wish to constrain the parameters of a linear model to have certain properties, for example, that the last coefficient is equal to 1, or that the sum of the coefficients are equal to one. How can we estimate the Least Squares Estimates using these constraints? Thus is constrained least squares, and this article will explore the theoretical basis for choosing a selection of coefficients that will minimize the least squares given some linear constraints on the coefficients. Constrained Least Squares -- Coefficient Calculation and Simulation We want to maximize the likelihood equation of $\\vec{\\beta}$. We know that $\\vec{y}$ in this context is normally distributed with mean $\\mathbf{X}\\vec{\\beta}$ and variance $\\sigma&#94;2$. This means that the distribution of $y_i$ given a vector of values $\\vec{x_i}$ is as follows: $$f_{Y}\\left( y_i \\middle|\\ \\vec{x}_i \\right) = {(2\\pi\\sigma&#94;{2})}&#94;{- 1/2}e&#94;{\\frac{- (y_i-\\vec{x}_i \\vec{\\beta})}{2\\sigma&#94;{2}}&#94;{2}}$$ So the likelihood of $\\vec{\\beta}$ is therefore: $$ L(\\vec{\\beta}, \\sigma&#94;{2} | \\vec{y}, \\mathbf{X}) = ( 2\\pi\\sigma&#94;{2})&#94;{- \\frac{N}{2}}e&#94;{- \\frac{\\sum_{i}&#94;{N}{(y_{i}-\\vec{x}_{i} \\vec{\\beta})}}{2\\sigma&#94;{2}}&#94;{2}} $$ Taking the natural logarithm gives us the log likelihood: $$l\\left( \\vec{\\beta},\\sigma&#94;{2} \\middle| y,\\mathbf{X} \\right) = - \\frac{1}{2\\sigma&#94;{2}}\\sum_{i}&#94;{N}{{(y_{i}-\\vec{x}_{i}\\vec{\\beta})}&#94;{2}-\\frac{N}{2}\\ln\\left( \\sigma&#94;{2} \\right)+ C}= - \\frac{1}{2\\sigma&#94;{2}}\\left( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right) - \\frac{N}{2}\\ln\\left( \\sigma&#94;{2} \\right)+ C$$ Here, I have truncated the result to only those involving $\\vec{\\beta}$ and $\\sigma&#94;{2}$, as we are only interested in maximizing with respect to $\\vec{\\beta}$ . Note that I have transitioned $\\sigma&#94;{2}$ to a constant, as we are assuming the errors are iid with constant variance $\\sigma&#94;{2}$. Here, we obtain the same result as minimizing the least squares estimator, so the MLE and the least squares estimators are the same. If we were unconstrained, we would simply take the partial derivative here with respect to $\\vec{\\beta}$ to estimate $\\vec{\\beta}$ using maximum likelihood. Since $\\sigma&#94;{2}$ is unconstrained, we can find the maximum likelihood estimator directly and use this to maximize with respect to $\\vec{\\beta}$. To do this, we differentiate the above equation with respect to $\\sigma&#94;2$. Doing so yields the normal estimate for $\\sigma&#94;{2}$: $${\\widehat{\\sigma}}&#94;{2} = - \\frac{1}{N}\\sum_{i}&#94;{N}{(y_{i}-x_{i}\\vec{\\beta}&#94;{c})}&#94;{2}=\\frac{1}{N}\\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)$$ Where $\\vec{\\beta}&#94;{c}$ is our new constrained estimates. Plugging this back into the log likelihood equation yields: $$ l(\\vec{\\beta}&#94;{c} | \\vec{y},\\mathbf{\\mathbf{X}}) = - \\frac{N}{2}[{(\\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c})}&#94;{\\intercal}( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} )]&#94;{-1} ( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c})&#94;{\\intercal} \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c}) - \\frac{N}{2}\\ln( \\frac{1}{N}( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c})&#94;{\\intercal}( \\vec{y} - \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c}))+ C $$$$l\\left( \\vec{\\beta} \\middle| \\vec{y},\\mathbf{X} \\right) = - \\frac{N}{2} - \\frac{N}{2}\\ln\\left( \\frac{1}{N}\\left( \\vec{y} -\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left( \\vec{y} -\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right) \\right)+ C$$ However, we want to maximize the above equation with the constraint that: M$\\vec{\\beta}$ = $\\vec{d}$, where d is a known vector and M is an r x p matrix of rank $r < p$. This is an equality constraint. We can use the Lagrange multipliers to solve this maximization problem with such a constraint. We can see that, since the natural logarithm is a monotonically increasing function, and the division within the above logarithm is a constant, and that all other terms do not involve $\\beta$, maximizing the above equation with respect to$\\ \\vec{\\beta}&#94;{c}$ is equivalent to minimizing: $$f = \\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right)&#94;{\\intercal}\\left(\\vec{y}-\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{c} \\right) - \\vec{\\lambda}&#94;{\\intercal}\\left( M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d} \\right)$$ Where $\\vec{\\lambda}$ is the Lagrange multiplier (and so $f$ is the Lagrange function). The Lagrange multiplier is especially useful in this case, as we are trying to minimize the least square error with some constraint on the parameters. Multiplying the error by the transpose gives: $$f =\\vec{y}&#94;{\\intercal}\\vec{y} -\\vec{y}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-{{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\vec{y} + {{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-\\vec{\\lambda}&#94;{\\intercal}\\left( M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d} \\right) = =\\vec{y}&#94;{\\intercal}\\vec{y} -2{{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\vec{y} + {{\\vec{\\beta}&#94;{c}}}&#94;{\\intercal}\\mathbf{X}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-\\vec{\\lambda}&#94;{\\intercal}\\left( M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d} \\right)$$ Now we take derivatives with respect to $\\vec{\\beta}&#94;{c}$ and $\\vec{\\lambda}$ to obtain $$\\frac{\\partial{f}}{\\partial{B}}= - 2\\mathbf{X}&#94;{\\intercal}\\vec{y}+2\\mathbf{X}&#94;{\\intercal}\\mathbf{X}\\vec{\\beta}&#94;{c}-M&#94;{\\intercal}\\vec{\\lambda}$$$$\\frac{\\partial{f}}{\\partial{\\lambda}}=M\\vec{\\beta}&#94;{c}\\ –\\ \\vec{d}$$ We can now set both of these equal to zero and solve this system of equations to find the appropriate MLE estimate of $\\vec{\\beta}&#94;{c}$ . However, there is a few simplifying steps which can make the result more useable. The first is to get the expression in terms of the original least squares estimator. If we multiply the first equation above by M(${\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}$, we obtain: $$- \\ 2M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\mathbf{X}&#94;{\\intercal}\\vec{y}+\\ 2M{\\widehat{\\beta}}&#94;{c}-\\ M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = 0$$ The first term on the right is the familiar $\\vec{\\beta}$ from unconstrained OLS (I will denote this $\\vec{\\beta}&#94;{u}$). Substituting, we get $$- \\ 2M\\vec{\\beta}&#94;{u}+\\ 2M{\\widehat{\\beta}}&#94;{c}-\\ M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = 0$$$$- \\ 2M\\vec{\\beta}&#94;{u}+\\ 2M{\\widehat{\\beta}}&#94;{c}-\\ M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = 0$$$$M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}\\vec{\\lambda} = \\ 2M{\\widehat{\\beta}}&#94;{c} - \\ 2M{\\widehat{\\beta}}&#94;{u}$$ Solving for lambda: $$\\vec{\\lambda} = {(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\left\\lbrack 2M{\\widehat{\\beta}}&#94;{c} - \\ 2M{\\widehat{\\beta}}&#94;{u} \\right\\rbrack= 2{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ Plugging this back into the original equation and solving for ${\\widehat{\\beta}}&#94;{c}$, $$0= - 2\\mathbf{X}&#94;{\\intercal}\\vec{y}+2\\mathbf{X}&#94;{\\intercal}\\mathbf{X}{\\widehat{\\beta}}&#94;{c}-M&#94;{\\intercal}\\vec{\\lambda}$$$$0= - 2\\mathbf{X}&#94;{\\intercal}\\vec{y}+2\\mathbf{X}&#94;{\\intercal}\\mathbf{X}{\\widehat{\\beta}}&#94;{c}-{2(M}&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack)$$ $$\\mathbf{X}&#94;{\\intercal}\\mathbf{X}{\\widehat{\\beta}}&#94;{c}=\\mathbf{X}&#94;{\\intercal}\\vec{y}+M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ $${\\widehat{\\beta}}&#94;{c}= ({{\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\mathbf{X}}&#94;{\\intercal}\\vec{y}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ $${\\widehat{\\beta}}&#94;{c}=\\vec{\\beta}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$$ This is the desired result, the maximum likelihood estimator given the constraint M$\\vec{\\beta} =d$. Now, let's look at an example. $$\\vec{\\beta}_{p} = 0$$ Is equivalent to the following constraint. $\\vec{\\alpha}&#94;{\\intercal}\\vec{\\beta} = 0$ where $\\vec{\\alpha}$ is a vector of zeros in all but the last row, which is a 1. So, $$\\vec{\\alpha} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$ Plugging it into the derived result, we get: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha}&#94;{\\intercal}{(\\vec{\\alpha}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha})}&#94;{- 1}\\lbrack - \\ \\vec{\\alpha}'{\\widehat{\\beta}}&#94;{u}\\rbrack$$$${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha}&#94;{\\intercal}{(\\vec{\\alpha}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha})}&#94;{- 1}\\lbrack - \\ {{\\widehat{\\beta}}&#94;{u}}_{p}\\rbrack$$ Since $\\vec{\\alpha}&#94;{- 1}= \\vec{\\alpha}$ in this case, we know that $\\vec{\\alpha}&#94;{\\intercal}{(\\vec{\\alpha}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\alpha})}&#94;{- 1} = \\vec{\\alpha}&#94;{\\intercal}\\left( \\vec{\\alpha}&#94;{\\intercal}\\left(\\mathbf{\\mathbf{X}}&#94;{\\intercal}\\mathbf{X} \\right)\\vec{\\alpha} \\right) = {\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}$ which is a scalar that is the bottom right element of the $\\mathbf{\\mathbf{X}}&#94;{\\intercal}\\mathbf{X}$ matrix. So we can simplify this to: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}{\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}\\lbrack - \\ {{\\widehat{\\beta}}&#94;{u}}_{p}\\rbrack$$ Since ${\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}\\lbrack 1 - \\ {\\beta&#94;{u}}_{p}\\rbrack$ is a constant: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}-{\\mathbf{X}&#94;{\\intercal}\\mathbf{X}}_{p,p}(\\left\\lbrack \\ {\\beta&#94;{u}}_{p} \\right\\rbrack){{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}$$ So, if we compute the unconstrained ${\\widehat{\\beta}}&#94;{u}$ and ${{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}$, we can solve directly for the constrained $\\vec{\\beta}$. Now, for the second part of part a), this is equivalent to M being a vector of 1s, lets call it ϕ. Then $d=1$ and $$ \\vec{\\phi} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$ Plugging it into the derived result, we get: $${\\widehat{\\beta}}&#94;{c}={\\widehat{\\beta}}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\phi}&#94;{\\intercal}{(\\vec{\\phi}&#94;{\\intercal}({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}\\vec{\\phi})}&#94;{- 1}\\lbrack 1 - \\vec{\\phi}'{\\widehat{\\beta}}&#94;{u}\\rbrack$$ The residual sum of squares is defined as: $$\\text{RSS} = \\ {(\\vec{y} -\\widehat{y})}&#94;{\\intercal}(\\vec{y} -\\widehat{y})$$ In this case, $$ \\widehat{y} =\\mathbf{\\mathbf{X}}{\\widehat{\\beta}}&#94;{c} = \\vec{\\beta}&#94;{u}+{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}) =\\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{u}+\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack $$ So it follows that $$ \\left(\\vec{y}-\\widehat{y_{c}} \\right) =\\vec{y}- \\mathbf{\\mathbf{X}}\\vec{\\beta}&#94;{u}+\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack =\\vec{\\varepsilon} +\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack $$ where $\\vec{\\varepsilon}$ is the error in the unconstrained case. So $$\\text{RSS}_c - \\text{RSS} =\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack)$$ Each of the errors are normally distributed with mean 0 and variance sigma squared. This implies that: $$ (y - \\check{y})\\ I\\frac{1}{\\sigma}\\sim\\text{MVN}(0,I) \\text{, i.i.d} $$ Where the estimator is the estimator from the restrained equation. We can write the sum of squares of the above as: $$\\left(\\vec{y}- \\widehat{y} \\right)&#94;{\\intercal}(I\\frac{1}{\\sigma&#94;{2}})\\left(\\vec{y}- \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} \\right)&#94;{\\intercal}(I\\frac{1}{\\sigma&#94;{2}})\\left(\\vec{y}- \\check{y} \\right) + \\ \\left( \\check{y}-\\widehat{y} \\right)&#94;{\\intercal}\\left( I\\frac{1}{\\sigma&#94;{2}} \\right)\\left( \\check{y}-\\widehat{y} \\right)+ 2(\\check{y}-\\widehat{y})'\\left( I\\frac{1}{\\sigma&#94;{2}} \\right)(y -\\check{y})$$ Where$\\ {\\check{y}}_{i}\\ $is the estimator of the unconstrained model. To show this, we remove the identical identity matrices and see that: $$\\left(\\vec{y}- \\widehat{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} + \\check{y} - \\widehat{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\check{y} + \\check{y} - \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\check{y} \\right) + \\ \\left( \\check{y} - \\widehat{y} \\right)&#94;{\\intercal}\\left( \\check{y} - \\widehat{y} \\right) + 2(y - \\check{y})'\\left( \\check{y} - \\widehat{y} \\right)$$ Now, $$\\left( \\check{y} - \\widehat{y} \\right) = \\left(\\vec{y}- \\widehat{y} - \\left(\\vec{y}- \\check{y} \\right) \\right) = \\left( \\ \\left(\\vec{y}- \\widehat{y} \\right) - \\vec{\\varepsilon} \\right)$$ which we know from the above is $(\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\lbrack \\vec{d} - \\text{M}{\\widehat{\\beta}}&#94;{u}\\rbrack$)' Rewriting, we have: $$\\left(\\vec{y}- \\check{y} \\right)'\\left( \\left(\\vec{y}- \\widehat{y} \\right) - \\vec{\\varepsilon} \\right) = \\vec{\\varepsilon}'(\\mathbf{X}{{(\\mathbf{X}}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal}{(M({\\mathbf{X}&#94;{\\intercal}\\mathbf{X})}&#94;{- 1}M&#94;{\\intercal})}&#94;{- 1}\\left\\lbrack \\vec{d} - \\ M{\\widehat{\\beta}}&#94;{u} \\right\\rbrack)$$ Using the fact that $\\vec{\\varepsilon}&#94;{\\intercal}\\mathbf{X} = 0$ , we can remove this to obtain. $${\\left(\\vec{y}- \\widehat{y} \\right)&#94;{\\intercal}\\left(\\vec{y}- \\widehat{y} \\right) = \\left(\\vec{y}- \\check{y} \\right)}&#94;{\\intercal}\\left(\\vec{y}- \\check{y} \\right) + \\ \\left( \\check{y} - \\widehat{y} \\right)&#94;{\\intercal}\\left( \\check{y} - \\widehat{y} \\right)$$ Thus the original standardized normal variable can be represented as a sum of $Q_1$, and $Q_2$ where: $$Q_1 = \\left(\\vec{y}-\\check{y} \\right)&#94;{\\intercal}\\left(\\vec{y}-\\check{y} \\right)$$$$Q_2 = \\left( \\check{y}-\\widehat{y} \\right)&#94;{\\intercal}\\left( \\check{y}-\\widehat{y} \\right) $$ $Q_1$ is the sum of squared errors for the unconstrained model, and therefore has rank $n - p$. $Q_2$ has rank $p - r$, as it is the differences between the two estimators. Thus rank($Q_1$) + rank ($Q_2$) = $n-r$ which is the rank of the errors of the constrained model, since $r<p$. Thus by Cochran's Theorem, the two are independently distributed. I have also produced simulations to prove that this is the case. I create three normal random variables, two of which are related to the others linearly and randomly. I compute the beta unconstrained, and then use this to compute the beta constrained. I then produce the errors for the constrained beta, and check the correlations in 100 samples of 100 each. The mean correlation is very close to zero, which is the expected result. This is included below. In [2]: const <- vector () unconst <- vector () correl <- vector () for ( j in 1 : 100 ) { for ( i in 1 : 100 ) { y <- rnorm ( 100 ) x1 <- y + rnorm ( 100 , 0 , . 2 ) x2 <- rnorm ( 100 , 0 , . 01 ) - . 3 * y Xmat <- matrix ( c ( rep ( 1 , length ( x1 )), x1 , x2 ), ncol = 3 ) data2 <- data.frame ( y , x1 , x2 ) mod.sim.1 <- lm ( y ~ x1 + x2 , data = data2 ) constraint <- matrix ( 1 , nrow = nrow ( data2 ), ncol = 1 ) d = 1 M = matrix ( rep ( 1 , 3 ), ncol = 1 ) mod.sim.1 BetaUnconstr <- summary ( mod.sim.1 ) $ coef [, 1 ] b <- d - t ( M ) %*% BetaUnconstr a <- t ( Xmat ) %*% Xmat c <- solve ( t ( M ) %*% solve ( a ) %*% M ) const [ i ] <- b * c * b unconst [ i ] <- sum ( resid ( mod.sim.1 ) &#94; 2 ) } correl [ j ] <- cor ( const , unconst ) } correl mean ( correl ) -0.0531688267764585 -0.114464495425658 0.030905848126332 -0.0804090923830123 -0.0282214216966975 0.194785864859665 0.0830930039710045 0.0724914248601695 0.0829089181916613 0.0288906516772889 0.0279474715925631 -0.0533135707611509 0.0658332419742044 -0.0799946756911054 -0.0941441499229167 -0.024909668515204 0.0714084573952652 -0.0455394491591305 0.166193108785476 -0.077429232110221 -0.128495293842719 -0.0786313087721033 -0.135204615732129 0.00402444738581924 -0.210414921426846 -0.0466002337555276 -0.0744914482523485 0.0253724403628865 -0.144593000144071 -0.0558067022331745 0.0941802108323231 -0.0326002696533416 0.1191407256456 0.0742989188901125 0.0498008210293263 0.02980912239177 -0.132207185159464 -0.154762140608869 -0.0311445812733598 0.0213674326991292 -0.0563900846752629 0.113499475702925 -0.125719333220632 0.0434611514397736 0.202868180728299 -0.113919008868015 -0.0382369212754716 0.0491389841646633 -0.0198221517858243 0.0175352131496739 0.0264657952051942 0.119278436715872 -0.218521793800235 0.0159366944834975 0.194276086012615 -0.121180977909389 -0.165054100432551 -0.0651733446635078 -0.07680468077657 -0.0503302517530004 0.00823376132659068 0.00388666785680694 -0.00915532185917745 -0.0930607404292774 -0.171362738860478 -0.0310668331585724 -0.0210004347148357 0.0549900820264029 -0.0468877624516252 0.0868626310356565 0.0532565684252016 -0.168838788927617 0.0339257077316963 0.0762184886512164 -0.0890754231460831 -0.0647727147695691 -0.0889513202649563 0.000667073625725037 0.0509714346818621 -0.0940193584932185 -0.0839669876254321 0.106982624440368 0.145030507757848 -0.0757386310783636 0.161739345883223 0.214585243128979 -0.0764856927072532 0.0477290142665558 0.0126102486334496 -0.0741476392314046 -0.0725679361079652 -0.0807334358109478 0.0186774645788979 0.0234449247050084 0.0176989561150896 0.0516204767156866 0.120367846574953 0.0233333600272549 -0.00113490805336294 0.118557812809982 -0.00814363230874542","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/constrained-least-squares.html","loc":"https://seanammirati.github.io/category/linear-models/constrained-least-squares.html"},{"title":"HMMs: Viterbi Algorithm","text":"This investigates HMMs (Hidden Markov Models) with two possible states and six possible emissions using the viterbi algorithm. Given a Markov process with two states (state 1 and state 2) and an arbitrary number of possible observed emissions , with transition probabilities and emission probabilities given for each state, we can find the most likely sequence of states given some observed emissions using the viterbi algorithm. This algorithm is an example of dynamic programming which produces results far faster than a brute force approach. This takes less time to run than considering all possible sequences of states -- we only need to consider the state probabilities forwards, rejecting paths that are unlikely at each point. The example which the following function defaults to is the case of a fair and loaded die, each with some probability of values 1-6. The roller does not show you what die he is using, but we know that he will generally switch between the two die with some transition probs. The goal is then to find the most likely sequence of fair and loaded die given the observed sequence of emissions (for example 1,2,2,1,3). In [12]: viterbialgorithm <- function ( values , initprobstates = c ( . 5 , . 5 ), probvectorstate1 = c ( rep ( 1 / 6 , 6 )), probvectorstate2 = c ( rep ( 1 / 10 , 5 ), 1 / 2 ), Ps1s2 = 0.05 , Ps2s1 = . 1 ){ ## values (vector): a series of observed values of two emissions. ## initprobstates (vector, len 2): the probabilities of entering the system in either state. (assuming .5, .5 is the least informative) ## probvectorstate1 (vector): the emission probabilities of the first state. ## probvectorstate2 (vector): the emission probabilities of the second state. ## Ps1s2 (numeric): probability of entering state 2 from state 1. ## Ps2s1 (numeric): probability of entering state 1 from state 2. iterations <- length ( values ) initprobstate1 = initprobstates [ 1 ] initprobstate2 = initprobstates [ 2 ] P11 <- 1 - Ps1s2 P22 <- 1 - Ps2s1 if ( iterations < 1 ) { stop ( \"Length of vector must be a positive integer.\" ) } # Initializes the ret list for the first sequence. ret <- list ( c ( s0 = 0 , s1 = - Inf , s2 = - Inf , st = 0 , prevst = NA )) ## Creates the transition matrix. transmatrix <- matrix ( c ( 0 , 0 , 0 , initprobstate1 , P11 , Ps2s1 , initprobstate2 , Ps1s2 , P22 ), ncol = 3 ) maxfun <- function ( potentialstate , prevvector ){ ## potentialstate (integer): integer referring to the potential state (1 or 2) ## prevvector (vector): previous probability of states. trans <- transmatrix [, potentialstate + 1 ] mx <- max ( prevvector [ 1 : 3 ] + log ( trans )) prevst <- which.max ( prevvector [ 1 : 3 ] + log ( trans )) - 1 return ( list ( val = mx , state = prevst )) } for ( i in 1 : iterations ) { maxst1 <- maxfun ( 1 , ret [[ i ]]) maxst2 <- maxfun ( 2 , ret [[ i ]]) ekst1 <- log ( probvectorstate1 [ values [ i ]]) ekst2 <- log ( probvectorstate2 [ values [ i ]]) V1 <- maxst1 [[ 'val' ]] + ekst1 V2 <- maxst2 [[ 'val' ]] + ekst2 end <- which.max ( c ( V1 , V2 )) ret [[ i + 1 ]] <- c ( s0 = - Inf , s1 = V1 , s2 = V2 , st = end , prevst = c ( maxst1 [[ 'state' ]], maxst2 [[ 'state' ]])) } finalstate <- ret [[ iterations + 1 ]][ 'st' ] path <- c ( rep ( 0 , iterations + 1 )) path [ iterations + 1 ] <- finalstate for ( i in ( iterations ) : 2 ) { path [ i ] <- ret [[ i + 1 ]][ 5 : 6 ][ path [ i + 1 ]] } res <- as.data.frame ( ret [ -1 ]) colnames ( res ) <- c ( values ) return ( list ( steps = res , path = path )) } In [13]: observed <- c ( 6 , 5 , 1 , 1 , 6 , 6 , 4 , 5 , 3 , 1 , 3 , 2 , 6 , 5 , 1 , 2 , 4 , 5 , 3 , 6 , 6 , 6 , 4 , 6 , 3 , 1 , 6 , 3 , 6 , 6 , 6 , 3 , 1 , 6 , 2 , 3 , 2 , 6 , 4 , 5 , 5 , 2 , 3 , 6 , 2 , 6 , 6 , 6 , 6 , 6 , 6 , 2 , 5 , 1 , 5 , 1 , 6 , 3 , 1 ) results <- viterbialgorithm ( observed ) resultswords <- c ( \"Fair\" , \"Loaded\" )[ results $ path ] resultswords 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Loaded' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' 'Fair' So this tells us that given the observed die rolls, the most likely sequence of states is as shown above. This did not have to be done manually -- we can use the HMM package to do this. It will serve as a check. In [14]: require ( HMM ) mod <- initHMM ( c ( \"Fair\" , \"Loaded\" ), c ( 1 , 2 , 3 , 4 , 5 , 6 ), startProbs = c ( . 5 , . 5 ), transProbs = rbind ( c ( . 95 , . 05 ), c ( . 1 , . 90 )), emissionProbs = rbind ( c ( rep ( 1 / 6 , 6 )), c ( rep ( 1 / 10 , 5 ), 1 / 2 ))) viterbi ( mod , observed ) == resultswords ## :) TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE","tags":"Markov Chains","url":"https://seanammirati.github.io/category/markov-chains/hmms-viterbi-algorithm.html","loc":"https://seanammirati.github.io/category/markov-chains/hmms-viterbi-algorithm.html"},{"title":"Hotelling's T-Test Example","text":"Here's an example of the Hotelling's MV T-test with responses and hypothesized means. Hotelling's MV T-test allows us to use a multivariate analogue of the t-test, comparing the observed values with some hypothesized mean. In this, similar to the univariate case, we assume that the population is Multivariate Normally Distributed, that is: $$ \\vec{x} \\sim MVN(\\vec{\\mu}, \\Sigma) $$ where $\\Sigma$ is a square, symmetric matrix. This test is implemented below. In [4]: ## Hotelling's t-test for multivariate normal. prob_null <- function ( data , hypothesized_mean_vector ) { n = nrow ( data ) p = ncol ( data ) tsqobs <- hotellings_tsq_statistic ( data , hypothesized_mean_vector ) f_dist <- hotellings_f_statistic_trans ( tsqobs , n , p ) return ( 1 - pf ( f_dist [ 'f' ], f_dist [ 'df1' ], f_dist [ 'df2' ])) } hotellings_f_statistic_trans <- function ( tsqobs , n , p ) { f <- (( n - p ) / ( p * ( n - 1 ))) * tsqobs df1 <- p df2 <- n - p return ( c ( f = f , df1 = df1 , df2 = df2 )) } hotellings_tsq_statistic <- function ( data , hypothesized_mean_vector ) { sample_mean <- apply ( data , 2 , mean ) sample_covar <- cov ( data ) S_inv <- solve ( sample_covar ) n <- nrow ( data ) mu <- hypothesized_mean_vector tsqobs <- n * t ( sample_mean - mu ) %*% S_inv %*% ( sample_mean - mu ) return ( tsqobs ) } find_discriminant <- function ( data , hypothesized_mean_vector ) { sample_mean <- apply ( data , 2 , mean ) sample_covar <- cov ( data ) discriminant <- solve ( sample_covar ) %*% ( sample_mean - hypothesized_mean_vector ) return ( discriminant ) } In [5]: response_data <- matrix ( c ( 51 , 27 , 37 , 42 , 27 , 43 , 41 , 38 , 36 , 26 , 29 , 36 , 20 , 22 , 36 , 18 , 32 , 22 , 21 , 23 , 31 , 20 , 50 , 26 , 41 , 32 , 33 , 43 , 36 , 31 , 27 , 31 , 25 , 35 , 17 , 37 , 34 , 14 , 35 , 25 , 20 , 25 , 32 , 26 , 42 , 27 , 30 , 27 , 29 , 40 , 38 , 16 , 28 , 36 , 25 ), ncol = 5 ) hypothesized_mean <- c ( 30 , 25 , 40 , 25 , 30 ) response_data prob_null ( response_data , hypothesized_mean ) find_discriminant ( response_data , hypothesized_mean ) 51 36 50 35 42 27 20 26 17 27 37 22 41 37 30 42 36 32 34 27 27 18 33 14 29 43 32 43 35 40 41 22 36 25 38 38 21 31 20 16 36 23 27 25 28 26 31 31 32 36 29 20 25 26 25 f: 0.00669952528414886 0.5298893 -0.2659554 -0.6946549 0.1483265 0.3206404 From this, because the p value is less than .05, we reject the hypothesis that this data comes from a normally distributed population with mean vector: $$ \\vec{\\mu} = \\begin{bmatrix} 30 \\\\ 25 \\\\ 40 \\\\ 25 \\\\ 30 \\end{bmatrix} $$ The discriminant indicates that the third variable contributes most to the difference between the hypothesized and sample mean. In [6]: require ( MASS ) ex <- mvrnorm ( 11 , mu = hypothesized_mean , Sigma = cov ( response_data )) prob_null ( ex , hypothesized_mean ) find_discriminant ( ex , hypothesized_mean ) f: 0.684532555387565 -0.03199313 -0.07721548 0.09417471 0.09898568 -0.13109563 Now we see that a multivariate normal distribution sampled with mean equal to the hypothesized mean and Sigma equal to the covariance of the observed data provides unsignificant results -- as expected. Now, let's see the t-test for the equality of means, assuming an equal covariance matrix and sample sizes. In [7]: differences <- response_data - ex prob_null ( differences , rep ( 0 , 5 )) find_discriminant ( differences , rep ( 0 , 5 )) f: 0.00818771246890448 0.35457592 -0.09934875 -0.70226005 0.01886289 0.33948696 We see the expected result: that the two samples produce significant results -- meaning that there is evidence to reject the null hypothesis that they come from a distribution with the same mean vector.","tags":"Multivariate Analysis","url":"https://seanammirati.github.io/category/multivariate-analysis/hotellings-t-test-example.html","loc":"https://seanammirati.github.io/category/multivariate-analysis/hotellings-t-test-example.html"},{"title":"Generalized Linear Models","text":"Here we will consider the marathon runners listed in this wikipedia page . This post will consider various generalized linear models (GLMS) with different families. In [10]: timesm <- c ( 123.98 , 128.633 , 134.266 , 139.484 , 139.533 , 156.5 , 161.95 , 174.8 , 184.9 , 195.9 , 236.567 , 340.017 , 505.283 ) timesf <- c ( 139.316 , 144.893 , 149.00 , 151.083 , 170.55 , 181.5 , 192.95 , 215.483 , 233.7 , 252.733 , 314.433 , 533.133 ) agem <- c ( 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 , 100 ) agef <- c ( 35 , 40 , 45 , 50 , 55 , 60 , 65 , 70 , 75 , 80 , 85 , 90 ) agemsq <- agem &#94; 2 time <- c ( timesf , timesm ) age <- c ( agef , agem ) gender <- c ( rep ( 1 , 12 ), rep ( 0 , 13 )) marathon <- data.frame ( time , age , gender ) head ( marathon ) time age gender 139.316 35 1 144.893 40 1 149.000 45 1 151.083 50 1 170.550 55 1 181.500 60 1 First, let's do an ordinary linear regression model with a Gaussian response variable assumption. Linear Regression (Gaussian) In [3]: lin1 <- lm ( timesm ~ agem , data = marathon ) lin2 <- lm ( timesm ~ agem + agemsq , data = marathon ) summary ( lin1 ) summary ( lin2 ) Call: lm(formula = timesm ~ agem, data = marathon) Residuals: Min 1Q Median 3Q Max -70.80 -47.41 -15.95 28.83 149.61 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -89.203 62.054 -1.438 0.17841 agem 4.449 0.910 4.889 0.00048 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 63.52 on 11 degrees of freedom Multiple R-squared: 0.6848, Adjusted R-squared: 0.6562 F-statistic: 23.9 on 1 and 11 DF, p-value: 0.0004801 Call: lm(formula = timesm ~ agem + agemsq, data = marathon) Residuals: Min 1Q Median 3Q Max -47.811 -15.503 5.468 19.742 40.354 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 498.57207 98.79145 5.047 0.000502 *** agem -14.89307 3.13305 -4.754 0.000776 *** agemsq 0.14557 0.02335 6.233 9.72e-05 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 30.14 on 10 degrees of freedom Multiple R-squared: 0.9355, Adjusted R-squared: 0.9226 F-statistic: 72.49 on 2 and 10 DF, p-value: 1.118e-06 Based on the summary, it appears that the quadratic term is quite important, it increases the R-squared significantly. So there does appear to be a quadratic effect of age on the time (or at the very least, a non-linear one). In [4]: lin3 <- lm ( time ~ age + gender ) summary ( lin3 ) Call: lm(formula = time ~ age + gender) Residuals: Min 1Q Median 3Q Max -74.18 -42.85 -14.10 26.96 181.20 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -104.3254 48.9768 -2.130 0.0446 * age 4.6801 0.6979 6.706 9.68e-07 *** gender 35.0534 25.7576 1.361 0.1873 --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 64.15 on 22 degrees of freedom Multiple R-squared: 0.6749, Adjusted R-squared: 0.6454 F-statistic: 22.84 on 2 and 22 DF, p-value: 4.285e-06 Here, the model shows that gender has some part in determining the time (in seconds) of completion, but it is not significant. If the coefficient is correct, it means that, holding age constant, a man will have 35.0534 faster time (or a woman will take 35.0534 additional seconds) to finish the race. Comparing the intercepts in the models shows a pretty large difference in the predictions. The reason why the male-only intercept is different from the intercept in the second model is because the second model is not considering the interaction of age and gender. When you account for this in the model, the intercepts will be the same. The gender coefficient in the model assumes that you are holding everything else constant, that is, the difference between a 35 year old male and female, for instance. When you consider the interaction term as well, this problem is eliminated. Gamma Distribution In [5]: gammalin1 <- glm ( timesm ~ agem , family = \"Gamma\" ) summary ( gammalin1 ) Call: glm(formula = timesm ~ agem, family = \"Gamma\") Deviance Residuals: Min 1Q Median 3Q Max -0.169540 -0.067254 0.001897 0.092610 0.116811 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 1.275e-02 5.748e-04 22.17 1.76e-10 *** agem -1.057e-04 6.839e-06 -15.46 8.30e-09 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for Gamma family taken to be 0.009790868) Null deviance: 2.42365 on 12 degrees of freedom Residual deviance: 0.10997 on 11 degrees of freedom AIC: 116.42 Number of Fisher Scoring iterations: 4 The link function here is the canonical link function for the Gamma distribution, which is the inverse ($\\frac{1}{\\mu}$). All variables come up as significant. The model can be written as follows $Y = X\\beta + \\epsilon$ where Y follows a Gamma distribution with link function $g(\\mu) = \\frac{1}{\\mu}$ Inverse-Gaussian distribution The exponential family has the following form: $$ f(y,\\theta ,\\phi ) = e&#94;{\\frac{y \\theta - b(\\theta )}{a(\\phi )} + c(y, \\phi)} $$ The inverse-Gaussian distribution is defined as: $$ f(y) = \\exp (-\\frac{\\lambda (y - \\mu )&#94;2)}{2\\mu &#94;2y} - \\frac{1}{2} \\log (\\frac{2\\pi y&#94;3}{\\lambda})) $$ $$ f(y) = \\exp (-\\frac{\\lambda (y&#94;2 - 2y\\mu + \\mu &#94;2)}{2\\mu &#94;2y} - \\frac{1}{2} \\log (\\frac{2\\pi y&#94;3}{\\lambda})) $$ $$ f(y) = \\exp (\\lambda [-\\frac{y}{2\\mu &#94;2} + \\frac{1}{\\mu}] - \\frac{\\lambda }{2y} - \\frac {1}{2}\\log(\\frac {2\\pi y&#94;3}{\\lambda})) $$ $$ f(y) = \\exp (\\frac {[-\\frac{y}{2\\mu &#94;2} + \\frac{1}{\\mu}]}{\\frac {1}{\\lambda}} - \\frac{\\lambda }{2y} - \\frac {1}{2}\\log(\\frac {2\\pi y&#94;3}{\\lambda})) $$ This is in the exponential form, that is, $$\\theta = - \\frac {1} {2 \\mu &#94;2}$$ $$a(\\phi) = \\frac {1}{\\lambda} = { \\phi}$$ $$b(\\theta) = - \\frac {1}{\\mu} = -\\sqrt{2 \\theta}$$ $$c(y,\\theta) = - \\frac{\\lambda}{2y} - \\frac{1}{2}\\log(\\frac{2\\pi y&#94;3}{\\lambda}) = - \\frac{1}{y\\phi} - \\frac{1}{2}\\log(2\\pi \\phi y&#94;3)$$ $b'(\\theta) = {\\sqrt {\\frac{2}{\\theta }}} = \\mu$ , which is $\\text{E}(Y)$. The canonical link function is $g(\\text{E}(Y))$ = $\\theta$. So, the canonical link is proportional to $\\frac {1}{\\mu &#94;2}$, which is equivalent to $X\\beta$. The model is then $Y = X\\beta + \\epsilon$ where Y follows a inverse Gaussian distribution with link function $g(\\text{E}(Y))$ = 1/$\\mu &#94;2$ In [7]: invgauslin1 <- glm ( timesm ~ agem , family = \"inverse.gaussian\" ) summary ( invgauslin1 ) Call: glm(formula = timesm ~ agem, family = \"inverse.gaussian\") Deviance Residuals: Min 1Q Median 3Q Max -0.0048497 -0.0023005 -0.0000681 0.0005923 0.0123111 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 9.958e-05 3.892e-06 25.59 3.75e-11 *** agem -9.617e-07 4.179e-08 -23.01 1.18e-10 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for inverse.gaussian family taken to be 2.180685e-05) Null deviance: 0.01111018 on 12 degrees of freedom Residual deviance: 0.00020857 on 11 degrees of freedom AIC: 102.69 Number of Fisher Scoring iterations: 4 Both are quite difficult to interpret by themselves, as the response variables are transformed via the link function. The predictions for the gamma are as follows: In [8]: predict ( gammalin1 ) pred1 <- 1 / predict ( gammalin1 ) pred2 <- 1 / (( predict ( invgauslin1 ))) &#94; . 5 pred3 <- predict ( lin1 ) sum (( pred1 - timesm ) &#94; 2 ) sum (( pred2 - timesm ) &#94; 2 ) sum (( pred3 - timesm ) &#94; 2 ) sum (( predict ( lin2 ) - timesm ) &#94; 2 ) 1 0.00904503120484234 2 0.00851640593952094 3 0.00798778067419954 4 0.00745915540887814 5 0.00693053014355673 6 0.00640190487823533 7 0.00587327961291393 8 0.00534465434759253 9 0.00481602908227113 10 0.00428740381694972 11 0.00375877855162832 12 0.00323015328630692 13 0.00217290275566411 6398.61022699518 5598.12999367698 44389.3243635469 9086.8812639335 Here, we can see the sum squared error of the transformed predictions from the models. We can see that the model using the gamma distribution does an exceptional job at predicting the response variable, time. It is better than both the linear and quadratic simple linear regression models. The inverse-gaussian performs the best out of the three candidate distributions. Below, I do the same thing, except using the identity function. This produces more easily interpretable coefficients, but they are less efficient at predicting the model. That is, when we take the sum of squares, the identity links are less successful at determining the response. In [9]: gammalin2 <- glm ( timesm ~ agem , family = \"Gamma\" ( link = \"identity\" )) summary ( gammalin2 ) invgauslin2 <- glm ( timesm ~ agem , family = \"inverse.gaussian\" ( link = identity )) summary ( invgauslin2 ) predict ( gammalin2 ) predict ( invgauslin2 ) sum (( predict ( gammalin2 ) - timesm ) &#94; 2 ) sum (( predict ( invgauslin2 ) - timesm ) &#94; 2 ) Call: glm(formula = timesm ~ agem, family = Gamma(link = \"identity\")) Deviance Residuals: Min 1Q Median 3Q Max -0.21209 -0.18853 -0.09241 0.08858 0.54268 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) -8.3100 38.5463 -0.216 0.833256 agem 3.1548 0.6898 4.574 0.000799 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for Gamma family taken to be 0.06523248) Null deviance: 2.42365 on 12 degrees of freedom Residual deviance: 0.60806 on 11 degrees of freedom AIC: 138.73 Number of Fisher Scoring iterations: 9 Call: glm(formula = timesm ~ agem, family = inverse.gaussian(link = identity)) Deviance Residuals: Min 1Q Median 3Q Max -0.012827 -0.011721 -0.006183 0.004403 0.032956 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 10.6841 32.6336 0.327 0.749516 agem 2.7958 0.6252 4.472 0.000944 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for inverse.gaussian family taken to be 0.000289055) Null deviance: 0.0111102 on 12 degrees of freedom Residual deviance: 0.0024039 on 11 degrees of freedom AIC: 134.47 Number of Fisher Scoring iterations: 10 1 102.107169679271 2 117.881054509177 3 133.654939339083 4 149.428824168989 5 165.202708998895 6 180.976593828801 7 196.750478658707 8 212.524363488613 9 228.29824831852 10 244.072133148426 11 259.846017978332 12 275.619902808238 13 307.16767246805 1 108.535713328251 2 122.514520425568 3 136.493327522885 4 150.472134620202 5 164.45094171752 6 178.429748814837 7 192.408555912154 8 206.387363009471 9 220.366170106788 10 234.344977204105 11 248.323784301422 12 262.302591398739 13 290.260205593373 52728.0806590428 58577.3835961494","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/generalized-linear-models.html","loc":"https://seanammirati.github.io/category/linear-models/generalized-linear-models.html"},{"title":"MANOVA","text":"Here we will be doing MANOVA for the dataset below. We want to know if the means of tear, gloss and opacity are statistically significantly different from one another depending on the rate groups -- Low or High. Data from manova example: Krzanowski (1998, p. 381) In evaluating MANOVA (Multivariate Analysis of Variance), we can consider a few different approaches to estimating the \"size\" of deviance from the between (the matrix $H$) and the within (the matrix $E$) distances. Each matrix represents something similar to the univariate ANOVA example, but we are now comparing the differences in matrices rather than in vectors. This makes the task more difficult, as we cannot simply use the F-distribution as a comparison of squared differences -- our results will also look like matrices, not single numbers. There have been four proposed solutions to this problem, named after Wilks, Roys, Pillai and Hotelling respectively. Each of these attempts to quantify the magnitude of the between matrix divided by the within matrix using the determinant or the eigenvalues of the product of these two matrices. If this value is greater than a certain threshold, we conclude that the groups are different, via a similar intuition with ANOVA. However, unlike ANOVA, no such simple distribution exists to describe the two error matrices. We also implement here the F approximations for each of these statistics to quantify our results. In [2]: return_MANOVA_results <- function ( dataset , group_column ) { F_dists <- find_approx_F_dist ( dataset , group_column ) results <- 1 - sapply ( X = F_dists [ c ( 'Wilks' , 'Roy' , 'Pillai' , 'Hotelling' )], FUN = function ( obj ) pf ( obj [ 1 ], obj [ 2 ], obj [ 3 ])) results <- rbind ( p_f = results , statistic = F_dists $ untransformed [ 1 : 4 ], data.frame ( F_dists [ c ( 'Wilks' , 'Roy' , 'Pillai' , 'Hotelling' )], row.names = c ( 'F Approx.' , 'df1' , 'df2' ))) return ( results ) } find_approx_F_dist <- function ( dataset , group_column ) { # dataset (dataframe): dataset with groupings as integer values. # group_column (string): string name of column with group entries, integer values. MANOVA_stat_lst <- find_MANOVA_statistics ( dataset , group_column ) Vh <- MANOVA_stat_lst [ 'n_groups' ] - 1 p <- ncol ( dataset ) - 1 Ve <- nrow ( dataset ) - Vh - 1 s <- min ( Vh , p ) Wilks_approx_F <- FforWilks ( Lambda = MANOVA_stat_lst [ 'Wilks' ], Vh = Vh , p = p , Ve = Ve ) Roy_approx_F <- FBoundforRoy ( R = MANOVA_stat_lst [ 'Roys' ], Vh = Vh , p = p , Ve = Ve ) Pillai_approx_F <- FapproxforPillai ( P = MANOVA_stat_lst [ 'Pillai' ], Vh = Vh , p = p , Ve = Ve , s = s ) Hotelling_approx_F <- FapproxforHotelling ( H = MANOVA_stat_lst [ 'Hotelling' ], Vh = Vh , p = p , Ve = Ve ) return ( list ( untransformed = MANOVA_stat_lst , Wilks = Wilks_approx_F , Roy = Roy_approx_F , Pillai = Pillai_approx_F , Hotelling = Hotelling_approx_F )) } find_MANOVA_statistics <- function ( dataset , group_column ){ # dataset (dataframe): dataset with groupings as integer values. # group_column (string): string name of column with group entries, integer values. grouped_ds_lst <- lapply ( 1 : length ( unique ( dataset [, group_column ])), function ( i ) dataset [ dataset [, group_column ] == i , ! names ( dataset ) %in% c ( group_column )]) H <- Reduce ( \"+\" , lapply ( grouped_ds_lst , function ( g ) hypothesis_iv ( g , dataset ))) E <- Reduce ( \"+\" , lapply ( grouped_ds_lst , function ( g ) error_iv ( g , dataset ))) Einv <- solve ( E ) HoverE <- H %*% Einv eigen_det <- eigen ( HoverE ) ev <- eigen_det $ values Wilks <- 1 / ( prod ( 1 + ev )) Roys <- ( max ( ev )) / ( 1 + max ( ev )) Pillai <- sum ( ev / ( 1 + ev )) Hotelling <- sum ( ev ) return ( c ( Wilks = Wilks , Roys = Roys , Pillai = Pillai , Hotelling = Hotelling , n_groups = length ( grouped_ds_lst ))) } hypothesis_iv <- function ( group , dataset ) { A <- sapply ( group , mean ) B <- sapply ( dataset [, 1 : ( ncol ( dataset ) - 1 )], mean ) dif <- A - B return (( dif %*% t ( dif )) * nrow ( group )) } error_iv <- function ( group , dataset ) { A <- sapply ( group , mean ) mattimestranspose <- function ( A ) return ( A %*% t ( A )) mylist <- lapply ( 1 : nrow ( group ), function ( i ) mattimestranspose ( t ( as.matrix ( group [ i ,] - A )))) a1 <- Reduce ( '+' , mylist ) return ( a1 ) } FforWilks <- function ( Lambda , Vh , p , Ve ) { t <- ((( p &#94; 2 ) * ( Vh &#94; 2 ) - 4 ) / (( p &#94; 2 ) + ( Vh &#94; 2 ) - 5 )) &#94; . 5 df1 <- p * Vh w <- Ve + Vh - (( p + Vh + 1 ) / 2 ) df2 <- w * t - ((( p * Vh ) - 2 ) / 2 ) Fapprox <- (( 1 - ( Lambda &#94; ( 1 / t ))) / ( Lambda &#94; ( 1 / t ))) * ( df2 / df1 ) return ( c ( Fapprox = Fapprox , df1 = df1 , df2 = df2 )) } FBoundforRoy <- function ( R , Vh , p , Ve ) { v1 <- ( Ve - p - 1 ) / 2 v2 <- ( p - Vh - 1 ) / 2 return ( c ( Fapprox = R * v1 / v2 , df1 = 2 * v1 + 2 , df2 = 2 * v2 + 2 )) } FapproxforPillai <- function ( P , Vh , p , Ve , s ) { N <- ( Ve - Vh + s ) / 2 m <- ( abs ( Vh - p ) - 1 ) / 2 num <- ( 2 * N + s + 1 ) * P den <- ( 2 * m + s + 1 ) * ( s - P ) Fapprox <- num / den df1 <- s * ( 2 * m + s + 1 ) df2 <- s * ( 2 * N + s + 1 ) return ( c ( Fapprox = Fapprox , df1 = df1 , df2 = df2 )) } FapproxforHotelling <- function ( H , Vh , p , Ve ) { a <- p * Vh B <- (( Ve + Vh - p - 1 ) * ( Ve - 1 )) / (( Ve - p - 3 ) * ( Ve - p )) b <- 4 + (( a + 2 ) / ( B - 1 )) cnum <- a * ( b - 2 ) cden <- b * ( Ve - p + 1 ) Fapprox <- ( H * cden ) / cnum return ( c ( Fapprox = Fapprox , df1 = a , df2 = b )) } In [3]: tear <- c ( 6.5 , 6.2 , 5.8 , 6.5 , 6.5 , 6.9 , 7.2 , 6.9 , 6.1 , 6.3 , 6.7 , 6.6 , 7.2 , 7.1 , 6.8 , 7.1 , 7.0 , 7.2 , 7.5 , 7.6 ) gloss <- c ( 9.5 , 9.9 , 9.6 , 9.6 , 9.2 , 9.1 , 10.0 , 9.9 , 9.5 , 9.4 , 9.1 , 9.3 , 8.3 , 8.4 , 8.5 , 9.2 , 8.8 , 9.7 , 10.1 , 9.2 ) opacity <- c ( 4.4 , 6.4 , 3.0 , 4.1 , 0.8 , 5.7 , 2.0 , 3.9 , 1.9 , 5.7 , 2.8 , 4.1 , 3.8 , 1.6 , 3.4 , 8.4 , 5.2 , 6.9 , 2.7 , 1.9 ) rate <- gl ( 2 , 10 , labels = c ( \"Low\" , \"High\" )) Y <- data.frame ( cbind ( tear , gloss , opacity , rate )) return_MANOVA_results ( Y , 'rate' ) Wilks Roy Pillai Hotelling p_f 0.002273044 0.05409917 4.282064e-04 0.001220981 statistic 0.413619230 0.58638077 5.863808e-01 1.417682561 F Approx. 7.560973658 8.20933078 9.451217e+00 8.641112752 df1 3.000000000 16.00000000 3.000000e+00 3.000000000 df2 16.000000000 3.00000000 2.000000e+01 16.000000000 We see that all four methods approximately agree that this would be unlikely if the two groups followed the same Multivariate Normal Distribution.","tags":"Multivariate Analysis","url":"https://seanammirati.github.io/category/multivariate-analysis/manova.html","loc":"https://seanammirati.github.io/category/multivariate-analysis/manova.html"},{"title":"To T-Test or Not to T-Test","text":"An Exploration of Using Bayesian Principles to Generalize the T-Test Procedure Standard T-Test Consider the t-test of equality of means. Using the standard procedure for a t-test: In [ ]: test_equality_means <- function ( norm_vec1 , norm_vec2 ){ sx <- var ( norm_vec1 ) sy <- var ( norm_vec2 ) xbar <- mean ( norm_vec1 ) ybar <- mean ( norm_vec2 ) n = length ( norm_vec1 ) m = length ( norm_vec2 ) tstat <- ( xbar - ybar ) / ( sqrt (( 1 / n + 1 / m ) * (( n - 1 ) * sx + ( m - 1 ) * sy ) / ( n + m - 2 ))) p_tstat <- pt ( tstat , n + m - 2 ) return ( list ( tstat = tstat , prob = p_tstat )) } In [2]: vec1 <- c ( 120 , 107 , 110 , 116 , 114 , 111 , 113 , 117 , 114 , 112 ) vec2 <- c ( 110 , 111 , 107 , 108 , 110 , 105 , 107 , 106 , 111 , 111 ) t_test_res = test_equality_means ( vec1 , vec2 ) t_test_res $tstat 3.48432421316996 $prob 0.998676366290831 We can see that the t-test gives us a significant result -- the two means would have a difference smaller than the observed difference with probability r t_test_res$prob . We then would reject the null hypothesis with 95% confidence (or even 99% confidence.) Bayesian Approach Now let's try to solve this problem with our Bayesian hats! Assuming that we have a prior distribution of $\\mu$ and $\\sigma&#94;2$ such that $x \\mid \\mu, \\sigma&#94;2 \\sim N(\\mu, \\sigma&#94;2)$ , we want the posterior $f(\\mu \\mid x_1, x_2, ..., x_n, \\sigma&#94;2)$ . Assuming a prior of $\\mu \\mid \\sigma&#94;2 \\sim N(\\beta, \\frac{\\sigma}{\\sqrt{n}})$ and $\\frac{S}{\\sigma&#94;2} \\sim \\chi&#94;2(k)$, we start with prior parameters $\\beta_0, n, S_0, k$ and update for each value of x. In [1]: simulatepostprob <- function ( vec , prior_params = c ( n = 0 , S = 0 , k = -1 , B = 0 )){ nnew = prior_params [ 'n' ] Snew = prior_params [ 'S' ] knew = prior_params [ 'k' ] Bnew = prior_params [ 'B' ] lst <- list () for ( i in 1 : length ( vec )) { obs <- vec [ i ] ninit = nnew Binit = Bnew Sinit = Snew kinit = knew knew = kinit + 1 nnew = ninit + 1 Bnew <- ( ninit * Binit + obs ) / nnew Snew <- Sinit + ninit * Binit &#94; 2 + obs &#94; 2 - nnew * Bnew &#94; 2 if ( knew == 0 ) { sampchi <- 0 sigma <- 0 } else { ## We randomly sample a value for sigma|x first sampchi <- sum ( rnorm ( knew , 0 , 1 ) &#94; 2 ) sigma <- sqrt ( Snew / sampchi ) } ## Using sigma, we sample for mu|x, sigma. mu <- rnorm ( 1 , obs + Bnew , sigma / sqrt ( nnew )) lst [[ i ]] <- mu } return ( unlist ( lst )) } rpost <- function ( n , vec , prior_params = c ( n = 0 , S = 0 , k = -1 , B = 0 )){ replicate ( n , expr = simulatepostprob ( vec )[ length ( vec )]) } In [3]: simulatepostprob ( vec = vec1 ) - simulatepostprob ( vec = vec2 ) 20 -0.503416712324565 1.45988251782074 17.6073011837844 7.22926620488519 9.24411515038176 8.35747543643981 15.4506594648444 8.61721035342939 6.22213384120991 By iterating for each observation, we arrive at a posterior distribution for $\\mu \\mid \\sigma&#94;2$ and $\\frac{S}{\\sigma&#94;2}$. We then sample randomly from these theoretical distributions to sample from the posterior distributions. If we replicate this many times, we will be able to estimate the differences between the posterior distributions of the two vectors. Using uninformative prior gives similar results to frequentist t-test method In [4]: sum ( rpost ( 1000 , vec1 ) > rpost ( 1000 , vec2 )) / 1000 0.999 We can then use our prior information about the sample to produce a mixed, updated result. For instance, with prior parameters: In [5]: prior_params = c ( n = 10 , S = 2 , k = 9 , B = 30 ) We can see our posterior: In [6]: sum ( rpost ( 1000 , vec1 , prior_params ) > rpost ( 1000 , vec2 , prior_params )) / 1000 0.999","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/t test or not t test.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/t test or not t test.html"},{"title":"Bayesian Analysis: Poisson Discrete","text":"A Simple Illustration of Bayesian Probability with a Binomial Distribution Suppose we have a discrete set of possible rates of occurrence $l$ of an event E with some prior beliefs about which rate is the true rate of occurrence, $\\lambda$, of the event E. That is, we have a probability mass function that takes discrete values within $l$ and maps them to a discrete probability space, where $$ P(\\lambda=w) = 0, \\forall{w \\notin l} $$$$ \\sum_{w}{P(\\lambda=w)} = 1, w \\in l $$ We then observe many events (that is, $n$ is very large, and $p$ is very small) and daily rates for these events and wish to update our prior beliefs accordingly. We observe a daily rate of $r$, and wish to update our probabilities accordingly. From Bayes' Rule, we know that $$ P(p=w | r) = \\frac{P(r | \\lambda=w)P(\\lambda=w)}{P(r)} $$ We can now use this information to calculate our posterior of $\\lambda$ by using the fact that this situation (the probability events with a constant rate) follows a Poisson distribution. Using this for $P(r|\\lambda=w)$, we can now solve for our posterior for each defined value of $\\lambda$. This is implemented below: In [4]: ## Poisson Discrete Updater ## Basic Bayesian analysis on discrete random variables. ## Given our data follows a Poisson distribution, finds the probability of various ## lambdas given an observed value using discrete probabilities for discrete lambda values. ## In this case I have generalized this to where lambda_new = lambda * t where t is the number of days. ## Consider that the number of events, k, in t days follows a Poisson distribution with lambda = t*l. ## For example, consider that the number of accidents in t days is distributed Poisson(t*l) ## Given prior beliefs about various l values (can be seen as the daily rate), we find the posterior distribution ## of these lambdas given new information. # Variables: ## pos_lambda (vector): a vector of hypothesized lambdas to check in posterior distribution. ## prior_probs (vector): a vector of prior probabilities for the hypothesized lambdas ## X (dataframe): a dataframe of new observed values, where the first column is the total number of events and t is the ## total number of days. For instance k=4, t=2 means there were two accidents per day, or 4 accidents in two days. k is the first column and t ## the second. ManualPoisson <- function ( k , t , l ){ # Same as dpois(x=k,lambda = l*t), or explictly return ((( t * l ) &#94; k ) * exp ( - t * l ) / factorial ( k )) } PosteriorFinder <- function ( k , t , plam , pp ){ # Finds the posterior and multiplicative constant for given probabilities and values. const <- sum ( mapply ( function ( l , p ) ManualPoisson ( k , t , l ) * p , plam , pp )) post <- mapply ( function ( l , p ) ManualPoisson ( k , t , l ) * p / const , plam , pp ) return ( list ( const = const , post = post )) } find_posterior <- function ( pos_lambda , prior_probs , X ){ ## sanity checks if ( sum ( pos_lambda > 0 ) != length ( pos_lambda )) { stop ( \"Lambdas must all be greater than zero.\" ) } if ( sum ( prior_probs ) != 1 ) { stop ( 'Priors must sum to 1, as they are probabilities.' ) } if ( ncol ( X ) != 2 ) { stop ( \"X can only have two columns (number of incidents and time)\" ) } if ( length ( pos_lambda ) != length ( prior_probs )) { stop ( \"The lambda vector and probability vector must be the same length.\" ) } ## Initialize probabilities as prior probabilities. probs <- prior_probs # Updates the posteriors for each observed value for ( i in nrow ( X )) { k <- X [ i , 1 ] t <- X [ i , 2 ] probs <- PosteriorFinder ( k , t , pos_lambda , probs ) $ post } posterior = probs return ( posterior ) } Example : Allergic Reactions Consider that we believe that people in most counties will have a severe allergic reaction around 1.5 times per day. We have an expert who has just moved to XYZ county who thinks that this is not the case in the county. He wants to test this hypothesis on stretches of the last two weeks, where he believes we have seen a lower number of allergic reactions. There was 12 incidents in the first 6 days, and zero in the last seven. Suppose that we know (by some empirical estimation) that counties in the US approximately have probability mass function: \\begin{equation} P(\\lambda = w) = \\begin{cases} .1 ,& \\text{if } w=0.5\\\\ .2 ,& \\text{if } w=1\\\\ .3 ,& \\text{if } w=1.5\\\\ .2 ,& \\text{if } w=2\\\\ .15 ,& \\text{if } w=2.5\\\\ .05 ,& \\text{if } w=4\\\\ 0, & \\text{otherwise} \\end{cases} \\end{equation} We believe that the distribution of the number of severe allergic reactions follows a Poisson distribution with rate $\\lambda$. That is, the rate is constant in a given county, and each event occurs independently of one another (i.e., one person having an allergic reaction does not effect the probability of another having one). Given these assumptions, we want to check the probability of the rates given our new data. In [5]: poslam_example <- c ( . 5 , 1 , 1.5 , 2 , 2.5 , 4 ) probs_example <- c ( . 1 , . 2 , . 3 , . 2 , . 15 , . 05 ) X_example <- data.frame ( k = c ( 12 , 6 ), t = c ( 0 , 7 )) find_posterior ( poslam_example , probs_example , X_example ) 0.14075345211024 0.544049403057648 0.280702619102117 0.031750965697214 0.00274313760334297 4.22429437384091e-07 What is the conclusion? The expert seems to be on to something -- there appears to be more evidence at the current time that lambda is actually 1 rather than 1.5. More trials would find tune this result further.","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/Poisson Discrete.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/Poisson Discrete.html"},{"title":"Bayesian Analysis: Binomial Discrete","text":"A Simple Illustration of Bayesian Probability with a Binomial Distribution Suppose we have a discrete set of possible probabilities $q$ of an event E with some prior beliefs about which probability is the true probability of the event E. That is, we have a probability mass function that takes discrete values within $q$ and maps them to a discrete probability space, where $$ P(p=w) = 0, \\forall{w \\notin q} $$$$ \\sum_{w}{P(p=w)} = 1, w \\in q $$ We then observe some independent events and wish to update our prior beliefs accordingly. We observe $k$ occurences over $n$ trials, and wish to update our probabilities accordingly. From Bayes' Rule, we know that $$ P(p=w | k, n) = \\frac{P(k | p=w, n)P(p=w|n)}{P(k|n)} $$ We can now use this information to calculate our posterior of $p$ by using the fact that independent events occuring with some probability $p$ follow a Bernoulli distribution, and so the total number of events follow a binomial distribution. Using this for $P(n, k|p=w)$, we can now solve for our posterior for each defined value of p. This is implemented below: Code to Find Posterior Distribution Given Binomial Prior In [1]: ## Finds posterior distribution of observed values assuming that k follows a binomial distribution. p is discrete in this case. numerator <- function ( k , n , posprobs , probsofprobs ){ ## finds the numerator -- ie P(k|p)P(p) ## k (integer) : the total number of observed successes. ## n (integer) : the total number of observed trials. ## posprobs (vector): possible p values associated with the random variable k. ## probsofprobs (vector): this is our prior, mapping a probability to each possible probability function ( p ){ ## For a particular p, this will find the posterior probability of p given observed values. This is a function generator -- given ## values of k and n, we can then find the probability of any particular p. x <- posprobs [ p ] px <- probsofprobs [ p ] res <- ( x &#94; k ) * (( 1 - x ) &#94; ( n - k )) * px return ( res ) } } denominator <- function ( k , n , posprobs , probsofprobs ){ ## Finds the denominator, the multiplicative constant. Same for all values of p. P(k) sum ( sapply ( X = 1 : length ( posprobs ), FUN = numerator ( k , n , posprobs , probsofprobs ))) } probabilityfinder <- function ( k , n , p , posprobs , probsofprobs ){ ## Finds the posterior probability of a particular p value. numerator ( k , n , posprobs , probsofprobs )( p ) / denominator ( k , n , posprobs , probsofprobs ) } posterior <- function ( k , n , posprobs , probsofprobs ){ ## Finds the posterior probability mass function of the p values. sapply ( X = 1 : length ( posprobs ), FUN = function ( p ) probabilityfinder ( k , n , p , posprobs , probsofprobs )) } log_transf_numerator <- function ( k , n ){ ## finds the numerator -- ie P(k|p)P(p), using log transform to avoid machine zeros ## k (integer) : the total number of observed successes. ## n (integer) : the total number of observed trials. ## posprobs (vector): possible p values associated with the random variable k. ## probsofprobs (vector): this is our prior, mapping a probability to each possible probability function ( p ){ ## For a particular p, this will find the posterior probability of p given observed values. This is a function generator -- given ## values of k and n, we can then find the probability of any particular p. x <- posprobs [ p ] px <- probsofprobs [ p ] mx <- max ( sapply ( posprobs , function ( pr ) k * log ( pr ) + ( n - k ) * log ( 1 - pr ))) res <- exp ( k * log ( x ) + ( n - k ) * log ( 1 - x ) - mx ) * probsofprobs [ p ] return ( res ) } } log_transf_denominator <- function ( k , n ){ ## Finds the denominator, the multiplicative constant, using log transform for machine zeros. Same for all values of p. P(k) sum ( sapply ( X = 1 : length ( posprobs ), FUN = log_transf_numerator ( k , n ))) } log_transf_probabilitydist <- function ( k , n , p ){ ## Finds the posterior probability of a particular p value using log transform. log_transf_numerator ( k , n )( p ) / log_transf_denominator ( k , n ) } log_transf_posterior <- function ( k , n ){ ## Finds the posterior probability mass function of the p values using log transform sapply ( X = 1 : length ( posprobs ), FUN = function ( x ) log_transf_probabilitydist ( k , n , x )) } Example: Thunderstorms Consider if we observed 11 positive events in 27 days -- as an example, the number of thunderstorms in a month. We want to estimate the probability of a thunderstorm given the new information. Assuming weather each day is independent (which is a simplifying assumption), we can assume that the number of thunderstorms in $n$ days can be modeled by a binomial distribution. Suppose we have the following prior probability mass function about the discrete values of $p$: \\begin{equation} P(p = w) = \\begin{cases} .07 ,& \\text{if } w=.05\\\\ .13 ,& \\text{if } w=.15\\\\ .26 ,& \\text{if } w=.25\\\\ .26 ,& \\text{if } w=.35\\\\ .13 ,& \\text{if } w=.45\\\\ .07 ,& \\text{if } w=.55\\\\ .02 ,& \\text{if } w=.65\\\\ .02 ,& \\text{if } w=.75\\\\ .02 ,& \\text{if } w=.85\\\\ .02 ,& \\text{if } w=.95\\\\ 0, & \\text{otherwise} \\end{cases} \\end{equation} If we believe prior to this that the distribution looks as it does above (with the highest belief in 35 and 45%), we can now find the posterior of the probabilities after observing the 11 thunderstorms as follows: In [2]: posprobs <- c ( . 05 , . 15 , . 25 , . 35 , . 45 , . 55 , . 65 , . 75 , . 85 , . 95 ) probsofprobs <- c ( . 07 , . 13 , . 26 , . 26 , . 13 , . 07 , . 02 , . 02 , . 02 , . 02 ) pos <- posterior ( 11 , 27 , posprobs , probsofprobs ) round ( pos , 3 ) 0 0.002 0.128 0.524 0.287 0.057 0.002 0 0 0 So we now believe with 0.524 probability that the actual $p$ is .35. This means the data supports the assumption of .35 chance of thunderstorms a day, given our prior beliefs and the independence of events. Behaviors for different $n$ and $k$ Now, let us investigate the behaviors for various sample sizes, $n$, and number of observed thunderstorms $k$. In [3]: n <- c ( 10 , 50 , 100 , 150 , 200 , 300 , 500 ) k <- lapply ( n , function ( n ) seq ( n / 10 , n , n / 10 )) col_names = as.factor ( 1 : 10 / 10 ) row_names = paste ( 'P(p=' , posprobs , ')' ) list_names = paste ( 'n=' , n ) iter_j = 1 : 10 ; names ( iter_j ) = col_names iter_i = 1 : 7 ; names ( iter_i ) = list_names dimthreedfs = lapply ( iter_i , function ( i ) data.frame ( sapply ( iter_j , function ( j ) posterior ( k [[ i ]][ j ], n [ i ], posprobs , probsofprobs )), check.names = FALSE )) In [4]: dimthreedfs = lapply ( dimthreedfs , function ( y ) round ( y , 3 )) In [5]: dimthreedfs $`n= 10` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.160 0.030 0.004 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.328 0.209 0.096 0.035 0.010 0.002 0.000 0.000 0.000 0.000 0.354 0.427 0.371 0.255 0.141 0.062 0.020 0.004 0.000 0.000 0.137 0.266 0.373 0.415 0.372 0.263 0.136 0.043 0.006 0.000 0.020 0.058 0.123 0.208 0.283 0.304 0.239 0.116 0.026 0.003 0.002 0.009 0.030 0.075 0.152 0.245 0.287 0.208 0.070 0.010 0.000 0.000 0.002 0.009 0.029 0.070 0.124 0.137 0.070 0.015 0.000 0.000 0.000 0.002 0.011 0.043 0.123 0.219 0.182 0.064 0.000 0.000 0.000 0.000 0.002 0.012 0.064 0.215 0.338 0.225 0.000 0.000 0.000 0.000 0.000 0.000 0.005 0.058 0.306 0.683 $`n= 50` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.232 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.703 0.292 0.009 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.065 0.646 0.471 0.054 0.001 0.000 0.000 0.000 0.000 0.000 0.001 0.061 0.489 0.613 0.135 0.005 0.000 0.000 0.000 0.000 0.000 0.000 0.031 0.311 0.555 0.160 0.006 0.000 0.000 0.000 0.000 0.000 0.000 0.023 0.299 0.640 0.179 0.003 0.000 0.000 0.000 0.000 0.000 0.000 0.010 0.180 0.408 0.047 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.016 0.392 0.499 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.015 0.451 0.602 0.004 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.370 0.996 $`n= 100` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.168 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.828 0.288 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.004 0.706 0.479 0.005 0.000 0.000 0.000 0.000 0.000 0 0.000 0.006 0.517 0.655 0.019 0.000 0.000 0.000 0.000 0 0.000 0.000 0.004 0.337 0.637 0.026 0.000 0.000 0.000 0 0.000 0.000 0.000 0.003 0.343 0.762 0.028 0.000 0.000 0 0.000 0.000 0.000 0.000 0.001 0.211 0.505 0.005 0.000 0 0.000 0.000 0.000 0.000 0.000 0.002 0.467 0.548 0.002 0 0.000 0.000 0.000 0.000 0.000 0.000 0.001 0.447 0.725 0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.273 1 $`n= 150` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.111 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.889 0.269 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0 0.000 0.730 0.471 0.000 0.000 0.000 0.000 0.000 0.000 0 0.000 0.001 0.529 0.656 0.002 0.000 0.000 0.000 0.000 0 0.000 0.000 0.001 0.343 0.648 0.004 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.349 0.782 0.004 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 0.214 0.527 0.000 0.000 0 0.000 0.000 0.000 0.000 0.000 0.000 0.469 0.575 0.000 0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.424 0.812 0 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.188 1 $`n= 200` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.071 0.00 0.000 0.000 0.00 0.000 0.000 0.0 0.000 0 0.929 0.25 0.000 0.000 0.00 0.000 0.000 0.0 0.000 0 0.000 0.75 0.461 0.000 0.00 0.000 0.000 0.0 0.000 0 0.000 0.00 0.538 0.653 0.00 0.000 0.000 0.0 0.000 0 0.000 0.00 0.000 0.346 0.65 0.000 0.000 0.0 0.000 0 0.000 0.00 0.000 0.000 0.35 0.787 0.000 0.0 0.000 0 0.000 0.00 0.000 0.000 0.00 0.212 0.538 0.0 0.000 0 0.000 0.00 0.000 0.000 0.00 0.000 0.461 0.6 0.000 0 0.000 0.00 0.000 0.000 0.00 0.000 0.000 0.4 0.876 0 0.000 0.00 0.000 0.000 0.00 0.000 0.000 0.0 0.124 1 $`n= 300` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.028 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.972 0.214 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.786 0.442 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.558 0.647 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.353 0.65 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.35 0.793 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.207 0.558 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.442 0.648 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.352 0.949 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.051 1 $`n= 500` 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.004 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.996 0.153 0.000 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.847 0.405 0.000 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.595 0.633 0.00 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.367 0.65 0.000 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.35 0.802 0.000 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.198 0.595 0.000 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.405 0.734 0.000 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.266 0.992 0 0.000 0.000 0.000 0.000 0.00 0.000 0.000 0.000 0.008 1 In [6]: library ( ggplot2 ) library ( reshape2 ) create_gg_heatmap <- function ( df ){ mat = as.matrix ( df ) melted = melt ( mat ) colnames ( melted ) = c ( 'k' , 'p' , 'posterior' ) plt = ggplot ( data = melted , aes ( x = k , y = p , fill = posterior )) + geom_tile () + scale_fill_gradientn ( limits = c ( 0 , 1 ), colors = c ( \"navyblue\" , \"darkmagenta\" , \"darkorange1\" )) return ( plt ) } In [7]: for ( df in dimthreedfs ){ print ( create_gg_heatmap ( df )) } Each list represents $n$ for $n \\in \\{10, 50, 100, 150, 200, 300, 500\\}$, and each column represents possible $k$ values from $\\frac{n}{10}$ to $n$. The heatmaps shows us that the probabilities become more polarized as $n$ increases, which is what we would expect. We see that with small $n$ the colors are fuzzy, reflecting our uncertainty concerning our estimate. We see what is expected with Bayesian probabilities: as $n$ increases, our confidence in our posterior probability becomes stronger. This is reflected in the fact that we get values that are nearly 1 for the observations which have p = k/n and 0 otherwise. This shows that as we get more observations, the data will overwhelm the prior. For small values of $n$, we see the opposite: it is highly skewed by our prior, so that we are not so confident that $p$ = $\\frac{k}{n}$ if $p$ was not deemed probable prior. Machine Zeros problem: In [8]: posterior ( 1000 , 2000 , posprobs , probsofprobs ) NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Notice that we have many machine zeros for large n. This interferes with our calculations, and gives us NaNs for the calculations. The following uses a log transformation to avoid machine zeros: In [9]: posterior ( 1000 , 2000 , posprobs , probsofprobs ) log_transf_posterior ( 1000 , 2000 ) NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 0 5.59622100226468e-289 3.46746548211335e-121 3.31238703307168e-37 0.650000000000052 0.349999999999948 2.54799002543976e-38 2.66728114008719e-122 8.60957077271489e-290 0","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/Binomial Discrete.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/Binomial Discrete.html"},{"title":"Determining Sampling Schemes for Hierarchies","text":"First, we will treat the csv \"schools_ds\" as the population that follows a random intercept model based on the group. We will use this to investigate the sampling strategies that one could use and see how the models compare. This data considers scores from various schools, with the score representing a students' score on a test, and the grpID represeting the school. In [2]: data <- read.csv ( \"./schools_ds.csv\" ) head ( data ) require ( lme4 ) mod1 <- lmer ( score ~ 1 + ( 1 | grpID ), data = data ) summary ( mod1 ) X score grpID 1 100 1 2 110 1 3 96 1 4 80 1 5 83 1 6 76 1 Loading required package: lme4 Loading required package: Matrix Linear mixed model fit by REML ['lmerMod'] Formula: score ~ 1 + (1 | grpID) Data: data REML criterion at convergence: 9142.6 Scaled residuals: Min 1Q Median 3Q Max -2.9995 -0.6230 -0.0388 0.6663 3.7416 Random effects: Groups Name Variance Std.Dev. grpID (Intercept) 32.01 5.658 Residual 233.57 15.283 Number of obs: 1100, groups: grpID, 10 Fixed effects: Estimate Std. Error t value (Intercept) 101.298 1.876 53.99 This exercise will be investigating the effect of sampling schema on the effectiveness of HLMS by looking at school data. Each school has a different number of students, and we wish to fit a random intercept model based on the school. I first create some functions that will be useful for me in creating the samples. The first creates a sample from a group of n elements. The second transfers a list into a data frame. I then create the samples by first randomly selecting 11 from each group, then selecting i*2 from each group (so 2 from group 1, 4 from group 2, etc.). Helper functions In [3]: samplefromgroup <- function ( grp , n ){ j = sample ( 1 : length ( data [ data $ grpID == grp , 1 ]), n ) return ( data [ data $ grpID == grp ,][ j ,]) } lt2df <- function ( l ){ return ( as.data.frame ( do.call ( rbind.data.frame , l ))) } Simulation In [4]: simresults <- list () for ( sim in 1 : 10 &#94; 3 ) { sample1 <- list () nschool <- length ( unique ( data $ grpID )) for ( i in 1 : nschool ){ sample1 [[ i ]] <- samplefromgroup ( i , 11 ) } sample1 <- lt2df ( sample1 ) sample1 sample2 <- list () for ( i in 1 : nschool ){ sample2 [[ i ]] <- samplefromgroup ( i , 2 * i ) } sample2 <- lt2df ( sample2 ) sample2 sample3 <- list () randomschool <- sample ( 1 : nschool , 5 ) trigger <- any ( randomschool == 1 ) if ( trigger ){ remaining <- ! randomschool == 1 remsch <- randomschool [ remaining ] randomselect23 <- sample ( remsch , 2 ) schoolselect22 <- remsch [ remsch != randomselect23 [ 1 ] & remsch != randomselect23 [ 2 ]] sample3 [[ 1 ]] <- data [ data $ grpID == 1 ,] for ( i in 1 : 2 ){ sample3 [[ i +1 ]] <- samplefromgroup ( randomselect23 [ i ], 23 ) sample3 [[ i +3 ]] <- samplefromgroup ( schoolselect22 [ i ], 22 ) } } else { for ( i in 1 : length ( randomschool )){ sample3 [[ i ]] <- samplefromgroup ( randomschool [ i ], 22 ) } } sample3 <- lt2df ( sample3 ) sample3 sample4 <- list () weights <- vector () for ( i in 1 : nschool ){ weights [ i ] <- length ( data [ data $ grpID == i , 1 ]) / nrow ( data ) } randomschool2 <- sample ( 1 : nschool , 5 , prob = weights ) trigger2 <- any ( randomschool2 == 1 ) if ( trigger2 ){ remaining2 <- ! randomschool2 == 1 remsch2 <- randomschool2 [ remaining2 ] randomselect232 <- sample ( remsch2 , 2 ) schoolselect222 <- remsch2 [ remsch2 != randomselect232 [ 1 ] & remsch2 != randomselect232 [ 2 ]] sample4 [[ 1 ]] <- data [ data $ grpID == 1 ,] for ( i in 1 : 2 ){ sample4 [[ i +1 ]] <- samplefromgroup ( randomselect232 [ i ], 23 ) sample4 [[ i +3 ]] <- samplefromgroup ( schoolselect222 [ i ], 22 ) } } else { for ( i in 1 : length ( randomschool )){ sample4 [[ i ]] <- samplefromgroup ( randomschool2 [ i ], 22 ) } } sample4 <- as.data.frame ( do.call ( rbind.data.frame , sample4 )) sample4 listofsamples <- list ( sample1 , sample2 , sample3 , sample4 ) parameters <- matrix ( nrow = 3 , ncol = 4 ) rownames ( parameters ) <- c ( \"Gamma00\" , \"Sigma\" , \"Tao0\" ) colnames ( parameters ) <- c ( \"Model 1\" , \"Model 2\" , \"Model 3\" , \"Model 4\" ) models <- list () for ( i in 1 : length ( listofsamples )){ models [[ i ]] <- lmer ( score ~ 1 + ( 1 | grpID ), data = listofsamples [[ i ]]) parameters [ 1 , i ] <- summary ( models [[ i ]]) $ coef [ 1 ] parameters [ 2 , i ] <- summary ( models [[ i ]]) $ sigma parameters [ 3 , i ] <- as.data.frame ( VarCorr ( models [[ i ]]))[ 1 , 5 ] } simresults [[ sim ]] <- parameters } fullmodelparameters <- matrix ( rep ( c ( summary ( mod1 ) $ coef [ 1 ], summary ( mod1 ) $ sigma , as.data.frame ( VarCorr ( mod1 ))[ 1 , 5 ]), 4 ), nrow = 3 ) simresults [[ 1 ]] - fullmodelparameters Warning message in optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp), : \"convergence code 3 from bobyqa: bobyqa -- a trust region step failed to reduce q\" Model 1 Model 2 Model 3 Model 4 Gamma00 -0.0801288 -2.981430985 -0.3801288 -0.9437652 Sigma 0.7517267 -0.005658724 -0.8614719 1.1448531 Tao0 2.2040381 -2.554488338 -0.4333093 -0.3124444 I then create a list of samples, and use each of these samples to sample using the models. I create a matrix that will store the parameters in parameters , and it stores the fixed effect ($\\gamma_{0,0}$) and the variances of the random effects ($\\sigma&#94;2$ and $\\tau&#94;2$) for each model, using each sample. It then stores this information for each simulation, which is done $10&#94;5$ times. This means the samples are created differently each time, and then the model is fit using each sample, and the information is stored in a list for each simulation. I then compare this to the model using the full population, which are the full model parameters. As an example, we can see the bias from the first simulation of each model, for each variable. I then compute the mean bias, mean squared error, and mean variance for each of the samples. I do this by summing all of the individual biases, squared errors, and variances, and dividing by the total number of simulations. In [5]: indbias <- lapply ( simresults , function ( x ) x - fullmodelparameters ) indsquarederror <- lapply ( simresults , function ( x ) ( x - fullmodelparameters ) &#94; 2 ) mean <- Reduce ( '+' , simresults ) / 10 &#94; 5 indvariance <- lapply ( simresults , function ( x ) ( x - mean ) &#94; 2 ) mse <- Reduce ( '+' , indsquarederror ) / 10 &#94; 5 variance <- Reduce ( '+' , indvariance ) / 10 &#94; 5 fullbias <- Reduce ( '+' , indbias ) / 10 &#94; 5 mse variance fullbias Model 1 Model 2 Model 3 Model 4 Gamma00 0.01748752 0.02072217 0.05179054 0.04740217 Sigma 0.01140245 0.01069759 0.01055099 0.01016488 Tao0 0.03968227 0.02583796 0.04847282 0.04262270 Model 1 Model 2 Model 3 Model 4 Gamma00 100.2610814 101.440250 100.5597129 101.5455572 Sigma 2.2049334 2.290984 2.2154111 2.2687420 Tao0 0.3043728 0.366400 0.2977824 0.3723673 Model 1 Model 2 Model 3 Model 4 Gamma00 -0.001651197 0.0042711199 -0.000320029 0.004667134 Sigma -0.003194664 -0.0002994215 -0.002816646 -0.001024023 Tao0 -0.004419130 0.0024195585 -0.005803453 0.001443499 Discussion The components of the MSE can be decomposed into the variance and the bias (MSE = $b&#94;2$ + $\\sigma&#94;2$). This discussion is especially important to application because it is often expensive or impossible to gather data from the entire population (thus why we sample in the first place), and in the same vein, it may be expensive or difficult to gather data from all of the subsequent groups. Sampling the schools may be seen as desirable in this instance, as it will likely require less resources to acquire information from 5 groups rather than 10. However, we can see from the above that the effects of this on the models are somewhat large, and sampling from the number of schools does not produce the same results as sampling from each school directly. While it may not be immediately obvious from this problem, if you were to try to sample from, say, 100,000 schools, 1,000 students in each school, this could become extremely expensive and difficult to do. Thus the importance of the results here cannot be stressed enough - it is not equivalent to randomly sample 5 (or any number) groups and select a comparable number of students. Due to the structure of the HLMs, a great deal of information is lost if we neglect to look at all of the groups. This is reflected in the results above. Models 3 and 4 show considerably worse results than the first two models. This suggest that sampling from the number of schools, which may be less expensive, is less accurate at predicting the true model underlying the data. That is, using more groups will produce more favorable results in the application of HLM, even though the number of students is exactly the same in all cases. We see that the estimates of $\\sigma$ are more accurate in terms of mse. This makes sense, as we can more accurately account for the individual differences, as there are less groups. However, this is a consequence of the sampling - and therefore, the $\\tau_{0}$ estimates are much, much worse for the later two models. Since we have constructed the HLM's to account for variations within groups, and we are often interested in the level 2 variations as well, gaining a small amount of accuracy in sigma for a large drop in accuracy of $\\tau_0$ is unfavorable. If we were that much more concerned with estimating $\\sigma$ than $\\tau$, we wouldn't be using a HLM in the first place. We also see a larger difference in the estimates for $\\gamma_0$ for the later two models. The other structure that is included here involves weighting the models by the number of students in each school. Thus Model 2 and Model 4 weigh the results by the number of students, while Model 1 and Model 3 treat them all as if they are equivalent. We see that using the weighing strategy produces worse results for the fixed coefficients, but in general is better for the random coefficients. This makes sense, since this weighing is based on the groups themselves, which will introduce more bias and variance in the fixed coefficients for the intercept. However, there is a trade off here, as the mse for both $\\sigma$ and $\\tau_0$ decreases, which suggests that the random components of the models are closer to the true random parameters. The weighed models produce more variance in estimating sigma, but less bias, and produce better results for both for $\\tau_0$. This makes sense, as weighing the groups will produce less biased estimates for the individual random effect, but produce a higher variance as these results have different amounts of students sampled. $\\tau_0$ will be more accurate in terms of both variance and bias, as we are accounting for the differences in the groups, which is reflected in the full model in a similar way.","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/determining-sampling-schemes-for-hierarchies.html","loc":"https://seanammirati.github.io/category/linear-models/determining-sampling-schemes-for-hierarchies.html"},{"title":"Approaches to Contingency Tables","text":"Frequentist and Bayesian Approaches to Contingency Tables Chi-Squared Test Here we will be investigating a few ways of dealing with contingency tables. The first two are standard procedures in frequentist statistics, the chi-squared test and the Fischer's Exact test. Consider a contingency table that looks like this: In [4]: row_names = c ( 'Has traveled outside US' , 'Has not traveled outside US' ) column_names = c ( 'Non-Artist' , 'Artist' ) cont_table <- matrix ( c ( 5 , 15 , 30 , 20 ), nrow = 2 , dimnames = list ( row_names , column_names )) cont_table Non-Artist Artist Has traveled outside US 5 30 Has not traveled outside US 15 20 Consider if we ask the question: are artists and non-artists equally likely to travel outside of the US? The first approach we can take is the chi-squared test, which is an asymptotically accurate. If each was equally likely to travel, we would expect a cont_table like this : In [7]: cont_table_exp <- matrix ( c ( 10 , 10 , 25 , 25 ), nrow = 2 , dimnames = list ( row_names , column_names )) cont_table_exp Non-Artist Artist Has traveled outside US 10 25 Has not traveled outside US 10 25 If we assume that artistry and travel likelihood are independent, our sum of the differences from this expectation divided by the square root of the expectation should be roughly normally distributed and so the squared differences will be approximately $\\chi &#94;{2}\\left ( 1 \\right )$. Here, we assume that the observed counts are Poisson distributed, so that their expectation and variance are defined by $E_{i}$ and $\\frac{1}{\\sqrt{E_{i}}}$. As $n \\to \\infty$, this is asymptotically normal, given the hypothesis of independence. Because we know the row and column totals, in this case we have a degree of freedom of 1: when one value is known on this contingency table, given the row and column totals we can deduce the others. In order to calculate an (approximate) probability of this example given independence (the null hypothesis), we perform the following calculation. In [8]: zsq <- sum (( cont_table - cont_table_exp ) &#94; 2 / cont_table_exp ) 1 - pchisq ( zsq , 1 ) 0.00815097159350264 So we would reject the notion of independence based on these observations. More generally: In [15]: chi_squared_test <- function ( cont_table , cont_table_exp = NULL ){ if ( cont_table_exp == NULL ) { cont_table_exp = t ( as.matrix ( rep ( apply ( cont_table , 2 , mean )))) } cont_table_exp df <- ( nrow ( cont_table ) - 1 ) * ( ncol ( cont_table ) - 1 ) zsq <- sum (( cont_table - cont_table_exp ) &#94; 2 / cont_table_exp ) probability <- 1 - pchisq ( zsq , df ) return ( probability ) } Fischer Exact Test However, the chi-squared test is only asympotically accurate. If we consider the counts to be distributed as a hyper-geometric distribution, we can use Fisher's Exact Test to calculate the probability of independence. Given some values in the contingency table, the probability of any distribution of values given independence is as follows: In [16]: probfun <- function ( i , j , n , m ){ # i: particular coordinate (for instance, non-artist, has not traveled) # j: the column total for the particular coordinate (for instance, non-artists) # n: the row total for particular coordinate (for instance, people who have traveled) # m: the row total for the other coordinate (for instance, people who have not traveled) choose ( m , i ) * choose ( n ,( j - i )) / choose ( m + n , j ) } This will give us the exact probability of each coordinate. If we wanted to find the probability that non-artist, people who traveled is less than or equal to five, we would do the following in this case: In [17]: sum ( sapply ( 0 : 5 , function ( x ) probfun ( x , 20 , 35 , 35 ))) 0.008027370811152 This shows that the probability of what we have observed is quite small if the columns and rows are independent. We again reject independence with 1% significance. The results of this test and the chisquared test are also quite close, which is as expected. As n goes to infinity, the chisquared test converges to the Fischer's Exact test. This is the hyper geometric distribution, which is included in R. The results are the same: In [18]: phyper ( 5 , 35 , 35 , 20 ) ##same result 0.00802737081115199 These are well known tests we can make. However, we can add some bayesian analysis to the mix here! Bayesian Approach -- using Beta distributions Let us assume that the probability of having traveled for artists and non-artists can be modeled using a Beta distribution. That is [P(travel \\mid nonartist) = p \\sim \\ {Beta}(\\alpha {1}, \\beta {1})] and [P(travel \\mid artist) = q \\sim \\ {Beta}(\\alpha {2}, \\beta {2})] We can now make an approximation to determine the probability that $p < q$. That is, we want the posterior distribution of $p$ and $q$ given our contingency table. Because our likelihood is multinomial, we know that the posterior distribution is also Beta distributed. After doing some manipulations, we can write the posterior distribution of $p$ and $q$. If we assume uninformative priors, we set $\\alpha = \\beta = 1$ to get the following: In [50]: posteriorb <- function ( p , q , cont_table , num = FALSE ){ if ( num == TRUE ) { if ( p >= q ) { return ( 0 ) } } obs_row_totals = apply ( cont_table , 1 , sum ) obs_coordinates = c ( cont_table ) obs_rows_w_prior = cont_table - 1 constant <- ( prod ( gamma ( obs_row_totals )) / ( prod ( gamma ( obs_coordinates )))) p_mat <- matrix ( c ( p , q , 1 - p , 1 - q ), nrow = 2 ) ret <- constant * ( prod ( p_mat &#94; obs_rows_w_prior )) return ( ret ) } Note that this will only work for 2x2 contingency tables and assuming an (invalid) prior of $\\alpha = \\beta = 1$ for each prior. If we wanted to include prior information, we could change these for each prior, which would result in a different number being added/subtracted above. This joint distribution is somewhat difficult to determine analytically. There are alternative here to estimate this probability (gtools, MCMCpack, etc), but I have included a numerical approximation. In [53]: approx <- function ( f , num = FALSE , ... ) { sum ( sapply ( seq ( 0.01 , 0.99 , 0.01 ), function ( p ) sapply ( seq ( 0.01 , 0.99 , 0.01 ), function ( q ) f ( p , q , num , ... )))) } estim <- approx ( posteriorb , TRUE , cont_table = cont_table ) / approx ( posteriorb , cont_table = cont_table ) estim 0.996557074981106 We estimate ${P}(p< q) = .9966$. Here, we individual values of p,q and compare their pdf evaluations at each point. Bayesian Approach -- Assuming Dependence Structure Let's use another formulation that allows $p$ and $q$ to depend on one another. That is, it is unlikely that $p$ and $q$ are distributed independently -- there is an underlying 'willing to travel' factor that is prevalent in both groups. We saw this when we considered the earlier tests. If we assume then that $\\ln \\frac{p}{1-p} - \\ln \\frac{q}{1-q}$, the difference in their log odds ratios is normally distributed with mean 0 and some arbitrary variance, we are allowing for some dependencies based on the value of sigma squared that we select. Selecting a small value of sigmasquared introduces a higher dependency structure. We can then test, for a certain level of expected variance in their difference, whether they are independent or not. In [42]: posteriorc <- function ( p , q , cont_table , num = FALSE , sigmasq = 1 ){ if ( num == TRUE ) { if ( p >= q ) { return ( 0 ) } } obs_rows_w_prior = cont_table - 1 logit1 <- log (( p / ( 1 - p ))) logit2 <- log (( q / ( 1 - q ))) expo <- exp (( - ( logit1 - logit2 ) &#94; 2 ) / ( 2 * sigmasq )) p_mat <- matrix ( c ( p , q , 1 - p , 1 - q ), nrow = 2 ) ret <- expo * prod (( p_mat &#94; obs_rows_w_prior )) return ( ret ) } We then approximate ${P}(p< q)$ given various assumed sigmas in the prior. In [52]: estimb1 <- approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 1 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 1 ) estimb2 <- approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 0.25 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 0.25 ) estimb3 <- approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 4 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 4 ) # sigmasq = 1 estimb1 # sigmasq = .25 estimb2 # sigma sq = 4 estimb3 # sigma sq = 400000 (close to complete independence) approx ( posteriorc , num = TRUE , cont_table = cont_table , sigmasq = 4e+05 ) / approx ( posteriorc , num = FALSE , cont_table = cont_table , sigmasq = 4e+05 ) 0.990231766938391 0.961061419233541 0.995239851507228 0.996557062782334 We see that the larger the variance, the higher ${P}(p< q)$ is. When we condsidered them to be independent, we got an estimate of .9965571. It makes sense, then, that as we increase the variance, and therefore the two become closer to being what we would consider as independent from one another, it approaches the value when they are assumed to be independent explicitly. Considering them to have a small variance (.25) will lead to the probability that $p < q$ to be smaller than any other method, including the Chi-Squared and Fischer Exact test. When we have a high variance (even at sigma sq = 4), we obtain a higher probability that the two probabilities are different. The conclusion then is that the two groups are different from one another. When the variance is small, we are less confident that the two groups are different from one another, both in prior and posterior distribution.","tags":"Bayesian Analysis","url":"https://seanammirati.github.io/category/bayesian-analysis/Approaches to Contingency Tables.html","loc":"https://seanammirati.github.io/category/bayesian-analysis/Approaches to Contingency Tables.html"},{"title":"Time Series Analysis: Lake Erie","text":"As a preface, I will define here some functions that will be useful later on in the analysis. In [1]: ## Functions used in Time Series Project -- Lake Erie Water Levelss library ( RColorBrewer ) qual_col_pals = brewer.pal.info [ brewer.pal.info $ category == 'qual' ,] col_vector = unlist ( mapply ( brewer.pal , qual_col_pals $ maxcolors , rownames ( qual_col_pals ))) sse <- function ( pred , data ){ true <- as.vector ( data ) sum (( pred - true ) &#94; 2 ) } colors2 <- col_vector addplot <- function ( pred , range = 1 : length ( lkerie ), color1 ){ lines ( x = c ( time ( lkerie ))[ range ], y = c ( lkerietrain , pred )[ range ], col = color1 , lty = \"dashed\" ) } get.best.arima <- function ( x_ts , maxord = c ( 1 , 1 , 1 , 1 , 1 , 1 )) { best.aic <- 1e8 n <- length ( x_ts ) for ( p in 0 : maxord [ 1 ]) for ( d in 0 : maxord [ 2 ]) for ( q in 0 : maxord [ 3 ]) for ( P in 0 : maxord [ 4 ]) for ( D in 0 : maxord [ 5 ]) for ( Q in 0 : maxord [ 6 ]) { fit <- arima ( x_ts , order = c ( p , d , q ), seas = list ( order = c ( P , D , Q ), frequency ( x_ts )), method = \"CSS\" ) fit.aic <- -2 * fit $ loglik + ( log ( n ) + 1 ) * length ( fit $ coef ) if ( fit.aic < best.aic ) { best.aic <- fit.aic best.fit <- fit best.model <- c ( p , d , q , P , D , Q ) } } list ( best.aic , best.fit , best.model ) } In [2]: library ( forecast ) library ( prophet ) library ( tidyr ) library ( TSA ) library ( lubridate ) Loading required package: Rcpp Loading required package: rlang Attaching package: ‘TSA' The following objects are masked from ‘package:stats': acf, arima The following object is masked from ‘package:utils': tar Attaching package: ‘lubridate' The following object is masked from ‘package:base': date The dataset I have chosen is a monthly recording of water levels in Lake Erie from the period between 1921 and 1970. The dataset is available at the following link: https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line&numberformat=n1 . Below is a plot of the time series over time: Importing the data In [4]: #lkerie<-dmseries(\"https://datamarket.com/data/set/22pw/monthly-lake-erie-levels-1921-1970#!ds=22pw&display=line\") #lkerie <- as.ts(lkerie,frequency=12) #Note: update as of 2019, datamarket is no longer availiable. As an alternative, I am using data from a different source # that has the lake erie water levels as well. lkerie = read.csv ( 'http://lre-wm.usace.army.mil/ForecastData/GLHYD_data_english.csv' , skip = 12 ) mask = ( lkerie $ year <= 1970 ) & ( lkerie $ year >= 1921 ) lkerie = lkerie [ mask , 'Erie' ] lkerie <- ts ( lkerie , frequency = 12 , start = c ( 1921 , 1 )) In [5]: plot ( lkerie ) We can see that the water levels seem to jump somewhat cyclically over time. This is typical of physical processes, and thus it is an interesting dataset to analyze. We see that over time the water levels decreased initially, and then began to increase again somewhat steadily from the 1940s onwards. While there is clearly a seasonal component at work here, it is not so clear how this seasonal component operates initially from the plot above. I first begin by fitting a seasonal means with linear trend model to summarize and get an idea of the seasonal component of the data. I then built four models – the Holt Winters model, an SARIMA model, a model decomposing the seasonal component with spectral analysis, and a model from the prophet package. I then compare the models using the sum squared errors and plotting them against the true data values. I have separated the time series into two sets, as shown below. These are the training and test sets I will be using to evaluate how effective each of the models I fit are at estimating the true time series. The following code initializes the dataset as a time series, and creates a training and test dataset to perform analysis on. The test set is the last 12 observations, or one year, of the time series. In [6]: lkerietrain <- ts ( lkerie [ 1 : ( length ( lkerie ) -12 )], frequency = 12 ) lkerietest <- lkerie [( length ( lkerie ) -11 ) : length ( lkerie )] tail ( lkerietrain ) 600 Jul Aug Sep Oct Nov Dec 49 573.13 572.93 572.44 571.92 571.65 571.65 After this initialization, the dataset is in good condition and doesn't need to be cleaned any further. The first 6 values of the time series are shown below: Seasonal Means Model The first model I fit was a seasonal means model, to get a sense of the seasonality of the dataset. Although this is a simplistic model that will not be used to model the data itself, it provides good insight into the seasonal nature of the time series. Below is a plot of the time series with the months added to the plot: In [7]: season_ <- season ( lkerie ) time_ <- time ( lkerie ) We can see that the time series tends to peak around the summer time, and has troughs in the colder months. The seasonal means model gives us more insight into how this is occurring: In [8]: help ( plot ) In [9]: seasonmean <- lm ( lkerie ~ season_ + time_ ) summary ( seasonmean ) plot ( lkerie ) points ( y = lkerie , x = time ( lkerie ), pch = as.vector ( season ( lkerie ))) Call: lm(formula = lkerie ~ season_ + time_) Residuals: Min 1Q Median 3Q Max -2.0887 -0.6366 -0.0174 0.6178 2.3333 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 526.266512 4.902892 107.338 < 2e-16 *** season_February -0.045883 0.178105 -0.258 0.796791 season_March 0.177633 0.178105 0.997 0.319006 season_April 0.725750 0.178106 4.075 5.24e-05 *** season_May 1.055267 0.178107 5.925 5.33e-09 *** season_June 1.165384 0.178108 6.543 1.31e-10 *** season_July 1.125900 0.178109 6.321 5.13e-10 *** season_August 0.918617 0.178111 5.158 3.43e-07 *** season_September 0.612734 0.178113 3.440 0.000623 *** season_October 0.274650 0.178115 1.542 0.123616 season_November 0.024167 0.178117 0.136 0.892120 season_December -0.017116 0.178120 -0.096 0.923479 time_ 0.022599 0.002519 8.971 < 2e-16 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.8905 on 587 degrees of freedom Multiple R-squared: 0.2928, Adjusted R-squared: 0.2784 F-statistic: 20.26 on 12 and 587 DF, p-value: < 2.2e-16 Indeed, we can see that the seasonal means model has quite a bit of significant components, suggesting that there is a good amount of seasonality in the data. However, this is not consistent, with some months having insignificant values. In general, we can see that the summer months have positive coefficients, while the colder months (which are not nearly as significant) sit somewhere close to zero. This agrees with the earlier findings from the time series plot. Next, I perform a decomposition of the time series, to see how this package can break up the time series into seasonal, trend and random components. In [10]: madecomp <- decompose ( lkerie ) plot ( madecomp ) madecomp $ seasonal [ 1 : 12 ] -0.50421626984126 -0.547915249433118 -0.325653344671198 0.221778628117899 0.550673185941034 0.664431689342403 0.627271825396811 0.420052437641727 0.11478883219956 -0.224964569160989 -0.477234977324255 -0.519012188208614 We can see again that there is clearly a seasonal component to the time series. We can also see the general trend of decreasing and increasing. When these components are removed, we see a relatively stationary random process. This cyclical nature of the seasonality lead me to consider spectral analysis. However, first we will look at two other models which can deal with seasonality – the Holt Winters' Model and a seasonal ARIMA model. Holt Winters' Model I use two Holt-Winters' Models (an additive and multiplicative model) to model the data. Below, I fit the two models, and plot the resulting fits. The Holt-Winters models both consider seasonal components, although the multiplicative model and additive model come up with different quantities for the coefficients (see the appendix for the summaries). However, the smoothing parameters are relatively the same in character, with alpha=0.8973698 beta=0.003048736 and gamma: 1. This indicates that the level and seasonality are smoothing based on observations in the distant past (with gamma = 1 meaning it is considering only the previous observations), while a small beta indicates that it is using the trend of only the nearest values. I have plotted one of the models below, but the other is very similar in nature. However, as we will see later, the prediction error is quite high – (and the prediction error for the multiplicative model is, predictably, much higher). In [11]: hw1 <- HoltWinters ( lkerietrain ) hw1 plot ( hw1 ) Holt-Winters exponential smoothing with trend and additive seasonal component. Call: HoltWinters(x = lkerietrain) Smoothing parameters: alpha: 0.8987307 beta : 0.002926917 gamma: 1 Coefficients: [,1] a 5.720476e+02 b -1.549754e-04 s1 -4.205078e-01 s2 -3.699342e-01 s3 -2.099025e-01 s4 3.027650e-01 s5 5.968531e-01 s6 6.435499e-01 s7 6.181102e-01 s8 4.454104e-01 s9 1.618096e-01 s10 -2.064551e-01 s11 -3.922436e-01 s12 -3.976096e-01 In [12]: hw2 <- HoltWinters ( lkerietrain , beta = NULL , seasonal = \"multiplicative\" ) hw2 plot ( hw2 ) hw1pred <- predict ( hw1 , n.ahead = 12 , prediction.interval = TRUE ) hw2pred <- predict ( hw2 , n.ahead = 12 , prediction.interval = TRUE ) plot ( forecast ( hw1 , 12 )) plot ( forecast ( hw2 , 12 )) ssehw1 <- sse ( hw1pred [, 1 ], lkerietest ) ssehw2 <- sse ( hw2pred [, 1 ], lkerietest ) Holt-Winters exponential smoothing with trend and multiplicative seasonal component. Call: HoltWinters(x = lkerietrain, beta = NULL, seasonal = \"multiplicative\") Smoothing parameters: alpha: 0.8986116 beta : 0.002922608 gamma: 1 Coefficients: [,1] a 5.720483e+02 b -1.676177e-04 s1 9.992631e-01 s2 9.993512e-01 s3 9.996315e-01 s4 1.000530e+00 s5 1.001045e+00 s6 1.001127e+00 s7 1.001082e+00 s8 1.000780e+00 s9 1.000284e+00 s10 9.996387e-01 s11 9.993134e-01 s12 9.993037e-01 SARIMA Modeling Next, we will turn to using an ARIMA (or in this case, a seasonal ARIMA model) to model and predict the data. The above time series is not stationary, so we must transform it. The transformation which creates an optimal situation for ARIMA modeling is one that is roughly stationary. After looking through various transformations (including logarithmic, differencing and BoxCox transformations), I settled upon a differencing of logs. By differencing the logarithms of the model, we get a result that is very close to stationary. The resulting plot is plotted below. In [13]: ts.plot ( lkerie ) ts.plot ( log ( lkerie )) ts.plot ( diff ( lkerie )) ts.plot ( diff ( log ( lkerie ))) BxCx <- BoxCox.ar ( lkerie ) root <- BxCx $ mle ts.plot ( diff ( lkerie &#94; ( root ))) Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\"Warning message in arima0(x, order = c(i, 0L, 0L), include.mean = demean): \"possible convergence problem: optim gave code = 1\" In [14]: lkerietrain.trans <- diff ( log ( lkerie )) plot ( lkerietrain.trans ) Except for the one outlier just before 1940, we can see that this time series is now stationary, and we can proceed to fit ARIMA models to it. Using the ACF, PACF and EACF plots, we can see that it will be difficult to find a fitting ARMA (without a seasonal component) model that fits the data well. Since we believe there is a good degree of seasonality to this data, this is not surprising. In the plots below of the ACF and PACF, we see some sine and cosine patterns in the autocorrelations. This indicates a seasonal model may be more appropriate than an ordinary ARMA or ARIMA model. In [15]: mean ( lkerie ) mean ( diff ( lkerie )) mean ( diff ( log ( lkerie )), lag = 12 ) mean ( diff ( lkerie &#94; root )) 570.745583333333 0.00160267111853076 2.80638701397251e-06 1.83050684474109 In [16]: acf ( lkerietrain.trans ) pacf ( lkerietrain.trans ) Using the arima.subsets function gives us a good idea of the seasonality and we can better infer the best models to use based on this. In [17]: a <- armasubsets ( lkerietrain.trans , nar = 12 , nma = 12 ) plot ( a ) We can see the most significant lags are those at times 1, 11, and 12 for the AR component, and lag 12 for the MA component. Using the auto.arima and get.best.arima functions, we can get the optimal model based on the AIC. In [18]: auto.arima ( lkerietrain.trans ) get.best.arima ( lkerietrain.trans , maxord = c ( 2 , 2 , 2 , 2 , 2 , 2 )) Series: lkerietrain.trans ARIMA(3,0,0)(2,1,0)[12] Coefficients: ar1 ar2 ar3 sar1 sar2 0.3252 -0.0717 -0.0758 -0.7303 -0.3584 s.e. 0.0413 0.0432 0.0412 0.0388 0.0385 sigma&#94;2 estimated as 1.525e-07: log likelihood=3772.65 AIC=-7533.29 AICc=-7533.15 BIC=-7507.04 Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\" [[1]] [1] -7761.115 [[2]] Call: arima(x = x_ts, order = c(p, d, q), seasonal = list(order = c(P, D, Q), frequency(x_ts)), method = \"CSS\") Coefficients: ma1 sar1 sma1 intercept 0.2951 0.9918 -0.9040 2e-04 s.e. 0.0375 0.0049 0.0208 3e-04 sigma&#94;2 estimated as 1.315e-07: part log likelihood = 3895.35 [[3]] [1] 0 0 1 1 0 1 The get.best.arima's model produces the minimal AIC, and therefore is the optimal model. We can see it is very similar to the models determined by the subsets, and adds an MA component to the seasonal part of the ARIMA model. I consider both models found (that is, SARIMA[1,0,0]x[2,0,2] and SARIMA[0,0,1]x[2,0,2]). In both models, all of the terms are significant, so there is no need to remove any. Looking at the residuals of these ARIMA models, we can see that they are roughly normally distributed, with the outliers skewing it somewhat. This is encouraging. Unfortunately, the residuals (and squared residuals) do seem to still exhibit autocorrelation. However, the ACF and PACF squared do not appear to have drastically more correlated values in the squared case. I have included the Q-Q plot and the ACF of the residuals below. For brevity, I have omitted the PACF, but it exhibits similar results. Regardless, this result is disappointing. This led me to use spectral analysis to interpret the seasonality, as I believed there was some underlying seasonal pattern that could not be found using these standard SARIMA processes. In [19]: eacf ( lkerietrain.trans ) sarima1 <- arima ( log ( lkerietrain ), order = c ( 0 , 1 , 1 ), seasonal = list ( order = c ( 2 , 0 , 2 ), frequency ( lkerietrain )), method = \"CSS\" ) sarima2 <- arima ( log ( lkerietrain ), order = c ( 1 , 0 , 1 ), seasonal = list ( order = c ( 0 , 1 , 1 ), frequency ( lkerietrain )), method = \"CSS\" ) sarima1pred <- predict ( sarima1 , n.ahead = 12 ) sarima2pred <- predict ( sarima2 , n.ahead = 12 ) predarima1 <- exp ( sarima1pred $ pred ) predarima2 <- exp ( sarima2pred $ pred ) searima1 <- exp ( sarima1pred $ se ) searima2 <- exp ( sarima2pred $ se ) ts.plot ( lkerietrain , predarima1 , col = \"red\" , main = \"SARIMA 1\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima1 +1.96 * searima1 )), col = \"red\" , lty = \"dashed\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima1 -1.96 * searima1 )), col = \"red\" , lty = \"dashed\" ) lines ( lkerietrain ) ts.plot ( lkerietrain , predarima2 , col = \"blue\" , main = \"SARIMA 2\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 +1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 -1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( lkerietrain ) ssesarima1 <- sse ( predarima1 , lkerietest ) ssesarima2 <- sse ( predarima2 , lkerietest ) plot ( residuals ( sarima1 )) plot ( residuals ( sarima2 )) AR/MA 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 x x o x x x x x o x x x x x 1 x x o x x x x x o o x x x x 2 x x o o o o o o o o o x o o 3 x x o o o o o o o x o x o o 4 x x x o o o o o o o o o o o 5 x x x x o o o o o o o x o o 6 x x x x o o o o o o o x o o 7 x x o x o x o o o o o x o o In [20]: qqnorm ( y = residuals ( sarima1 )) acf ( residuals ( sarima1 )) acf ( residuals ( sarima1 ) &#94; 2 ) In [21]: pacf ( residuals ( sarima1 )) pacf ( residuals ( sarima1 ) &#94; 2 ) acf ( residuals ( sarima2 )) acf ( residuals ( sarima2 ) &#94; 2 ) pacf ( residuals ( sarima2 )) pacf ( residuals ( sarima2 ) &#94; 2 ) Spectral Analysis + ARIMA The cyclical component of the time series led me to believe that using spectral analysis to analyze the seasonality of the data may be fruitful. The method I used here was to find the highest frequencies on the periodogram, and use these to model the seasonal components. Using the ten frequencies with this highest spectrum on the log of the time series, I created twenty variables with a sin/cosine function pertaining to each. I then performed linear regression on the transformed time series with these ten values to create a harmonic model. In [22]: spec.per <- spec ( log ( lkerietrain )) frequencies <- spec.per $ freq [ order ( spec.per $ spec , decreasing = TRUE )] t <- 1 : length (( log ( lkerietrain ))) length ( t ) cc1 <- as.numeric ( cos ( 2 * pi * frequencies [ 1 ] * t )) d1 <- sin ( 2 * pi * frequencies [ 1 ] * t ) c2 <- cos ( 2 * pi * frequencies [ 2 ] * t ) d2 <- sin ( 2 * pi * frequencies [ 2 ] * t ) c3 <- cos ( 2 * pi * frequencies [ 3 ] * t ) d3 <- sin ( 2 * pi * frequencies [ 3 ] * t ) c4 <- cos ( 2 * pi * frequencies [ 4 ] * t ) d4 <- sin ( 2 * pi * frequencies [ 4 ] * t ) c5 <- cos ( 2 * pi * frequencies [ 5 ] * t ) d5 <- sin ( 2 * pi * frequencies [ 5 ] * t ) c6 <- cos ( 2 * pi * frequencies [ 6 ] * t ) d6 <- sin ( 2 * pi * frequencies [ 6 ] * t ) c7 <- cos ( 2 * pi * frequencies [ 7 ] * t ) d7 <- sin ( 2 * pi * frequencies [ 7 ] * t ) c8 <- cos ( 2 * pi * frequencies [ 8 ] * t ) d8 <- sin ( 2 * pi * frequencies [ 8 ] * t ) c9 <- cos ( 2 * pi * frequencies [ 9 ] * t ) d9 <- sin ( 2 * pi * frequencies [ 9 ] * t ) c10 <- cos ( 2 * pi * frequencies [ 10 ] * t ) d10 <- sin ( 2 * pi * frequencies [ 10 ] * t ) spec.m1 <- lm ( log ( lkerietrain ) ~ cc1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9 + d10 + t ) summary ( spec.m1 ) 588 Call: lm(formula = log(lkerietrain) ~ cc1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10 + d1 + d2 + d3 + d4 + d5 + d6 + d7 + d8 + d9 + d10 + t) Residuals: Min 1Q Median 3Q Max -2.371e-03 -4.185e-04 -4.084e-05 4.940e-04 1.747e-03 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.346e+00 1.536e-04 41304.497 < 2e-16 *** cc1 -4.134e-04 3.943e-05 -10.483 < 2e-16 *** c2 -1.104e-03 3.827e-05 -28.837 < 2e-16 *** c3 6.794e-04 3.941e-05 17.237 < 2e-16 *** c4 1.714e-04 3.933e-05 4.358 1.56e-05 *** c5 8.460e-04 3.921e-05 21.575 < 2e-16 *** c6 6.117e-04 3.928e-05 15.574 < 2e-16 *** c7 3.266e-04 3.881e-05 8.415 3.22e-16 *** c8 -3.230e-04 3.914e-05 -8.252 1.10e-15 *** c9 -2.637e-04 3.906e-05 -6.752 3.62e-11 *** c10 6.280e-05 3.836e-05 1.637 0.10221 d1 -5.615e-04 1.048e-04 -5.356 1.24e-07 *** d2 -2.710e-04 3.830e-05 -7.075 4.44e-12 *** d3 1.012e-03 6.148e-05 16.458 < 2e-16 *** d4 -6.820e-04 4.453e-05 -15.314 < 2e-16 *** d5 -1.193e-04 4.073e-05 -2.929 0.00354 ** d6 -2.688e-04 4.208e-05 -6.386 3.55e-10 *** d7 -3.401e-04 3.876e-05 -8.775 < 2e-16 *** d8 6.069e-05 3.992e-05 1.520 0.12897 d9 -6.551e-05 3.941e-05 -1.662 0.09706 . d10 2.577e-04 3.855e-05 6.685 5.55e-11 *** t 2.417e-06 5.135e-07 4.708 3.16e-06 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.0006562 on 566 degrees of freedom Multiple R-squared: 0.8769, Adjusted R-squared: 0.8724 F-statistic: 192 on 21 and 566 DF, p-value: < 2.2e-16 The results here are very significant, with a very high R-squared (.8804). We can see that the plot below produced a moderately good fit, considering it is just a sum of sine and cosine waves. However, the danger of overfitting is present. To mitigate this issue, I used the training data and a ARIMA (1,0,1) to find the sum squared errors related to different amounts of frequencies. We can see that the error plateaus at 2 frequencies. I used frequencies of 2 and 4 to create these fits. I then used these frequencies as a prediction of the seasonal components. In [23]: plot ( exp ( fitted ( spec.m1 )), type = \"l\" , col = \"blue\" ) lines ( t , lkerietrain ) In [24]: test <- cbind ( c ( 589 : 600 ), cos ( 2 * pi * frequencies [ 1 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 1 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 2 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 2 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 3 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 3 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 4 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 4 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 5 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 5 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 6 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 6 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 7 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 7 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 8 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 8 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 9 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 9 ] * ( 589 : 600 )), cos ( 2 * pi * frequencies [ 10 ] * ( 589 : 600 )), sin ( 2 * pi * frequencies [ 10 ] * ( 589 : 600 ))) colnames ( test ) = c ( \"t\" , \"cc1\" , \"d1\" , \"c2\" , \"d2\" , \"c3\" , \"d3\" , \"c4\" , \"d4\" , \"c5\" , \"d5\" , \"c6\" , \"d6\" , \"c7\" , \"d7\" , \"c8\" , \"d8\" , \"c9\" , \"d9\" , \"c10\" , \"d10\" ) plot ( lkerietrain ) models <- list () In [25]: for ( i in 1 : 10 ){ beg <- \"cc1+d1\" if ( i > 1 ){ for ( s in 2 : i ){ beg <- paste ( beg , paste ( \"c\" , s , sep = \"\" ), paste ( \"d\" , s , sep = \"\" ), sep = \"+\" ) } } end <- paste ( beg , \"t\" , sep = \"+\" ) models [[ i ]] <- lm ( formula ( paste ( \"log(lkerietrain)\" , \"~\" , end , sep = \"\" ))) pre <- exp ( fitted ( models [[ i ]])) sub <- lkerietrain - exp ( fitted ( models [[ i ]])) arimasub <- Arima ( sub , order = c ( 1 , 0 , 1 )) print ( i ) print ( sum ((( arimasub $ fitted + pre ) - lkerietrain ) &#94; 2 )) } [1] 1 [1] 42.29154 [1] 2 [1] 27.36564 [1] 3 [1] 26.89214 [1] 4 [1] 26.63287 [1] 5 [1] 25.86952 [1] 6 [1] 25.1265 [1] 7 [1] 24.43297 [1] 8 [1] 24.10715 [1] 9 [1] 23.83364 [1] 10 [1] 23.4628 In [26]: summary ( models [[ 2 ]]) summary ( models [[ 4 ]]) ##c4 is insignificant, remove it models [[ 4 ]] <- lm ( log ( lkerietrain ) ~ cc1 + c2 + c3 + d1 + d2 + d3 + d4 + t ) Call: lm(formula = formula(paste(\"log(lkerietrain)\", \"~\", end, sep = \"\"))) Residuals: Min 1Q Median 3Q Max -0.0030522 -0.0009396 -0.0000317 0.0008021 0.0046639 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.347e+00 1.822e-04 34832.880 < 2e-16 *** cc1 -5.413e-04 8.497e-05 -6.371 3.83e-10 *** d1 -1.220e-03 1.389e-04 -8.786 < 2e-16 *** c2 -1.100e-03 8.384e-05 -13.122 < 2e-16 *** d2 -2.828e-04 8.386e-05 -3.373 0.000793 *** t -1.070e-06 5.851e-07 -1.829 0.067857 . --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.001437 on 582 degrees of freedom Multiple R-squared: 0.3927, Adjusted R-squared: 0.3875 F-statistic: 75.27 on 5 and 582 DF, p-value: < 2.2e-16 Call: lm(formula = formula(paste(\"log(lkerietrain)\", \"~\", end, sep = \"\"))) Residuals: Min 1Q Median 3Q Max -0.0027355 -0.0007332 -0.0001049 0.0008003 0.0036466 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 6.346e+00 2.050e-04 30958.858 < 2e-16 *** cc1 -4.664e-04 6.620e-05 -7.044 5.32e-12 *** d1 -4.834e-04 1.442e-04 -3.351 0.000857 *** c2 -1.104e-03 6.497e-05 -16.991 < 2e-16 *** d2 -2.688e-04 6.501e-05 -4.136 4.07e-05 *** c3 6.267e-04 6.618e-05 9.470 < 2e-16 *** d3 1.055e-03 9.067e-05 11.641 < 2e-16 *** c4 1.202e-04 6.608e-05 1.819 0.069362 . d4 -6.514e-04 7.133e-05 -9.133 < 2e-16 *** t 2.811e-06 6.783e-07 4.144 3.92e-05 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Residual standard error: 0.001114 on 578 degrees of freedom Multiple R-squared: 0.6378, Adjusted R-squared: 0.6321 F-statistic: 113.1 on 9 and 578 DF, p-value: < 2.2e-16 Using the models with 2 and 4 frequencies, I subtracted these fitted spectral models from the original time series. As the plots are very similar in nature, I will only produce one of them here: In [27]: sub1 <- lkerietrain - exp ( fitted ( models [[ 2 ]])) sub2 <- lkerietrain - exp ( fitted ( models [[ 4 ]])) plot ( sub1 ) plot ( sub2 ) This produced two time series that are very reminiscent of random noise, which is what is expected, as this was to remove the seasonal component from the data. After a similar analysis to this data to the above SARIMA models, I arrived at two models to fit this ‘white noise' like data, SARIMA[1,0,1]x[0,1,1] for the first and SARIMA[2,1,1]x[1,0,1] for the second (the code is included in the appendix, but this result had the smallest AIC from the get.best.arima() function.) In [28]: acf ( sub1 ) pacf ( sub1 ) acf ( sub2 ) pacf ( sub2 ) eacf ( sub1 ) eacf ( sub2 ) plot ( armasubsets ( sub1 , nar = 10 , nma = 10 )) plot ( armasubsets ( sub2 , nar = 10 , nma = 10 )) auto.arima ( sub1 ) auto.arima ( sub2 ) get.best.arima ( sub1 , c ( 2 , 2 , 2 , 2 , 2 , 2 )) get.best.arima ( sub2 , c ( 2 , 2 , 2 , 2 , 2 , 2 )) a1spec1 <- Arima ( sub1 , c ( 1 , 0 , 1 )) a2spec1 <- auto.arima ( sub1 ) a1spec2 <- Arima ( sub2 , c ( 1 , 0 , 1 )) a2spec2 <- auto.arima ( sub2 ) a3spec1 <- Arima ( y = sub1 , order = c ( 1 , 0 , 1 ), seasonal = c ( 0 , 1 , 1 ), method = \"CSS\" ) a3spec2 <- Arima ( y = sub2 , order = c ( 2 , 1 , 1 ), seasonal = c ( 1 , 0 , 1 ), method = \"CSS\" ) AR/MA 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 x x x x x x x x x x x x x x 1 x x x o o o o o o x x x o o 2 x x o o o o o o o o o x x o 3 x x o o o o o o o o o x x o 4 x x o o o o o o o o o o o o 5 o x o o o o o o o o o o o o 6 x o x o o o o o o o o x o o 7 x x o o o o o o o o o x o o AR/MA 0 1 2 3 4 5 6 7 8 9 10 11 12 13 0 x x x x x x x x x x x x x x 1 x o x o o o o o o x x x x o 2 x x o o o o o o o o o x x o 3 o x o o o o o o o o o x x o 4 x x o o o o o o o o o o o o 5 x x o o o o o o o o o o o o 6 x o x o o o o o o o o x o o 7 o x o x o o o o o o o o o o Series: sub1 ARIMA(1,0,2)(2,0,0)[12] with zero mean Coefficients: ar1 ma1 ma2 sar1 sar2 0.9309 0.3420 0.0113 0.1461 0.1965 s.e. 0.0172 0.0457 0.0474 0.0412 0.0415 sigma&#94;2 estimated as 0.04377: log likelihood=85.88 AIC=-159.77 AICc=-159.63 BIC=-133.51 Series: sub2 ARIMA(1,0,1)(2,0,0)[12] with zero mean Coefficients: ar1 ma1 sar1 sar2 0.9032 0.3462 0.1385 0.1797 s.e. 0.0187 0.0399 0.0409 0.0416 sigma&#94;2 estimated as 0.04297: log likelihood=91.14 AIC=-172.28 AICc=-172.17 BIC=-150.39 Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\"Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\" [[1]] [1] -169.6077 [[2]] Call: arima(x = x_ts, order = c(p, d, q), seasonal = list(order = c(P, D, Q), frequency(x_ts)), method = \"CSS\") Coefficients: ar1 ar2 sar1 sma1 intercept 1.2697 -0.3110 0.9526 -0.8858 0.3867 s.e. 0.0393 0.0394 0.0149 0.0307 0.5862 sigma&#94;2 estimated as 0.04121: part log likelihood = 103.25 [[3]] [1] 2 0 0 1 0 1 Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\"Warning message in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg, : \"possible convergence problem: optim gave code = 1\" [[1]] [1] -185.2687 [[2]] Call: arima(x = x_ts, order = c(p, d, q), seasonal = list(order = c(P, D, Q), frequency(x_ts)), method = \"CSS\") Coefficients: ar1 ar2 sar1 sma1 intercept 1.2491 -0.3189 0.9536 -0.8916 0.0008 s.e. 0.0391 0.0391 0.0138 0.0277 0.3225 sigma&#94;2 estimated as 0.04013: part log likelihood = 111.08 [[3]] [1] 2 0 0 1 0 1 The plots below show that the residuals for the first model are relatively normal and that they do not exhibit very high auto-correlation or partial auto-correlation in the squared cases, which suggests that a these ARIMA models are sufficient, and a GARCH model is unneccessary. In [29]: resid1 <- residuals ( a3spec1 ) resid2 <- residuals ( a3spec2 ) In [30]: plot ( resid1 ) acf ( resid1 ) acf ( resid1 &#94; 2 ) In [31]: plot ( resid2 ) pacf ( resid2 ) pacf ( resid2 &#94; 2 ) In [32]: # Predictions specpred1 <- exp ( predict ( models [[ 2 ]], newdata = as.data.frame ( test ))) specpred2 <- exp ( predict ( models [[ 4 ]], newdata = as.data.frame ( test ))) arimapred1 <- predict ( a3spec1 , n.ahead = 12 ) $ pred arimapred2 <- predict ( a3spec2 , n.ahead = 12 ) $ pred arimase1 <- predict ( a3spec1 , n.ahead = 12 ) $ se arimase2 <- predict ( a3spec2 , n.ahead = 12 ) $ se finalpred1 <- specpred1 + arimapred1 finalpred2 <- specpred2 + arimapred2 ssesp1 <- sse ( finalpred1 , lkerietest ) ssesp2 <- sse ( finalpred2 , lkerietest ) ssesp1 / 12 ssesp2 / 12 Warning message in predict.Arima(a3spec2, n.ahead = 12): \"MA part of model is not invertible\"Warning message in predict.Arima(a3spec2, n.ahead = 12): \"MA part of model is not invertible\" 0.146456905447528 0.127569058857754 Prophet The final model that I considered was one produced by Facebook's prophet package for R. All that is needed is to initialize the data and set it up appropriately. Although the prophet package is somewhat of a \"black box\" – and it is most effective on daily data – I thought it would be interesting to include it and see how it performs as compared with the other models. In [40]: ds <- as.Date ( lkerietrain ) y <- as.numeric (( log ( lkerietrain ))) df <- data.frame ( ds , y ) prop <- prophet ( df , weekly.seasonality = TRUE ) future <- make_future_dataframe ( prop , period = 12 , freq = \"m\" ) forecast <- prophet ::: predict.prophet ( prop , future ) fitproph <- exp (( forecast $ yhat )[ 1 : ( length ( forecast $ yhat ) -12 )]) predproph <- exp (( forecast $ yhat )[( length ( forecast $ yhat ) -11 ) : length ( forecast $ yhat )]) confintup <- exp (( forecast $ yhat_upper )[( length ( forecast $ yhat ) -11 ) : length ( forecast $ yhat )]) confintlow <- confintup <- exp (( forecast $ yhat_lower )[( length ( forecast $ yhat ) -11 ) : length ( forecast $ yhat )]) sseproph <- sse ( predproph , lkerietest ) Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. Results Below is a table representing the sum squared errors of each of the models when forecasting for the next 12 months. These are the errors of each model in comparison to the test data. In [41]: results <- data.frame ( c ( ssehw1 , ssehw2 , ssesarima1 , ssesarima2 , ssesp1 , ssesp2 , sseproph )) colnames ( results ) <- \"SSE\" rownames ( results ) <- ( c ( \"HW 1\" , \"HW 2\" , \"SARIMA 1\" , \"SARIMA 2\" , \"Spec 1\" , \"Spec 2\" , \"Prophet\" )) results SSE HW 1 2.216213 HW 2 2.222284 SARIMA 1 2.842606 SARIMA 2 1.682859 Spec 1 1.757483 Spec 2 1.530829 Prophet 2.811629 We can see that the models which use the frequencies from spectral analysis to remove the seasonal components are the best at predicting the data. This makes sense, as many geophysical processes can be modeled well using the sin and cosine waves in spectral analysis. Besides these two models, the SARIMA 2 model predicted with the least error. Below, I produce a plot which shows all the models' predictions as well as the true values for the time series. In [46]: length ( lkerie ) range <- 560 : 600 plot ( time ( lkerie )[ range ], lkerie [ range ], ty = \"l\" , xlab = \"Time\" , ylab = \"Water Level\" , main = \"Overview of Models Used\" ) addplot ( finalpred1 , range , colors2 [ 1 ]) addplot ( finalpred2 , range , colors2 [ 2 ]) addplot ( predarima1 , range , colors2 [ 3 ]) addplot ( predarima2 , range , colors2 [ 4 ]) addplot ( hw1pred , range , colors2 [ 5 ]) addplot ( hw2pred , range , colors2 [ 6 ]) addplot ( predproph , range , colors2 [ 7 ]) lines ( lkerie [ range ]) legend ( 'topleft' , c ( \"True\" , \"Spec 1\" , \"Spec 2\" , \"SARIMA 1\" , \"SARIMA 2\" , \"HW 1\" , \"HW 2\" , \"Prophet\" ), col = c ( \"black\" , colors2 ), lty = \"dashed\" , bty = 'n' , cex = . 75 ) 600 Finally, I produce graphs with confidence intervals for the most effective models of each type. In [61]: plot ( forecast ( hw1 , 12 )) plot ( x = time ( lkerie ), c ( lkerietrain , finalpred2 ), ty = \"l\" , main = \"Forecast of Spec 2\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain , finalpred1 +1.96 * arimase1 ), lty = \"dashed\" , col = \"blue\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain , finalpred1 -1.96 * arimase1 ), lty = \"dashed\" , col = \"blue\" ) lines ( lkerietrain , col = \"black\" ) plot ( time ( lkerie ), c ( lkerietrain , predarima2 ), ty = 'l' , col = \"blue\" , main = \"SARIMA 2\" , xlab = \"Time\" , ylab = \"Water Level\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 +1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( x = c ( time ( lkerie )), c ( lkerietrain ,( predarima2 -1.96 * searima2 )), col = \"blue\" , lty = \"dashed\" ) lines ( lkerietrain )","tags":"Projects","url":"https://seanammirati.github.io/category/projects/tsa_lake_erie.html","loc":"https://seanammirati.github.io/category/projects/tsa_lake_erie.html"},{"title":"ANOVA","text":"A common argument against ANOVA is non-normality -- when the variables in question do not follow a normal distribution. This exercise will explore that contention in more detail. I will be comparing simulated exponential and beta distributed variables to results of the F distribution of ANOVA to determine how inaccurate ANOVA is when used on variables that are independent and identically distributed to a non-Normal distribution. Helper Functions First, let's define some helper functions -- these define the sum of squares for the treatment and error. In [2]: sst <- function ( a , n , r ) { a <- matrix ( a , n , r ) rowmean <- apply ( a , 2 , mean ) sum ( n * ( rowmean - mean ( rowmean )) &#94; 2 ) } sse <- function ( a , n , r ) { a <- matrix ( a , n , r ) rowmean <- apply ( a , 2 , mean ) b <- numeric () for ( i in 1 : n ) { b [ i ] <- sum (( a [ i ,] - rowmean ) &#94; 2 ) } sum ( b ) } The following code creates $10&#94;5$ sets of 4 groups of 3, finds the $F$ statistic associated with each and stores it in an R object. So exemp1 has the empirical F statistics for $10&#94;5$ such sets using an exponential distribution, and bemp has the empirical $F$ statistics for $10&#94;5$ sets using a beta distribution. In [4]: help ( rexp ) In [3]: expemp1 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rexp ( 12 , 1 ) f <- ( sst ( a , 3 , 4 ) / 3 ) / ( sse ( a , 3 , 4 ) / 8 ) expemp1 [ l ] <- f } expemp2 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rexp ( 120 , 1 ) f <- ( sst ( a , 30 , 4 ) / 3 ) / ( sse ( a , 30 , 4 ) / 116 ) expemp2 [ l ] <- f } bemp1 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rbeta ( 12 , 5 , 2 ) f <- ( sst ( a , 3 , 4 ) / 3 ) / ( sse ( a , 3 , 4 ) / 8 ) bemp1 [ l ] <- f } bemp2 <- numeric ( 10 &#94; 5 ) for ( l in 1 : 10 &#94; 5 ){ a <- rbeta ( 120 , 5 , 2 ) f <- ( sst ( a , 30 , 4 ) / 3 ) / ( sse ( a , 30 , 4 ) / 116 ) bemp2 [ l ] <- f } This has created all of the $F$ statistics needed to determine. This then uses $exp(12)$ with 4 groups of 3 and 30, and $beta(5,2)$ with 4 groups of 3 and 30. Below I create a table F to compare them with the true mean of the $F$ distribution ($\\frac{df_2}{df_2-2}$) and the quantiles (the median and 95th quantile.) In [5]: f1mean <- 9 / 7 f2mean <- 116 / 114 table_F <- matrix ( c ( f1mean , mean ( expemp1 ), mean ( bemp1 ), f2mean , mean ( expemp2 ), mean ( bemp2 ), qf ( . 5 , 3 , 9 ), median ( expemp1 ), median ( bemp1 ), qf ( . 5 , 3 , 116 ), median ( expemp2 ), median ( bemp2 ), qf ( . 95 , 3 , 9 ), quantile ( expemp1 , . 95 ), quantile ( bemp1 , . 95 ), qf ( . 95 , 3 , 116 ), quantile ( expemp2 , . 95 ), quantile ( bemp2 , . 95 )), 6 ) colnames ( table_F ) <- c ( \"Mean\" , \"Median\" , \"95th Percentile\" ) rownames ( table_F ) <- c ( \"F,3,9\" , \"Empirical Exp,3,4\" , \"Empirical Beta,3,4\" , \"F,3,116\" , \"Empirical Exp,30,4\" , \"Empirical Beta,30,4\" ) table_F Mean Median 95th Percentile F,3,9 1.285714 0.8516840 3.862548 Empirical Exp,3,4 1.319407 0.8701090 3.778306 Empirical Beta,3,4 1.353786 0.8590901 4.150790 F,3,116 1.017544 0.7933195 2.682809 Empirical Exp,30,4 1.013416 0.8021038 2.628236 Empirical Beta,30,4 1.017539 0.7929869 2.691883 Discussion We can see that, for the mean, the true F distribution has a lower mean than the empirical beta and exponential using 4 groups of 3. This suggests that if we were to use ANOVA with data with an exponential or beta distribution, the mean would be underestimated. That is, the true mean would be higher than that given by the F-distribution. For the median, the true F distribution again would underestimate the empirical results from the beta and exponential distributions. This implies that if we were to use ANOVA and found a P-value of .5 using the F-distribution, the true p-value would be lower than this. So our estimations would be skewed in favor of not rejecting the null hypothesis if we were to set the significance to .5. The 95th percentile is perhaps the most interesting case, where we can see some difference between the two empirical distributions. In the case of the exponential distribution, we can see that the empirical exponential F statistic produced lower 95th quantiles than the true F distribution. Suppose we were to run ANOVA on data that was exponentially distributed and got an F statistic of 3.8. Using the true F distribution, we would conclude that we cannot reject the null hypothesis that the two groups behave the same with 95% confidence. However, the true exponential distribution would have a 95th percentile of 3.72, meaning we would reject the null hypothesis that the groups are the same. It results in a type II error - we fail to reject the null hypothesis when it is false. The reverse is true for the beta distribution. Let's say we run ANOVA with data coming from an underlying beta(5,2) distribution and we get an F statistic of 4. The true F distribution would suggest that we reject the null that the groups are the same with 95% confidence - however, the empirical F using beta has a 95th percentile of 4.12, leading us to a Type I error, we have rejected the null hypothesis when it is true. When we increase the value of n, we see that the discrepancies between the true F distribution and the empirical distributions shrink. This makes sense intuitively; due to the central limit theorem, we would expect this to occur as we increase the number in each group. There are still some discrepancies, but the difference is not very large and would likely not give very different results except at the margin (when it is exactly one of the values of the other distributions). Interestingly, the beta distribution produces a lower F empirical than the true distribution for the 95th percentile and the median in this case. This seems preferable, because it means that Type I error (a false positive) is no more likely (in fact, less likely) using the true F distribution, as it is in fact more conservative than the actual distributions. So if we have a large sample size (n=120), and we are able to reject the null hypothesis, knowing only that the original distribution of each data point was either exponential, beta(5,2) or normally distributed (as is the normal case in ANOVA), we can be confident that our conclusion is correct regardless of which distribution it originally followed. It is in this sense that the original assumption of normality for ANOVA is perhaps not so important - compared to the independent and identically distributed assumption which has a larger effect if it is not satisfied.","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/anova.html","loc":"https://seanammirati.github.io/category/linear-models/anova.html"},{"title":"Factor Analysis","text":"Introduction Note: All of the code associated with this project can be found on my GitHub repository located here . What are the core factors of human personality? It's a hard question to answer! Data doesn't give us a direct answer to this question -- we may, using some sort of model , be able to determine how individuals are clustered into groups based on their responses to surveys. This is an example of unsupervised learning . We can use this information to cluser similar people together, and infer that they have similar personalities based on this. However, what we really want to know when we answer this question may be things that data cannot tell us. For example, what kinds of questions pertain to aggression? Introversion? Reliability? We can determine clusters of similar behaviors -- for instance, the answer to the questions \"Do you enjoy parties?\" and \"Do you have a large social network?\" will likely be correlated , so we may believe that these are influenced by some factor called extroversion. Most of the time, however, we do not model these explicitly -- we only wish to cluster individuals together, to make predictions of some measure (say, a happiness score), or make inferences about the questions themselves. For instance, people who answered question A and B in a certain way are likely to be similar to others in their responses to questions. This distinction is important -- we can speculate that there is some unknown factor involved, but it is not necessarily the case. In clustering algorithms like K-Means, SVD, LDA and Gaussian Mixture Models, we do not concern ourselves with what the actual underlying factors actually are -- we only use them as a way to categorize the data in order to make inferences or to predict another variable of interest. We can see that these approaches are data-driven . We assume that the data contains relevant information by which we can group people together. But does this truly answer the question we wish to solve? In most cases, the answer is no, at least not explicitly. Factor Analysis attempts to model these underlying constructs explicitly. In this way, it is hypothesis driven , i.e., we model for the hypothesis we wish to verify or reject. Hypothesis testing is a common exercise in Statistics, but in this case our hypothesis is quite specific -- we hyptohesize that there are some unknown, unseen, unobservable factors underlying the patterns we see. It is for this reason that is particularly controversial -- who is to say that such factors even exist, and if they do, how can we categorize them in abstract terms? This is an open ended question -- but factor analysis can begin to give us a sense of how these factors may interact. Here's the objective : from our concrete features, we wish to derive information about unknown, and generally unknowable, latent variables. Let's dive in! Overview Factor analysis is similar to principal components analysis in that we are working only with a single group of variables (in this case, the response) and we wish to in some way reduce the number of variables in order to simplify our data. This gives us a sense of what the underlying nature of the data is, and how it is organized. In factor analysis, we wish to reduce the redundancy of the variables by limiting them to a smaller number of factors which can adequately explain the variation in the full set of variables. Factor analysis is considered somewhat controversial by statisticians and is not encouraged by all schools of thought. This is because factor analysis is often difficult to validate in practice, as the number of factors or the interpretations are not always clear from the analysis itself. Although at first glance factor analysis and principal component analysis may seem very similar, there are key differences which separate the two methods. In principal component analysis we aim to maximize the total variance of the variables in question, but in factor analysis we wish to account for the covariance between the variables. While principal components analysis uses linear combinations of the variables themselves, in factor analysis we create a linear combination of factors , which are unobserved, supposed latent variables. The factors that we are looking at are often some underlying attributes of the variables that we believe to be \"seperate\" in some way. For instance, let's say there we have students' test scores for different classes: physics, chemistry, statistics, English, history, etc. We may expect that there would be some underlying factors that would affect an individual's ability to perform well in these classes. Perhaps this would be something like quantitative reasoning skills, critical thinking ability, or reading level and skill. We wish to reduce the variables to a smaller subset that can use these factors (which are not observed but can be derived from the data using factor analysis) to simplify our dataset. We would then use factor analysis to determine both the \"correct\" number of factors and the effects these factors have on each of the variables. It is quite easy to see how in practice this could be quite difficult to implement. This contributes to the skepticism of some statisticians to its use in the first place. Often times, the data doesn't easily lend itself to such a simplistic interpretation, and it is unclear what these factors can be and how they should be interpreted. The Model Our model is constructed as follows: For each observation vector of $p$ variables, we have $$ y_1 - \\mu_1 = \\lambda_{1 1}f_1 + \\lambda_{1 2}f_2 + ... + \\lambda_{1 m}f_m + \\epsilon_1 \\\\ y_2 - \\mu_2 = \\lambda_{2 1}f_1 + \\lambda_{2 2}f_2 + ... + \\lambda_{2 m}f_m + \\epsilon_2 \\\\ ... \\\\ y_p - \\mu_p = \\lambda_{p 1}f_1 + \\lambda_{p 2}f_2 + ... + \\lambda_{p m}f_m + \\epsilon_p $$ On the left side of the equations, $y_1, ..., y_p$ are the variables in the dataset, and $\\mu_1, ..., \\mu_p$ are the corresponding means of these variables. We do this in order to center the variables. On the right side of the equations, $\\lambda_{i j}$ are the loadings for the $ith$ variable and $jth$ factor, $f_1, f_2, ..., f_m$ are the $m$ factors, and $\\epsilon_1, \\epsilon_2, ..., \\epsilon_p$ are the $p$ error terms associated with each variable. Our goal in doing factor analysis is to find some $m << p$ such that the factors specified above are appropriate for the $p$ variables of interest. In doing this, we are defining the original $p$ variables into a linear combination of $m$ factors. The $f$s in the above model are the $m$ factors themselves, while the lambdas are the loadings, which serve as weights for each factor for each of the $p$ variables. While this may seem similar to a more typical multiple regression model , there are key differences. Perhaps the most important thing to note here is that the $fs$ are unobserved random variables, not fixed effects like in multiple regression. Factor analysis only represents one observation vector, while multiple regression represents all of the observations simultaneously -- in this way, factor analysis is looking more to individual variation as opposed to aggregate or population level aggregation. Our model can be written more simply in matrix notation as: $$ \\vec{y} - \\vec{\\mu} = \\Lambda\\vec{f} + \\vec{\\epsilon} $$ where $$ \\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_p \\end{bmatrix} \\\\ \\vec{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{bmatrix} \\\\ \\vec{f} = \\begin{bmatrix} f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_m \\end{bmatrix} \\\\ \\vec{\\epsilon} = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_p \\end{bmatrix} \\\\ \\Lambda = \\begin{bmatrix} \\lambda_{1 1} ... \\lambda_{1 m} \\\\ \\vdots \\ddots\\vdots \\\\ \\lambda_{p 1} ... \\lambda_{p m} \\end{bmatrix} $$ Assumptions : In performing factor analysis, we assume that the following holds: $\\mathbb{E}[\\vec{f}] = \\vec{0}$ $\\Sigma_{\\vec{f}} = I_{mxm}$ $\\mathbb{E}[\\vec{\\epsilon}] = \\vec{0}$ $\\Sigma_{\\vec{\\epsilon}} = \\Phi_{mxm} \\text{, where } \\Phi{pxp} = \\begin{pmatrix} \\sigma&#94;2_{\\epsilon_1} & 0 & \\dots & 0 \\\\ 0 & \\sigma&#94;2_{\\epsilon_2} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\dots & \\sigma&#94;2_{\\epsilon_p} \\end{pmatrix}$ $\\Sigma_{\\vec{f}, \\vec{\\epsilon}} = 0_{mxm}$ Notationally, I mean $\\Sigma_{\\vec{X}}$ to be the square covariance matrix of a vector $\\vec{X}$ with itself, where the $i, j$th element is the covariance of the $i$th entry in $\\vec{X}$ with the $j$th entry. It is therefore a positive semi-definite symmetric matrix with variances in the diagonal. I mean $\\Sigma_{\\vec{X}, \\vec{Y}}$ to be the covariances of each element in $\\vec{X}$ with each element in $\\vec{Y}$. This is not a square matrix, but relates the covariance of the individual elements of the two vectors. These assumptions follow naturally from the model itself. For instance, as $\\mathbb{E}[\\vec{y} - \\vec{\\mu}] = \\vec{0}$, it follows that both episilon and the factors themselves must also have an expected value of zero. We assume that the factors are uncorrelated, as we wish to minimize the number of variables used (so, we wish to have no correlation between the factors themselves.) The $\\Phi$ matrix shows that there are no covariance between the errors, only a specific variance for each error term, meaning the factors account for the correlations amongst the $y$s. This is exactly the goal of factor analysis, so these assumptions are self-checking -- if in recreating the model it is not clear what the factors should be or what number of them there should be, it is saying that these assumptions have not been met. In achieving our goal in factor analysis of reducing the variables used, we wish to express the covariance of $\\vec{y}$ in terms of the loadings $\\Lambda$ and the variances of the errors, $Phi$ using some number of factors $m$ which is less than the original number of variables $p.$ We can show that the population covariance matrix $\\Sigma_{\\vec{y}-\\vec{\\mu}}$ can be written as follows: $$ \\Sigma_{\\vec{y}-\\vec{\\mu}} = \\Lambda \\Lambda{'} + \\Phi$$ This follows from assumption 2 and 5 above. We can see then that the variance in this case will be a combination of some signal -- or explained -- variance in the latent variable loadings, and some random or unexplained variance in the error terms. This seperates Factor analysis from Principle Component Analysis -- in PCA, we make no distinction between explained and unexplained variance except in terms of the number of components selected. Here, we make this distinction for a number of latent variables. It is also of note that we can rotate the loadings by an orthogonal matrix without effecting their ability to reproduce this population covariance matrix. As such the loadings are not unique. We will use these rotations when we perform our analysis. The rotations prove useful as the results given in the loadings may not make it clear which variables are effected by which factors. By rotating the coordinate axis, we can more easily interpret the results from the factor analysis. The variances of each of the individual $y$s can be written as follows: $$ Var[y_i] = h&#94;2_i + \\phi_i $$ where $h_i&#94;2 = \\lambda_{i 1}&#94;2 + \\lambda_{i 2}&#94;2 + ... + \\lambda_{i m}&#94;2$ and $\\phi_i$ is the specific variance for the ith error term, that is, the ith diagonal of $\\Phi$. Thus we can separate the variance of each of the $y$s into a communal and specific part, which is $h_i&#94;2$ and $\\phi_i$ respectively. As such, the diagonal elements can be easily estimated using the loadings and the specific variance, while the off diagonal elements depend on the selection of the loadings alone. Since factor analysis is largely dealing with the estimations of the loadings, it accounts for the covariations between the variables, rather than the total variance as in principal component analysis. There are four different methods to obtain the loadings $\\Lambda$ from a sample. These are the principal component method, the principal factor method, the iterated principal factor method, and the maximum likelihood method. These will be explored later in the coding. Another difficulty is determining the number of factors, $m$. There are four approaches: we can select the number of variables $m$ which accounts for a prespecified amount of variance of the variables, we can choose $m$ to be the number of eigenvalues of the correlation matrix $R$ which is greater than the average of the eigenvalues, we can use a scree plot to determine where there is a leveling of the eigenvalues of $R$, or we can test the hypothesis using a chi-squared distribution if the number of factors is the true number of factors. Again, this will be shown further in the coding. Data The dataset I will be using to illustrate the benefits and limitations of factor analysis is collected data from an online personality questionnaire. The link can be found here . In this study, 1,005 participants were prompted with 40 statements to rate on a scale of one to five. This scale is used frequently and is known as the likert scale. It ranges from 1 (strongly disagree) to 5 (strongly agree). The data was partitioned into four sections, with every ten statements designed to be related to some attributes of the population; in particular, it was looking for their assertiveness, social confidence, adventurousness, and dominance. The questions were designed to discover the prevalence of these attributes through the responses to the statements in the survey. This seems like an ideal dataset to use factor analysis on, because the goal of the dataset seems to be precisely what factor analysis is used for. That is, we have some variables (the questions) that have some underlying, unobservable factor (initially hypothesized to be assertiveness, social confidence, adventurousness and dominance) that ties them all together. We are not very interested in the individual answers to the questions themselves, but of the behavior of the underlying factors that exist which are unobservable. Some examples of questions are printed below: AS2 I try to lead others. SC4 I express myself easily. AD6 I dislike changes. DO8 I challenge others' points of view. By looking at the coding markers of each of the questions, we can get a sense of how this survey was distributed and why. If, after doing factor analysis, there are four factors (that is, $m = 4$) and these factors are inclusive of each of the ten questions, we can then say that the data reflects these attributes well. Therefore, the goal of this analysis is to see if the data supports the idea that these factors are well separated in the variables, and if not, is there a better set of factors that can describe the individual more effectively. After doing some research, it seems that many personality scales fail to truly describe individuals completely. A very popular scale, the Myers Briggs Type Indicator (MBTI), has been purported by many to effectively separate all personalities into one of sixteen types. However, when researchers used factor analysis on data testing for the four underling dimensions of the Myers Briggs tests, they found conflicting results. One of these analyses confirmed the four-dimensionality of the data, while the other suggested there to be six, rather than the four purported by the Myers Briggs test.The motivation for choosing this topic and subsequent dataset was to test these findings for myself. Although I was unable to find an adequate dataset with questions relating to the Myers Briggs tests, I used this dataset to see how well factor analysis can truly separate personality types from a series of related statements. Analysis In [7]: require ( psych ) tmp <- tempfile () download.file ( \"http://personality-testing.info/_rawdata/AS+SC+AD+DO.zip\" , tmp ) data <- read.csv ( unz ( tmp , 'AS+SC+AD+DO/data.csv' )) unlink ( tmp ) data <- data [, 1 : 40 ] data [ 1 : 5 , c ( 1 , 11 , 21 , 31 )] R <- cor ( data ) Loading required package: psych Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, : \"there is no package called ‘psych'\" AS1 SC1 AD1 DO1 4 5 5 1 4 4 2 4 5 4 4 4 4 2 4 3 4 2 4 4 In [5]: round ( R , 2 )[ 1 : 5 , 1 : 5 ] AS1 AS2 AS3 AS4 AS5 AS1 1.00 0.41 0.37 0.44 0.32 AS2 0.41 1.00 0.63 0.42 0.45 AS3 0.37 0.63 1.00 0.38 0.49 AS4 0.44 0.42 0.38 1.00 0.30 AS5 0.32 0.45 0.49 0.30 1.00 I begin by importing the dataset and only using the variables pertaining to the questions. The next step is to find the correlation matrix. This is the preferred matrix to work with, as it makes many of the computations easier to do (as compared with the covariance matrix itself). We can see immediately that this is a covariance matrix, as there are ones in the diagonal. However, it does not appear to have particularly strong or easily discernible groups of correlations between the variables, which implies that factor analysis may not perform as well as we had hoped. I did not produce the entire matrix, as it is a 40x40 matrix. Instead, I only look at the first five rows and columns for illustrative purposes. There were forty questions on the questionaire, with every ten representing a \"factor\" of one's personality. We are using factor analysis to determine whether there is statistical evidence that these groupings represent latent 'factors'. Selecting $m$ As mentioned before, it often is not clear how many factors, $m$, we should use in the factor analysis. In this example, it would be preferable to use four, as that is how the data is designed. However, as we will see, four factors are not sufficient to separate the variables. In order to perform factor analysis, we must find the correlation matrix of the dataset. This makes many of the calculations quite simple. The next step will be determining how many factors we should use. In [108]: R <- cor ( data ) Once we have the correlation matrix, the eigenvalues will tell us the optimal number of ways to reduce the features. In [17]: head ( round ( eigen ( R ) $ values , 2 ), 15 ) 9.21 4.25 2.71 2.12 1.59 1.4 1.25 1.07 0.98 0.93 0.88 0.81 0.79 0.77 0.73 There are numerous methods to select the number of eigenvectors/eigenvalues we want to include. Each eigenvalue represents the proportion of variance explained by that eigenvector (a linear combination of the variables). We want to select eigenvectors/values such that we can explain a good deal of the variance with a smaller number of linear combinations of the features. The first method involves choosing an arbitrary threshold. In this example, at 19 latent variables (that is, reducing the forty variables nearly in half) we have acheieved a threshold of variance explained greater than 60%. In [110]: sum ( eigen ( R ) $ values [ 1 : 19 ]) / 40 ##choose 19 0.80183840681608 Another method is to select eigenvectors such that each corresponding eigenvalue is greater than one. This means that the eigenvector of variables contributes more to the variance of the features than any single feature alone. Using this method, we come up with 8 linear combinations. In [111]: sum ( eigen ( R ) $ values > 1 ) ##choose 8 8 The final methodology is using a scree plot and determining where the eigenvalues level off. This is at 9 linear combinations of the features. In [112]: scree ( R ) ##choose 9 fa.parallel ( data ) Parallel analysis suggests that the number of factors = 9 and the number of components = 7 Three different methods were explored to determine the amount of factors, $m$, to use. Unfortunately, it seems right from the beginning that four factors will not be enough to adequately describe the variables. The three methods provide very different numbers: 19, 8 and 9. I will use 9, as it appears to be a middle ground between the two, and the scree plot does flatten out significantly. Also, we do not wish to use nearly half of the number of variables as factors! Even 9 factors is quite high. This is where factor analysis and principle component analysis begin to differ. In PCA, we simply use the eigenvectors of the correlation matrix as variables, or 'principle components'. This does not account for the noise in the principle components themselves -- it is not trying to find any signal here. Factor analysis considers that these components are representations of some latent variables, that which are the true 'signal' in the correlation matrix, while the remainder is noise. Factor analysis is controversial because it attempts to estimate this latency by approximating correlation matrices, as we saw above. Now that we have determined m, the number of factors, we will aim to create the loadings matrix to estimate the covariance matrix, as explained above. There are four methods by which we can estimate the loadings matrices. The loadings matrix in each case will be a $p x m$ matrix, and will use the eigenvalues of the correlation matrix. The first method we will use is the principal components method. Principal Components Method In [16]: eigenvectors <- eigen ( R ) $ vectors [, 1 : 9 ] loadings <- eigenvectors %*% ( diag ( 1 , nrow = 9 ) * ( eigen ( R ) $ values [ 1 : 9 ]) &#94; . 5 ) head ( round ( loadings , 2 )) -0.63 -0.12 -0.33 -0.05 0.20 -0.26 -0.01 -0.11 -0.22 -0.66 0.13 -0.12 0.22 0.22 0.13 -0.25 0.00 0.07 -0.64 0.20 -0.06 0.23 0.24 0.11 -0.31 0.07 -0.01 -0.60 0.00 -0.25 0.03 0.20 -0.10 0.22 0.39 0.18 -0.56 0.08 0.04 0.09 0.32 0.10 -0.34 0.03 0.09 -0.56 0.20 0.02 0.13 0.30 0.07 -0.28 -0.07 0.09 We first determine the loadings by multiplying the first $m$ eigenvectors (in this case, 9) by the square roots of their corresponding eigenvalues. This produces the above matrix, a unsightly $40x9$ matrix. Each column corresponds to a different loading. To determine the communalities and specific variances of the variables, we can square the entries in each of the rows of the loadings matrix. This will give us the $h&#94;2_i$ vector, from which we can also determine the specific variances. In [21]: hisq <- apply ( apply ( loadings , 2 , function ( x ) x &#94; 2 ), 1 , sum ) psi <- rep ( 1 , 40 ) - hisq cbind ( communalities = round ( hisq , 3 ), specific = round ( psi , 3 )) communalities specific 0.684 0.316 0.654 0.346 0.675 0.325 0.713 0.287 0.567 0.433 0.559 0.441 0.635 0.365 0.532 0.468 0.610 0.390 0.485 0.515 0.707 0.293 0.736 0.264 0.539 0.461 0.697 0.303 0.745 0.255 0.713 0.287 0.634 0.366 0.711 0.289 0.685 0.315 0.506 0.494 0.528 0.472 0.602 0.398 0.625 0.375 0.590 0.410 0.563 0.437 0.701 0.299 0.661 0.339 0.573 0.427 0.362 0.638 0.511 0.489 0.774 0.226 0.806 0.194 0.501 0.499 0.621 0.379 0.594 0.406 0.529 0.471 0.559 0.441 0.556 0.444 0.573 0.427 0.574 0.426 The above are the communalities and the specific variances of each of the variables. To determine the amount of the total variance of which each loading contributes, we can divide the eigenvalues of $R$ by the number of variables, $p = 40$. Thus, all nine factors account for about 60% of the variance of the variables. This is not very good, especially considering there were supposed to be only four underlying factors! Rotating the Loadings Because we have $m = 9$, I will use the varimax rotations from the psych package to rotate the loadings we have above. This will make it easier to see which variables correspond to the factors. In [115]: ## varimax includes a print method which makes the loading distributions easier to see. loadingsrot <- print ( varimax ( loadings ) $ loadings , cutoff = . 3 , sort = T ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] -0.681 [2,] -0.713 [3,] -0.681 [4,] -0.655 [5,] 0.506 0.486 [6,] -0.657 [7,] -0.742 [8,] -0.765 [9,] -0.616 [10,] -0.642 [11,] -0.623 [12,] -0.685 [13,] -0.698 [14,] 0.716 [15,] 0.784 [16,] 0.769 [17,] 0.731 [18,] 0.643 [19,] -0.397 -0.534 [20,] -0.718 [21,] -0.747 [22,] -0.688 [23,] -0.313 -0.728 [24,] -0.753 [25,] 0.759 [26,] 0.727 0.301 [27,] 0.722 [28,] 0.794 [29,] -0.349 0.786 [30,] 0.744 [31,] 0.745 [32,] 0.338 -0.618 [33,] 0.302 0.681 [34,] -0.408 -0.522 [35,] 0.578 [36,] -0.396 0.337 -0.501 [37,] -0.407 0.481 [38,] -0.478 0.355 [39,] 0.360 0.327 0.457 [40,] 0.391 0.326 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.970 4.308 3.507 2.557 3.954 1.746 2.289 1.437 1.819 Proportion Var 0.074 0.108 0.088 0.064 0.099 0.044 0.057 0.036 0.045 Cumulative Var 0.074 0.182 0.270 0.334 0.432 0.476 0.533 0.569 0.615 We can see from the above that the factors do not separate so easily. The values of the loadings below .3 have been omitted to make it clearer what the underling relationships are. Although they do not separate cleanly into the four theorized factors, we can see that the first factor, for instance, largely is between the first and tenth variables (which was assertiveness), the second factor between the 30th and 40th variables (which was dominance). Similarly, the fifth factor is between the 20th and 30th variables (social confidence) and the third and factor is largely between the 30th and 40th variables (adventurousness). However, the split is not so simple as we assumed before the analysis, as the other factors show relationships between variables which are not in the same groups. The second function simply sorts the variables by factor, so we can get a better sense of how they separate. We can see that towards the top there are larger values for the loading matrix, which indicates a stronger association with the factor. We can also see that the 6th-9th factors also have some large values, which means that their correlations cannot be ignored. This suggests that the initial separation into only four factors may have been overly simplistic. Principle Factors Method In the previous method, we had ignored the structure of the specific variances. Now, using the principal factors method, we will take the $\\Phi$ matrix into account. We first make an initial estimate of the $h_i$s, and use this to account for the specific variances. Below I create a diagonal matrix which has the specific variances in each of the diagonal elements. We will then subtract this matrix from the covariance matrix, and use the new matrix to compute our loadings. In [25]: hinit <- 1 - 1 / diag ( solve ( R )) psiint <- 1 - hinit cbind ( communalities = round ( hinit , 3 ), specific = round ( psiint , 3 )) communalities specific AS1 0.703 0.297 AS2 0.548 0.452 AS3 0.551 0.449 AS4 0.505 0.495 AS5 0.391 0.609 AS6 0.418 0.582 AS7 0.479 0.521 AS8 0.327 0.673 AS9 0.268 0.732 AS10 0.219 0.781 SC1 0.578 0.422 SC2 0.656 0.344 SC3 0.457 0.543 SC4 0.696 0.304 SC5 0.572 0.428 SC6 0.639 0.361 SC7 0.486 0.514 SC8 0.580 0.420 SC9 0.572 0.428 SC10 0.378 0.622 AD1 0.362 0.638 AD2 0.382 0.618 AD3 0.402 0.598 AD4 0.413 0.587 AD5 0.428 0.572 AD6 0.611 0.389 AD7 0.579 0.421 AD8 0.393 0.607 AD9 0.230 0.770 AD10 0.361 0.639 DO1 0.619 0.381 DO2 0.658 0.342 DO3 0.424 0.576 DO4 0.539 0.461 DO5 0.439 0.561 DO6 0.415 0.585 DO7 0.442 0.558 DO8 0.462 0.538 DO9 0.458 0.542 DO10 0.485 0.515 In [27]: New <- R - diag ( psiint , nrow = 40 ) eigs <- eigen ( New ) head ( round ( eigs $ values , 2 )) round ( eigs $ vectors , 3 )[ 1 : 5 , 1 : 5 ] 8.72 3.73 2.22 1.56 1.13 0.95 -0.212 0.064 0.217 -0.022 -0.256 -0.219 -0.065 0.061 0.177 -0.133 -0.211 -0.098 0.024 0.184 -0.150 -0.199 0.000 0.147 0.036 -0.166 -0.183 -0.036 -0.032 0.068 -0.188 We will then use this new matrix to compute our eigenvalues and vectors, and subsequently create our new loadings matrix. In a similar way, we will determine the specific variances and communalities, and then rotate the loadings matrix. In [28]: eigenvectors2 <- eigen ( New ) $ vectors [, 1 : 9 ] loadings2 <- eigenvectors2 %*% ( diag ( 1 , nrow = 9 ) * ( eigen ( New ) $ values [ 1 : 9 ]) &#94; . 5 ) hisq2 <- apply ( apply ( loadings2 , 2 , function ( x ) x &#94; 2 ), 1 , sum ) psi <- rep ( 1 , 40 ) - hisq loadings2rot <- print ( varimax ( loadings2 ) $ loadings , cutoff = . 35 ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] 0.713 [2,] -0.635 [3,] -0.662 [4,] -0.574 [5,] -0.549 [6,] -0.554 [7,] 0.500 [8,] [9,] 0.368 [10,] [11,] 0.636 [12,] 0.729 [13,] 0.460 [14,] 0.717 [15,] -0.620 [16,] -0.739 [17,] -0.360 0.505 [18,] -0.650 [19,] -0.688 [20,] 0.371 [21,] -0.466 [22,] -0.625 [23,] -0.627 [24,] -0.597 [25,] 0.649 [26,] 0.754 [27,] 0.732 [28,] 0.635 [29,] 0.362 [30,] 0.578 [31,] 0.727 [32,] -0.375 0.730 [33,] -0.609 [34,] -0.711 [35,] -0.686 [36,] -0.588 [37,] -0.582 [38,] -0.581 [39,] -0.631 [40,] -0.651 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.508 3.963 3.089 1.957 3.511 1.279 1.401 1.402 0.995 Proportion Var 0.063 0.099 0.077 0.049 0.088 0.032 0.035 0.035 0.025 Cumulative Var 0.063 0.162 0.239 0.288 0.376 0.408 0.443 0.478 0.503 We can see a similar pattern to that of the principal component method above. The variance is not as well explained using this method, but when we perform the next step (using iterations of the above method), it will become a better representation of the data. Again, we can see similar relationships between the first five factors and the variables, but it is not all encompassing. Principal Factors Method We will now use a similar method to above, but iterate it each time and update it with the new initial squared loadings. That is, when we calculate the new $h&#94;2_i$ values, we do the iterate until it converges. For completeness, I perform the process 100 times below. In [30]: hinit2 <- 1 - 1 / diag ( solve ( R )) for ( i in 1 : 100 ) { New2 <- R - diag (( 1 - hinit2 ), nrow = 40 ) eigenvectors3 <- eigen ( New2 ) $ vectors [, 1 : 9 ] loadings3 <- eigenvectors3 %*% ( diag ( 1 , nrow = 9 ) * ( eigen ( New2 ) $ values [ 1 : 9 ]) &#94; . 5 ) hinit2 <- apply ( apply ( loadings3 , 2 , function ( x ) x &#94; 2 ), 1 , sum ) } hisq3 = hinit2 round ( loadings3 , 3 )[ 1 : 5 , 1 : 5 ] -0.634 0.130 0.342 -0.038 -0.299 -0.650 -0.126 0.095 0.225 -0.120 -0.629 -0.191 0.040 0.241 -0.147 -0.591 0.003 0.226 0.040 -0.169 -0.540 -0.070 -0.047 0.089 -0.185 In [32]: head ( round ( hisq3 , 3 )) 0.829 0.602 0.638 0.568 0.43 0.454 In [35]: loadings3rot <- print ( varimax ( loadings3 ) $ loadings , cutoff = . 3 ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] 0.328 0.771 [2,] -0.639 [3,] -0.688 -0.300 [4,] 0.593 [5,] -0.551 [6,] -0.555 [7,] 0.487 -0.362 [8,] -0.357 [9,] -0.376 [10,] -0.326 [11,] 0.605 [12,] 0.755 [13,] 0.463 [14,] 0.324 0.749 [15,] 0.300 0.703 [16,] -0.757 [17,] -0.361 -0.509 -0.340 [18,] -0.624 [19,] -0.706 [20,] -0.327 -0.398 [21,] -0.350 -0.455 [22,] -0.647 [23,] -0.662 [24,] -0.606 [25,] 0.653 [26,] 0.771 [27,] 0.734 [28,] 0.641 [29,] 0.347 [30,] 0.573 [31,] 0.730 [32,] -0.367 0.834 [33,] -0.606 [34,] -0.717 [35,] -0.698 [36,] -0.592 [37,] -0.575 [38,] -0.578 [39,] -0.628 [40,] -0.649 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.456 3.967 3.049 1.971 3.529 1.417 1.485 1.511 1.206 Proportion Var 0.061 0.099 0.076 0.049 0.088 0.035 0.037 0.038 0.030 Cumulative Var 0.061 0.161 0.237 0.286 0.374 0.410 0.447 0.485 0.515 Again, we can see a similar pattern to the two above methods. The iterated method also created loadings which are closer to the principal component method. There are a few differences; for instance, in the iterated method we see that the third variable is part of both the 1st and 2nd factor, which wasnâ€™t present in the principal component method nor the principal factor method. Still, the analysis from before holds: much of the expected separation is contained in the factors (or some combination of them), but there are other correlations which those four factors cannot account for. I will use the rotated matrix above to make some considerations about the level of effectiveness of the analysis. We know that the main goal of factor analysis is to acquire factors for which each variable can be loaded into only one factor. If we can do this, we have essentially reached our goal â€\" we have separated the variables completely into a smaller number of factors which accounts for the covariates between them. However, in practice, this is very difficult to achieve. In the loading matrix above, there are many instance where the variables correlate strongly with more than one factor at once -- that is there is no clear distinction between what these questions relate to in terms of our factors. This indicates that factor analysis is not doing a very good job at separating the factors with the number of factors we have used. Some Considerations In an ideal situation, we would use a very small value of $m$ relative to the amount of variables $p$. Although $m = 9$ performed decently well, it is quite a large number of factors. Even with nine factors, we see that the factor analysis was only able to account for around 60% of the covariation of the original variables. A fundamental issue with factor analysis is that the correlation matrix $R$ contains both structure and error, and factor analysis is not able to separate the two. Thus, the original assumptions of no relationships between the errors and the factors are too optimistic, as this rarely happens in practice. As a result, the loadings above are quite difficult to interpret. It is difficult to say what the exact factors are in plain English. Here, we can see some of the difficulty in working with factor analysis in practice â€\" it is often questioned if these factors truly exist. Conclusions and Relation to Psychological Tests In [37]: print ( varimax ( loadings ) $ loadings , cutoff = . 3 ) Loadings: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [1,] -0.408 -0.522 [2,] -0.681 [3,] -0.713 [4,] 0.744 [5,] -0.681 [6,] -0.655 [7,] 0.506 0.486 [8,] 0.578 [9,] 0.302 0.681 [10,] -0.407 0.481 [11,] -0.313 -0.728 [12,] -0.753 [13,] -0.478 0.355 [14,] -0.396 0.337 -0.501 [15,] 0.745 [16,] 0.759 [17,] 0.338 -0.618 [18,] 0.727 0.301 [19,] 0.722 [20,] 0.360 0.327 0.457 [21,] 0.397 -0.534 [22,] -0.718 [23,] -0.747 [24,] -0.688 [25,] -0.716 [26,] -0.784 [27,] -0.769 [28,] -0.731 [29,] -0.391 0.326 [30,] -0.643 [31,] 0.794 [32,] 0.349 0.786 [33,] 0.657 [34,] 0.742 [35,] 0.765 [36,] 0.616 [37,] 0.642 [38,] 0.623 [39,] 0.685 [40,] 0.698 [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] SS loadings 2.970 4.308 3.507 2.557 3.954 1.746 2.289 1.437 1.819 Proportion Var 0.074 0.108 0.088 0.064 0.099 0.044 0.057 0.036 0.045 Cumulative Var 0.074 0.182 0.270 0.334 0.432 0.476 0.533 0.569 0.615 We know that the original variables were constructed considering four groups (assertiveness, social confidence, adventurousness, and dominance). These were the intended characteristics that the statements were testing for. Above, we can see how factors 1-5 capture a lot of the variables in each set. So, we could say that factor 1 represents assertiveness, factor 2 represents dominance, factor 3 and 4 represent adventurousness, and factor 5 represents social confidence. However, this would not be so accurate, as there are other factors to consider as well. We can see that factors 6-9 have members from all the different groups. This suggests that the relationship between the variables was not so simple as we once thought. Additionally, variable 10 failed to have a loading greater than .3 into any of the factors, which suggests that even 9 factors are inadequate to capture the variation in the data. In this example, which seemed relatively straightforward, we see that problems emerged quite quickly. What, for instance, is factor six? And why are factors 3 and 4 separated? It is not so clear that our prior ideas about the factors are accurate. One thing is clear, though. The four factors used to create the questionnaire are insufficient to truly describe the structure of the data. To say that these statements measure these factors would be jumping to conclusions too quickly. Doing so is, in a way, oversimplifying the data. In conclusion, we have learned that this dataset cannot be so simply separated into the four purported factors. In fact, even separating the variables into 9 factors can only account for 60% of the variation of the variables. We can see that while factor analysis is a powerful technique, it is very dependent on the dataset which it is performed on. The self-checking property of the assumptions of factor analysis ensures that only models which fit the assumptions will produce results which separate the variables adequately to the different factors.","tags":"Projects","url":"https://seanammirati.github.io/category/projects/factor_analysis.html","loc":"https://seanammirati.github.io/category/projects/factor_analysis.html"},{"title":"Summary Statistics Part 2: Measures of Spread","text":"Measures of Spread In the last post on central tendency , we discussed the basics of what a statistic and a population is as well as the common measures of central tendency, the mean , median and mode . In this post, we will dive deeper into how to describe sample and population data using Measures of Spread. When we talked about central tendency, we discussed the idea behind the \"average\" observation in a pool of data. When it comes to describing a dataset, the central tendency is a great place to start -- it gives us a sense of the location of our data. For instance, if we had the weight of a variety of individuals, we can get a sense of what a \"typical\" weight might be using measures of central tendency. Alien Brains Imagine you are an observer who knows nothing about a particular subject. Let's say this subject is the number of neurons in a sample of aliens' brains. Or, you know, something. Based on this information alone, an ordinary, human observer would have absolutely no sense of where to begin. That is, we have absolutely no prior information about aliens' brains. Let's say the sample of neurons in 100 aliens found in the first UFO crashing ever known to man look something like this: In [1]: import numpy as np import pandas as pd import scipy.stats import plotly.offline as plt import plotly.graph_objs as go plt . init_notebook_mode ( connected = True ) In [47]: alien_neurons = pd . Series ( np . random . negative_binomial ( 424242 , . 0042 , 100 )) In [48]: data = [ go . Histogram ( x = alien_neurons . tolist ())] layout = go . Layout ( title = 'Alien Brains' , xaxis = dict ( title = 'Number of Neurons' ), yaxis = dict ( title = 'Number of Aliens' ), bargap = 0.2 , bargroupgap = 0.1 ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Woah! These values are in the hundreds of millions! Well actually, the human brain has over 100,000,000,000 neurons! So, this alien is not the most sophisticated in terms of neurons. Is there a way to see this, the location, more quantitatively? Your answer should be ... yes! Using the techniques we learned last time, we can find the mean and median of this distribution. To spare the agony of performing this arithmetic on such huge numbers, we can use pandas to find these values. In [49]: alien_mean , alien_median = alien_neurons . aggregate ([ 'mean' , 'median' ]) print ( '''Mean of Alien Neurons: {} , Median of Alien Neurons: {} ''' . format ( alien_mean , alien_median )) Mean of Alien Neurons: 100544873.04, Median of Alien Neurons: 100546471.0 This means that the mean of this sample is 100,584,186.03 and the median is 100,592,749. These values are quite \"close\", which is expected because we see that the distribution is somewhat symmetric. But... wait a minute! How are we certain that these two values are \"close\"? We can look at our histogram to see if this assertion is correct. In [50]: alien_mean - alien_median Out[50]: -1597.9599999934435 In [51]: data = [ go . Histogram ( x = alien_neurons . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : alien_mean , 'y0' : 0 , 'x1' : alien_mean , 'y1' : 20 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : alien_median , 'y0' : 0 , 'x1' : alien_median , 'y1' : 20 , }] layout = go . Layout ( title = 'Alien Brains' , xaxis = dict ( title = 'Number of Neurons' ), yaxis = dict ( title = 'Number of Aliens' ), bargap = 0.2 , bargroupgap = 0.1 , shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Well, they look pretty close. However, this is far too hand-wavy of an answer. In determining the \"closeness\" of these two statistics, we would like to have an objective measure relative to scale. The actual difference is around {{alien_mean - alien_median}}! With just that information alone, we would likely not say that the two are \"close\". If the difference between what you got in a paycheck was \\$8,563 less than what you expected, you would definitely be upset! The more we think about it, relative differences like these are reliant of the <b> scale </b> of the numbers we're talking about. 8,000 stars may be astronomically small compared to the total number of stars, but \\$8,000 is significant to most people! However, because of the scale of this distribution, this is not a large difference relative to the differences in observations. Imagine we had a distribution that looked like this instead: In [52]: log_norm_sigma = 1.158 log_norm_mu = np . log (( alien_mean - alien_median ) / ( 1 - np . exp ( ( log_norm_sigma ** 2 ) / 2 ))) sim = pd . Series ( np . random . lognormal ( mean = log_norm_mu , sigma = log_norm_sigma , size = 100 )) In [53]: data = [ go . Histogram ( x = sim . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : sim . mean (), 'y0' : 0 , 'x1' : sim . mean (), 'y1' : 50 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : sim . median (), 'y0' : 0 , 'x1' : sim . median (), 'y1' : 50 , }] layout = go . Layout ( title = 'Not Alien Brains' , xaxis = dict ( title = 'Counts' ), yaxis = dict ( title = 'Volume' ), bargap = 0.2 , bargroupgap = 0.1 , shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) As a sidenote, the way this distribution was simulated was using the fact that we want a relative difference in the mean and median in the population to be equal to 4,692, but on a completely different scale. This was using the lognormal distribution, which is a skewed distribution, so it works well for this purpose! In [54]: sim . mean () - sim . median () Out[54]: 1260.074316668763 We can now clearly see that the absolute differences in the mean and median (and in fact, any of the observations) is not sufficient to describe their \"closeness\" to one another. We have an idea of the location, but we need a sense of scale. Why does spread matter? We can already see why the measures of central tendency are insufficient to give us an idea of what a distribution looks like. To investigate further, suppose we know only that the mean of some observations is 100. What else can we say? Well, not much. Consider the two distributions below. Both populations have mean 100, and we sample 100 values from them. The resulting samples have means very close to one another (and to 100), but they are quite different from one another. In [55]: ex_1 = np . random . normal ( 100 , 2 , 1000 ) ex_2 = np . random . normal ( 100 , 10 , 1000 ) data = [ go . Histogram ( x = ex_1 . tolist (), name = 'Less Variant' ), go . Histogram ( x = ex_2 . tolist (), name = 'More Variant' ) ] layout = go . Layout ( title = 'Mean of 100' , xaxis = dict ( title = 'Value' ), yaxis = dict ( title = 'Volume' ), ) figure = go . Figure ( data = data , layout = layout ) plt . iplot ( figure ) In [56]: ex_1 . mean () Out[56]: 100.06843195281844 In [57]: ex_2 . mean () Out[57]: 99.93551822004906 We can now see how the spread of these distributions are quite different. How can we measure spread? Range The simplest measure of spread is known as the range . This is the largest value in the sample subtracted from the smallest value in the sample. This can give us an idea of where the possible values of the sample lay. That is: $$ \\bar{S}_{range} = x_{max} - x_{min} $$ For instance, in our alien example, we have: In [58]: max_alien_neurons , min_alien_neurons = alien_neurons . agg ([ 'max' , 'min' ]) rng = max_alien_neurons - min_alien_neurons print ( '''Maximum: {} Minimum: {} Range: {} ''' . format ( max_alien_neurons , min_alien_neurons , rng )) Maximum: 100856570 Minimum: 100223744 Range: 632826 From this, we know the difference between the largest and smallest values. To see how the original difference compares to this, we can look at the ratio of the difference and the range. In [59]: abs ( alien_mean - alien_median ) / rng Out[59]: 0.002525117488841235 Here we can get a sense of how \"close\" the median and mean are. This ratio is necessarily between 0 and 1, with 0 representing the same point, and 1 representing the largest difference in the sample (that is $x_1 - x_2 = x_{range}$). Although simple, the range suffers from some problems in that it only can measure the differences between the largest and smallest points. It is therefore as variable as those points. Suppose we had a dataset that looked like the following: 0, 50, 50, 50, 90, 95, 100. Then, the mean is: In [60]: example_ds = pd . Series ([ 0 , 50 , 50 , 50 , 90 , 95 , 100 ]) example_ds . mean () Out[60]: 62.142857142857146 In [61]: example_ds . median () Out[61]: 50.0 Using the ratio calculation as before, we would calculate: In [62]: ( example_ds . mean () - example_ds . median ()) / ( example_ds . max () - example_ds . min ()) Out[62]: 0.12142857142857146 In [63]: data = [ go . Histogram ( x = example_ds . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . mean (), 'y0' : 0 , 'x1' : example_ds . mean (), 'y1' : 5 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . median (), 'y0' : 0 , 'x1' : example_ds . median (), 'y1' : 5 , }] layout = go . Layout ( title = 'Dummy' , xaxis = dict ( title = 'Number' ), yaxis = dict ( title = 'Volume' ), shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Now let's do something crazy -- let's change the last value of the series to 1 million, and see what happens to the \"ratio\" of the difference between the median and mean to the range. In [64]: example_ds . replace ({ 100 : 1000000 }, inplace = True ) In [65]: example_ds . median () Out[65]: 50.0 In [66]: ( example_ds . mean () - example_ds . median ()) / ( example_ds . max () - example_ds . min ()) Out[66]: 0.142855 In [67]: data = [ go . Histogram ( x = example_ds . tolist ())] shapes = [{ 'type' : 'line' , 'name' : 'Mean' , 'line' : { 'color' : 'red' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . mean (), 'y0' : 0 , 'x1' : example_ds . mean (), 'y1' : 6 , }, { 'type' : 'line' , 'name' : 'Median' , 'line' : { 'color' : 'black' }, 'xref' : 'x' , 'yref' : 'y' , 'x0' : example_ds . median (), 'y0' : 0 , 'x1' : example_ds . median (), 'y1' : 6 , }] layout = go . Layout ( title = 'Dummy' , xaxis = dict ( title = 'Number' ), yaxis = dict ( title = 'Volume' ), shapes = shapes ) fig = go . Figure ( data = data , layout = layout ) plt . iplot ( fig ) Huh. Here, the mean and median are very far apart -- and the mean is quite a bit off from the \"average\" person, as only one observation pushed it up into the thousands. This is, again, due to the lack of robustness of the mean. However, our ratio measure does not seem to think there is much of a difference between the difference in this case and in the previous case. We would expect that our most extreme values in the sample may be more variable then values closer to the center. In this case, we can take ranges of specific quartiles , as discussed in the previous post, to determine variability. Interquartile range In a similar way, we can define a range between any two percentiles of the data. In this case, the range is a specific case of this, where we take the 0th and 100th percentile of the data. As discussed earlier, this may be quite variant -- these are the most extreme values of our distribution. Suppose instead we took the difference between the 25th and 75th percentiles, or the 1st and 3rd quantiles. It would follow that this measure would be more robust then the max-min range. This is known as the inter-quartile range and is (unsurprisingly) robust to outliers. That is, we can say that the interquartile range in a finite sample is: $$ \\tilde{S}_{IQR} = Q_x(.75) - Q_x(.25) $$ Where $Q_x(p)$ is the quantile function, i.e. the function that finds data which is the above $p%$ of the data. Arithmetically, we can determine this by finding the medians of the values above and below the median. Most software packages contain functions which can determine this automatically. In the case of the alien brains, we have: In [68]: iqr = alien_neurons . quantile ( . 75 ) - alien_neurons . quantile ( . 25 ) iqr Out[68]: 210608.0 This represents how distant the closest two points which account for 50% of the data are. In other words, we have determined a measure of how far away things are. Taking a similar ratio, we can determine how distant two points are. For example: In [69]: abs ( alien_mean - alien_median ) / iqr Out[69]: 0.007587366101921311 This value has a range between 0 and infinity. In general, each \"unit\" of this ratio measures how far away the two points are relative to the center 50% of the data . A small value indicates that two points are relatively \"close\", while a large value (>1) indicates they are \"distant\" relative to the center 50%. For more information on this robust measure of spread, see here Boxplots Using the information we have learned about the measures of spread and measures of central tendency, we can now culminate it together into a plot called a box and whisker plot. This plot shows the median, the inter-quartile range, and the total range in one figure. This can give us a good \"overview\" of what the data looks like in terms of quantiles. In [70]: data = go . Box ({ 'x' : alien_neurons , 'name' : 'Alien Neurons' }) plt . iplot ([ data ]) The center line represents the median, the two lines surrounding in the 25th and 75th quartiles, respectively, and the endpoints (and whiskers) represent the minimum and maximum. To compare to the non-symmetric simulated plot we discussed before, we can see large differences between the two: In [71]: data = go . Box ({ 'x' : sim , 'name' : 'Not Alien Neurons' }) plt . iplot ([ data ]) We can see now how much of the inferences about spread can be understood through the use of quantiles and differences between them. These measures are quite robust, but suffer in the sense of efficiency. In a manner equivalent to the median and mean, these measures of variability do not take into account every data point, as the mean does, but only considers the quantiles. The next measure of spread, and the most common historically and in practice, is the variance which is defined in terms of the mean instead of in terms of quantiles. But first, we will discuss the degrees of freedom implicit in the mean. Freedom -- the dream, and the degree to which we have it It is at this point that we should discuss degrees of freedom , before jumping into variance. Degrees of freedom are a concept which many struggle with in elementary statistics, and is often glossed over in respect to statistics courses. I believe that understanding the degrees of freedom are essential in understanding how randomness ties into statistical treatment of data. It is for this reason that I have included this here. If this feels too math heavy, feel free to disregard it -- it is not necessary to understand, but helpful. If you're fond of mathematics, read on. Consider it this way. Let's say I told you we have 100 points. How many degrees of freedom do these points have? When I ask this, I essentially am asking the question how many different ways can these points vary? Well, we only know that there are 100 points -- we know nothing of how these points are distributed. Therefore, they can conceivably be any 100 points, since we have no prior information. Now, suppose that I tell you that the mean of these points is 100. Have the degrees of freedom changed? The answer is yes. We can no longer vary the numbers absolutely freely -- we are constrained in the fact that the number must have some property -- the mean -- which equals some known value. In this case, n - 1 of the points can vary, after which the final number is fixed. Let's consider an example. Imagine we pick some random numbers out of a hat and get the following: 9, 8, 2, 42, 100, -2. Suppose there is only one paper left in the hat and I told you that I, an omniscient observer, know that the mean of all the values in the hat is 1000. What is the final value? Well, we know that $$ \\bar{X}_{mean} = \\frac{\\sum_{1}&#94;{n} x_i}{n} = \\frac{\\sum_{1}&#94;{n - 1}}{n} + \\frac{x_n}{n} $$ So, it follows that: $$ x_n = n\\bar{X}_{mean} - \\sum_{1}&#94;{n-1} x_i $$ This is only using elementary algebra. In our case, the final value is $$ 7 \\bar{X}_{mean} - \\sum_{1}&#94;{6} x_i\\\\ 7 \\times 1000 - (9 + 8 + 2 + 42 + 100 - 2) $$ This is: In [72]: points_picked = pd . Series ([ 9 , 8 , 2 , 42 , 100 , - 2 ]) last_value = 7000 - points_picked . sum () last_value Out[72]: 6841 To confirm that this is correct, lets see what the mean looks like. In [73]: points_picked . append ( pd . Series ( last_value )) . mean () Out[73]: 1000.0 But we could do this for any mean! Suppose the mean was instead -1000. Then In [74]: last_value = - 7000 - points_picked . sum () last_value Out[74]: -7159 In [75]: points_picked . append ( pd . Series ( last_value )) . mean () Out[75]: -1000.0 This describes what the mean tells us about the data -- if we know the mean, one of the data points essentially becomes redundant. Now, suppose I wanted to create my own hat game, where I wanted the mean to be 42. How could I do this? There are infinitely many sets of numbers I could choose that have a mean of 42. However, they all have the constraint that if we were to remove one value, we could reconstruct it in the way above ! That is, the final value is not random but determined . This means that we have n - 1 degrees of freedom -- or that the dimensionality of our random vector is now n -1. Then, I could randomly select n - 1 numbers, and select the final number deterministically such that $$ x_7 = 7 \\times 42 - \\sum_{1}&#94;{6} x_i $$ This will ensure that my mean is always 42. In fact, there is no such other number I could choose that has a mean of 42. Conversely, the number I select will uniquely determine the mean. So the mean has one degree of freedom -- it can be determined entirely from one number, if the other numbers are known. This becomes important because the more degrees of freedom we have, the more we allow random variation to play a part in our estimates. Although randomness may seem bad, in fact having more degrees of freedom can be a good thing, as is the case in linear regression -- a topic for a different day. In general, we have In [76]: def find_point_from_mean ( series_of_less , mean ): n = len ( series_of_less ) + 1 n_final = n * mean - series_of_less . sum () return n_final In [77]: find_point_from_mean ( points_picked , 1000 ) Out[77]: 6841 Freedom isn't free Now let's tackle the first case -- we have 100 points with a mean of 100. How many different ways can we select these 100 points? Consider the following: knowing only the mean of the distribution, we can select any n - 1 points, in this case 99, arbitrarily and completely at random, and then select the final point such that: $$ x_{final} = n\\bar{X}_{mean} - \\sum_{i=1}&#94;{n-1} x_i $$ In this case, this is $$ x_{final} = 10,000 - \\sum_{i=1}&#94;{99} x_i $$ Taken another way, we can say the following: Knowing the mean, we can calculate: $$ \\tilde{x}_i = x_i - \\bar{X}_{mean}\\\\ \\forall x_i \\in S $$ Only n -1 of these values are allowed to vary -- once we know n-1 of them, the last is determined by the calculation of the mean. In general, we have that the degrees of freedom can be decomposed as follows: $$ \\begin{bmatrix}x_1 \\\\ x_2 \\\\ ... \\\\ x_n\\end{bmatrix} = \\begin{bmatrix}\\bar{X}_{mean} \\\\ \\bar{X}_{mean}\\\\ ... \\\\ \\bar{X}_{mean}\\end{bmatrix} + \\begin{bmatrix} x_1 - \\bar{X}_{mean} \\\\ x_2 - \\bar{X}_{mean}\\\\ ... \\\\ x_n - \\bar{X}_{mean}\\end{bmatrix} $$ Note that this is in vector notation, but it makes the results easier to see. The vector on the left, our data points $x_1, x_2, ..., x_n$ have n degrees of freedom -- given no additional information, they are free to vary randomly. The first vector on the right, the vector of means, has 1 degree of freedom -- if we do not know it, it can vary in one dimension. That means it can be any single number. The final vector on the right now has n - 1 degrees of freedom (the degrees of freedom of the original vector minus the degrees of freedom of the mean). This means that given a mean, our data points can vary with dimension n - 1. The final dimension is then a linear combination of the other data points. Variance But how does this have any connection to the spread of a dataset? Happy you asked! One way we can consider spread is: how far away are the observations from the mean? In order to do this, we could take the differences between each point and the mean. This would look something like this: In [78]: x_differences = alien_neurons - alien_mean x_differences Out[78]: 0 -37191.04 1 56131.96 2 -208377.04 3 78291.96 4 209167.96 5 311696.96 6 157524.96 7 184399.96 8 -133574.04 9 11597.96 10 152764.96 11 -3920.04 12 -133156.04 13 -66259.04 14 29614.96 15 -45663.04 16 307873.96 17 106871.96 18 153068.96 19 -60630.04 20 -147816.04 21 7425.96 22 -156595.04 23 240394.96 24 1339.96 25 -22053.04 26 -118010.04 27 -262313.04 28 -146881.04 29 45270.96 ... 70 23517.96 71 234281.96 72 -129149.04 73 -226458.04 74 -48473.04 75 -61272.04 76 -32908.04 77 -146455.04 78 68237.96 79 -76984.04 80 151819.96 81 144539.96 82 111319.96 83 -132128.04 84 3298.96 85 -125061.04 86 59073.96 87 86979.96 88 -91383.04 89 112969.96 90 -253920.04 91 -157460.04 92 -32293.04 93 -321129.04 94 -107901.04 95 -152860.04 96 291216.96 97 91285.96 98 -4254.04 99 123267.96 Length: 100, dtype: float64 This will give us differences for each of the points. It would be nicer to have one number to measure the variability of the data, so we can take the average of these values: In [79]: x_differences . mean () Out[79]: -6.556510925292969e-09 Hm. This value is very close to zero. This is actually expected, simply because of the definition of the arithmetic mean . Consider the following: $$ \\bar{X}_{mean} = \\frac{\\sum_{i=1}&#94;{n} x_i}{n}\\\\ \\sum_{i=1}&#94;{n}(x_i - \\bar{X}_{mean}) =\\\\ \\sum_{i=1}&#94;{n}x_i - n\\bar{X}_{mean} = \\\\ \\sum_{i=1}&#94;{n}x_i - n\\frac{\\sum_{i=1}&#94;{n} x_i}{n} =\\\\ 0 $$ Intuitively, the mean is the value which is in the \"center\" of the distribution, in the sense that the differences above the mean are equal to the differences below the mean. The mean is specifically selected such that this sum will equal to zero. But in our case, we are not very happy with this. We are not as concerned with the sign of the distance from the mean, but how far away it is. We are not interested in the direction but the magnitude . For example, if the mean is 2 and we have two points around it, 1 and 3, we would say that they are equally distanced from the mean -- the sign of the difference (-1 and 1) does not matter. How can we eliminate the sign? Mathematically, there are two general approaches to this -- we can take the absolute value or we can take the squares . The absolute value preserves the exact magnitude, while squaring does not. Why might we want to take the squares? Squaring the distance values will essentially put more weight on values which are far from the mean, and less on those close. In this sense, we get a penalization in the case of extreme values -- extreme values will make our variance larger than \"common\" values. Additionally, historically the squares have been used because they are analytically \"nicer\" -- they are smooth functions for which we can take the derivative, while the absolute value is not. In the computing age, this has become less of an issue as numerical approaches to optimization problems have taken hold, but I digress. Since the squared version is used the most in statistical literature, we will define this first. We can define the sum of squares as the following: $$ SS = \\sum_{i=1}&#94;{n}(x_i - \\bar{X}_{mean})&#94;2 $$ This gives us a sense of the total amount of squared deviations from the mean. This is a measure of spread. It may be useful to get an average of these values, i.e. the \"typical\" squared deviation from the mean. This is known as the mean squared deviation from the mean, or the variance . In the case of a known population mean , we can calculate this variance directly from our sample. In this case, all of our values are allowed to vary freely -- they are not reliant on this known population mean. So we can take the mean as follows: $$ \\hat\\sigma&#94;2 = \\frac{1}{n}\\sum_{i=1}&#94;{n}(x_i - \\mu)&#94;2 $$ Of course, this would not happen often in practice. But since the mean was known, our values are not linearly computable directly from it -- in this sense, they can be any values. If we do not, however, we must make a correction known as the Bessel's correction . The sample variance is the variance which takes this into account, as follows: $$ \\hat{s}&#94;2 = \\frac{1}{n-1}\\sum_{i=1}&#94;{n}(x_i - \\bar{X}_{mean})&#94;2 $$ Here, we divide by n-1 because the vector of $x_i - \\bar{x}_i$ is only allowed to vary in n-1 dimensions -- because it is determined once the other points are calculated. This is how degrees of freedom relates to the sample variance.","tags":"Beginner Statistics","url":"https://seanammirati.github.io/category/beginner-statistics/sumstatsmeasuresofspread.html","loc":"https://seanammirati.github.io/category/beginner-statistics/sumstatsmeasuresofspread.html"},{"title":"Summary Statistics Part 1: Central Tendency","text":"Summary Statistics (and what they mean) Welcome! In this (very first) post, we are going to be going over a few basic statistical concepts involving summary statistics and how we can use them. This is a great place to start for getting some idea on how statistics can work for you! Statistics courses usually start with a discussion of summary statistics, and for good reason. Summary statistics are the essential building blocks of statistics. When we talk about expectation, or prediction, we are actually talking about a summary statistic -- in many cases, and algorithms, the mean. For the most part, summary statistics can give us a good idea of the underlying distribution and, in some cases, are sufficient in describing a set of data points for a particular algorithm. Don't Be Another Statistic The first question that will come up immediately when discussing this is what exactly a statistic is. In everyday language many people use statistic to mean a way of summarizing data into a single data point. When people say things like \"You are not just a statistic!\", what they are generally referring to is how statistics are aggregates of data. That is, statistics cannot capture all of the truth, but only an average. So is it fair to say that statistics are purporting to be cold-hard facts gathered from data? Not exactly. Statistics are gathered from sample data, and are often used as an estimation of the population. In statistician speak, a population is the theoretical overarching group that we wish to infer about. For instance, the population can be men living in the United States, all of the atoms in the universe, or the set of all possible combinations of people into couples. A sample is a subset of the population, a smaller group derived from it. We use statistics derived from the sample to make inferences about the population, but by definition these are not exact -- they depend on the sample that we choose. Almost surely, if we were to choose another sample we would come up with a different statistic. Therefore, a statistician would not take a summary statistic to be the hard, cold \"truth\", but a signal, or estimation, of the \"cold hard truth.\" Statistics vs Statistic Unfortunately, much of the confusion about the nature of a statistic has to do with the field known as statistics. Confusing, but true. Statistics is generally what people are referring to when they talk about the hand-wavy idea -- \"gathering insights from data.\" But this treats data as though it is a fixed, constant thing. This couldn't be further from the case! This is exactly what makes statistics such an interesting field -- statistics deals with random variation. Statistics isn't used for cases of absolute certainty, like many of the sciences do, but revels in the uncertainty of real life. Indeed, nearly everything has random variation involved in it, and therefore nearly everything is in the scope of statistics. The way we can quantify uncertainty is by using the theories of probability. But that is for another post! In general, we do not say that a statistic is the \"ground truth\" but a description of sample data. This sample statistic is then used to estimate the </b> population parameters.</b> This comes from the initial distinction that many people make -- samples themselves are not a representation of the ground truth. When a statistic is used as a statement about the population, we call this statistic an estimator of the population parameter. In fact, population parameters are not nearly as interesting as sample statistics. A population parameter is not probabilistic at all -- if it is obtainable, then it will be a single value with no variation. Sample statistics, however, do vary, and sometimes considerably based on how the data is sampled and how large the sample is. Throughout this exercise, I am going to be using some code to give examples of how these things work. I will only be displaying code that I believe is helpful for the task at hand, and I will create a future article describing how this code is written using python. In this post, which I will be calling a \"framework\" post, I will only be describing how these things work. Then, in a \"application\" post, I will show how to write code to solve these problems with real-world data. Class Grades Now, onto an example. Suppose we have a class of 7th graders who had gotten the following grades on the previous exam: 95, 80, 50, 80, 100, 94. Let's start with the situation where this is the entire class. In this case, we are dealing with the population. That means, if we take the common 'statistics', we will not have a statistic at all but a parameter. This distinction makes it clear that we are not estimating anything, but are looking at the \"cold, hard truth.\" The population is then \"the grades of 7th graders on math test last week.\" If, however, we were trying to estimate their performance in the class overall from this test alone, then this is a sample , presuming there were other tests. Thus, the phrasing of the question is a large distinction. In [1]: from itertools import combinations import numpy as np import pandas as pd from scipy import stats import matplotlib as plt % matplotlib inline In [2]: class_grades = pd . Series ({ 'John' : 95 , 'Mary' : 80 , 'Jacob' : 50 , 'Jose' : 80 , 'Sarah' : 100 , 'Laura' : 94 }) class_grades Out[2]: John 95 Mary 80 Jacob 50 Jose 80 Sarah 100 Laura 94 dtype: int64 Central Tendency Given a set of data points, where do we start? A good start is determining the central tendency or what we often call the average. Here, I use the word average as a general case of central tendency -- we want to know what the average student in this class looks like. All of the following examples assumes that we have some finite sample of a finite population, but can be extended in the more general case. Arithmetic Mean One common example of an average is the arithmetic mean. We are all familiar with this from our schooling days: \\begin{equation} \\bar{X}_{mean} = \\frac{ \\sum_{i=0}&#94;{n} x_{i} }{n} \\end{equation} Here, we say that we sum over all of the possible values, and divide by the total number. We can see the results on our simple dataset above: $$ 95 + 80 + 50 + 80 + 100 + 94 = 499 = T \\\\ \\bar{x} = \\frac{T}{6} = 83.166 $$ In [3]: mean = class_grades . mean () mean Out[3]: 83.16666666666667 Median Another measure of central tendency is the median. The median is the value which half of the data is smaller and half is larger than. If there is an even number of points, we take the average of the two closest to the \"center\" of the ranked numbers. For example, we first list the points from smallest to largest: $$ 50, 80, 80, 94, 95, 100 $$ Then, we iteratively remove points from each side until we arrive at a single value (if the total number of points is odd) or two values (if it is even). This is shown below: $$ 50, 80, 80, 94, 95, 100\\\\ 80, 80, 94, 95\\\\ 80, 94\\\\ $$ Now that we have two values (since 6 = n is even), we take an average of these values. That is: $$ \\frac{(80 + 94)}{2} = 87 $$ Note that we can also express this as follows: Rank the $X_i$ from smallest to largest, where $i = 1, ..., n$. Define $X&#94;{(i)}$ as the $i$th order statistic , where $i$ is the rank in this list. Then: $$ \\tilde{X}_{median} = \\begin{cases} X&#94;{(\\frac{n+1}{2})}, & \\text{if n is odd}\\\\ \\frac{X&#94;{(\\frac{n+2}{2})} + X&#94;{(\\frac{n}{2})}}{2}, & \\text{if n is even} \\end{cases} $$ Then, $n$ in this case is 6, which is even. So we take the average of the 3rd and 4th order statistic, 80 and 94. This is the same result we had derived earlier. In [4]: median = class_grades . median () median Out[4]: 87.0 Mode Finally there is the mode. The mode is simply the most likely value, or the most recurring value. Thus there can be multiple modes if there are two values which are equally likely and are the most recurring in the dataset. We can summarize this by looking at the frequencies of each data point. For convenience, I have done this using pandas. In [5]: class_grades . value_counts () Out[5]: 80 2 95 1 94 1 100 1 50 1 dtype: int64 Because 80 occurs twice, it is the mode. In [6]: mode = class_grades . mode () mode Out[6]: 0 80 dtype: int64 For all unique values $u_j$ of $x_i$ in the sample, let $f(u_j) = \\sum_{x_i} 1, \\forall x_i = u_j$ Then, $f(u_j)$ is the frequency of unique value $u_j$. Then: $$ \\hat{X}_{mode} = \\{ u_j \\mid f(u_j) = \\max_{u_j}f(u_j) \\} $$ The set notation implies that it is possible for there to be multiple modes. If there is only one mode, such as in the case of the class grades, then we say that the sample is unimodal. Other Measures of Central Tendency These are all of the canonical measures of central tendency that you have likely heard about before. However, there are others as well such as those listed here . Check them out -- they're pretty interesting! For fun, I will show some examples below. Trimean The trimean is the weighted arithmetic mean of the median and two quartiles. That's a lot to unpack, so let me explain. By weighted, we mean that the observations are multiplied by some set of constants that place a magnitude on a particular observation. Higher weighted variables are considered more \"important\" to the calculation. Instead of dividing by the sample size, we divide instead by the sum of the weights . Another way of looking at this is that each observation is considered to have a probability of occurring, $\\frac{w_i}{\\sum_{i}w_i}$. Then, the weighted average is a generalization of the arithmetic average explained before, where the arithmetic average weighs each value equally, i.e, $w_i = c$ for all $i$. Then we arrive at the same formula we saw in equation (1). A quartile is the point that is greater then some multiple of 25% of the data. If there is no such discrete number, we take a weighted average to determine its value. It is called a quartile because it separates the data into four segments. The first quartile is the data point which is greater than 25% of the data, the second is the median (50% of the data), the third is for 75% of the data, and the forth quartile is also the maximum, greater than all of the data. Interestingly, this is often not considered a quartile, but by the same logic it can be considered one. The trimean is then defined as: $$ TM = \\frac{Q_1 + 2M + Q_3}{4} $$ Intuitively, this is the average of the Median (M) and the midhinge, defined as: $$ MH = \\frac{Q_1 + Q_3}{2} $$ This is calculated below: In [7]: def trimean ( series ): T = series . quantile ( . 25 ) + 2 * series . median () + series . quantile ( . 75 ) TM = T / 4 return ( TM ) TM = trimean ( class_grades ) TM Out[7]: 87.1875 In practice, it turns out that this estimator is a remarkably efficient estimator of the population, meaning it can accurately estimate the population mean if calculated on a sample with relatively fewer points than other estimators. For a symmetric distribution like the normal distribution, it is the most efficient as compared with the median, and other L-3 estimators. See here for more information. It is also is more robust than the mean, which will be discussed further below. Winsorized Mean This will be a good segue into the next section, which examines the most appropriate method of central tendency to use. The Winsorized Mean attempts to mitigate the effect of outliers -- values that are very far from what we would call the \"average\" value. If this doesn't make sense yet, don't worry. I'll be discussing it further later in the section on robustness. For now, we will just examine what this will look like. In the Winsorized mean, we will take the most extreme values of our sample and replace them with the most extreme remaining values. In this case, this is 50 and 100. So, our new, adjusted, sample to calculate the central tendency is: $$ \\tilde{S} = {80, 80, 80, 94, 95, 95} $$ Here, we replaced 50 with 80 -- the next smallest value -- and 100 with 95 -- the next largest value. In this case, the data is quite small, so we would likely not want to do this, but for a large dataset this can be useful. We then take the average of the remaining values. $$ \\frac{ \\sum_i \\tilde{s}_{i} }{n},\\\\ \\forall \\tilde{s}_i \\in \\tilde{S} $$ If you don't understand the notation, don't worry. We're just taking an average of our \"new\" sample. I just did it to be precise. In general, we can take the largest and smallest p% of data and replace it with the most extreme values within this range. This value is arbitrary, but is often taken to be 10 to 25% of the ends replaced. In this example, we take the top and bottom 10%, which trims the most extreme values on both sides. In [8]: def winsorized_mean ( series , percent_removed ): ## Taking the value which is greater than/less than percent_removed bot_quant = series . quantile ( percent_removed ) top_quant = series . quantile ( 1 - percent_removed ) #Finding the values to replace bottom = series [ series < bot_quant ] top = series [ series > top_quant ] #Replacements rep_bottom = min ( series [ series > bot_quant ]) rep_top = max ( series [ series < top_quant ]) replacement_dic = { b : rep_bottom for b in bottom } replacement_dic . update ({ t : rep_top for t in top }) # Make replacements, find mean S_tilde = series . replace ( replacement_dic ) winsorized_mean = S_tilde . mean () return ( winsorized_mean ) wm = winsorized_mean ( class_grades , . 1 ) wm Out[8]: 87.33333333333333 But wait! How can you just replace values like that! I know, it seems dishonest. The idea is that these values are extremely unlikely. In the case of 6 people, this is probably not the case. But imagine we had the following values: $$ 0, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 100 $$ In [9]: ex_outliers = pd . Series ( np . concatenate ([[ 0 ], np . repeat ( 95 , 18 ), np . repeat ( 96 , 12 ), [ 100 ]])) ex_outliers . mean () Out[9]: 92.5625 But, would we say that the average student received a grade of 92? This seems like an underestimation. Naively, if we choose the Winsorized mean, we would get the following: In [10]: winsorized_mean ( ex_outliers , . 1 ) Out[10]: 95.40625 This seems like a more accurate representation of the data. In the cases of outliers, one must be careful to examine the distribution of the data well before naively using one measure of central tendency, as the results may be very misleading. Which to choose? This is not always clear, as we saw above for the Windsorized mean. If we look at a histogram of the values, we can get a sense of the shape of the distribution. It turns out that the shape can help us determine which measure of central tendency best describes the distribution. In [11]: plt . pyplot . figure ( figsize = ( 15 , 10 )) plot = class_grades . hist () plot . axvline ( x = mean , color = 'red' , label = 'Arithmetic Mean' ) plot . axvline ( x = median , color = 'black' , label = 'Median' ) plot . axvline ( x = mode [ 0 ], color = 'green' , label = 'Mode' ) plot . axvline ( x = TM , color = 'yellow' , label = 'Trimean' ) plot . axvline ( x = wm , color = 'purple' , label = 'Winsorized Mean' ) plot . legend () Out[11]: <matplotlib.legend.Legend at 0x11662c5da58> All of these represent central tendency. To see this more easily, let's consider it on a larger dataset. In [12]: likelihood_estim = stats . beta . fit ( class_grades / 100 ) random_var = pd . Series ( np . random . beta ( likelihood_estim [ 0 ], likelihood_estim [ 1 ], 100 )) * 100 random_var = random_var . floordiv ( 1 ) C:\\Users\\ginge\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:437: RuntimeWarning: invalid value encountered in sqrt sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b) In [13]: plt . pyplot . figure ( figsize = ( 15 , 10 )) plot = random_var . hist ( bins = 30 ) plot . axvline ( x = random_var . mean (), color = 'red' , label = 'Arithmetic Mean' ) plot . axvline ( x = random_var . median (), color = 'black' , label = 'Median' ) plot . axvline ( x = random_var . mode ()[ 0 ], color = 'green' , label = 'Mode' ) plot . axvline ( x = winsorized_mean ( random_var , . 1 ), color = 'yellow' , label = 'Winsorized Mean' ) plot . axvline ( x = trimean ( random_var ), color = 'purple' , label = \"Trimean\" ) plot . legend () Out[13]: <matplotlib.legend.Legend at 0x11662e02d30> We see that this is a skewed distribution -- in particular it is left skewed . A majority of the students did well on the exam, but there were some who did particularly poorly, with some extreme values on the far left. In this case, where there are many extreme values, the mean is not a good \"average\" -- it is dragged down by the very small values in the tail. We can see this as the mean is close to 70, but it appears that the \"average\" student did better than this. The median is a more robust measure, because it is less sensitive to anomalies in the sample. The mode is not very useful in this case, as it is at the bin with the largest values. The \"average\" student likely did worse than this. In general, as a rule of thumb, for a unimodal distribution (meaning there is only one mode), it is often but not always the case that: Let $\\bar{X}_{median}$ be the median of a distribution and $\\bar{X}_{mean}$ be the arithmetic mean. Then: if $\\bar{X}_{median} << \\bar{X}_{mean}$, X is right skewed if $\\bar{X}_{median} \\approx \\bar{X}_{mean}$, X is approximately symmetric if $\\bar{X}_{median} >> \\bar{X}_{mean}$, X is left skewed This can be used as a guideline to see how skewed our distribution is. However, it is not foolproof - in some cases this fails. See this wikipedia article for more information. For instance, consider -6, -4, 0, 0, 2, 8. The mean and median are zero, but this distribution is skewed to the left, i.e. not symmetric. In [14]: plt . pyplot . figure ( figsize = ( 15 , 10 )) example = pd . Series ([ - 6 , - 4 , 0 , 0 , 2 , 8 ]) example . hist () plot . axvline ( x = example . mean (), color = 'red' , label = 'Arithmetic Mean' ) plot . axvline ( x = example . median (), color = 'black' , label = 'Median' ) plot . legend () Out[14]: <matplotlib.legend.Legend at 0x11662ea32e8> In general, since the median is a more robust measure, if we are given a sample and wish to estimate the behavior of the population and we believe that the skewed values are anomalies , the median is a better \"guess\" of the central tendency. Interestingly, the two \"non-typical\" central tendency estimators, the Windsorized mean and the Trimean, lay between the mean and the median. The Windsorized mean and Trimean are both more robust then the mean, meaning they are less sensitive to extreme values. This is directly dealt with in the Windsorized mean and is implicit in the Trimean in a method similar to taking the median. Notice also that the median and mean tell us different things about the data. The mean uses all of the information available, and each data point contributes equally to the evaluation of it. On the other hand, the median \"cuts away\" all of the data not at the 50% mark. In this way, the median can be seen as an extreme version of the Windsorized Mean where we cut out the bottom 49% and 51%. In this case, we are seeing a trade-off between robustness which is the variability of the estimator under anomalies and varying sample distributions, and power (or efficiency ), which can determine a more stable solution with less data assuming certain assumptions are met. For instance, if we know that the population distribution follows some \"known\" distribution, it can often be shown that the mean is a sufficient, unbiased estimator of the true population mean. In the case of the normal distribution, it is the most efficient estimator of the population mean, while the median is relatively inefficient. This means that the median will require more data to obtain the same level of accuracy as the mean. To sum it all up -- the mean is often the most efficient estimator in certain simplifying conditions, but is not robust. On the other hand, the median is robust , but not as efficient. Samples of samples of samples, oh my! Why do we care about these measures of central tendency? Well, let's take a look at how they function in terms of predicting a population parameter. Suppose, as we talked about before, the population in a simplistic example is 95, 80, 50, 80, 100, 94. Let's consider all possible samples of 3 of this data. There are 6 data points and we wish to choose 3. This means there are 6 choose 3, or 20 possible samples. The possible samples are: In [15]: possible_samples = combinations ( class_grades , 3 ) list ( possible_samples ) Out[15]: [(95, 80, 50), (95, 80, 80), (95, 80, 100), (95, 80, 94), (95, 50, 80), (95, 50, 100), (95, 50, 94), (95, 80, 100), (95, 80, 94), (95, 100, 94), (80, 50, 80), (80, 50, 100), (80, 50, 94), (80, 80, 100), (80, 80, 94), (80, 100, 94), (50, 80, 100), (50, 80, 94), (50, 100, 94), (80, 100, 94)] We remember from before that the population mean is 83.16666. This is a population parameter -- it is not a statistic. However, suppose we sampled randomly from this population and retrieved the sample data: 80, 94, 80. Then our estimate of the population mean would be 84.6. Remember when I said that the statistic is never the \"cold hard\" truth? Here is a direct example. However, there are some nice properties of this. Consider the means and medians of all the samples: In [16]: sample_dict = {} for i , x in enumerate ( combinations ( class_grades , 3 )): sample_dict [ i ] = x sample_results = pd . DataFrame ( sample_dict ) . T sample_results [ 'mean' ] = sample_results . mean ( axis = 1 ) sample_results [ 'median' ] = sample_results . iloc [:, 0 : 3 ] . median ( axis = 1 ) sample_results Out[16]: 0 1 2 mean median 0 95 80 50 75.000000 80.0 1 95 80 80 85.000000 80.0 2 95 80 100 91.666667 95.0 3 95 80 94 89.666667 94.0 4 95 50 80 75.000000 80.0 5 95 50 100 81.666667 95.0 6 95 50 94 79.666667 94.0 7 95 80 100 91.666667 95.0 8 95 80 94 89.666667 94.0 9 95 100 94 96.333333 95.0 10 80 50 80 70.000000 80.0 11 80 50 100 76.666667 80.0 12 80 50 94 74.666667 80.0 13 80 80 100 86.666667 80.0 14 80 80 94 84.666667 80.0 15 80 100 94 91.333333 94.0 16 50 80 100 76.666667 80.0 17 50 80 94 74.666667 80.0 18 50 100 94 81.333333 94.0 19 80 100 94 91.333333 94.0 We see that both the mean and the median vary around the true population mean for each sample. This is the sampling distribution of the sample mean. This is what has a distribution and can vary! Remember, our population parameter is fixed, but the variation implicit in each sample is what controls the distribution here. Since this too is a distribution, we can examine statistics about it. In [17]: sample_results . drop ([ 0 , 1 , 2 ], axis = 1 , inplace = True ) sample_results . mean ( axis = 0 ) Out[17]: mean 83.166667 median 87.200000 dtype: float64 In [18]: mean Out[18]: 83.16666666666667 Hmm, that seems odd. It turns out that the expectation, or arithmetic mean, of the sampling distribution of the sample mean is equal to the true population mean! This property actually follows directly from the definition of the arithmetic mean. We're going to go into some maths here, but feel free to skip over it if you don't want to. The point here is that the sample mean is an unbiased estimator of the population mean . Theorum 1: If $S$ is a sample of size $k$ from a population of size $N$ drawn randomly and without replacement , then the mean of the sample, $\\bar{X}_S$, is an unbiased estimator of the population mean. Let $\\bar{X}_P$ be the population mean. Then $$ \\bar{X}_P = \\frac{ \\sum_{i=1}&#94;{N} x_{i} }{N}\\\\ $$ Let $\\bar{X}_s$ be the mean of a sample of size k, called $S_s$. Then, $$ \\bar{X}_s = \\frac{ \\sum_j x_{j, s} }{k}\\\\ $$ Where $X_{j,s}$ is the $j$th observation from sample $s$. It follows that there are ${N}\\choose{k}$ possible samples, ie, $s$ runs from $1$ to ${N}\\choose{k}$. Let $M =$ ${N}\\choose{k}$. Then: $$ \\sum_{s=1}&#94;{M} \\frac{\\bar{X}_s}{M} =\\frac{\\sum_{s=1}&#94;{M} \\sum_j \\frac{x_{j, s}}{k}}{M}\\\\ =\\frac{\\sum_{s=1}&#94;{M} \\sum_j x_{j, s}}{kM} $$ Each value of the population will appear in ${N}\\choose{k}$ - ${N-1}\\choose{k}$ = $M - \\frac{M(N-k)}{N}$ samples. We can say this because it is the total number of possible samples ${N}\\choose{k}$ minus the total number of samples of size $k$ which do not contain observation $j$, ${N-1}\\choose{k}$. So then we have: $$ \\frac{M - \\frac{M(N-k)}{N}}{kM} \\sum_i x_i $$ Expanding the left hand side: $$ \\frac{M - \\frac{M(N-k)}{N}}{kM} = \\frac{1}{k} - \\frac{N-k}{kN} =\\frac{1}{N} $$ so $$ \\sum_{s=1}&#94;{M} \\frac{\\bar{X}_s}{M} = \\frac{ \\sum_{i=1}&#94;{N} x_{i} }{N}\\\\ $$ But, we now see that this is exactly equal to the population mean, $\\bar{X}_P$! Since we weighed each sample equally in this expectation, this is equivalent to sampling randomly without replacement. This means that we have proven the result -- the sample mean is an unbiased estimator of the population mean. This is a powerful result! Hopefully you can begin to see why statistics is so exciting now! We have proven that any inference about the population mean using a sample mean will, on average, be accurate. However, it is important to note that while the sample mean is unbiased, it does not make it the most accurate estimator. This will be covered in the next post, when we discuss the Measures of Spread. We've seemed to disregard the median in this case. Why is this? Well, the median is not an unbiased estimator of the population mean. However, we see that the case changes quite a bit when we take the medians! In [19]: sample_results . median ( axis = 0 ) Out[19]: mean 83.166667 median 87.000000 dtype: float64 In [20]: median Out[20]: 87.0 In this case we see that both the mean and the median are equal to the population values. What gives?! Well, it turns out that the sample median is a median unbiased estimator of the population median. This one is a bit tougher to prove, so I will leave it for now. Perhaps we can get back to it when we discuss probability density functions, as it requires some use of this. What about the mean? Why is the mean a median unbiased estimator of the population mean? The reason is because of the Central Limit Theorum, but this is something that will take some time to discuss! However, because the sampling distribution of the sample mean will tend to a normal distribution relatively quickly, it also becomes symmetric quickly. In this case, the median converges towards the mean. That is why we see that the mean in this case is again equal to the median. We can look at a histogram to see the details: In [21]: plt . pyplot . figure ( figsize = ( 15 , 10 )) sample_results [ 'mean' ] . hist () Out[21]: <matplotlib.axes._subplots.AxesSubplot at 0x11662e70278> Conclusion In this first post, we went over what a statistic is and the distinction between a sample and a population. We also discussed measures of central tendencies and their relation to sampling distributions. In the next post, we will be discussing the Measures of Spread which will shed light further on the sampling distribution of the sample mean. Hope you enjoyed!","tags":"Beginner Statistics","url":"https://seanammirati.github.io/category/beginner-statistics/sumstatscentraltendency.html","loc":"https://seanammirati.github.io/category/beginner-statistics/sumstatscentraltendency.html"},{"title":"Bootstrapping Exercise","text":"This is a simple example of using the boot function in R to produce bootstrapped estimates of the bias and standard error of the parameters. In [2]: install.packages ( 'ISLR' ) Updating HTML index of packages in '.Library' Making 'packages.html' ... done In [3]: library ( ISLR ) library ( boot ) attach ( Default ) head ( Default ) glm.fit <- glm ( default ~ income + balance , family = \"binomial\" , data = Default ) summary ( glm.fit ) boot.fn <- function ( data , index ) { return ( glm ( default ~ income + balance , family = \"binomial\" , data = data , subset = index ) $ coefficients ) } boot.fn ( Default , 1 : 100 ) boot ( Default , boot.fn , 1000 ) summary ( glm.fit ) default student balance income No No 729.5265 44361.625 No Yes 817.1804 12106.135 No No 1073.5492 31767.139 No No 529.2506 35704.494 No No 785.6559 38463.496 No Yes 919.5885 7491.559 Call: glm(formula = default ~ income + balance, family = \"binomial\", data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4725 -0.1444 -0.0574 -0.0211 3.7245 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.154e+01 4.348e-01 -26.545 < 2e-16 *** income 2.081e-05 4.985e-06 4.174 2.99e-05 *** balance 5.647e-03 2.274e-04 24.836 < 2e-16 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1579.0 on 9997 degrees of freedom AIC: 1585 Number of Fisher Scoring iterations: 8 (Intercept) -26.5660685235383 income 9.0174230280541e-20 balance -3.21887798899407e-20 ORDINARY NONPARAMETRIC BOOTSTRAP Call: boot(data = Default, statistic = boot.fn, R = 1000) Bootstrap Statistics : original bias std. error t1* -1.154047e+01 -4.009952e-02 4.329822e-01 t2* 2.080898e-05 1.740632e-07 4.721751e-06 t3* 5.647103e-03 1.964705e-05 2.331222e-04 Call: glm(formula = default ~ income + balance, family = \"binomial\", data = Default) Deviance Residuals: Min 1Q Median 3Q Max -2.4725 -0.1444 -0.0574 -0.0211 3.7245 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.154e+01 4.348e-01 -26.545 < 2e-16 *** income 2.081e-05 4.985e-06 4.174 2.99e-05 *** balance 5.647e-03 2.274e-04 24.836 < 2e-16 *** --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2920.6 on 9999 degrees of freedom Residual deviance: 1579.0 on 9997 degrees of freedom AIC: 1585 Number of Fisher Scoring iterations: 8 As we can see above, the boot function found standard errors that were greater than the standard errors of the summary function. This makes sense, as the bootstrap method does not depend on estimates of the deviations of the error terms -- it simply samples observations repeatedly and creates models with them, and uses the difference between that and the full model to determine the standard error. So, as expected, the true standard error is likely larger than that given by the summary, since the summary is dependent on the type of model itself, while the bootstrap is not.","tags":"Model Validation","url":"https://seanammirati.github.io/category/model-validation/bootstrapping-exercise.html","loc":"https://seanammirati.github.io/category/model-validation/bootstrapping-exercise.html"},{"title":"Mixed Models vs GEE","text":"This is an example using 'fake' data for the difference between lmer (which assumes a constant (independent) covariance matrix for the error terms) and geepack (general estimating equations, which can handle an unknown convariance structure)) This example highlights the power of using a GEE model over a HLM model when the covariance structure differs at different levels of the data. The GEE model can be seen as a population estimating model -- i.e., it is preferable when we are trying to make inferences about the population rather than the individuals in the sample. We do not explicitly parametrize over the groups -- instead, we assign some covariance structure between members of each group and find the marginal distribution. The problem is formulated as follows: consider 6 athletes with characteristics given by: In [1]: library ( lme4 ) library ( geepack ) Loading required package: Matrix In [2]: athlete <- c ( 1 , 1 , 1 , 2 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 6 , 6 , 6 ) age <- c ( 38 , 40 , 43 , 53 , 55 , 56 , 58 , 37 , 40 , 42 , 41 , 45 , 46 , 54 , 58 , 57 , 60 , 62 ) club <- c ( 0 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 1 ) time <- c ( 95 , 94 , 93 , 96 , 98 , 91 , 93 , 83 , 82 , 82 , 91 , 94 , 99 , 105 , 111 , 90 , 89 , 95 ) logtime <- log ( time ) This is an example of a repeated measures problem. We create a dataframe from these elements: In [3]: df <- data.frame ( athlete , age , club , time , logtime ) df athlete age club time logtime 1 38 0 95 4.553877 1 40 0 94 4.543295 1 43 1 93 4.532599 2 53 1 96 4.564348 2 55 1 98 4.584967 2 56 1 91 4.510860 2 58 1 93 4.532599 3 37 1 83 4.418841 3 40 1 82 4.406719 3 42 0 82 4.406719 4 41 0 91 4.510860 4 45 1 94 4.543295 4 46 0 99 4.595120 5 54 1 105 4.653960 5 58 1 111 4.709530 6 57 1 90 4.499810 6 60 1 89 4.488636 6 62 1 95 4.553877 Now, we want to estimate the (log) time based on age and club. However, the data is clearly correlated -- each individual will have different specifications. Consider that we believe that the average (log) time will be different for each individual, as a baseline. This is a mixed model more specifically a Random Intercept model. This is highlighted by the following equations. In [4]: mod1 <- lmer ( logtime ~ club + log ( age ) + ( 1 | athlete ), data = df , REML = FALSE ) mod2 <- lmer ( logtime ~ log ( age ) + ( 1 | athlete ), data = df , REML = FALSE ) summary ( mod1 ) summary ( mod2 ) Linear mixed model fit by maximum likelihood ['lmerMod'] Formula: logtime ~ club + log(age) + (1 | athlete) Data: df AIC BIC logLik deviance df.resid -49.2 -44.7 29.6 -59.2 13 Scaled residuals: Min 1Q Median 3Q Max -1.3445 -0.7551 -0.1840 0.7895 1.2651 Random effects: Groups Name Variance Std.Dev. athlete (Intercept) 0.0042173 0.06494 Residual 0.0008858 0.02976 Number of obs: 18, groups: athlete, 6 Fixed effects: Estimate Std. Error t value (Intercept) 3.78158 0.45714 8.272 club -0.01049 0.02101 -0.499 log(age) 0.19753 0.11837 1.669 Correlation of Fixed Effects: (Intr) club club 0.184 log(age) -0.998 -0.216 Linear mixed model fit by maximum likelihood ['lmerMod'] Formula: logtime ~ log(age) + (1 | athlete) Data: df AIC BIC logLik deviance df.resid -50.9 -47.4 29.5 -58.9 14 Scaled residuals: Min 1Q Median 3Q Max -1.3343 -0.6924 -0.3215 0.7936 1.2927 Random effects: Groups Name Variance Std.Dev. athlete (Intercept) 0.0042948 0.06554 Residual 0.0008962 0.02994 Number of obs: 18, groups: athlete, 6 Fixed effects: Estimate Std. Error t value (Intercept) 3.8241 0.4527 8.448 log(age) 0.1846 0.1164 1.586 Correlation of Fixed Effects: (Intr) log(age) -0.998 However, this assumes that the covariance structure of the errors at each level are independent. This assumption is too simplifying -- it is likely that the variables are correlated across individuals (for instance, it appears that the clubs are for particular age ranges). To account for this, we could add more random effects to the model (which would subsequently break the data down into even smaller groups), or we could use a GEE to forgo the subject-level estimations and estimate on the populations, assuming some covariance structure between groups. In [5]: mod3 <- geeglm ( logtime ~ club + log ( age ), data = df , id = athlete , family = gaussian , corstr = \"exchangeable\" ) mod4 <- geeglm ( logtime ~ log ( age ), data = df , id = athlete , family = gaussian , corstr = \"exchangeable\" ) summary ( mod3 ) summary ( mod4 ) Call: geeglm(formula = logtime ~ club + log(age), family = gaussian, data = df, id = athlete, corstr = \"exchangeable\") Coefficients: Estimate Std.err Wald Pr(>|W|) (Intercept) 3.67345 0.42962 73.109 <2e-16 *** club -0.01429 0.01422 1.010 0.3149 log(age) 0.22584 0.11276 4.011 0.0452 * --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Estimated Scale Parameters: Estimate Std.err (Intercept) 0.004443 0.001609 Correlation: Structure = exchangeable Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.6112 0.1829 Number of clusters: 6 Maximum cluster size: 4 Call: geeglm(formula = logtime ~ log(age), family = gaussian, data = df, id = athlete, corstr = \"exchangeable\") Coefficients: Estimate Std.err Wald Pr(>|W|) (Intercept) 3.742 0.488 58.84 1.7e-14 *** log(age) 0.206 0.127 2.61 0.11 --- Signif. codes: 0 ‘***' 0.001 ‘**' 0.01 ‘*' 0.05 ‘.' 0.1 ‘ ' 1 Estimated Scale Parameters: Estimate Std.err (Intercept) 0.00458 0.00154 Correlation: Structure = exchangeable Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.624 0.181 Number of clusters: 6 Maximum cluster size: 4 The main shift in inference here is from likelihood models (which are generally prefered) to a semi-parametric model that depends only on the first two moments. GEEs are better estimators of a population-level variance -- they cannot account for individual differences explicitly, as a GLMM could. In this example, however, we see that the GEE finds the log(age) value significant where the previous model had not. Depending on the goal of the analysis, the GEE will produce better estimates asymptotically at the cost of sample-level inference.","tags":"Linear Models","url":"https://seanammirati.github.io/category/linear-models/mixed-models-vs-gee.html","loc":"https://seanammirati.github.io/category/linear-models/mixed-models-vs-gee.html"},{"title":"NYPD Stop and Frisk","text":"The Results of Stop and Frisk: What Contributes Most to Arrests? Note: All of the code associated with this project, as well as the data used, is accessible at my GitHub repository here . The code is omitted here for brevity. For the complete code used in this project, please visit the GitHub repository listed above, or check out this article which also includes the R code as well as some unannotated intermediate results. Introduction In this project I will be looking at a dataset available from the city of New York about the results of the Stop, Question and Frisk policy. Which factors lead to the highest likelihood of arrest? The data is a collection of 22,564 cases of Stop, Question and Frisk in the City of New York in 2015. There are 112 variables which were measured. A majority of the variables in the dataset are categorical variables relating to the incident itself, while a few represent the age, race, height, weight, and sex of the person who was on the receiving end of the policy. Stop and Frisk has been a controversial policy in New York City for quite some time. Many feel that it is a violation of privacy, that it is unconstitutional, or that the policy is inherently racist - that it disproportionately affects minorities and people of color. Meanwhile, there are others who support the policy, believing stop and frisk to keep people safe from terrorists and other wrongdoers. As a result of this and other recent controversies involving police across the country, the NYPD has been getting a lot of negative press over the way it addresses these racial problems. What I aim to discover by analyzing this dataset are the main contributing factors to whether a person is arrested or not in a stop and frisk incident. I will be using a few different methods of analysis to determine the best model to use in predicting whether a person will be arrested or not. Because the response variable, arstmade, is a qualitative, binary variable, I will be using categorical methods. These include logistic regression, K-Nearest Neighbors analysis and tree methods (like bagging, random forests, boosting). Data The data comes from the City of New York. It is a collection of 22,564 observations of 112 variables. Some of these variables had to be removed, as they did not contribute any information (for instance, they were the same value for all observations) or they had too many missing values. After removing these variables from the dataset, I had 75 variables to work with, with 22,563 observations. For example, lineCM was a redundant variable, as it had 1's for all observations. I also removed apt number, state and zip, as many of these values were left blank. Overall, none of the variables I was very interested in had any missing values, so removing these variables likely had very little effect on the analysis, if any. There were two variables for height (ht_inch and ht_feet) which I combined into one (height). The response variable I used for all of the following analyses was arstmade, which was a categorical variable that had values of \"Y\" for yes and \"N\" for no. Many of the categorical variables in the dataset followed this pattern. The percentage of arrests in all cases was 0.18, meaning that around 18% of the stops resulted in an arrest. The mean age of people who were stopped and frisked was around 29 years. Most of the people who were stopped were males (92.42%). Force was used in about 27% of the cases. In investigating whether blacks were targeted more frequently and if they were arrested more frequently, I created a categorical variable that was 1 if the person was black and 0 otherwise. Below is a bar graph of the arrests, with the colorings indicating the percentage who were black and non-black. It does not seem from these graphs alone that blacks were more likely to be arrested, since they have roughly half of the bars in both instances. However, it does seem like there are a disproportionate number of blacks who are stopped and frisked in the first place, which is shown in the second graph. With blacks making up roughly 23% of the population, it is surprising that almost double that (53%) of the people who were stopped were black In [4]: ggplot ( mydata , aes ( arstmade )) + geom_bar ( aes ( fill = black )) + ggtitle ( \"Bar Graph of Arrests for Blacks vs. Non-Blacks\" ) + xlab ( \"Was the person arrested?\" ) + ylab ( \"Numer of Incidences\" ) + scale_fill_discrete ( name = \"Race\" , labels = c ( \"Non-Blacks\" , \"Blacks\" )) In [5]: ggplot ( mydata , aes ( black )) + geom_bar ( fill = \"red\" ) + ggtitle ( \"Blacks vs. Non-Blacks Who Were Stopped and Frisked\" ) + xlab ( \"Was the person black?\" ) + ylab ( \"Number of Incidents\" ) Some more interesting things found in the descriptive statistics: Around 6.7 times more people were arrested than given summonses. That is, only 2% of the people were given summonses. None of the people who were stopped had a machine gun, so I removed this variable from the data. I removed any variable which had levels that were exactly one, as this indicated that there was no variation between observations. Although the suspected crime variable (crimsusp) was of interest to me in my analysis, I found the data to be inconsistent in how it was recorded. For instance, if it was a felony, the officer had data entered that had both \"FEL\" and \"FELONY\" in two different cases. Of course, R would not be able to recognize the fact that these are the same, and would treat them as separate variables. As such, I decided to remove any variables which had levels larger than 10, as this would indicate that the data was too separated to be meaningful. Since much of this information is contained in the categorical variables (such as the rs variables, which had different values for different reasons for stopping the person), it was relatively inconsequential in my analysis to do so. There were also groupings which could raise doubt as to whether the information recorded was accurate. Of primary concern to me was pf_other (physical force used: other), rs_other (reason for search: other) and ac_other (additional circumstances: other). These are not very informative, and could be raise a speculative point about whether these were truly extraordinary cases or if they were omitted for other reasons. Of course, that is merely speculation, but it is important to note that this is a source of potential inconsistency and unreliability from the dataset. Even worse, some of these variables contributed a lot to the models which I have fit. Below shows a bar graph of the number of arrests conditional on whether the reason for search was classified as \"other.\" In [6]: ggplot ( mydata , aes ( sb_other )) + geom_bar ( aes ( fill = arstmade )) + ggtitle ( \"Arrests Conditional on if Reason for Search classified as OTHER\" ) + xlab ( \"Was the reason for search classifed as other?\" ) + ylab ( \"Number of Incidents\" ) + scale_fill_discrete ( name = \"Arrest Status\" , labels = ( c ( \"Not Arrested\" , \"Arrested\" ))) This confirms the results of the analysis below, as this variable has a large effect on whether the person was arrested or not. If the person was classified as being searched for other reasons not specified, they were arrested quite often. This leads to a limitation of the data and interpretation - we can not determine what the true reasons were. sb_other accounted for around 10.7% of all the cases, and 57.6% of all the people who were searched. Analysis and Model Fitting Logistic Regression I separated the data into two sets - test data and training data. The training data was randomly sampled from ¾ of the data. I began my analysis by using a logistic regression model to predict arstmade on all of the other variables. This was because the response variable was categorical, so it was necessary to use a logistic model instead of a linear model with normal errors. I then used stepwise reduction to determine the best fit and to reduce the amount of variables used. Since there were a great deal of variables that came up insignificant in the full model, it reduced the model quite a lot. I used the glm function using the binomial family to model the data. Below is a summary of the results of the models: In [11]: table1 arrest.test fit1 N Y 0 4513 397 1 126 590 In [12]: misclasserrorsfull table2 misclasserrorstepwise 0.0945609669392108 arrest.test fitted2 N Y 0 4508 401 1 131 586 0.0929612513330963 Using the stepwise regression model determined by the AIC produced a marginally better misclassification error, from .095 to .093 (or 9.5 % to 9.3%). This isn't a very big improvement, though. Surprisingly, the model does not find any of the race factors to be significant in determining whether an arrest was made or not besides Q, which stands for the White-Hispanics. That is, all of the levels of the \"race\" variable other than \"White-Hispanic\" came up insignificant. All of these coefficients are positive besides \"U\", which stood for \"unknown\", which essentially means that if the person's race was known, the odds of arrest are higher. The full summary of the stepwise regression is included in the appendix(1). When the stepwise regression was performed, we could see that contraband and knife/cutting instrument variables were highly significant in determining whether a person would get arrested or not. These variables are both very significant in the reduced logistic model. The variables that represent the different boroughs were all found to be significant, all with negative values. This suggests that the variable that was not included in the factors (the Bronx) had a higher likelihood of being arrested than any of the other four boroughs. Since the largest coefficient is that of Staten Island it means that, all else equal, a person from Staten Island who was stopped and frisked was less likely to be arrested than a person from Manhattan. The model also indicates that people who had a weapon were more likely to be arrested during a stop and frisk. This is logical, as if the police had found a weapon, it is quite likely that the person would be arrested. If the officer was in uniform, interestingly, a person would be less likely to be arrested, according to the model. If the police officer stopped the person because they knew them (indicated by rf_knowlY), the person was less likely to be arrested. If the reason the officer used force was to defend themselves, the person was less likely to be arrested than if the reason for force was that the person was suspected of running away. If the person is searched, they had a higher chance of being arrested than if they were not searched. KNN The next model I used was the K-Nearest Neighbors approach. K-Nearest Neighbors is a non-parametric method, so it does not make assumptions about the underlying distribution of the variables. K-Nearest Neighbors works by finding the nearest data points to the ones you are testing them against and assigning a classification based on the nearest k points. I have used k=5 and k=10 in this model to see if they can accurately predict the chances of arrest. Below are the tables for predicted vs actual arrest values for the test set, as well as the misclassification error, for the 10-nearest neighbor method (as this was the most successful.) In [15]: table3 round ( KNNMisclass , 3 ) arrest.test knn.predict N Y N 4479 749 Y 160 238 0.162 The 10-Nearest Neighbor model was the most successful of the different k's I used (I tried 1, 5, 10, and 20). Still, it doesn't produce very good outcomes compared to the logistic model. In fact, if I were to guess that none of the people would get arrested, I would have an error of about 18%, so this model is only marginally better than the trivial method. The failure of this method is due to the fact that the variables cannot exactly be scaled, and the distances are affected by this. As there are many categorical variables and few continuous ones, high numbers of the continuous variables may have influenced the predictions, since the KNN function in R uses Euclidian distances to predict the outcomes. To rectify this, I perform the K-NN method using only the categorical variables. The predictions are slightly better in this model, as shown by the confusion matrix and misclassification rate below. In [16]: table4 round ( KNNMisclass2 , 3 ) arrest.test knn.predict2 N Y N 4547 582 Y 92 405 0.12 Interestingly, while the error is still larger than the logistic models, or in fact any of the models I used in the analysis, it is more accurate at predicting an arrest when an arrest was made than any of the earlier models, suggested by the second row of the confusion matrix above. That is, while it produces more overall error than any of the models I've used, it also produces the smallest type I error out of any of the models (the false positives). Tree-Based Methods I will now use tree-based methods (a single tree, Random Forest, Bagging and Boosting) in order to predict whether a person will be arrested or not. The first model I use will be an unpruned tree using all the variables. The tree selects groupings of the variables which produces the least errors in predictions, and lists decision trees based on the conditional results of each tree. It determines these factors by a popular vote in the case of classification trees, so that the most common outcome is the one which is predicted by the tree. The tree is posted below. In [17]: tree . arrest <- tree ( arstmade ~ . , mydata [ train , ]) In [18]: plot ( tree . arrest , main = \"Unpruned Tree\" ) text ( tree . arrest , cex = 0.9 ) This tree has six nodes, so it is relatively simple. If the statement is true, you go to the right, and if it is false you go to the left. The top node is whether the person was searched or not. This makes sense, as if a person is searched, the police are more likely to find a reason to arrest the person (because of illegal goods, etc). It is already evident that this tree can be pruned. For the trhsloc and pf_hcuff variables, either decision results in the same response. This means that the variable is redundant, and can be removed from the tree. The values that are included are: searched, repcmd (reporting officers command, which takes values from 1 to 999), contrabn (contraband), sb_hdobj (basis of search being a hard object), trhsloc P,T (whether the location was a transit location or housing location), and pf_hcuff(force used was handcuffing). Some of these are quite interesting: the fact that the officers command has an influence on whether the person was arrested or not, and also the fact that regardless of whether the person was handcuffed or not, they were not reported as arrested if they reach the bottom left node. This defies our common sense - as we often expect someone who is handcuffed to be arrested. Based on the prior conditional factors, the tree says that if someone is handcuffed given they don't have contraband, they weren't searched, and the reporting officers command was less than 805, they would be classified as not being arrested. Since we already have seen that we should prune this tree, I will find the correct number of nodes to prune to by plotting the cross-validation errors. In [19]: cv . arrest <- cv . tree ( tree . arrest , FUN = prune . misclass ) In [20]: plot ( cv . arrest $ size , cv . arrest $ dev , type = \"b\" , xlab = \"Number of Nodes\" , ylab = \"Deviance\" , main = \"Determining Best Reduction of Trees by Deviance\" ) points ( 3 , cv . arrest $ dev [ 3 ], col = \"red\" ) There is a clear leveling off at 3, marked with a red point, so we will prune the next tree to three nodes. In [21]: prune . arrest <- prune . misclass ( tree . arrest , best = 3 ) In [22]: plot ( prune . arrest ) text ( prune . arrest ) This tree is extremely small, and only considers two variables - whether a person was searched and whether the reason for the search was that they had a hard object. Below, I post the tables for the pruned and unpruned trees, along with their misclassification errors. In [23]: tree . pred <- predict ( tree . arrest , newdata = mydata . test , type = \"class\" ) table5 <- table ( tree . pred , arrest . test ) misclassificationunprune <- 1 - sum ( diag ( table5 )) / nrow ( mydata . test ) pruned . pred <- predict ( prune . arrest , newdata = mydata . test , type = \"class\" ) table6 <- table ( pruned . pred , arrest . test ) misclassificationprune <- 1 - sum ( diag ( table6 )) / nrow ( mydata . test ) In [24]: table5 round ( misclassificationunprune , 3 ) table6 round ( misclassificationprune , 3 ) arrest.test tree.pred N Y N 4426 447 Y 213 540 0.117 arrest.test pruned.pred N Y N 4466 486 Y 173 501 0.117 The misclassification errors are quite high for the single trees, which is expected since one tree will rarely be sufficient in predicting the outcome. They perform roughly the same on the data, which indicates that the pruning was effective - we were able to reduce the nodes without significantly effecting the accuracy of the models. It is notable that the pruned tree reduced Type I error, which is more desirable in this case. Still, both trees performed better on the overall error than the KNN approach. Next, I will use the bagging method. The bagging method, or bootstrap aggregation, uses bootstrap samples repeatedly to create many trees, and then averages these trees to make predictions. In the case of a classification problem such as this one, bagging will predict by the majority vote. Performing bagging with 300 trees reports the following confusion matrix and misclassification error: In [25]: bag . arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 300 ) bag . pred <- predict ( bag . arrest , newdata = mydata . test , type = \"class\" ) table7 <- table ( bag . pred , arrest . test ) misclassificationbagging <- 1 - sum ( diag ( table7 )) / nrow ( mydata . test ) In [26]: table7 round ( misclassificationbagging , 3 ) arrest.test bag.pred N Y N 4505 285 Y 134 702 0.074 This is a large improvement over any of the previous methods we have used. In order to make the results more easily interpretable and prevent overfitting, I will reduce the number of trees used. Looking at the plot below can help us determine the correct number of trees to use : In [27]: plot ( bag . arrest , main = \"Trees vs Errors\" ) This plot determines the errors produced by each amount of trees. Using this, we can see visually where the line appears to stabilize and use this information to minimize the loss in accuracy from removing the trees. The green line shows the errors for the affirmative case (which is more likely), the red line for the negative case, and the black line for the overall error (the same as the misclassification error above). The plot appears to level off at around n=25. When I recreated the model using only 25 trees, I found the misclassification error to be 0.076. The error and confusion matrix will be included in the appendix. Although this is slightly higher than the model using 300 trees, it it a very small reduction in accuracy for a rather large increase in the interpretability and utility of the model. Now that we have determined a good amount of trees, we can look at an importance plot to see which variables create the most variations in the errors. Since this is a categorical variable, it will be most useful to use the right graph, which uses the Gini index. In [28]: bag . arrest2 <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 25 ) bag . pred2 <- predict ( bag . arrest2 , newdata = mydata . test , type = \"class\" ) table8 <- table ( bag . pred2 , arrest . test ) misclassificationbagging2 <- 1 - sum ( diag ( table8 )) / nrow ( mydata . test ) varImpPlot ( bag . arrest2 ) The variable with the most importance is sb_other, which is reason for search:other. This is a troubling thing, as described in the data section, because this is not a very informative variable. We can see that race now does have some importance in determining the arrests, as well as some other variables that were ommited from the logistic regression model. Interestingly, age, weight and height are included quite highly as well. Once again, whether the person was searched, had contraband, or was handcuffed is determined as important, which is consistent with our findings from the logistic regression. By using a sample of the variables in each tree and performing the bagging method on a different subset for each iteration, we can implement a Random Forest method. This method is useful since it allows some variation in the trees, since they will not only be dominated by the most important variables. Using two numbers for the number of variables used, I produced two random forest models. Below are their confusion matrices and misclassification errors. In [29]: table7 misclassificationbagging table8 misclassificationbagging2 arrest.test bag.pred N Y N 4505 285 Y 134 702 0.0744756487735514 arrest.test bag.pred2 N Y N 4490 277 Y 149 710 0.0757198720227515 The misclassification errors for the two are nearly identical. They perform very similarly to the bagging method. I would prefer the second model, as it predicts better in the case that the prediction and observed value are both yes than they are both no. By using a similar method to the bagging method earlier, we determine a good reduction in the amount of trees is to 40. Using 40 trees, we get the following confusion matrix and misclassification error. In [31]: table11 misclassRF3 arrest.test RF3.pred N Y N 4515 293 Y 124 694 0.0741201564166371 In this case, the error actually was reduced. This can be accounted for by the fact that it simplified the model, which resulted in the random forest model with 40 trees actually predicting better than the more complex model. This suggests that the increase in bias by using a simpler model with 40 trees was less than the decrease in variance of the model on the testing data. This model performs the best of all the models I have used, and is the most successful at predicting whether a person is arrested or not when they are stopped and frisked, with about a 7.2% misclassification rate. Conclusions Below is a table summarizing all of the models that I've used, and their misclassifcation errors on the test data. In [32]: Method <- c ( \"Full Logistic Model\" , \"Reduced Logistic Model\" , \"10-NN Full Model\" , \"5-NN Reduced Model\" , \"Pruned Tree\" , \"Unpruned Tree\" , \"Bagging (n=300)\" , \"Bagging (n=25)\" , \"Random Forest I (n=300)\" , \"Random Forest II (n=300)\" , \"Random Forest III (n=40)\" ) TestError <- c ( misclasserrorsfull , misclasserrorstepwise , KNNMisclass , KNNMisclass2 , misclassificationprune , misclassificationunprune , misclassificationbagging , misclassificationbagging2 , misclassRF , misclassRF2 , misclassRF3 ) TestError <- round ( TestError , 3 ) data . frame ( Method , TestError ) # error: Yes when No Er1 <- table1 [ 2 , 1 ] / sum ( table1 [ 2 , ]) Er2 <- table2 [ 2 , 1 ] / sum ( table2 [ 2 , ]) Er3 <- table3 [ 2 , 1 ] / sum ( table3 [ 2 , ]) Er4 <- table4 [ 2 , 1 ] / sum ( table4 [ 2 , ]) Er5 <- table5 [ 2 , 1 ] / sum ( table5 [ 2 , ]) Er6 <- table6 [ 2 , 1 ] / sum ( table6 [ 2 , ]) Er7 <- table7 [ 2 , 1 ] / sum ( table7 [ 2 , ]) Er8 <- table8 [ 2 , 1 ] / sum ( table8 [ 2 , ]) Er9 <- table9 [ 2 , 1 ] / sum ( table9 [ 2 , ]) Er10 <- table10 [ 2 , 1 ] / sum ( table10 [ 2 , ]) Er11 <- table11 [ 2 , 1 ] / sum ( table11 [ 2 , ]) # error: No when Yes Er21 <- table1 [ 1 , 2 ] / sum ( table1 [ 1 , ]) Er22 <- table2 [ 1 , 2 ] / sum ( table2 [ 1 , ]) Er23 <- table3 [ 1 , 2 ] / sum ( table3 [ 1 , ]) Er24 <- table4 [ 1 , 2 ] / sum ( table4 [ 1 , ]) Er25 <- table5 [ 1 , 2 ] / sum ( table5 [ 1 , ]) Er26 <- table6 [ 1 , 2 ] / sum ( table6 [ 1 , ]) Er27 <- table7 [ 1 , 2 ] / sum ( table7 [ 1 , ]) Er28 <- table8 [ 1 , 2 ] / sum ( table8 [ 1 , ]) Er29 <- table9 [ 1 , 2 ] / sum ( table9 [ 1 , ]) Er210 <- table10 [ 1 , 2 ] / sum ( table10 [ 1 , ]) Er211 <- table11 [ 1 , 2 ] / sum ( table11 [ 1 , ]) FalsePositive <- round ( c ( Er1 , Er2 , Er3 , Er4 , Er5 , Er6 , Er7 , Er8 , Er9 , Er10 , Er11 ), 3 ) FalseNegative <- round ( c ( Er21 , Er22 , Er23 , Er24 , Er25 , Er26 , Er27 , Er28 , Er29 , Er210 , Er211 ), 3 ) Method TestError Full Logistic Model 0.095 Reduced Logistic Model 0.093 10-NN Full Model 0.162 5-NN Reduced Model 0.120 Pruned Tree 0.117 Unpruned Tree 0.117 Bagging (n=300) 0.074 Bagging (n=25) 0.076 Random Forest I (n=300) 0.074 Random Forest II (n=300) 0.073 Random Forest III (n=40) 0.074 In [33]: data . frame ( Method , TestError , FalsePositive , FalseNegative ) Method TestError FalsePositive FalseNegative Full Logistic Model 0.095 0.176 0.081 Reduced Logistic Model 0.093 0.183 0.082 10-NN Full Model 0.162 0.402 0.143 5-NN Reduced Model 0.120 0.185 0.113 Pruned Tree 0.117 0.283 0.092 Unpruned Tree 0.117 0.257 0.098 Bagging (n=300) 0.074 0.160 0.059 Bagging (n=25) 0.076 0.173 0.058 Random Forest I (n=300) 0.074 0.137 0.064 Random Forest II (n=300) 0.073 0.146 0.060 Random Forest III (n=40) 0.074 0.152 0.061 I have also included the false positive and false negative results, as these can help to determine which models fit the best. The Random Forest model which used 40 trees and 39 variables performed the best out of all the methods used. It is clear that the bagging and random forest methods were superior to the logistic and KNN methods. Of the simpler methods, the reduced Logistic Model using stepwise reduction with the AIC performed the best. Despite my original inclinations, we can see from the importance plots on the bagging model and the summaries of the logistic model that race was not a very significant factor in determining whether a person would be arrested or not. However, as we saw before from the bar graphs, there was a significantly larger portion of blacks who were stopped and frisked than the general population. This suggests that a black person is more likely to be stopped, but not significantly more or less likely than other races to be arrested after they are stopped. Which raises the question: why stop more black people if they are no more likely to be arrested after the frisk than other races? Another variable of interest was sb_other. This variable indicates that there was an \"other reason for stopping the subject.\" As such, this variable is difficult to interpret. However, the variable was quite significant with a high coefficient in the logistic model, and was rated as the most important variable by a large margin in the bagging models. This is a point of difficulty in the interpretations. This may indicate that there needs to be more disclosure about the reason a person was searched, since the sb variables in their current state fail to capture much of the reasons for arrest. In both the logisitic and bagging methods, the variables for contraband and knife/cutting object came up as significant, and had a significant effect on whether the person was arrested or not. This is logical: if the officer found a weapon on contraband on the person, they were much more likely to be arrested. This was far more significant than other factors about the person, in particular the cs variables, which list the reasons why the person was stopped, or the age, weight, gender and race variables, which are the physical attributes of the person. This suggests that the physical attributes of a person are not nearly as important as criminal possession in predicting an arrest, which is in support of the stop and frisk's usage. For instance, being stopped due to the clothing you wear (cs_cloth) or being perceived as a lookout (cs_lkout) were not nearly as significant as carrying a suspicious object (cs_object) or if they were searched (searched), and actually had negative coefficients. So it appears that, most of the time, the probability of being arrested was largely reliant on whether the person was searched or not, or found to have an object. This matches up with the significance and postive coefficient of cs_drgtr, which was the variable representing the cause of search being for a suspected drug transaction. One would conclude that if a person was being suspected for a drug transaction, they would also be more likely to have contraband or perhaps a weapon than if they were not. Interestingly, the frisked variable was not found to be significant in the logistic regression model, suggesting that frisking, as compared to searching, was not very significant in arrest, and therefore not too effective a measure to stop criminal activity. In both models, searching had a stronger effect than frisking in determining whether a person was arrested or not. The success of the random forest/bagging methods as compared to the simpler methods suggests that the relationship between whether a person is arrested or not and the various variables in the dataset is quite complex, and that it cannot be estimated as well using the simpler techniques.","tags":"Projects","url":"https://seanammirati.github.io/category/projects/nypd_stop_frisk.html","loc":"https://seanammirati.github.io/category/projects/nypd_stop_frisk.html"},{"title":"NYPD Stop and Frisk -- Full Code","text":"The Results of Stop and Frisk: What Contributes Most to Arrests? Note: All of the code associated with this project, as well as the data used, is accessible at my GitHub repository here . This contains all of the code used in this project , unannotated. Introduction In this project I will be looking at a dataset available from the city of New York about the results of the Stop, Question and Frisk policy. Which factors lead to the highest likelihood of arrest? The data is a collection of 22,564 cases of Stop, Question and Frisk in the City of New York in 2015. There are 112 variables which were measured. A majority of the variables in the dataset are categorical variables relating to the incident itself, while a few represent the age, race, height, weight, and sex of the person who was on the receiving end of the policy. Stop and Frisk has been a controversial policy in New York City for quite some time. Many feel that it is a violation of privacy, that it is unconstitutional, or that the policy is inherently racist - that it disproportionately affects minorities and people of color. Meanwhile, there are others who support the policy, believing stop and frisk to keep people safe from terrorists and other wrongdoers. As a result of this and other recent controversies involving police across the country, the NYPD has been getting a lot of negative press over the way it addresses these racial problems. What I aim to discover by analyzing this dataset are the main contributing factors to whether a person is arrested or not in a stop and frisk incident. I will be using a few different methods of analysis to determine the best model to use in predicting whether a person will be arrested or not. Because the response variable, arstmade, is a qualitative, binary variable, I will be using categorical methods. These include logistic regression, K-Nearest Neighbors analysis and tree methods (like bagging, random forests, boosting). In [3]: set.seed ( 43 ) install.packages ( 'tree' ) require ( ggplot2 ) require ( tree ) require ( class ) require ( randomForest ) download_link <- \"http://www.nyc.gov/html/nypd/downloads/zip/analysis_and_planning/2015_sqf_csv.zip\" tmp <- tempfile () download.file ( download_link , tmp ) mydata <- read.csv ( unz ( tmp , \"2015_sqf_csv.csv\" )) unlink ( tmp ) sum ( is.na ( mydata )) nrow ( mydata ) * ncol ( mydata ) names ( mydata ) napercol <- list () for ( i in 1 : ncol ( mydata )) { napercol [[ i ]] <- sum ( is.na ( mydata [, i ])) } mydata <- mydata [, napercol == 0 ] lvllist <- list () for ( i in 1 : ncol ( mydata )) { lvllist [[ i ]] <- length ( levels ( mydata [, i ])) } attach ( mydata ) height <- 12 * ht_feet + ht_inch mydata <- data.frame ( mydata , height ) mydata <- mydata [, ( lvllist < 10 ) & ( lvllist != 1 )] mydata <- mydata [, - c ( 1 , 2 , 3 , 4 , 15 , 16 , 18 , 78 , 79 , 83 , 84 )] mydata <- na.omit ( mydata ) mydata <- mydata [ mydata $ age < 100 , ] train <- sample ( 1 : nrow ( mydata ), 3 * nrow ( mydata ) / 4 ) mydata.test <- mydata [ - train , ] arrest.test <- mydata $ arstmade [ - train ] attach ( mydata ) # descriptive sum ( arstmade == \"Y\" ) / nrow ( mydata ) sum ( sex == \"M\" ) / nrow ( mydata ) sum ( forceuse != \" \" ) / nrow ( mydata ) sum ( race == \"B\" ) / nrow ( mydata ) sum ( sumissue == \"Y\" ) / nrow ( mydata ) sum ( arstmade == \"Y\" ) / sum ( sumissue == \"Y\" ) sum ( sb_other == \"Y\" ) / nrow ( mydata ) sum ( sb_other == \"Y\" ) / sum ( searched == \"Y\" ) sum ( pf_other == \"Y\" ) / nrow ( mydata ) mean ( mydata $ age ) black <- as.factor ( race == \"B\" ) white <- as.factor ( race == \"W\" ) nonwhite <- as.factor ( race != \"W\" ) summary ( mydata ) Installing package into 'C:/Users/ginge/Anaconda3/Lib/R/library' (as 'lib' is unspecified) package 'tree' successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\\Users\\ginge\\AppData\\Local\\Temp\\RtmpMrc3nQ\\downloaded_packages Loading required package: ggplot2 Loading required package: tree Loading required package: class Loading required package: randomForest randomForest 4.6-12 Type rfNews() to see new features/changes/bug fixes. Attaching package: 'randomForest' The following object is masked from 'package:ggplot2': margin 173895 2527056 'year' 'pct' 'ser_num' 'datestop' 'timestop' 'recstat' 'inout' 'trhsloc' 'perobs' 'crimsusp' 'perstop' 'typeofid' 'explnstp' 'othpers' 'arstmade' 'arstoffn' 'sumissue' 'sumoffen' 'compyear' 'comppct' 'offunif' 'officrid' 'frisked' 'searched' 'contrabn' 'adtlrept' 'pistol' 'riflshot' 'asltweap' 'knifcuti' 'machgun' 'othrweap' 'pf_hands' 'pf_wall' 'pf_grnd' 'pf_drwep' 'pf_ptwep' 'pf_baton' 'pf_hcuff' 'pf_pepsp' 'pf_other' 'radio' 'ac_rept' 'ac_inves' 'rf_vcrim' 'rf_othsw' 'ac_proxm' 'rf_attir' 'cs_objcs' 'cs_descr' 'cs_casng' 'cs_lkout' 'rf_vcact' 'cs_cloth' 'cs_drgtr' 'ac_evasv' 'ac_assoc' 'cs_furtv' 'rf_rfcmp' 'ac_cgdir' 'rf_verbl' 'cs_vcrim' 'cs_bulge' 'cs_other' 'ac_incid' 'ac_time' 'rf_knowl' 'ac_stsnd' 'ac_other' 'sb_hdobj' 'sb_outln' 'sb_admis' 'sb_other' 'repcmd' 'revcmd' 'rf_furt' 'rf_bulg' 'offverb' 'offshld' 'forceuse' 'sex' 'race' 'dob' 'age' 'ht_feet' 'ht_inch' 'weight' 'haircolr' 'eyecolor' 'build' 'othfeatr' 'addrtyp' 'rescode' 'premtype' 'premname' 'addrnum' 'stname' 'stinter' 'crossst' 'aptnum' 'city' 'state' 'zip' 'addrpct' 'sector' 'beat' 'post' 'xcoord' 'ycoord' 'dettypCM' 'lineCM' 'detailCM' The following object is masked _by_ .GlobalEnv: height The following objects are masked from mydata (pos = 3): ac_assoc, ac_cgdir, ac_evasv, ac_incid, ac_inves, ac_other, ac_proxm, ac_rept, ac_stsnd, ac_time, age, arstmade, asltweap, build, city, contrabn, cs_bulge, cs_casng, cs_cloth, cs_descr, cs_drgtr, cs_furtv, cs_lkout, cs_objcs, cs_other, cs_vcrim, detailCM, explnstp, forceuse, frisked, inout, knifcuti, offshld, offunif, offverb, othpers, othrweap, perobs, pf_baton, pf_drwep, pf_grnd, pf_hands, pf_hcuff, pf_other, pf_pepsp, pf_ptwep, pf_wall, pistol, race, radio, recstat, repcmd, revcmd, rf_attir, rf_bulg, rf_furt, rf_knowl, rf_othsw, rf_rfcmp, rf_vcact, rf_vcrim, rf_verbl, riflshot, sb_admis, sb_hdobj, sb_other, sb_outln, searched, sex, sumissue, timestop, trhsloc, typeofid, weight 0.175984356946049 0.924317838414363 0.274686694516043 0.529419607146031 0.0261310105768376 6.73469387755102 0.107501555417296 0.577326968973747 0.026486534530264 27.6311883388143 timestop recstat inout trhsloc perobs typeofid Min. : 0 : 2845 I: 4211 H: 3357 Min. : 0.00 O: 432 1st Qu.: 507 1:14661 O:18291 P:18245 1st Qu.: 1.00 P:12960 Median :1623 9: 10 T: 900 Median : 1.00 R: 623 Mean :1377 A: 4986 Mean : 2.64 V: 8487 3rd Qu.:2052 3rd Qu.: 2.00 Max. :2359 Max. :535.00 explnstp othpers arstmade sumissue offunif frisked searched N: 28 N:15780 N:18542 N:21914 N: 9965 N: 7289 N:18312 Y:22474 Y: 6722 Y: 3960 Y: 588 Y:12537 Y:15213 Y: 4190 contrabn pistol riflshot asltweap knifcuti othrweap pf_hands N:21379 N:22337 N:22499 N:22501 N:21813 N:22228 N:18771 Y: 1123 Y: 165 Y: 3 Y: 1 Y: 689 Y: 274 Y: 3731 pf_wall pf_grnd pf_drwep pf_ptwep pf_baton pf_hcuff pf_pepsp N:21279 N:22191 N:22087 N:22234 N:22497 N:19111 N:22496 Y: 1223 Y: 311 Y: 415 Y: 268 Y: 5 Y: 3391 Y: 6 pf_other radio ac_rept ac_inves rf_vcrim rf_othsw ac_proxm N:21906 N:13142 N:15906 N:19561 N:17472 N:18858 N:14543 Y: 596 Y: 9360 Y: 6596 Y: 2941 Y: 5030 Y: 3644 Y: 7959 rf_attir cs_objcs cs_descr cs_casng cs_lkout rf_vcact cs_cloth N:20949 N:21580 N:14248 N:17837 N:20095 N:20834 N:21555 Y: 1553 Y: 922 Y: 8254 Y: 4665 Y: 2407 Y: 1668 Y: 947 cs_drgtr ac_evasv ac_assoc cs_furtv rf_rfcmp ac_cgdir rf_verbl N:21138 N:18194 N:20724 N:15766 N:19860 N:17541 N:22287 Y: 1364 Y: 4308 Y: 1778 Y: 6736 Y: 2642 Y: 4961 Y: 215 cs_vcrim cs_bulge cs_other ac_incid ac_time rf_knowl ac_stsnd N:20590 N:20804 N:14637 N:12148 N:15037 N:21242 N:21837 Y: 1912 Y: 1698 Y: 7865 Y:10354 Y: 7465 Y: 1260 Y: 665 ac_other sb_hdobj sb_outln sb_admis sb_other repcmd N:19947 N:21063 N:22204 N:22229 N:20083 Min. : 1 Y: 2555 Y: 1439 Y: 298 Y: 273 Y: 2419 1st Qu.: 66 Median :106 Mean :229 3rd Qu.:186 Max. :878 revcmd rf_furt rf_bulg offverb offshld forceuse sex Min. : 1.0 N:15913 N:20577 :15175 :12749 :16321 F: 1509 1st Qu.: 66.0 Y: 6589 Y: 1925 V: 7327 S: 9753 DO: 98 M:20799 Median :106.0 DS: 2145 Z: 194 Mean :229.4 OR: 284 3rd Qu.:186.0 OT: 1687 Max. :878.0 SF: 1461 SW: 506 race age weight build city B :11913 Min. : 0.00 Min. : 1.0 H: 2159 BRONX :4740 Q : 5082 1st Qu.:19.00 1st Qu.:150.0 M:10958 BROOKLYN :6334 W : 2505 Median :24.00 Median :170.0 T: 8861 MANHATTAN:3923 P : 1407 Mean :27.63 Mean :171.3 U: 191 QUEENS :5710 A : 1100 3rd Qu.:33.00 3rd Qu.:185.0 Z: 333 STATEN IS:1795 Z : 298 Max. :99.00 Max. :999.0 (Other): 197 detailCM height Min. : 6.00 Min. :36.00 1st Qu.: 20.00 1st Qu.:67.00 Median : 27.00 Median :69.00 Mean : 37.79 Mean :68.83 3rd Qu.: 46.00 3rd Qu.:71.00 Max. :113.00 Max. :95.00 Data The data comes from the City of New York. It is a collection of 22,564 observations of 112 variables. Some of these variables had to be removed, as they did not contribute any information (for instance, they were the same value for all observations) or they had too many missing values. After removing these variables from the dataset, I had 75 variables to work with, with 22,563 observations. For example, lineCM was a redundant variable, as it had 1's for all observations. I also removed apt number, state and zip, as many of these values were left blank. Overall, none of the variables I was very interested in had any missing values, so removing these variables likely had very little effect on the analysis, if any. There were two variables for height (ht_inch and ht_feet) which I combined into one (height). The response variable I used for all of the following analyses was arstmade, which was a categorical variable that had values of \"Y\" for yes and \"N\" for no. Many of the categorical variables in the dataset followed this pattern. The percentage of arrests in all cases was 0.18, meaning that around 18% of the stops resulted in an arrest. The mean age of people who were stopped and frisked was around 29 years. Most of the people who were stopped were males (92.42%). Force was used in about 27% of the cases. In investigating whether blacks were targeted more frequently and if they were arrested more frequently, I created a categorical variable that was 1 if the person was black and 0 otherwise. Below is a bar graph of the arrests, with the colorings indicating the percentage who were black and non-black. It does not seem from these graphs alone that blacks were more likely to be arrested, since they have roughly half of the bars in both instances. However, it does seem like there are a disproportionate number of blacks who are stopped and frisked in the first place, which is shown in the second graph. With blacks making up roughly 23% of the population, it is surprising that almost double that (53%) of the people who were stopped were black In [4]: ggplot ( mydata , aes ( arstmade )) + geom_bar ( aes ( fill = black )) + ggtitle ( \"Bar Graph of Arrests for Blacks vs. Non-Blacks\" ) + xlab ( \"Was the person arrested?\" ) + ylab ( \"Numer of Incidences\" ) + scale_fill_discrete ( name = \"Race\" , labels = c ( \"Non-Blacks\" , \"Blacks\" )) In [5]: ggplot ( mydata , aes ( black )) + geom_bar ( fill = \"red\" ) + ggtitle ( \"Blacks vs. Non-Blacks Who Were Stopped and Frisked\" ) + xlab ( \"Was the person black?\" ) + ylab ( \"Number of Incidents\" ) Some more interesting things found in the descriptive statistics: Around 6.7 times more people were arrested than given summonses. That is, only 2% of the people were given summonses. None of the people who were stopped had a machine gun, so I removed this variable from the data. I removed any variable which had levels that were exactly one, as this indicated that there was no variation between observations. Although the suspected crime variable (crimsusp) was of interest to me in my analysis, I found the data to be inconsistent in how it was recorded. For instance, if it was a felony, the officer had data entered that had both \"FEL\" and \"FELONY\" in two different cases. Of course, R would not be able to recognize the fact that these are the same, and would treat them as separate variables. As such, I decided to remove any variables which had levels larger than 10, as this would indicate that the data was too separated to be meaningful. Since much of this information is contained in the categorical variables (such as the rs variables, which had different values for different reasons for stopping the person), it was relatively inconsequential in my analysis to do so. There were also groupings which could raise doubt as to whether the information recorded was accurate. Of primary concern to me was pf_other (physical force used: other), rs_other (reason for search: other) and ac_other (additional circumstances: other). These are not very informative, and could be raise a speculative point about whether these were truly extraordinary cases or if they were omitted for other reasons. Of course, that is merely speculation, but it is important to note that this is a source of potential inconsistency and unreliability from the dataset. Even worse, some of these variables contributed a lot to the models which I have fit. Below shows a bar graph of the number of arrests conditional on whether the reason for search was classified as \"other.\" In [6]: ggplot ( mydata , aes ( sb_other )) + geom_bar ( aes ( fill = arstmade )) + ggtitle ( \"Arrests Conditional on if Reason for Search classified as OTHER\" ) + xlab ( \"Was the reason for search classifed as other?\" ) + ylab ( \"Number of Incidents\" ) + scale_fill_discrete ( name = \"Arrest Status\" , labels = ( c ( \"Not Arrested\" , \"Arrested\" ))) This confirms the results of the analysis below, as this variable has a large effect on whether the person was arrested or not. If the person was classified as being searched for other reasons not specified, they were arrested quite often. This leads to a limitation of the data and interpretation - we can not determine what the true reasons were. sb_other accounted for around 10.7% of all the cases, and 57.6% of all the people who were searched. Analysis and Model Fitting Logistic Regression I separated the data into two sets - test data and training data. The training data was randomly sampled from ¾ of the data. I began my analysis by using a logistic regression model to predict arstmade on all of the other variables. This was because the response variable was categorical, so it was necessary to use a logistic model instead of a linear model with normal errors. I then used stepwise reduction to determine the best fit and to reduce the amount of variables used. Since there were a great deal of variables that came up insignificant in the full model, it reduced the model quite a lot. I used the glm function using the binomial family to model the data. Below is a summary of the results of the models: In [7]: logistfullmodel <- glm ( arstmade ~ . , data = mydata [ train , ], family = binomial ) summary ( logistfullmodel ) # step(logistfullmodel) stepwiselogmodel <- glm ( formula = arstmade ~ recstat + inout + trhsloc + perobs + typeofid + sumissue + offunif + frisked + searched + contrabn + pistol + knifcuti + othrweap + pf_hands + pf_wall + pf_grnd + pf_drwep + pf_hcuff + pf_pepsp + pf_other + radio + ac_rept + rf_vcrim + rf_othsw + ac_proxm + rf_attir + cs_objcs + cs_casng + cs_lkout + cs_cloth + cs_drgtr + ac_evasv + ac_assoc + rf_rfcmp + rf_verbl + cs_vcrim + cs_bulge + cs_other + rf_knowl + ac_other + sb_hdobj + sb_other + revcmd + rf_furt + rf_bulg + forceuse + sex + race + age + weight + city + detailCM , family = binomial , data = mydata [ train , ]) summary ( stepwiselogmodel ) Call: glm(formula = arstmade ~ ., family = binomial, data = mydata[train, ]) Deviance Residuals: Min 1Q Median 3Q Max -3.7188 -0.3688 -0.2544 -0.1509 3.8891 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -3.603e-01 1.098e+00 -0.328 0.742736 timestop -5.247e-05 3.830e-05 -1.370 0.170732 recstat1 3.513e-01 1.029e-01 3.414 0.000640 *** recstat9 -1.984e+00 2.315e+00 -0.857 0.391350 recstatA -1.814e-01 1.186e-01 -1.530 0.126081 inoutO -8.448e-01 8.111e-02 -10.415 < 2e-16 *** trhslocP -8.829e-01 1.264e-01 -6.987 2.81e-12 *** trhslocT -1.433e+00 1.497e-01 -9.576 < 2e-16 *** perobs 8.119e-03 3.385e-03 2.398 0.016477 * typeofidP 4.639e-02 2.062e-01 0.225 0.822032 typeofidR -1.221e+00 3.381e-01 -3.612 0.000303 *** typeofidV -1.538e-01 2.087e-01 -0.737 0.461056 explnstpY -5.317e-01 7.951e-01 -0.669 0.503720 othpersY 8.174e-02 7.156e-02 1.142 0.253323 sumissueY -2.921e+00 2.354e-01 -12.411 < 2e-16 *** offunifY -2.842e-01 3.264e-01 -0.871 0.383957 friskedY 1.868e-01 1.029e-01 1.816 0.069326 . searchedY 1.743e+00 2.208e-01 7.893 2.95e-15 *** contrabnY 3.129e+00 1.397e-01 22.406 < 2e-16 *** pistolY 2.889e+00 3.064e-01 9.428 < 2e-16 *** riflshotY -9.084e+00 1.970e+02 -0.046 0.963216 asltweapY 9.640e+00 1.970e+02 0.049 0.960966 knifcutiY 2.090e+00 1.459e-01 14.327 < 2e-16 *** othrweapY 1.924e+00 2.089e-01 9.209 < 2e-16 *** pf_handsY 1.730e-01 9.642e-02 1.794 0.072817 . pf_wallY -2.660e-01 1.438e-01 -1.850 0.064294 . pf_grndY 7.378e-01 2.149e-01 3.433 0.000597 *** pf_drwepY -6.845e-01 2.622e-01 -2.610 0.009050 ** pf_ptwepY -1.893e-01 2.868e-01 -0.660 0.509344 pf_batonY 2.162e+00 1.520e+00 1.422 0.155040 pf_hcuffY 1.308e+00 9.645e-02 13.564 < 2e-16 *** pf_pepspY 3.710e+00 1.173e+00 3.162 0.001568 ** pf_otherY -2.753e-01 1.954e-01 -1.409 0.158894 radioY -4.752e-01 6.943e-02 -6.844 7.70e-12 *** ac_reptY 2.874e-01 7.863e-02 3.655 0.000257 *** ac_invesY 5.950e-02 9.443e-02 0.630 0.528644 rf_vcrimY -2.614e-01 9.061e-02 -2.885 0.003912 ** rf_othswY -2.411e-01 9.555e-02 -2.523 0.011633 * ac_proxmY 1.581e-01 6.608e-02 2.393 0.016729 * rf_attirY -3.667e-01 1.361e-01 -2.695 0.007042 ** cs_objcsY 5.588e-01 1.324e-01 4.219 2.45e-05 *** cs_descrY 1.136e-01 8.865e-02 1.281 0.200124 cs_casngY -1.454e-01 9.321e-02 -1.560 0.118861 cs_lkoutY -4.338e-01 1.205e-01 -3.599 0.000319 *** rf_vcactY -8.072e-02 1.335e-01 -0.605 0.545450 cs_clothY -3.673e-01 1.809e-01 -2.031 0.042294 * cs_drgtrY 3.498e-01 1.206e-01 2.901 0.003719 ** ac_evasvY 2.235e-01 8.377e-02 2.668 0.007640 ** ac_assocY -5.466e-01 1.345e-01 -4.063 4.84e-05 *** cs_furtvY -2.918e-02 8.263e-02 -0.353 0.723996 rf_rfcmpY -2.335e-01 1.029e-01 -2.270 0.023184 * ac_cgdirY 1.199e-01 8.096e-02 1.481 0.138624 rf_verblY -3.992e-01 2.927e-01 -1.364 0.172692 cs_vcrimY -3.357e-01 1.384e-01 -2.426 0.015256 * cs_bulgeY -4.862e-01 1.482e-01 -3.281 0.001033 ** cs_otherY -1.467e-01 7.321e-02 -2.004 0.045122 * ac_incidY 4.553e-02 7.148e-02 0.637 0.524196 ac_timeY -4.144e-02 7.612e-02 -0.544 0.586213 rf_knowlY -4.205e-01 1.562e-01 -2.692 0.007095 ** ac_stsndY -2.085e-01 1.926e-01 -1.083 0.278930 ac_otherY -3.115e-01 9.911e-02 -3.143 0.001673 ** sb_hdobjY -6.270e-01 2.079e-01 -3.016 0.002558 ** sb_outlnY 2.030e-01 2.446e-01 0.830 0.406604 sb_admisY 1.097e-01 2.662e-01 0.412 0.680126 sb_otherY 1.280e+00 2.151e-01 5.951 2.66e-09 *** repcmd -3.224e-04 6.206e-04 -0.520 0.603369 revcmd 8.321e-04 6.175e-04 1.348 0.177806 rf_furtY -1.473e-01 9.015e-02 -1.634 0.102289 rf_bulgY -3.050e-01 1.374e-01 -2.220 0.026416 * offverbV -2.356e-02 1.051e-01 -0.224 0.822675 offshldS -3.050e-02 3.115e-01 -0.098 0.922004 forceuseDO -7.150e-01 4.305e-01 -1.661 0.096764 . forceuseDS -6.897e-01 1.305e-01 -5.286 1.25e-07 *** forceuseOR -8.379e-02 2.432e-01 -0.345 0.730402 forceuseOT 3.347e-01 1.257e-01 2.664 0.007725 ** forceuseSF -5.223e-02 1.308e-01 -0.399 0.689594 forceuseSW -5.030e-01 2.170e-01 -2.318 0.020476 * sexM -2.052e-01 1.171e-01 -1.752 0.079809 . sexZ -7.004e-01 4.190e-01 -1.671 0.094626 . raceB 3.875e-02 1.589e-01 0.244 0.807283 raceI 8.685e-02 5.556e-01 0.156 0.875776 raceP 2.571e-01 1.889e-01 1.361 0.173603 raceQ 3.531e-01 1.623e-01 2.176 0.029566 * raceU 2.086e-01 4.130e-01 0.505 0.613431 raceW 1.073e-01 1.809e-01 0.593 0.552966 raceZ 5.422e-01 3.251e-01 1.668 0.095401 . age 8.302e-03 2.664e-03 3.117 0.001830 ** weight -1.064e-03 1.022e-03 -1.041 0.297954 buildM -4.270e-02 1.113e-01 -0.384 0.701347 buildT -1.117e-02 1.210e-01 -0.092 0.926426 buildU -1.004e-01 3.264e-01 -0.308 0.758299 buildZ -7.310e-01 2.973e-01 -2.459 0.013934 * cityBROOKLYN -8.185e-01 8.534e-02 -9.592 < 2e-16 *** cityMANHATTAN -6.850e-01 9.098e-02 -7.529 5.10e-14 *** cityQUEENS -5.335e-01 9.414e-02 -5.667 1.45e-08 *** citySTATEN IS -1.102e+00 1.562e-01 -7.053 1.75e-12 *** detailCM 8.680e-03 1.159e-03 7.492 6.78e-14 *** height -2.539e-03 1.019e-02 -0.249 0.803203 --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 15712.6 on 16875 degrees of freedom Residual deviance: 8277.2 on 16778 degrees of freedom AIC: 8473.2 Number of Fisher Scoring iterations: 10 Call: glm(formula = arstmade ~ recstat + inout + trhsloc + perobs + typeofid + sumissue + offunif + frisked + searched + contrabn + pistol + knifcuti + othrweap + pf_hands + pf_wall + pf_grnd + pf_drwep + pf_hcuff + pf_pepsp + pf_other + radio + ac_rept + rf_vcrim + rf_othsw + ac_proxm + rf_attir + cs_objcs + cs_casng + cs_lkout + cs_cloth + cs_drgtr + ac_evasv + ac_assoc + rf_rfcmp + rf_verbl + cs_vcrim + cs_bulge + cs_other + rf_knowl + ac_other + sb_hdobj + sb_other + revcmd + rf_furt + rf_bulg + forceuse + sex + race + age + weight + city + detailCM, family = binomial, data = mydata[train, ]) Deviance Residuals: Min 1Q Median 3Q Max -3.7416 -0.3693 -0.2555 -0.1531 3.8643 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -1.1394455 0.3563078 -3.198 0.001384 ** recstat1 0.3575529 0.1023959 3.492 0.000480 *** recstat9 -2.0037225 2.3393201 -0.857 0.391699 recstatA -0.1811463 0.1180715 -1.534 0.124978 inoutO -0.8198646 0.0803864 -10.199 < 2e-16 *** trhslocP -0.8738981 0.1249270 -6.995 2.65e-12 *** trhslocT -1.4117556 0.1472958 -9.584 < 2e-16 *** perobs 0.0085851 0.0033665 2.550 0.010767 * typeofidP 0.0553679 0.2048434 0.270 0.786934 typeofidR -1.2301359 0.3365018 -3.656 0.000257 *** typeofidV -0.1420737 0.2072662 -0.685 0.493051 sumissueY -2.9274054 0.2337174 -12.525 < 2e-16 *** offunifY -0.2300162 0.0709762 -3.241 0.001192 ** friskedY 0.1596321 0.0999233 1.598 0.110144 searchedY 1.8727029 0.1564972 11.966 < 2e-16 *** contrabnY 3.1108031 0.1387840 22.415 < 2e-16 *** pistolY 2.9397144 0.3037615 9.678 < 2e-16 *** knifcutiY 2.0781429 0.1408105 14.758 < 2e-16 *** othrweapY 1.9085458 0.2061663 9.257 < 2e-16 *** pf_handsY 0.1829775 0.0958719 1.909 0.056318 . pf_wallY -0.2684473 0.1426597 -1.882 0.059872 . pf_grndY 0.7069531 0.2111336 3.348 0.000813 *** pf_drwepY -0.7202203 0.2427153 -2.967 0.003004 ** pf_hcuffY 1.3038209 0.0958149 13.608 < 2e-16 *** pf_pepspY 3.6381377 1.1662366 3.120 0.001811 ** pf_otherY -0.2736084 0.1936878 -1.413 0.157766 radioY -0.4540335 0.0686508 -6.614 3.75e-11 *** ac_reptY 0.3203196 0.0716227 4.472 7.74e-06 *** rf_vcrimY -0.2302783 0.0885105 -2.602 0.009276 ** rf_othswY -0.2195720 0.0938505 -2.340 0.019305 * ac_proxmY 0.1764237 0.0639188 2.760 0.005778 ** rf_attirY -0.3508092 0.1347952 -2.603 0.009254 ** cs_objcsY 0.5277556 0.1306460 4.040 5.35e-05 *** cs_casngY -0.1567943 0.0897938 -1.746 0.080783 . cs_lkoutY -0.4306545 0.1190220 -3.618 0.000297 *** cs_clothY -0.3454175 0.1787932 -1.932 0.053367 . cs_drgtrY 0.3362326 0.1183804 2.840 0.004507 ** ac_evasvY 0.2346666 0.0809086 2.900 0.003727 ** ac_assocY -0.5555065 0.1340808 -4.143 3.43e-05 *** rf_rfcmpY -0.2040480 0.1014848 -2.011 0.044365 * rf_verblY -0.4083410 0.2922657 -1.397 0.162366 cs_vcrimY -0.3965895 0.1264841 -3.135 0.001716 ** cs_bulgeY -0.4881904 0.1440797 -3.388 0.000703 *** cs_otherY -0.1770344 0.0673632 -2.628 0.008587 ** rf_knowlY -0.4071034 0.1555024 -2.618 0.008845 ** ac_otherY -0.3129862 0.0982278 -3.186 0.001441 ** sb_hdobjY -0.7305307 0.1589738 -4.595 4.32e-06 *** sb_otherY 1.1601221 0.1583057 7.328 2.33e-13 *** revcmd 0.0004988 0.0001588 3.142 0.001681 ** rf_furtY -0.1420974 0.0805927 -1.763 0.077874 . rf_bulgY -0.2908946 0.1356321 -2.145 0.031974 * forceuseDO -0.7594367 0.4329658 -1.754 0.079425 . forceuseDS -0.6950447 0.1298365 -5.353 8.64e-08 *** forceuseOR -0.0864302 0.2400623 -0.360 0.718823 forceuseOT 0.3274358 0.1251247 2.617 0.008874 ** forceuseSF -0.0552919 0.1300915 -0.425 0.670820 forceuseSW -0.4997659 0.2145224 -2.330 0.019824 * sexM -0.2124173 0.1120586 -1.896 0.058014 . sexZ -0.6879402 0.4163837 -1.652 0.098498 . raceB 0.0408446 0.1577469 0.259 0.795693 raceI 0.0878653 0.5519119 0.159 0.873510 raceP 0.2519675 0.1881200 1.339 0.180441 raceQ 0.3582209 0.1613654 2.220 0.026423 * raceU 0.2148683 0.4095903 0.525 0.599866 raceW 0.1083503 0.1798309 0.603 0.546833 raceZ 0.5206662 0.3238037 1.608 0.107842 age 0.0073152 0.0026085 2.804 0.005042 ** weight -0.0012546 0.0008145 -1.540 0.123469 cityBROOKLYN -0.8072589 0.0847163 -9.529 < 2e-16 *** cityMANHATTAN -0.6764624 0.0904685 -7.477 7.58e-14 *** cityQUEENS -0.5136538 0.0932366 -5.509 3.61e-08 *** citySTATEN IS -1.0791310 0.1548114 -6.971 3.16e-12 *** detailCM 0.0090240 0.0011467 7.870 3.56e-15 *** --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 15712.6 on 16875 degrees of freedom Residual deviance: 8297.6 on 16803 degrees of freedom AIC: 8443.6 Number of Fisher Scoring iterations: 6 In [8]: plot ( stepwiselogmodel ) In [9]: fit1 <- predict ( stepwiselogmodel , newdata = mydata.test , type = \"response\" ) fit1 <- ifelse ( fit1 > 0.5 , 1 , 0 ) sum ( fit1 ) / nrow ( mydata.test ) table1 <- table ( fit1 , arrest.test ) misclasserrorstepwise <- 1 - sum ( diag ( table1 )) / nrow ( mydata.test ) round ( misclasserrorstepwise , 3 ) fitted3 <- ifelse ( fit1 > 0.17 , 1 , 0 ) sum ( fitted3 ) / nrow ( mydata.test ) fit2 <- predict ( logistfullmodel , newdata = mydata.test , type = \"response\" ) fitted2 <- ifelse ( fit2 > 0.5 , 1 , 0 ) sum ( fitted2 ) / nrow ( mydata.test ) table2 <- table ( fitted2 , arrest.test ) misclasserrorsfull <- 1 - sum ( diag ( table2 )) / nrow ( mydata.test ) round ( misclasserrorsfull , 3 ) 0.127266263775329 0.093 0.127266263775329 0.127444009953786 0.095 In [11]: table1 arrest.test fit1 N Y 0 4513 397 1 126 590 In [12]: misclasserrorsfull table2 misclasserrorstepwise 0.0945609669392108 arrest.test fitted2 N Y 0 4508 401 1 131 586 0.0929612513330963 Using the stepwise regression model determined by the AIC produced a marginally better misclassification error, from .095 to .093 (or 9.5 % to 9.3%). This isn't a very big improvement, though. Surprisingly, the model does not find any of the race factors to be significant in determining whether an arrest was made or not besides Q, which stands for the White-Hispanics. That is, all of the levels of the \"race\" variable other than \"White-Hispanic\" came up insignificant. All of these coefficients are positive besides \"U\", which stood for \"unknown\", which essentially means that if the person's race was known, the odds of arrest are higher. The full summary of the stepwise regression is included in the appendix(1). When the stepwise regression was performed, we could see that contraband and knife/cutting instrument variables were highly significant in determining whether a person would get arrested or not. These variables are both very significant in the reduced logistic model. The variables that represent the different boroughs were all found to be significant, all with negative values. This suggests that the variable that was not included in the factors (the Bronx) had a higher likelihood of being arrested than any of the other four boroughs. Since the largest coefficient is that of Staten Island it means that, all else equal, a person from Staten Island who was stopped and frisked was less likely to be arrested than a person from Manhattan. The model also indicates that people who had a weapon were more likely to be arrested during a stop and frisk. This is logical, as if the police had found a weapon, it is quite likely that the person would be arrested. If the officer was in uniform, interestingly, a person would be less likely to be arrested, according to the model. If the police officer stopped the person because they knew them (indicated by rf_knowlY), the person was less likely to be arrested. If the reason the officer used force was to defend themselves, the person was less likely to be arrested than if the reason for force was that the person was suspected of running away. If the person is searched, they had a higher chance of being arrested than if they were not searched. KNN The next model I used was the K-Nearest Neighbors approach. K-Nearest Neighbors is a non-parametric method, so it does not make assumptions about the underlying distribution of the variables. K-Nearest Neighbors works by finding the nearest data points to the ones you are testing them against and assigning a classification based on the nearest k points. I have used k=5 and k=10 in this model to see if they can accurately predict the chances of arrest. Below are the tables for predicted vs actual arrest values for the test set, as well as the misclassification error, for the 10-nearest neighbor method (as this was the most successful.) In [14]: train1 <- model.matrix ( arstmade ~ . , data = mydata [ train , ]) test1 <- model.matrix ( arstmade ~ . , data = mydata.test ) knn.predict <- knn ( train1 , test1 , arstmade [ train ], k = 10 ) table3 <- table ( knn.predict , arrest.test ) KNNMisclass <- 1 - sum ( diag ( table3 )) / nrow ( mydata.test ) round ( KNNMisclass , 3 ) train2 <- model.matrix ( arstmade ~ . , data = mydata [ train , ])[, 7 : 60 ] test2 <- model.matrix ( arstmade ~ . , data = mydata.test )[, 7 : 60 ] knn.predict2 <- knn ( train2 , test2 , arstmade [ train ], k = 5 ) table4 <- table ( knn.predict2 , arrest.test ) KNNMisclass2 <- 1 - sum ( diag ( table4 )) / nrow ( mydata.test ) round ( KNNMisclass2 , 3 ) 0.162 0.12 In [15]: table3 round ( KNNMisclass , 3 ) arrest.test knn.predict N Y N 4479 749 Y 160 238 0.162 The 10-Nearest Neighbor model was the most successful of the different k's I used (I tried 1, 5, 10, and 20). Still, it doesn't produce very good outcomes compared to the logistic model. In fact, if I were to guess that none of the people would get arrested, I would have an error of about 18%, so this model is only marginally better than the trivial method. The failure of this method is due to the fact that the variables cannot exactly be scaled, and the distances are affected by this. As there are many categorical variables and few continuous ones, high numbers of the continuous variables may have influenced the predictions, since the KNN function in R uses Euclidian distances to predict the outcomes. To rectify this, I perform the K-NN method using only the categorical variables. The predictions are slightly better in this model, as shown by the confusion matrix and misclassification rate below. In [16]: table4 round ( KNNMisclass2 , 3 ) arrest.test knn.predict2 N Y N 4547 582 Y 92 405 0.12 Interestingly, while the error is still larger than the logistic models, or in fact any of the models I used in the analysis, it is more accurate at predicting an arrest when an arrest was made than any of the earlier models, suggested by the second row of the confusion matrix above. That is, while it produces more overall error than any of the models I've used, it also produces the smallest type I error out of any of the models (the false positives). Tree-Based Methods I will now use tree-based methods (a single tree, Random Forest, Bagging and Boosting) in order to predict whether a person will be arrested or not. The first model I use will be an unpruned tree using all the variables. The tree selects groupings of the variables which produces the least errors in predictions, and lists decision trees based on the conditional results of each tree. It determines these factors by a popular vote in the case of classification trees, so that the most common outcome is the one which is predicted by the tree. The tree is posted below. In [17]: tree.arrest <- tree ( arstmade ~ . , mydata [ train , ]) In [18]: plot ( tree.arrest , main = \"Unpruned Tree\" ) text ( tree.arrest , cex = 0.9 ) This tree has six nodes, so it is relatively simple. If the statement is true, you go to the right, and if it is false you go to the left. The top node is whether the person was searched or not. This makes sense, as if a person is searched, the police are more likely to find a reason to arrest the person (because of illegal goods, etc). It is already evident that this tree can be pruned. For the trhsloc and pf_hcuff variables, either decision results in the same response. This means that the variable is redundant, and can be removed from the tree. The values that are included are: searched, repcmd (reporting officers command, which takes values from 1 to 999), contrabn (contraband), sb_hdobj (basis of search being a hard object), trhsloc P,T (whether the location was a transit location or housing location), and pf_hcuff(force used was handcuffing). Some of these are quite interesting: the fact that the officers command has an influence on whether the person was arrested or not, and also the fact that regardless of whether the person was handcuffed or not, they were not reported as arrested if they reach the bottom left node. This defies our common sense - as we often expect someone who is handcuffed to be arrested. Based on the prior conditional factors, the tree says that if someone is handcuffed given they don't have contraband, they weren't searched, and the reporting officers command was less than 805, they would be classified as not being arrested. Since we already have seen that we should prune this tree, I will find the correct number of nodes to prune to by plotting the cross-validation errors. In [19]: cv.arrest <- cv.tree ( tree.arrest , FUN = prune.misclass ) In [20]: plot ( cv.arrest $ size , cv.arrest $ dev , type = \"b\" , xlab = \"Number of Nodes\" , ylab = \"Deviance\" , main = \"Determining Best Reduction of Trees by Deviance\" ) points ( 3 , cv.arrest $ dev [ 3 ], col = \"red\" ) There is a clear leveling off at 3, marked with a red point, so we will prune the next tree to three nodes. In [21]: prune.arrest <- prune.misclass ( tree.arrest , best = 3 ) In [22]: plot ( prune.arrest ) text ( prune.arrest ) This tree is extremely small, and only considers two variables - whether a person was searched and whether the reason for the search was that they had a hard object. Below, I post the tables for the pruned and unpruned trees, along with their misclassification errors. In [23]: tree.pred <- predict ( tree.arrest , newdata = mydata.test , type = \"class\" ) table5 <- table ( tree.pred , arrest.test ) misclassificationunprune <- 1 - sum ( diag ( table5 )) / nrow ( mydata.test ) pruned.pred <- predict ( prune.arrest , newdata = mydata.test , type = \"class\" ) table6 <- table ( pruned.pred , arrest.test ) misclassificationprune <- 1 - sum ( diag ( table6 )) / nrow ( mydata.test ) In [24]: table5 round ( misclassificationunprune , 3 ) table6 round ( misclassificationprune , 3 ) arrest.test tree.pred N Y N 4426 447 Y 213 540 0.117 arrest.test pruned.pred N Y N 4466 486 Y 173 501 0.117 The misclassification errors are quite high for the single trees, which is expected since one tree will rarely be sufficient in predicting the outcome. They perform roughly the same on the data, which indicates that the pruning was effective - we were able to reduce the nodes without significantly effecting the accuracy of the models. It is notable that the pruned tree reduced Type I error, which is more desirable in this case. Still, both trees performed better on the overall error than the KNN approach. Next, I will use the bagging method. The bagging method, or bootstrap aggregation, uses bootstrap samples repeatedly to create many trees, and then averages these trees to make predictions. In the case of a classification problem such as this one, bagging will predict by the majority vote. Performing bagging with 300 trees reports the following confusion matrix and misclassification error: In [25]: bag.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 300 ) bag.pred <- predict ( bag.arrest , newdata = mydata.test , type = \"class\" ) table7 <- table ( bag.pred , arrest.test ) misclassificationbagging <- 1 - sum ( diag ( table7 )) / nrow ( mydata.test ) In [26]: table7 round ( misclassificationbagging , 3 ) arrest.test bag.pred N Y N 4505 285 Y 134 702 0.074 This is a large improvement over any of the previous methods we have used. In order to make the results more easily interpretable and prevent overfitting, I will reduce the number of trees used. Looking at the plot below can help us determine the correct number of trees to use : In [27]: plot ( bag.arrest , main = \"Trees vs Errors\" ) This plot determines the errors produced by each amount of trees. Using this, we can see visually where the line appears to stabilize and use this information to minimize the loss in accuracy from removing the trees. The green line shows the errors for the affirmative case (which is more likely), the red line for the negative case, and the black line for the overall error (the same as the misclassification error above). The plot appears to level off at around n=25. When I recreated the model using only 25 trees, I found the misclassification error to be 0.076. The error and confusion matrix will be included in the appendix. Although this is slightly higher than the model using 300 trees, it it a very small reduction in accuracy for a rather large increase in the interpretability and utility of the model. Now that we have determined a good amount of trees, we can look at an importance plot to see which variables create the most variations in the errors. Since this is a categorical variable, it will be most useful to use the right graph, which uses the Gini index. In [28]: bag.arrest2 <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ), importance = TRUE , ntree = 25 ) bag.pred2 <- predict ( bag.arrest2 , newdata = mydata.test , type = \"class\" ) table8 <- table ( bag.pred2 , arrest.test ) misclassificationbagging2 <- 1 - sum ( diag ( table8 )) / nrow ( mydata.test ) varImpPlot ( bag.arrest2 ) The variable with the most importance is sb_other, which is reason for search:other. This is a troubling thing, as described in the data section, because this is not a very informative variable. We can see that race now does have some importance in determining the arrests, as well as some other variables that were ommited from the logistic regression model. Interestingly, age, weight and height are included quite highly as well. Once again, whether the person was searched, had contraband, or was handcuffed is determined as important, which is consistent with our findings from the logistic regression. By using a sample of the variables in each tree and performing the bagging method on a different subset for each iteration, we can implement a Random Forest method. This method is useful since it allows some variation in the trees, since they will not only be dominated by the most important variables. Using two numbers for the number of variables used, I produced two random forest models. Below are their confusion matrices and misclassification errors. In [29]: table7 misclassificationbagging table8 misclassificationbagging2 arrest.test bag.pred N Y N 4505 285 Y 134 702 0.0744756487735514 arrest.test bag.pred2 N Y N 4490 277 Y 149 710 0.0757198720227515 The misclassification errors for the two are nearly identical. They perform very similarly to the bagging method. I would prefer the second model, as it predicts better in the case that the prediction and observed value are both yes than they are both no. By using a similar method to the bagging method earlier, we determine a good reduction in the amount of trees is to 40. Using 40 trees, we get the following confusion matrix and misclassification error. In [30]: RF.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = 9 , importance = TRUE , ntree = 300 ) RF.pred <- predict ( RF.arrest , newdata = mydata.test , type = \"class\" ) table9 <- table ( RF.pred , arrest.test ) misclassRF <- 1 - sum ( diag ( table9 )) / nrow ( mydata.test ) varImpPlot ( RF.arrest , main = \"Imporance of Variables\" ) RF2.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ) / 2 , importance = TRUE , ntree = 300 ) RF2.pred <- predict ( RF2.arrest , newdata = mydata.test , type = \"class\" ) table10 <- table ( RF2.pred , arrest.test ) misclassRF2 <- 1 - sum ( diag ( table10 )) / nrow ( mydata.test ) varImpPlot ( RF2.arrest ) plot ( RF2.arrest ) RF3.arrest <- randomForest ( arstmade ~ . , data = mydata [ train , ], mtry = ( ncol ( mydata ) - 1 ) / 2 , importance = TRUE , ntree = 40 ) RF3.pred <- predict ( RF3.arrest , newdata = mydata.test , type = \"class\" ) table11 <- table ( RF3.pred , arrest.test ) misclassRF3 <- 1 - sum ( diag ( table11 )) / nrow ( mydata.test ) In [31]: table11 misclassRF3 arrest.test RF3.pred N Y N 4515 293 Y 124 694 0.0741201564166371 In this case, the error actually was reduced. This can be accounted for by the fact that it simplified the model, which resulted in the random forest model with 40 trees actually predicting better than the more complex model. This suggests that the increase in bias by using a simpler model with 40 trees was less than the decrease in variance of the model on the testing data. This model performs the best of all the models I have used, and is the most successful at predicting whether a person is arrested or not when they are stopped and frisked, with about a 7.2% misclassification rate. Conclusions Below is a table summarizing all of the models that I've used, and their misclassifcation errors on the test data. In [32]: Method <- c ( \"Full Logistic Model\" , \"Reduced Logistic Model\" , \"10-NN Full Model\" , \"5-NN Reduced Model\" , \"Pruned Tree\" , \"Unpruned Tree\" , \"Bagging (n=300)\" , \"Bagging (n=25)\" , \"Random Forest I (n=300)\" , \"Random Forest II (n=300)\" , \"Random Forest III (n=40)\" ) TestError <- c ( misclasserrorsfull , misclasserrorstepwise , KNNMisclass , KNNMisclass2 , misclassificationprune , misclassificationunprune , misclassificationbagging , misclassificationbagging2 , misclassRF , misclassRF2 , misclassRF3 ) TestError <- round ( TestError , 3 ) data.frame ( Method , TestError ) # error: Yes when No Er1 <- table1 [ 2 , 1 ] / sum ( table1 [ 2 , ]) Er2 <- table2 [ 2 , 1 ] / sum ( table2 [ 2 , ]) Er3 <- table3 [ 2 , 1 ] / sum ( table3 [ 2 , ]) Er4 <- table4 [ 2 , 1 ] / sum ( table4 [ 2 , ]) Er5 <- table5 [ 2 , 1 ] / sum ( table5 [ 2 , ]) Er6 <- table6 [ 2 , 1 ] / sum ( table6 [ 2 , ]) Er7 <- table7 [ 2 , 1 ] / sum ( table7 [ 2 , ]) Er8 <- table8 [ 2 , 1 ] / sum ( table8 [ 2 , ]) Er9 <- table9 [ 2 , 1 ] / sum ( table9 [ 2 , ]) Er10 <- table10 [ 2 , 1 ] / sum ( table10 [ 2 , ]) Er11 <- table11 [ 2 , 1 ] / sum ( table11 [ 2 , ]) # error: No when Yes Er21 <- table1 [ 1 , 2 ] / sum ( table1 [ 1 , ]) Er22 <- table2 [ 1 , 2 ] / sum ( table2 [ 1 , ]) Er23 <- table3 [ 1 , 2 ] / sum ( table3 [ 1 , ]) Er24 <- table4 [ 1 , 2 ] / sum ( table4 [ 1 , ]) Er25 <- table5 [ 1 , 2 ] / sum ( table5 [ 1 , ]) Er26 <- table6 [ 1 , 2 ] / sum ( table6 [ 1 , ]) Er27 <- table7 [ 1 , 2 ] / sum ( table7 [ 1 , ]) Er28 <- table8 [ 1 , 2 ] / sum ( table8 [ 1 , ]) Er29 <- table9 [ 1 , 2 ] / sum ( table9 [ 1 , ]) Er210 <- table10 [ 1 , 2 ] / sum ( table10 [ 1 , ]) Er211 <- table11 [ 1 , 2 ] / sum ( table11 [ 1 , ]) FalsePositive <- round ( c ( Er1 , Er2 , Er3 , Er4 , Er5 , Er6 , Er7 , Er8 , Er9 , Er10 , Er11 ), 3 ) FalseNegative <- round ( c ( Er21 , Er22 , Er23 , Er24 , Er25 , Er26 , Er27 , Er28 , Er29 , Er210 , Er211 ), 3 ) Method TestError Full Logistic Model 0.095 Reduced Logistic Model 0.093 10-NN Full Model 0.162 5-NN Reduced Model 0.120 Pruned Tree 0.117 Unpruned Tree 0.117 Bagging (n=300) 0.074 Bagging (n=25) 0.076 Random Forest I (n=300) 0.074 Random Forest II (n=300) 0.073 Random Forest III (n=40) 0.074 In [33]: data.frame ( Method , TestError , FalsePositive , FalseNegative ) Method TestError FalsePositive FalseNegative Full Logistic Model 0.095 0.176 0.081 Reduced Logistic Model 0.093 0.183 0.082 10-NN Full Model 0.162 0.402 0.143 5-NN Reduced Model 0.120 0.185 0.113 Pruned Tree 0.117 0.283 0.092 Unpruned Tree 0.117 0.257 0.098 Bagging (n=300) 0.074 0.160 0.059 Bagging (n=25) 0.076 0.173 0.058 Random Forest I (n=300) 0.074 0.137 0.064 Random Forest II (n=300) 0.073 0.146 0.060 Random Forest III (n=40) 0.074 0.152 0.061 I have also included the false positive and false negative results, as these can help to determine which models fit the best. The Random Forest model which used 40 trees and 39 variables performed the best out of all the methods used. It is clear that the bagging and random forest methods were superior to the logistic and KNN methods. Of the simpler methods, the reduced Logistic Model using stepwise reduction with the AIC performed the best. Despite my original inclinations, we can see from the importance plots on the bagging model and the summaries of the logistic model that race was not a very significant factor in determining whether a person would be arrested or not. However, as we saw before from the bar graphs, there was a significantly larger portion of blacks who were stopped and frisked than the general population. This suggests that a black person is more likely to be stopped, but not significantly more or less likely than other races to be arrested after they are stopped. Which raises the question: why stop more black people if they are no more likely to be arrested after the frisk than other races? Another variable of interest was sb_other. This variable indicates that there was an \"other reason for stopping the subject.\" As such, this variable is difficult to interpret. However, the variable was quite significant with a high coefficient in the logistic model, and was rated as the most important variable by a large margin in the bagging models. This is a point of difficulty in the interpretations. This may indicate that there needs to be more disclosure about the reason a person was searched, since the sb variables in their current state fail to capture much of the reasons for arrest. In both the logisitic and bagging methods, the variables for contraband and knife/cutting object came up as significant, and had a significant effect on whether the person was arrested or not. This is logical: if the officer found a weapon on contraband on the person, they were much more likely to be arrested. This was far more significant than other factors about the person, in particular the cs variables, which list the reasons why the person was stopped, or the age, weight, gender and race variables, which are the physical attributes of the person. This suggests that the physical attributes of a person are not nearly as important as criminal possession in predicting an arrest, which is in support of the stop and frisk's usage. For instance, being stopped due to the clothing you wear (cs_cloth) or being perceived as a lookout (cs_lkout) were not nearly as significant as carrying a suspicious object (cs_object) or if they were searched (searched), and actually had negative coefficients. So it appears that, most of the time, the probability of being arrested was largely reliant on whether the person was searched or not, or found to have an object. This matches up with the significance and postive coefficient of cs_drgtr, which was the variable representing the cause of search being for a suspected drug transaction. One would conclude that if a person was being suspected for a drug transaction, they would also be more likely to have contraband or perhaps a weapon than if they were not. Interestingly, the frisked variable was not found to be significant in the logistic regression model, suggesting that frisking, as compared to searching, was not very significant in arrest, and therefore not too effective a measure to stop criminal activity. In both models, searching had a stronger effect than frisking in determining whether a person was arrested or not. The success of the random forest/bagging methods as compared to the simpler methods suggests that the relationship between whether a person is arrested or not and the various variables in the dataset is quite complex, and that it cannot be estimated as well using the simpler techniques.","tags":"Projects","url":"https://seanammirati.github.io/category/projects/nypd_stop_frisk_full.html","loc":"https://seanammirati.github.io/category/projects/nypd_stop_frisk_full.html"}]};