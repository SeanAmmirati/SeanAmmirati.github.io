<!DOCTYPE html>


<script src="//cdnjs.cloudflare.com/ajax/libs/require.js/2.3.5/require.min.js"></script>


<html lang="english">
    <head>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/require.js/2.3.5/require.min.js"></script>

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="author" content="Sean Ammirati" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="factor analysis, abstract, feature reduction, Projects, " />

<meta property="og:title" content="Factor Analysis "/>
<meta property="og:url" content="../../category/projects/factor_analysis.html" />
<meta property="og:description" content="A project detailing the methodology and use of Factor Analysis on a psychological survey." />
<meta property="og:site_name" content="Stats Works" />
<meta property="og:article:author" content="Sean Ammirati" />
<meta property="og:article:published_time" content="2018-09-16T00:00:00-04:00" />
<meta name="twitter:title" content="Factor Analysis ">
<meta name="twitter:description" content="A project detailing the methodology and use of Factor Analysis on a psychological survey.">

        <title>Factor Analysis  Â· Stats Works
</title>
        <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.1/css/font-awesome.css" rel="stylesheet">
        <link rel="stylesheet" type="text/css" href="../../theme/css/pygments.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../../theme/tipuesearch/tipuesearch.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../../theme/css/elegant.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../../theme/css/admonition.css" media="screen">
        <link rel="stylesheet" type="text/css" href="../../theme/css/custom.css" media="screen">
        <link rel="shortcut icon" href="../../theme/images/favicon.ico" type="image/x-icon" />
        <link rel="icon" href="../../theme/images/apple-touch-icon-152x152.png" type="image/png" />
        <link rel="apple-touch-icon" href="../../theme/images/apple-touch-icon.png"  type="image/png" />
        <link rel="apple-touch-icon" sizes="57x57" href="../../theme/images/apple-touch-icon-57x57.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="72x72" href="../../theme/images/apple-touch-icon-72x72.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="76x76" href="../../theme/images/apple-touch-icon-76x76.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="114x114" href="../../theme/images/apple-touch-icon-114x114.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="120x120" href="../../theme/images/apple-touch-icon-120x120.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="144x144" href="../../theme/images/apple-touch-icon-144x144.png" type="image/png" />
        <link rel="apple-touch-icon" sizes="152x152" href="../../theme/images/apple-touch-icon-152x152.png" type="image/png" />



    </head>
    <body>
        <div id="content-sans-footer">
        <div class="navbar navbar-static-top">
            <div class="navbar-inner">
                <div class="container-fluid">
                    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </a>
                    <a class="brand" href="../../"><span class=site-name>Stats Works</span></a>
                    <div class="nav-collapse collapse">
                        <ul class="nav pull-right top-menu">
                            <li ><a href="../..">Home</a></li>
                            <li ><a href="../../pages/about/">About This Website</a></li>
                            <li ><a href="../../categories">Categories</a></li>
                            <li ><a href="../../tags/">Tags</a></li>
                            <li ><a href="../../archives">Archives</a></li>
                            <li><form class="navbar-search" action="../../search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="container-fluid">
            <div class="row-fluid">
                <div class="span1"></div>
                <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
    <h1><a href="../../category/projects/factor_analysis.html"> Factor Analysis  </a></h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">

            
            <style type="text/css">/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area 
div.output_area 
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}



.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}






.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}








.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}


.rendered_html pre,



.rendered_html tr,
.rendered_html th,

.rendered_html td,


.rendered_html * + table {
  margin-top: 1em;
}

.rendered_html * + p {
  margin-top: 1em;
}

.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,

.rendered_html img.unconfined,

div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered 
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
</style>
<style type="text/css">.highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */</style>
<style type="text/css">
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }
</style>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Note: All of the code associated with this project can be found on my GitHub repository located <a href="https://github.com/SeanAmmirati/stats-works/blob/master/projects/Factor%20Analysis%20-%20Case%20Study/factor_analysis.R">here</a>.</em></p>
<p>What are the core factors of human personality?</p>
<p>It's a hard question to answer!</p>
<p>Data doesn't give us a direct answer to this question -- we may, using some sort of <strong>model</strong>, be able to determine how individuals are clustered into groups based on their responses to surveys. This is an example of <strong>unsupervised learning</strong>.</p>
<p>We can use this information to cluser similar people together, and infer that they have similar personalities based on this.</p>
<p>However, what we really want to know when we answer this question may be things that data cannot tell us. For example, what kinds of questions pertain to aggression? Introversion? Reliability?</p>
<p>We can determine clusters of similar behaviors -- for instance, the answer to the questions "Do you enjoy parties?" and "Do you have a large social network?" will likely be <strong>correlated</strong>, so we may believe that these are influenced by some <strong>factor</strong> called extroversion.</p>
<p>Most of the time, however, we do not model these explicitly -- we only wish to cluster individuals together, to make predictions of some measure (say, a happiness score), or make inferences about the questions themselves. For instance, people who answered question A and B in a certain way are likely to be similar to others <strong>in their responses to questions.</strong></p>
<p>This distinction is important -- we can speculate that there is some unknown factor involved, but it is not necessarily the case.</p>
<p>In clustering algorithms like K-Means, SVD, LDA and Gaussian Mixture Models, we do not concern ourselves with what the actual underlying factors actually are -- we only use them as a way to categorize the data in order to make inferences or to predict another variable of interest.</p>
<p>We can see that these approaches are <strong>data-driven</strong>. We assume that the data contains relevant information by which we can group people together.</p>
<p>But does this truly answer the question we wish to solve? In most cases, the answer is no, at least not explicitly.</p>
<p><strong>Factor Analysis</strong> attempts to model these underlying constructs explicitly. In this way, it is <strong>hypothesis driven</strong>, i.e., we model for the hypothesis we wish to verify or reject.</p>
<p>Hypothesis testing is a common exercise in Statistics, but in this case our hypothesis is quite specific -- we hyptohesize that there are some unknown, unseen, unobservable <em>factors</em> underlying the patterns we see.</p>
<p>It is for this reason that is particularly controversial -- who is to say that such factors even exist, and if they do, how can we categorize them in abstract terms? This is an open ended question -- but factor analysis can begin to give us a sense of how these factors may interact.</p>
<p>Here's the objective : from our concrete features, we wish to derive information about unknown, and generally unknowable, <strong>latent</strong> variables.</p>
<p>Let's dive in!</p>
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Overview">Overview<a class="anchor-link" href="#Overview">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Factor analysis is similar to <strong>principal components analysis</strong> in that we are working only with a single group of variables (in this case, the response) and we wish to in some way reduce the number of variables in order to simplify our data. This gives us a sense of what the underlying nature of the data is, and how it is organized.</p>
<p>In factor analysis, we wish to reduce the redundancy of the variables by limiting them to a smaller number of <strong>factors</strong> which can adequately explain the variation in the full set of variables.</p>
<p>Factor analysis is considered somewhat controversial by statisticians and is not encouraged by all schools of thought. This is because factor analysis is often difficult to validate in practice, as the number of factors or the interpretations are not always clear from the analysis itself.</p>
<p>Although at first glance factor analysis and principal component analysis may seem very similar, there are key differences which separate the two methods.</p>
<ol>
<li><p>In principal component analysis we aim to maximize the <strong>total variance</strong> of the variables in question, but in factor analysis we wish to account for the <strong>covariance between the variables.</strong></p>
</li>
<li><p>While principal components analysis uses linear combinations of the variables themselves, in factor analysis we create a linear combination of <strong>factors</strong>, which are unobserved, supposed latent variables.</p>
</li>
</ol>
<p>The factors that we are looking at are often some underlying attributes of the variables that we believe to be "seperate" in some way.</p>
<p>For instance, let's say there we have students' test scores for different classes: physics, chemistry, statistics, English, history, etc. We may expect that there would be some underlying factors that would affect an individual's ability to perform well in these classes.</p>
<p>Perhaps this would be something like quantitative reasoning skills, critical thinking ability, or reading level and skill. We wish to reduce the variables to a smaller subset that can use these factors (which are not observed but can be derived from the data using factor analysis) to simplify our dataset. We would then use factor analysis to determine both the "correct" number of factors and the effects these factors have on each of the variables.</p>
<p>It is quite easy to see how in practice this could be quite difficult to implement. This contributes to the skepticism of some statisticians to its use in the first place. Often times, the data doesn't easily lend itself to such a simplistic interpretation, and it is unclear what these factors can be and how they should be interpreted.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-Model">The Model<a class="anchor-link" href="#The-Model">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our model is constructed as follows: For each observation vector of $p$ variables, we have</p>
$$
y_1 - \mu_1 = \lambda_{1 1}f_1 + \lambda_{1 2}f_2 + ... + \lambda_{1 m}f_m + \epsilon_1 \\
y_2 - \mu_2 = \lambda_{2 1}f_1 + \lambda_{2 2}f_2 + ... + \lambda_{2 m}f_m + \epsilon_2 \\
... \\
y_p - \mu_p = \lambda_{p 1}f_1 + \lambda_{p 2}f_2 + ... + \lambda_{p m}f_m + \epsilon_p
$$<p>On the left side of the equations, $y_1, ..., y_p$ are the variables in the dataset, and $\mu_1, ..., \mu_p$ are the corresponding means of these variables. We do this in order to center the variables.</p>
<p>On the right side of the equations,  $\lambda_{i j}$ are the <strong>loadings</strong> for the $ith$ variable and $jth$ factor, $f_1, f_2, ..., f_m$ are the $m$ factors, and $\epsilon_1, \epsilon_2, ..., \epsilon_p$ are the $p$ error terms associated with each variable.</p>
<p>Our goal in doing factor analysis is to find some $m &lt;&lt; p$ such that the factors specified above are appropriate for the $p$ variables of interest.</p>
<p>In doing this, we are defining the original $p$ variables into a linear combination of $m$ factors. The $f$s in the above model are the $m$ factors themselves, while the lambdas are the loadings, which serve as weights for each factor for each of the $p$ variables.</p>
<p>While this may seem similar to a more typical <strong>multiple regression model</strong>, there are key differences.</p>
<p>Perhaps the most important thing to note here is that the $fs$ are unobserved random variables, not fixed effects like in multiple regression. Factor analysis only represents one observation vector, while multiple regression represents all of the observations simultaneously -- in this way, factor analysis is looking more to <em>individual</em> variation as opposed to <em>aggregate</em> or population level aggregation.</p>
<p>Our model can be written more simply in matrix notation as:</p>
$$
\vec{y} - \vec{\mu} = \Lambda\vec{f} + \vec{\epsilon}
$$<p>where
$$
\vec{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_p \end{bmatrix} \\
\vec{\mu} = \begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_p \end{bmatrix} \\
\vec{f} = \begin{bmatrix} f_1 \\ f_2 \\ \vdots \\ f_m \end{bmatrix} \\
\vec{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_p \end{bmatrix} \\
\Lambda = \begin{bmatrix} \lambda_{1 1}  ... \lambda_{1 m} \\
                          \vdots       \ddots\vdots         \\
                          \lambda_{p 1} ... \lambda_{p m}
          \end{bmatrix}
$$</p>
<p><strong>Assumptions</strong>: In performing factor analysis, we assume that the following holds:</p>
<ol>
<li><p>$\mathbb{E}[\vec{f}] = \vec{0}$</p>
</li>
<li><p>$\Sigma_{\vec{f}} = I_{mxm}$</p>
</li>
<li><p>$\mathbb{E}[\vec{\epsilon}] = \vec{0}$</p>
</li>
<li><p>$\Sigma_{\vec{\epsilon}} = \Phi_{mxm} \text{, where } \Phi{pxp} = \begin{pmatrix} \sigma^2_{\epsilon_1}  &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \sigma^2_{\epsilon_2}  &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \sigma^2_{\epsilon_p} \end{pmatrix}$</p>
</li>
<li>$\Sigma_{\vec{f}, \vec{\epsilon}} = 0_{mxm}$</li>
</ol>
<p><em>Notationally, I mean $\Sigma_{\vec{X}}$ to be the square covariance matrix of a vector $\vec{X}$ with itself, where the $i, j$th element is the covariance of the $i$th entry in $\vec{X}$  with the $j$th entry. It is therefore a <strong>positive semi-definite symmetric</strong> matrix with variances in the diagonal.</em></p>
<p><em>I mean $\Sigma_{\vec{X}, \vec{Y}}$ to be the covariances of each element in $\vec{X}$  with each element in $\vec{Y}$. This is not a square matrix, but relates the covariance of the individual elements of the two vectors.</em></p>
<p>These assumptions follow naturally from the model itself. For instance, as $\mathbb{E}[\vec{y} - \vec{\mu}] = \vec{0}$, it follows that both episilon and the factors themselves must also have an expected value of zero. We assume that the factors are uncorrelated, as we wish to minimize the number of variables used (so, we wish to have no correlation between the factors themselves.)</p>
<p>The $\Phi$ matrix shows that there are no covariance between the errors, only a <em>specific variance</em> for each error term, meaning the factors account for the correlations amongst the $y$s. This is exactly the goal of factor analysis, so these assumptions are self-checking -- if in recreating the model it is not clear what the factors should be or what number of them there should be, it is saying that these assumptions have not been met.</p>
<p>In achieving our goal in factor analysis of reducing the variables used, we wish to express the covariance of $\vec{y}$ in terms of the loadings $\Lambda$ and the variances of the errors, $Phi$ using some number of factors $m$ which is less than the original number of variables $p.$</p>
<p>We can show that the population covariance matrix $\Sigma_{\vec{y}-\vec{\mu}}$ can be written as follows:
  $$ \Sigma_{\vec{y}-\vec{\mu}} = \Lambda \Lambda{'} + \Phi$$
This follows from assumption 2 and 5 above. We can see then that the variance in this case will be a combination of some <em>signal</em> -- or explained -- variance in the latent variable loadings, and some <em>random</em> or unexplained variance in the error terms.</p>
<p>This seperates Factor analysis from Principle Component Analysis -- in PCA, we make no distinction between explained and unexplained variance except in terms of the number of components selected. Here, we make this distinction for a number of latent variables.</p>
<p>It is also of note that we can rotate the loadings by an orthogonal matrix without effecting their ability to reproduce this population covariance matrix. As such the loadings are not unique.</p>
<p>We will use these rotations when we perform our analysis. The rotations prove useful as the results given in the loadings may not make it clear which variables are effected by which factors. By rotating the coordinate axis, we can more easily interpret the results from the factor analysis.</p>
<p>The variances of each of the individual $y$s can be written as follows:
$$
Var[y_i] = h^2_i + \phi_i
$$
where $h_i^2 = \lambda_{i 1}^2 + \lambda_{i 2}^2 + ... + \lambda_{i m}^2$ and $\phi_i$ is the <em>specific variance</em> for the ith error term, that is, the ith diagonal of $\Phi$. Thus we can separate the variance of each of the $y$s into a communal and specific part, which is $h_i^2$ and $\phi_i$ respectively. As such, the diagonal elements can be easily estimated using the loadings and the specific variance, while the off diagonal elements depend on the selection of the loadings alone. Since factor analysis is largely dealing with the estimations of the loadings, it accounts for the covariations between the variables, rather than the total variance as in principal component analysis.</p>
<p>There are four different methods to obtain the loadings $\Lambda$ from a sample. These are the principal component method, the principal factor method, the iterated principal factor method, and the maximum likelihood method. These will be explored later in the coding.</p>
<p>Another difficulty is determining the number of factors, $m$. There are four approaches: we can select the number of variables $m$ which accounts for a prespecified amount of variance of the variables, we can choose $m$ to be the number of eigenvalues of the correlation matrix $R$ which is greater than the average of the eigenvalues, we can use a scree plot to determine where there is a leveling of the eigenvalues of $R$, or we can test the hypothesis using a chi-squared distribution if the number of factors is the true number of factors. Again, this will be shown further in the coding.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Data">Data<a class="anchor-link" href="#Data">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dataset I will be using to illustrate the benefits and limitations of factor analysis is collected data from an online personality questionnaire. The link can be found <a href="http://personality-testing.info/_rawdata/AS+SC+AD+DO.zip">here</a>.</p>
<p>In this study, 1,005 participants were prompted with 40 statements to rate on a scale of one to five. This scale is used frequently and is known as the likert scale. It ranges from 1 (strongly disagree) to 5 (strongly agree). The data was partitioned into four sections, with every ten statements designed to be related to some attributes of the population; in particular, it was looking for their assertiveness, social confidence, adventurousness, and dominance. The questions were designed to discover the prevalence of these attributes through the responses to the statements in the survey.</p>
<p>This seems like an ideal dataset to use factor analysis on, because the goal of the dataset seems to be precisely what factor analysis is used for. That is, we have some variables (the questions) that have some underlying, unobservable factor (initially hypothesized to be assertiveness, social confidence, adventurousness and dominance) that ties them all together. We are not very interested in the individual answers to the questions themselves, but of the behavior of the underlying factors that exist which are unobservable.</p>
<p>Some examples of questions are printed below:</p>
<p><b>AS2</b> I try to lead others.</p>
<p><strong>SC4</strong> I express myself easily.</p>
<p><strong>AD6</strong> I dislike changes.</p>
<p><strong>DO8</strong> I challenge others' points of view.</p>
<p>By looking at the coding markers of each of the questions, we can get a sense of how this survey was distributed and why. If, after doing factor analysis, there are four factors (that is, $m = 4$) and these factors are inclusive of each of the ten questions, we can then say that the data reflects these attributes well.</p>
<p>Therefore, the goal of this analysis is to see</p>
<ol>
<li>if the data supports the idea that these factors are well separated in the variables,
and</li>
<li>if not, is there a better set of factors that can describe the individual more effectively.</li>
</ol>
<p>After doing some research, it seems that many personality scales fail to truly describe individuals completely. A very popular scale, the Myers Briggs Type Indicator (MBTI), has been purported by many to effectively separate all personalities into one of sixteen types. However, when researchers used factor analysis on data testing for the four underling dimensions of the Myers Briggs tests, they found conflicting results. One of these analyses confirmed the four-dimensionality of the data, while the other suggested there to be six, rather than the four purported by the Myers Briggs test.The motivation for choosing this topic and subsequent dataset was to test these findings for myself. Although I was unable to find an adequate dataset with questions relating to the Myers Briggs tests, I used this dataset to see how well factor analysis can truly separate personality types from a series of related statements.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Analysis">Analysis<a class="anchor-link" href="#Analysis">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kn">require</span><span class="p">(</span>psych<span class="p">)</span></span>
<span class="code-line"></span>
<span class="code-line">tmp <span class="o">&lt;-</span> <span class="kp">tempfile</span><span class="p">()</span></span>
<span class="code-line">download.file<span class="p">(</span><span class="s">&quot;http://personality-testing.info/_rawdata/AS+SC+AD+DO.zip&quot;</span><span class="p">,</span> tmp<span class="p">)</span></span>
<span class="code-line">data <span class="o">&lt;-</span> read.csv<span class="p">(</span><span class="kp">unz</span><span class="p">(</span>tmp<span class="p">,</span> <span class="s">&#39;AS+SC+AD+DO/data.csv&#39;</span><span class="p">))</span></span>
<span class="code-line"><span class="kp">unlink</span><span class="p">(</span>tmp<span class="p">)</span></span>
<span class="code-line"></span>
<span class="code-line">data <span class="o">&lt;-</span> data<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">40</span><span class="p">]</span></span>
<span class="code-line"><span class="kp">head</span><span class="p">(</span>data<span class="p">)</span></span>
<span class="code-line"></span>
<span class="code-line">R <span class="o">&lt;-</span> cor<span class="p">(</span>data<span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[107]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kp">head</span><span class="p">(</span>R<span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<table>
<thead><tr><th scope=col>AS1</th><th scope=col>AS2</th><th scope=col>AS3</th><th scope=col>AS4</th><th scope=col>AS5</th><th scope=col>AS6</th><th scope=col>AS7</th><th scope=col>AS8</th><th scope=col>AS9</th><th scope=col>AS10</th><th scope=col>...</th><th scope=col>DO1</th><th scope=col>DO2</th><th scope=col>DO3</th><th scope=col>DO4</th><th scope=col>DO5</th><th scope=col>DO6</th><th scope=col>DO7</th><th scope=col>DO8</th><th scope=col>DO9</th><th scope=col>DO10</th></tr></thead>
<tbody>
	<tr><td>4  </td><td>4  </td><td>3  </td><td>3  </td><td>5  </td><td>4  </td><td>1  </td><td>3  </td><td>1  </td><td>1  </td><td>...</td><td>1  </td><td>1  </td><td>3  </td><td>1  </td><td>3  </td><td>2  </td><td>5  </td><td>4  </td><td>2  </td><td>1  </td></tr>
	<tr><td>4  </td><td>3  </td><td>4  </td><td>4  </td><td>3  </td><td>2  </td><td>3  </td><td>3  </td><td>4  </td><td>3  </td><td>...</td><td>4  </td><td>4  </td><td>3  </td><td>2  </td><td>3  </td><td>2  </td><td>3  </td><td>3  </td><td>2  </td><td>2  </td></tr>
	<tr><td>5  </td><td>4  </td><td>4  </td><td>5  </td><td>3  </td><td>3  </td><td>2  </td><td>2  </td><td>1  </td><td>1  </td><td>...</td><td>4  </td><td>3  </td><td>3  </td><td>3  </td><td>3  </td><td>4  </td><td>4  </td><td>5  </td><td>2  </td><td>3  </td></tr>
	<tr><td>4  </td><td>3  </td><td>3  </td><td>2  </td><td>3  </td><td>3  </td><td>4  </td><td>3  </td><td>4  </td><td>1  </td><td>...</td><td>3  </td><td>3  </td><td>3  </td><td>3  </td><td>4  </td><td>4  </td><td>4  </td><td>5  </td><td>3  </td><td>1  </td></tr>
	<tr><td>4  </td><td>4  </td><td>4  </td><td>4  </td><td>4  </td><td>3  </td><td>2  </td><td>1  </td><td>2  </td><td>0  </td><td>...</td><td>4  </td><td>4  </td><td>4  </td><td>3  </td><td>4  </td><td>3  </td><td>5  </td><td>5  </td><td>4  </td><td>4  </td></tr>
	<tr><td>3  </td><td>4  </td><td>4  </td><td>3  </td><td>3  </td><td>3  </td><td>3  </td><td>2  </td><td>3  </td><td>3  </td><td>...</td><td>4  </td><td>3  </td><td>3  </td><td>4  </td><td>4  </td><td>4  </td><td>3  </td><td>4  </td><td>4  </td><td>3  </td></tr>
</tbody>
</table>

</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<table>
<thead><tr><th></th><th scope=col>AS1</th><th scope=col>AS2</th><th scope=col>AS3</th><th scope=col>AS4</th><th scope=col>AS5</th><th scope=col>AS6</th><th scope=col>AS7</th><th scope=col>AS8</th><th scope=col>AS9</th><th scope=col>AS10</th><th scope=col>...</th><th scope=col>DO1</th><th scope=col>DO2</th><th scope=col>DO3</th><th scope=col>DO4</th><th scope=col>DO5</th><th scope=col>DO6</th><th scope=col>DO7</th><th scope=col>DO8</th><th scope=col>DO9</th><th scope=col>DO10</th></tr></thead>
<tbody>
	<tr><th scope=row>AS1</th><td>1.0000000 </td><td>0.4137723 </td><td>0.3682398 </td><td>0.4448159 </td><td>0.3221669 </td><td>0.2931655 </td><td>-0.3267101</td><td>-0.2608312</td><td>-0.1858184</td><td>-0.1295499</td><td>...       </td><td>0.1031671 </td><td>0.09393072</td><td>0.05655698</td><td>0.1736394 </td><td>0.1128714 </td><td>0.2045945 </td><td>0.2821300 </td><td>0.1982907 </td><td>0.1183617 </td><td>0.1488767 </td></tr>
	<tr><th scope=row>AS2</th><td>0.4137723 </td><td>1.0000000 </td><td>0.6344424 </td><td>0.4177133 </td><td>0.4526988 </td><td>0.4913432 </td><td>-0.4684408</td><td>-0.2919509</td><td>-0.3102848</td><td>-0.1228974</td><td>...       </td><td>0.2551933 </td><td>0.26932527</td><td>0.20737706</td><td>0.3264103 </td><td>0.2162572 </td><td>0.3004144 </td><td>0.2971938 </td><td>0.2598124 </td><td>0.3015476 </td><td>0.2841464 </td></tr>
	<tr><th scope=row>AS3</th><td>0.3682398 </td><td>0.6344424 </td><td>1.0000000 </td><td>0.3838178 </td><td>0.4944641 </td><td>0.5456192 </td><td>-0.4765066</td><td>-0.2718920</td><td>-0.2233892</td><td>-0.1374873</td><td>...       </td><td>0.2574269 </td><td>0.27971084</td><td>0.24749086</td><td>0.3568029 </td><td>0.2513066 </td><td>0.3279393 </td><td>0.2882716 </td><td>0.2657068 </td><td>0.2976032 </td><td>0.3326947 </td></tr>
	<tr><th scope=row>AS4</th><td>0.4448159 </td><td>0.4177133 </td><td>0.3838178 </td><td>1.0000000 </td><td>0.3028295 </td><td>0.3159420 </td><td>-0.2751444</td><td>-0.2203457</td><td>-0.1744766</td><td>-0.2328334</td><td>...       </td><td>0.1601098 </td><td>0.15038853</td><td>0.12135381</td><td>0.2166474 </td><td>0.1918464 </td><td>0.2673106 </td><td>0.2415963 </td><td>0.2472153 </td><td>0.1542765 </td><td>0.2298038 </td></tr>
	<tr><th scope=row>AS5</th><td>0.3221669 </td><td>0.4526988 </td><td>0.4944641 </td><td>0.3028295 </td><td>1.0000000 </td><td>0.4661230 </td><td>-0.3747829</td><td>-0.1977530</td><td>-0.2327827</td><td>-0.1506424</td><td>...       </td><td>0.2189824 </td><td>0.22601369</td><td>0.15205653</td><td>0.2137453 </td><td>0.1715335 </td><td>0.1811936 </td><td>0.2429682 </td><td>0.2532139 </td><td>0.2470171 </td><td>0.2718944 </td></tr>
	<tr><th scope=row>AS6</th><td>0.2931655 </td><td>0.4913432 </td><td>0.5456192 </td><td>0.3159420 </td><td>0.4661230 </td><td>1.0000000 </td><td>-0.3557865</td><td>-0.2394912</td><td>-0.2095667</td><td>-0.1155672</td><td>...       </td><td>0.2377874 </td><td>0.25424424</td><td>0.22620592</td><td>0.3082356 </td><td>0.2454524 </td><td>0.2617313 </td><td>0.2924134 </td><td>0.3185667 </td><td>0.2968619 </td><td>0.2707137 </td></tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I begin by importing the dataset and only using the variables pertaining to the questions. The next step is to find the correlation matrix. This is the preferred matrix to work with, as it makes many of the computations easier to do (as compared with the covariance matrix itself). We can see immediately that this is a covariance matrix, as there are ones in the diagonal. However, it does not appear to have particularly strong or easily discernible groups of correlations between the variables, which implies that factor analysis may not perform as well as we had hoped.</p>
<p>I did not produce the entire matrix, as it is a 40x40 matrix. Instead, I use the head function to display the first six rows of the matrix.</p>
<p>There were forty questions on the questionaire, with every ten representing a "factor" of one's personality. We are using factor analysis to  determine whether there is statistical evidence that these groupings represent latent 'factors'.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Selecting-$m$">Selecting $m$<a class="anchor-link" href="#Selecting-$m$">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As mentioned before, it often is not clear how many factors, $m$, we should use in the factor analysis. In this example, it would be preferable to use four, as that is how the data is designed. However, as we will see, four factors are not sufficient to separate the variables.</p>
<p>In order to perform factor analysis, we must find the correlation matrix of the dataset. This makes many of the calculations quite simple.</p>
<p>The next step will be determining how many factors we should use.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[108]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>R <span class="o">&lt;-</span> cor<span class="p">(</span>data<span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once we have the correlation matrix, the eigenvalues will tell us the optimal number of ways to reduce the features.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[109]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kp">eigen</span><span class="p">(</span>R<span class="p">)</span><span class="o">$</span>values</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<ol class=list-inline>
	<li>9.21029045767209</li>
	<li>4.24760230352397</li>
	<li>2.71094042312592</li>
	<li>2.12362289107785</li>
	<li>1.58576259730588</li>
	<li>1.40297421933113</li>
	<li>1.25498754107964</li>
	<li>1.0702244847308</li>
	<li>0.980598047554073</li>
	<li>0.932908788172687</li>
	<li>0.875987401716021</li>
	<li>0.810352421774326</li>
	<li>0.794043533485721</li>
	<li>0.770980301512266</li>
	<li>0.730096544235721</li>
	<li>0.680693097946611</li>
	<li>0.665770057061438</li>
	<li>0.636014100079021</li>
	<li>0.589687061258025</li>
	<li>0.563270135901251</li>
	<li>0.550068374680893</li>
	<li>0.521441549385526</li>
	<li>0.5079724127076</li>
	<li>0.49008858321543</li>
	<li>0.471300978751053</li>
	<li>0.455906836189933</li>
	<li>0.435144284728272</li>
	<li>0.412711543929556</li>
	<li>0.391866415262251</li>
	<li>0.375715323212406</li>
	<li>0.359819660931453</li>
	<li>0.349986696198294</li>
	<li>0.339350022467012</li>
	<li>0.315884675549187</li>
	<li>0.295294865270752</li>
	<li>0.255552072439942</li>
	<li>0.235430143959798</li>
	<li>0.225839012349674</li>
	<li>0.203852564039258</li>
	<li>0.169967576187254</li>
</ol>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are numerous methods to select the number of eigenvectors/eigenvalues we want to include. Each eigenvalue represents the proportion of variance explained by that eigenvector (a linear combination of the variables). We want to select eigenvectors/values such that we can explain a good deal of the variance with a smaller number of linear combinations of the features.</p>
<p>The first method involves choosing an arbitrary threshold. In this example, at 19 latent variables (that is, reducing the forty variables nearly in half) we have acheieved a threshold of variance explained greater than 60%.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[110]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kp">sum</span><span class="p">(</span><span class="kp">eigen</span><span class="p">(</span>R<span class="p">)</span><span class="o">$</span>values<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">19</span><span class="p">])</span><span class="o">/</span><span class="m">40</span> <span class="c1">##choose 19</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
0.80183840681608
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another method is to select eigenvectors such that each corresponding eigenvalue is greater than one. This means that the eigenvector of variables contributes more to the variance of the features than any single feature alone. Using this method, we come up with 8 linear combinations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[111]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kp">sum</span><span class="p">(</span><span class="kp">eigen</span><span class="p">(</span>R<span class="p">)</span><span class="o">$</span>values <span class="o">&gt;</span> <span class="m">1</span><span class="p">)</span> <span class="c1">##choose 8</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
8
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The final methodology is using a scree plot and determining where the eigenvalues level off. This is at 9 linear combinations of the features.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[112]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>scree<span class="p">(</span>R<span class="p">)</span><span class="c1">##choose 9</span></span>
<span class="code-line">fa.parallel<span class="p">(</span>data<span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAYFBMVEUAAAAAiwBFpkVNTU1d
r11oaGhvtm98fHx+vH6LwYuMjIyWxpaampqgyqCnp6eqzaqysrKy0bK71Lu9vb3C18LHx8fK
2srQ0NDR3dHY39jZ2dnh4eHl5eXp6enw8PD///8UOFWWAAAACXBIWXMAABJ0AAASdAHeZh94
AAAgAElEQVR4nO3dDZeixrqG4eohhIQYj4m7Y9KO8v//5RH8aD9QgXqoegvua609022j1O7x
DgUiugqANxd7AMAUEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIg
QEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBI
gAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBCSBZtF5ly+3Cof07n2f9uFciU4
IyQDlu5kJXzQ9pC+Mv7FR8GvNb61u/jSPWp7SM+2U/DErzW+3Lnlvqp2hXOl7lEJKSh+rfGd
n9z7yxfLQ1vF5vzDXe6Wza2Zy5a76rTI1TffD7M+3HGxu3nUww7Y4cvF5nSbI6VR8EuN7/Dk
L6/ndLvs+Hyv66mf+3VV37d+XS9yfbfDt2VzY7Y7fdvcXJzqKStCGhG/1PiafaRs8XnevmTn
PaZ6K3L86vP71ux6kezqYb73tPLTt/Vf5eXWkpBGxC/VgPOTPW/mX4eusm21L45BHG4u9sdb
D3/vF01UN9+c1V0dtlCb7FLg4c/N4a/1YdGVu74VcvxaLdjk7ns2Vx6f8vt8VW+iTgHUt9Y9
Vc2W5eabM3fK6tDOojonc2ht3fx0eX0r5Pi12rD7XDTTtfX9c91dmjnL7r65WvL8RX759nz3
anf5Psj/n9nh12rHrmwSeAjp/PfZ3Tf3S1bXyXwvQUij4tcaXXbeaJye5u0hZde3Zm05XDY+
dyFdtkhZRUij4dca3eK491Kdp1/F/T5S87PTnlP1+M2ZO+0ObS6Huo9Lso8UAr/W6OoDa82L
qPXxtkXLUbtmqc/jMbnP5jWlm2/OTofJ60dZX+7YdtRu/zgG+CKk+L5f6nGuDuryOtJ3D9e3
ft1/c3J3BOJ0x+Jy6+L8KMug/+9mgpAMuDzZs2bC9nV3ZsNxoY27uvXmm5P62+OjtJ7ZcJw+
1qcLXW3FoEJIFjTvR3Ll6jTpqk+kc+XlXLvqfGt+ufX2m+qy5GfusuX+9o7Ngy/Oi5aXPTIo
EdJkcBwhJn73k0FIMfG7nwxCionf/WQQUkz87ieDkGLidw8IEBIgQEiAACEBAoQECBASIEBI
gAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAA
IQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiAQICQHJGbAs1wfToRVYPrcz3AI
CZNFSIQEAUIiJAgQEiFBgJAICQKEREgQICRCggAhERIECImQIEBIhAQBQiIkCBASIUGAkAgJ
AoRESBAgJEKCACEREgQIiZAgQEjPVjHoHb2YK0JqX8XgN8djnrxD+mj8+P3f47f//P7j4/e/
CQkzIwrpkFJT0h/Hb375N/GQPK7Xgll6FtLhOdQ1pPrP/379+OPw158fPw5bo//+PGVFSJiL
9lxOz6LuIf387+PHz5//ngP6/eN3QsKcqEJq/v7j48/jN//99n9ph8Q+EvppjeXy3+PuITVb
pF8//nm1KCFhstxtOa89D+nfZh/pvHWaQEi8joReBFuk01G7/yYWEtCDYB/p+3UkQsJcyQ42
1H677CP9/R8hYU5EryMd/Xk+ave/j18ICXMiOLPh++vL60i/fqR++BvoRRrSz9+bMxv+/a0+
Fk5ImBFtSIdN0TTOtQP6EYf086/fPj5+/at90dRCIjF0xvuRCAkChERIECAkQoIAIRESBAiJ
kCBASIQEAUIiJAgQEiFBgJAICQKE9GIVlISuZNe1O50p9KP9fFVCwrSJQ/r78MWT66wSEiZM
+sa++n0Uf7Re0o6QMG3P3mp+/qNnSIeJ3Y+nF24gJEzWk5Cu/uwT0l8ff/z84+PJuygICdPV
fjmuu787h/Trx/9+/u/jV0LC3JwvEHnzP3f39+V/rSF9H2toLrd6mNu1XUKIkDBlgi3Sd0h/
NR9J8XRuR0iYLOk+0i+Hmd3Pn/+0XouLkDBlyqN2/15eU2q99gkhYbqUryP9eQnpT0LCvCiv
IvTLaUv075O5HSFhsoQh/fPx2+mrJ5+TREiYLGFIf1zOsvu7OXpHSJgPYUg/vs/7/jGNSxYT
Erri/UiEBAFCIiQIEBIhQYCQXq2CktARIT1bRf2p5oSEjgipfRXOnf8AOiCkJyGFWj2mgZBa
V3G6kU0SOiIkQoIAIRESBAiJfSQIEBJH7SBASLyOBAFC4swGCBASIUGAkAgJAoRESBBwIQ0Y
nv7/cZ9VEBIMIiRAILWQOPwNkwgJEEgtJKZ2MCm9kCgJBqUWkiMkWERIgEBqIYVZP9ATIQEC
qYXE1A4mERIgkFpIYdYP9ERIgEByIXGOECwiJEAgZEi7hctWVbXOXbb0WAUhwZ6AIe2z+o2E
61XzfsJi+CoICfYEDGnpDtuhZeYW+2rffD1oFUztYFHAkLLmjs7tm7+ygasgJFgUMCTnvv9s
uWBq5ytJEBLsibBFqv/cD94iERIsirCPtNyfvh60CqZ2sCi5o3aEBIsSfB2JkmBPcmc2hBkA
0E9yIfE+ClhESIBAciGFGQDQDyEBAsmFxNQOFhESIJBcSGEGAPRDSIBAeiFxjhAMIiRAIL2Q
mNrBIEICBNILiakdDCIkQCC9kJjawSBCAgTSC4mpHQxKMSRKgjnphRRmBEAvhAQIpBcSUzsY
REiAQHohhRkB0AshAQLphcTUDgalFxKvyMKgBENiiwR7CAkQ8A1pnVfVLnf5l2pAj6t4/CEh
wRrPkDb1B/A1n3skLYmQkBjPkAr3WW1dXn2++eQwj1X0/zEQnGdI9QZpW3+M5evPT/ZZRf8f
A8EJQirdJmhITO1gj/fUbrupP6A85NSOkGCP/8EG51b1BmkjG1LF1A7J8T78ndV7SFX+KRpP
yyp6/xgILsEXZDnZDvYQEiAgOGrXyDLFaNpWMeTnQGCikHZBX0ciJFjjEVJzxO4iDzYqpnaw
x2eLlF93FPJcO0KCNap9JC2mdkhMgkftwgwB6CPBkJjawR7fkFaXHSXViB5W0fJTQoIxniGt
vg83yIZUMbVDcjxDytxaNpQnqxjycyCwFI/aMbWDOZ4hlW4vG8qTVbT9lJBgjGdIu6zQXj/o
cRVDfg4E5j2142ADkGZITO1gToIvyBIS7EkxJKZ2MMc7pE3ZXJJrJxpP2yqGLAAE5RtScdw9
cpm0pHdTO0KCMZ4hrV2xr0Nau4VsSBUhITnepwjtj2c3hD38TUgwRnCKECEBniHlpy3SNuA1
G5jawR7NPtJGfBY4ISExvkftytN5DdJr6DO1Q2okryO5Unvpb0JCapI8s4FzhGANIQECSYbE
1A7WpHgVoS4LAEEleRUhpnawJsmrCBESrEnxKkJdFgCCSvEqQl0WAIJK8ipCTO1gTYoXPyEk
mJNkSJ2WAAJK9AVZQoItSYbE+yhgjXdIn0X4s78JCdZIriIU/v1IhARbvN8hm20Of4V9h2y3
JYCAvK/ZsG3+DnrNBqZ2MEd1ilDg15EICbbItkiZZjyPqxi6BBAQ+0iAQJpH7ThHCMb4v44U
4ypChARjkjyzIcwggO4ICRDQTO0WG9FwWlfR9nNCgimqgw2lakCPq2j9OSHBFM+Qlhz+BirB
VYRinCLUaQkgoCRPEWJqB2u8p3bnLZJ0J+l9SJQEU7wvWdzsI31lod+PREgwRXfxE+UFUAgJ
iUkzJKZ2MCbNMxsICcakGVKYUQCdERIg4BvSMotxpVWmdjDG+3WkKJcsJiQY433ULsYHjXVc
BAhGdYqQFiEhMd5Tu1gfNEZIsMT7/UjFTjWUZ6toX4CQYIlvSJtIn49ESDDFM6RVrA8aIySY
4v3GvjhH7ZjawZZEj9oREmzxntrFOWrH1A62eL+xr/hSDeXZKgYvAgSjez+SbEhVp6kdJcES
QgIEkn0bBSHBkpAh7Zf1p5GtcueKN59eQUhIjHdIn0XXj3XZZYf53z7r8nlKTO2QGO9z7bp/
0NjClfvDH4vdoamFW3qNipBgi2dIfT760tWvObnjC0/71585y9QOifEMKe9x7e/mwF7mrr65
+3G/Q4CEBENUpwh1eO4v6uhWx/L2r+eCTO2QGNkW6eVU7bzMcluV2eEem9y9/GgyQkJiAu4j
1UtdrLxHRUgwJOBRu4PPRd58vN/qzdtqCQmJ8X8dqez6OtLgVbQvQUgwJNVThAgJpqQaElM7
mOIb0vH8uWypfX8fISExniE158/VryJl0qtyMbVDYjxDKtyi3hbtl2E/Q5aQYEzAMxsGrsJn
GSAQ78txHXeO9uFDoiQY4hnS0jUXP/kqXr8twmcVT5cgJNgR9syGQat4tgQhwQ7NmQ2F+Hqr
TO2QmHRfkCUkGJJqSEztYAohAQKphtR1ISAIQgIEUg2JqR1MISRAINWQui4EBOERkrsVfFSE
BDuSDYn3UcAS36ld2VyO6ytbiMbTsopnixAS7PA++/t8gcjAZ393XQgIIt039hESDPF+Y1/3
SxYPXMXTRQgJdnhP7bL6jX2b7M01iD1W8XQRQoIdqjf2Sa990nFUlAQzRJcsfvnZEr6r8FsK
CCDZMxs4RwiWEBIgkGxInZcCAvANaZVHO9eOkGCHZ0iriOfaERLs8H5BVnwhrsdVPF+EkGCG
6hQhLaZ2SIxnSKXTfjBSyyp8lwIC8AxplzXX/lZjaofEeE/tor1DlpPtYEi6IbFFgiG8IAsI
pBsSUzsYogrpK/BnyBISTPENack+EiB4h+yZ9B1JhITEeJ8i9FkVbrcrnPTlJKZ2SIzgFKHV
YWu01X6IbLeQKAlmCELa1CeuxthHIiSY4X2u3We1c3n1RUiYNc+QNnVAzZWEpNcsZmqHxHi/
Q7b+buG0VywmJKQm3TMbui8GjI6QAIF0Q2JqB0MICRBIN6TuiwGjIyRAIN2QmNrBkHRD4qxV
GOIb0jqvql3ucu21hJjaITGKU4Sy+hSh4G+j6L4YMDrPkAr3WW1dXn2GfxsFUzsYIngbxbY+
0S7Kp5oTEqwQhFTWbzPnbRSYNe+p3XbjsirK1I6QYIf/wQbnVvUGKfzFT3ghCXZ4H/7Omrci
5Z+i8bSs4sVChAQjEn5BtsdywMh8r9mgfWds2yoUywEjExy1GwFTOyTGM6Q84if2ERLs8Axp
X8b6xL5eywEj857aRbyIPiHBjIRDYmoHOxI+/E1IsCPhkHosB4zMO6TP+oLFpfbEBkJCanxD
Kk57SNJzVjuOivdRwAzPkNYuq89W3WT1R7voEBIS4/2C7Lb5u36XrBBTOyRGdYoQryNh1mRb
pEwznsdVvFqKkGAE+0iAQMpH7ZjawQz/15HKiK8jURKMSPnMBs4RghmEBAioDn9nEY7a9VkQ
GJUopF2c15EICUZ4hLRx1yKc2cDUDmb4bJHy644ifBoFIcGMhK8i1GtBYFQpH7XrsyAwKt+Q
9sv6cF221F6Vi6kdEuMZ0i5rJnfOZTvViO5X4bsUEIBnSIVb1Nui/dKVqhHdrwJIQNrvRwKM
8AwpO12yeM8n9mHWPENauuaSxV+Fk34sBSEhMWm/HwkwQvN+pEL6/lhCQnKSfkHWVeJrjgMD
pR2S/vL9wCCqkL5ivI5ESLDCN6RlxM9HGmXVwBDeh7/PNrIhVYSE5Hi/IPtZFW63K1yM9yMR
EqwQnCK0OmyNttoXkthHQmIEIW3qq6zGOUWIkGCEZ0jlYWq3c3n1FelcOzKCDZ4hbeoncnOa
0EI2pKpHSIANvoe/V/V3C6c9Z5VCkBqPkJYr6UjaVgEkwiOkelYX9ypC9AYrvELaERLQ8Ahp
cXOlVd5qjjnzCGlfDg3p7cKEhMREudKqLCTXY1lgRAFDct2ngoSExAR8Y99XJg/JZziAUMh3
yB52qormgqytFQ3e4Ro6HEAn7FvNP52rP7ZZuo9ESTAg8DUbdoUr94SEyQl+8ZOVyzbaw9+E
hPjCn2u3zd/vAxESEhPjXLsFUztMTdLn2hESrJjCuXaUhOiinGsnHhUhIboo59r1WsX75QgJ
0RESIBD8daQRVkFIiM47pM/6GkLlp2g4ravQLg2MwDekuJ/Y5/otDozFM6S1y+qr528yJ/3M
PkJCYjxDyt22+Xvrcs14HlehXxyQUx21i3rxE0JCbLItUqYZz+MqOixISIgt7X0kQoIRaR+1
G7o8IOb/OlIZ/XUkQkJ0iZ/ZwPFv2EBIgEDiIQ1cHhAjJEBgGiFREiIjJECAkAABQgIEfENa
51W1y13+pRrQ4ypGuQMg5RlSc/Xh5uNapCUREhLjGVLhPpv3In1qT7brPypKQlSC9yNt3TLy
+5EG3QMQEoRUuk20kC4XpiQkROU9tdtu6vf0xZnaNRUdUyIkROV/sMG5Vf1s3siGVHUP6ftP
QkJU3oe/s3oPqcq1b0jqNip39TchIaqUX5C9DomSEBUhAQLeIW3K5sjdTjSetlW8WYwLoCA+
ycVPDrdl0pL6H7UjJETlfTmuYl8/k9duIRtS1WNU3x9wRkiIyTOkzO1vtgsiNvfcgKcEZzbY
CImSEJNnSPlpixT3IvqD7wOIaPaRol2y2PM+gIjv3khp4pLFQ+8DiEheR4p+yeKh9wFEbB4f
IyQkJv2Q3MMXQHCEBAh4v450IRtSNbAJQkI8hAQIaKZ2X0XpP5TXq3i/MCEhHtE+0j7SSavX
CxMS4lEdbDAwtaMkxCMKaV1fSkiHkJAY2cGGlWxI1cCpHSEhHlFIufSc1Z6jYicJ0aX/gqzv
vQABQgIEPEJyt+KNip0kREdIgMCUpnaEhGgICRBQhfQlPdmu56g4/o3YfENaGthHIiRE5xnS
d0cxPh9Jdj/Ak/eVVj+rwu12ReRPNfe8H+BJcKXV1WFrtI36qeYc/0ZsgpA29cUh476NgoAQ
mWdI5WFqt3N59WXh/UhANJ4hbeqAms9IivcOWSA+38Pfq/q7hWs+kVmHqR0SM40zGwgJkXmG
tJcN5OkqgAT4HrUrpC/Etq2i3x21Bz2AjjxDyg/P3KX0tdiHVfS4wwgnKwGd+O4j7VaHlvKV
eIpHSEiM4GDDbpk58RRv4FvNxzh/FuhCc9RubeHa34SEeBRbpGZ2J/3MvmFTO0JCNJJ9pGy5
U42nZRXd70FHiEVw1G5h4Kjd6W6EhEi8X0cSfwzz4yr63ZGMEMVUzmygH0Q1jXPtCAmRTSUk
ICpCAgQmExLtISabT1mbowKesvmU9VwFUSE0QgIEvK/ZkBu4ZPHDfSgJgXmGtLLw+UiP9yEk
BOZ9yWLtpzC3rCLOAwC9CK60OgJCQmK8r7Q6ytl2vlM7SkJgniHtskL/JgpCQnK8p3ZmDjbo
HwLojJAAgQm9IOtefguMiZAAAe+QNmU9qyu1Vz+RREBJCMc3pOK4e+QyaUmEhMR4hrR2xb4O
aW3hg8bu70VJCMb7FKH98ewGC0ftCAnRCE4RMhMSEI33BSKPW6Sty2VDqggJydHsI23EZ4Fr
pnZAML5H7crTeQ2FakCPqxj5XoCA5HUkV4ovXEwSSMyEzmwA4plUSC1346r6CEJ39nexlA1K
FhKf84JAdCE5l8UcVfvjEBLC8J3aLbL6U5g3mfuqSifbJome+nwWJkLxDGnpts3fW1dUe92L
skOf+ffvpCAkBKK6itDlXCEJQkJivE9aPW+RMgshPTwOHSEM76ndeR9pWX3qTm8gJCRG8sa+
4ylC7t35dvvFYbnN6UFerlj31CcjBCE6RajOw61e32+fNcmVxwcJFBIQRMAzG5b1Fmu/zpoJ
ICFhUgKGlB3vuMvyHSFhYjxCOh7x7n6I+bzIvijaQro5SaL/qICYAoaUXy64nxdskTAtAad2
31ca2rlipJDYmCGOkG+jWF6e5Zs3T/ihZ38/uwwLdWFknlO77xu7PNC2PH+1W4wS0rM7ExJG
5h3SqSALl+Nyd397PyDQFSEBAjMJiZIwrimF9HwfiZAwsmmF9Oyo3fCHBDqZVEivXkciJIxp
YiEFfkzgxCuk0U6PIyQkZj4hURJGFPIUocirICSMZ0YhURLGM6eQgNFMMiQ6RGiEBAhMMiQg
NEICBCYa0tMH4L3oGMW8QuIaRRjJREN69riEhHHMKiQum4exTDak1rfJEhJGQkiAwGRDan9c
OsI4CAkQmHBI7RdBISOMYW4hAaOYcEhAOIQECMw+JHaaoDDzkDiMBw1CIiQIzDskTnWACCER
EgQIiZAgMPWQ3n1YLR1BYtohvfqcl9MChASFiYf0/tHICAqTDsnd/d3pPoSFAQjp9h5M9TAI
Id3eg5AwyKRD6rKPdLs8h8MxzMRDenfU7mF5QsIg0w6p77EDQsJAUw+p74rpCIMQ0u2KCQmD
ENL9qskIAxASIEBIgMBMQqJMjGsmIQ1/RPaZ0MVcQhqIo3johpBeIiR0M5uQBj0kZzqgo9mE
NOgxCQkdzSekIdpCIiu0IKSXHjpiC4VWhPQSIaEboyEBiRnwLNeHE2EVwzz82gb/FjEthNQL
IaEdIfXT3hElzR4h9dMhJLKao7mF5P8sdy0H8a5uYgs1T/MKqe9Vhbo84l03hDRPMwtJ/uhP
NkiUNDezCqn/lVe7POj9DlLLNkq5PphESOJVvD8+TlhTREjqdbw5+NAy86OsCZhVSCPsIz2u
4s3xcY5OTNPMQpIftWtdyYvj4xwvn6h5hRR+GvU2JI5OTMPcQgqu41SPs/cSR0gj63jwgZAS
R0ijc/cHv18ebGiZ6jHXSwAhBeceDn6/DomwUkBI8b08zNfplShER0jWDNiJIqv4Zh+SuSfh
u5A4OmHSzEMK8gptX61HJ3qFRFfBzT2ksKsb5M3RiC5HJzC6eYcU4ixWb2866RISWY2OkEKu
b6DWud7dt/3OlaAsNUIKuT6Rl2FxwnkU8w4piX2kd96FxNwvhLmHZPGoXW+P8zbmfqHNPKTH
Z8wEnkH+IbHJ6m32Id2teBJbqJ47Ue/nfoT1FiG1rHhiz5F3G5z+myzmgg8IqW29U3tOeO5E
MRd8j5Da1jv554Df3G/IXHDqZRFS23on/U/+qO/cr/9ccPovXRFSy4on/O/9xOu5n/9ccPpz
P0K6XfE0jtr56zT3u//eZ5OVeliEdL/qtP89x/I6rCDHAW3/yxAShug3FxQcB+xQWlSE9Iap
fy3D/DZZD2G9D6mlrJgI6SX2mYZ6vcnquoVK5/AFIb0016N4I3i5yeof0rMbnq5xZIT0ykxf
Vwqg40zOI6TAUz9CeoWQxvNyCzXG4YtxN1GE9AohhfP4pH+9gek7Fxx5E0VIL7GPFM/9k/51
aUNCesjKozNCeqnlqF3AeTdeejk5fDbVG+sUdkJ64+G/g9V9WTCh707VoNeAX6x+wIj738Xg
KgZirmfXyy2U5ID6q5UPGG//uxhcxTAtRx+Y6lnVb6eqw17Wq39rQurlISSmesnwDunlFoqQ
enkM6fZbWPZ67te1tCePPWA4/e9icBUD3YXDC03pehfS27BuH23AAPrfxeAqBrqbyhFSyh6i
eLnJIiSt2/+I3f2NSWnbYBHSKNhHmg/2kUbEqQ/zQUijcvcZVfdlYSp4HSmcx7keW6g5CBrS
16psNo7l8musVcTGK7YzFTCkfe6+FaOsIj5esZ2pgCEtXfa5bb7abTK3HGMV8d2HxMl5MxEw
pMxtL19vXTbGKgx4d+oDh/mmKWBId68aP/74ysBVGPDu1IeHqd5jWUn//58rtkhyd+eZXP1Z
tYT1UBZHJ5IUdh9ps2u+mvA+0oNuWyiOTqQu5OHv4mrulu9HWYVFr0/O63B0AgkI+zrSsnkd
KStXk30d6a2Hmdybv5uvmemZx5kNgT3sAr3fieJoRAIIKbg3J+c9Hta7u4GjERYRUnz3byer
WsJ6eTSCLVR8hGRP2/HzFztRvMRrASFZ9z6k22+Z+0VBSOa9ORrx/iVeBEBI5r05GvH+JV4E
QEgJeHk0oktIby6XA3+ElKCXZ/N1OBrBK1N6hJS8dy/xDnplirJ6IqQJeP0S7/u5X4fjfoT1
BiFNke95su83WYR1h5Amr39IbzdZzAUfENL0vdtHGloac8ErhDR9b4/avTvup5gLTr0sQpqD
tx+7UFWvjvv5zwWnv8kiJFTvjvv5zwW7bLLSRkho0+WtHd3ngh02WamffWE0JCAxA57l+nBM
rGsoxqgxuzES0i3GqDG7MRLSLcaoMbsxEtItxqgxuzES0i3GqDG7MRLSLcaoMbsxEtItxqgx
uzES0i3GqDG7MRLSLcaoMbsxEtItxqgxuzES0i3GqDG7MRLSLcaoMbsxpvB/GDCPkAABQgIE
CAkQICRAgJAAAUICBAgJECAkQICQAAFCAgQICRAgJECAkAABQgIECAkQCBbSMnPZch9qbQOs
z78KsyNd55eBWR3jfuHcYnv82uoYa1+nf2zdGEOFVDQX+c8DrW2A7fkjCMyOdNkMLKv/2c2O
MWsG1pRkdowH++z4jy0cY6CQvly2rbaZ+wqzuv4OYzv+KsyOdOsW+3q7uTA8xmU9uqUrK8Nj
rJXHf2zlGAOFtHSbw5+fbhVmdb2tXXEKyexIy9PHeTnDY8xcvb1sfpNmx1jVozr+YyvHGCik
0u2q+j+qZZjV9eaW58+wMz9SZ36MLqtMj3F3/q+mcoyBQnLu+i97tvdDtDrSvSusj3Hp1pXp
MRZudxyWcoyEdJZGSOt6NmJ5jIdp07L+2+4YV+6zIqTxJBHSLqunIZbHuC6zZp/D7BibmRwh
jSeFkPZZUf9leoxVtajndmbHmNcvICQbUmb11/rtNDbTIy2OL3mYHmO9H5fZHeOiOVJ3HJZy
jEGP2u0sHsM5uzlqZ3Kku7zYNV8YHmPj+8iivTG6C+0YA4W0av47sDnuh9p0CsnuSDeuOH1l
dozH15F29bkCVsd4HZJyjJzZcGb9zIbdpSO7Y2zObNiX9T6S2TE2kj2zocqb/1VI91gAAAPr
SURBVAgU7xeM5jxTtjrSxfd/Sc2O8XSuXTMws2Osnf6xhWMMFdK+Oc820MoGOYdkdaRXUxKz
Y2xOp87XzVd2x1hd/rGFY7R2UAVIEiEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQI
EBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBAS
IEBIgAAhAQKEBAgQEiBASIAAIfnaL9yzT/PdhB3JWz3H43hydMfvylfpnFu1/iQ39svtOx5C
6oHflS/ndk9/EnQgb/Udj7Xxm8bvytfzp5u1JyIhjYjflZ/zB41vDjO88+djLzNX7E4/qr9f
56dP+nZun7vyct9nt5/uf7tEtXLZYQa5PO6QHb5fXtZ3tdSubJY63pjd3ngZz+2ix9vqP9+t
JGsbLI4Iyc8ppNXx7+ZJV9RfZfvLE7f53hXNwuXVgYmXtx/uf7tEs4ZNcVrL6fvi/nEyd9pj
Kx9vvAvpvOh1SPcrKS8ruX688unRlRkjJF/NE9G5z6r6bL78dMW+WhyfiFXzfbattlm9gKt/
dPb89vP975dYn/7Mmg7OP3pcKj9sIOuv9oXbXN94PVW7v/UY0tOV3D8e7hGSr6unZ/Nl6b6q
an98IlbN9/VR5039X3NX/+js+e3n+z8scTyucXzOH39Uti5V31g/2ff1z79vvA3p9tbjVy9W
cvd4uENIvk5Pz91mVZw2Trc/OX1//0x+d3vbEje7M0+/PH7lzntvD3eq2h7q7UruHg93+K34
Oj6xivPzjJDmid+Kr+aJtXD5erOzFNLd43mH1DI+XOG34uu7nmNIxZN9pPLuOfjs9uJhH6ls
e45/NT9aPFnqeGN1f6e7UX/f+tUe0ld1syN2dU/c4bfi6xTSV7U97iOt66Nay+NRu3q3/eao
2tX9nt3+ff+HJa6f48cfbZ4s1dx4eKjy9k7f52B835q7dX08rjWk25VcPx7u8Fvx1Tyxlqdd
iPo/4ZfXgXJXb1duXue5vuPL2x9eR6qq2+d487Oy7XFO28XmUXa3yTTjqW4fat08UHtIi7uV
fD8e7vBb8XV8Yh2ecsXX5vikO1RV1v/t/8qPT9x19n1+wrVnt5/v/7DEzXysPP3oyVLrQzaL
3c2N5/HcLbrKDjPEJ/tIy/OJEvePhzv8VpLEs9ka/kGSREjW8A+SJEKyhn+QJBGSNfyDAAKE
BAgQEiBASIAAIQEChAQIEBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiBASIAAIQEChAQI
EBIgQEiAACEBAoQECBASIEBIgAAhAQKEBAgQEiDw/3Sx1D9KE1dEAAAAAElFTkSuQmCC"
>
</div>

</div>

<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="code-line">Parallel analysis suggests that the number of factors =  9  and the number of components =  7 </span>
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAZlBMVEUAAAAAAP8AiwBFpkVN
TU1dr11oaGhvtm98fHx+vH6LwYuMjIyWxpaampqgyqCnp6eqzaqysrKy0bK71Lu9vb3C18LH
x8fK2srQ0NDR3dHY39jZ2dnh4eHl5eXp6enw8PD/AAD///+NpOwdAAAACXBIWXMAABJ0AAAS
dAHeZh94AAAgAElEQVR4nO2dDXuaOhhAiaXWzTrn2nVubfXy///klW8SAkTzAsGe8zyT8JlU
OUsI4SVKAMCbaO4CANwDiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBI
AAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQg
ACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIg
EoAAiNRFVBBvj1fs0pjYVjXYXxbtrzjqjRscd3EUrfcfLjn15aB9F1057rxyWTSI1EVU43S+
J1eKlB35iqPetsG+/BsOLln15KB9F/Yc3+MvfDZ94T99gMbJE70779KY2FbVvGUHfnM/6k0b
vF79N3TloB3HnqPjfwz3yRf+0wcoT4vjpWnk2GS5RqTN5T/3KNpcUZBbWF9yOSfJ6ZLb9uaD
tL8LRGrxhf/0AarT4qNIvW2j9GrjVKw8rbNmjrG0ued5H0dxvqZ1kp0uFxzpaX6qMztezvZd
cS3TOmq6eU6czp4Pl42j7Vvz2Noy448412W6ZLqprnSKP6JZUn3G/l1U88fdJbk7FtsUjVVb
Qe4eROqiPvXz1MZo26yz+sRc2tjzFDfaVKZI+/Sq5VBdfkVZ/ZSSmdQ+6uWEzc/+Y7pPeei8
RsuPrS8ruBRy22zTlRuVVzr5H6GVVJvp+C7K+bKc26QhkrUgdw8iddH8XzjOrjY250yA+qx5
ay9t7FmeT7F2tILLynNaT8RVZgVpy8ly1PeybbZNT/FddnV1vpzHr9Wx9WUF2TVSvHs71dnm
HBt/hF5Sbcb+XZTz26rU24ZI1oLcPYjURXnyFNcFZTOs+i85PdXbS+uZ3IZzflqZIh3z/6+3
ZT2TnrjH7ByMOo5aLLs0Cdd5oyzJGmzr5maNZSXlub7O8rmUKf7IzvF144/QSqoX2/5dFPOX
PyJ6vWx7qMVMOgty7yBSF1GDD21x/nk0Nk4MkbZpnZPNNSqrksKgY1nPFMc765s1j1o0A/fZ
//NprbE76pvpyyqO6+Jv2NfZnteHU+OP0EqqF7vju6jqwLzO2WtdEB0FuXMQqYvGuVOcFae3
/SaqTu5zYluaJI3zv9FI0g2p2nRxddpGSXPaPmqxS77DIT/wrnl7VF/W4PS2y5prr2Ypojrz
uqR6sTu+C+NLOGl/fmdB7hpE6qI8czb7/Gx5K/9r17qtbEtNkbQdMhq3d/L/1A2RbEfNapNj
0RVf3meNT/UG2jKD07ZsEup/ofaXRnn3YLPYHd+F+Tfp830FuVsQqQv9pMtuoK53r0b/r3Vp
VDa1Oo+2rs/W/EJCF8l61OyyalNWj+e3vMds09i5uSwnrivO0hJLmbSS6sW2fxeWGkmrddsF
uX8QqQvj5FmbrZrupVF5jXTsONp71KQ5VCAqOxYseV1O8aPW4Dru2rVduSxnV91LzltfG/Ma
KVunlXRrXP3Zvov6L7RdI9kKcv98pb/1OozzoJg1aiTr0qpaid+zycY82r7uEnvNewH049nz
yltM2Zi5dXV1U9cE+rKctF9tlzpTdLe1eu2yrbSS6sW2fxc9vXZne0HuH0Tqwjh5NtkZn56Q
zZPbujSqe69aVU556LLFdbaJac/rlB0s2zFt5p2qnrzqtG4sK6jv9OR90lWZXhsH1kuqzdi/
i3K+um+c13vpnvuOgtw9iNSFcfKUzbE4O8PKldalUXVRk9EaL/3WHLy3zWonXSR7XtmVVdEn
XV7PN0Y26MtKqnM9zhps77GtTFpJtRn7d9H8f6T2KLsLtukqyL2DSF2YJ8/HLh0j8HEqb+N3
Ly1XZuPaLE/wbJqXIfmtWV2kjrzSPohyx+wSZPOqHbu5rD5+1ve9PRRVYDqQrl2mZkn1Gft3
USuYHr3q6N4WSlkLcucg0nJ4/VpXHcsCkRbDR/y1rjqWBSIthPyyw++JcRgPRFoImUdez4vD
mCDSQkifHPpaj8otC0QCEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQCEACRAARAJAAB
EAlAAEQCEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQCEACRAARAJAABEAlAAEQCEACR
AARAJAABEAlAAEQCEACRAARAJAABEAlAAEQCEACRAASYQKQIYGHccJbLizNDFnA10Sd0gkjg
CiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjg
CiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1gEjgCiL1sCCRVCsBk4JIPSxIpFIg
NX7+YAOReliSSLlCavzswYqoSKuMh+9/89k/3x9W31+sWz6sHlrL7FvmxzWyeHz+57737SxK
pFQiNX7uYMcqkmol3FgVPGQmPRcn/d/2hi+X5eaZ/7hqb1cdtyMLp71vZ1kiJUqNnzl0YK+R
lDZxJj/h/z2tni+TH6uHiyv/frTO+QvfV8+r79ade47bSP59Wj057307iASudDTtVPVxDcXZ
/C9tt/0tBfreUuay4cOlcWffuee4zeSjUaMhEk27Wem6RlI3eFSdzen0efUjn/n37ae53a9L
lfW8+pXPPD+snv4WbbbyENnny7dLC+65edxG8iXTs9qk2LuxjwSLEklVHzADnZ0N6nqPtBrp
afWnc7un1e/P30Xj7Cm74vnXFulHfjH0/GkV6d/qsblJsXdjHwmWJJLSJjA1uUjKjaETr7qA
ee5ta2WiXdp2ac/br9XTv/SSSauLPnOlfqWrV59WkeybNBZIsCCRVCsBkyLctCu61P71ivQr
qzLytt23S+WUm2WKVB7ys1sk+yZy10sLEglmRrizob6P1HM2P6byfP5JG2emD9rn35cfTwMi
tTapFkiASODKCN3fOd+qa6QX4+bp37LiWv3tFelpVXYhWET6m11itTapF0iASOCK8A3ZOv2j
7LX7ndU8n801JT/6RPq+evz58rdLpKx52NqksUACRAJXhIcI1enqPtLTyuj+flz9LbZ4zDvw
WtdIv6v+u06RsuZha5PGAgkQCVwZTaRL7ZCObPj7zRxV92f1rUilPeQ/016757zXLvXr8aLd
v6dcpN+ffzqukYqRDdomf/UFEiASuDKeSOX1ijnW7rkak/CS+lPeR7oolCr3M539lt/Rzflt
iNQYa9fYJN+7sUACRAJXRhTp89dFiKdf5kYPD3rycvJ/S634/ZjVXT8eVt/zA32/7P37Ja2/
WiI9Fddf9SbF3vUCCZYmkho/c+iA55F6QCRwBZF6QCRwBZF6QCRwBZF6QCRwBZF6QCRwBZF6
QCRwBZF6QCRwBZF6QCRwBZF6QCRwBZF6WJpImDQfswSI/Pf8uFo9/cx36T+gPt8RBXKsAJKI
BK6MINJggMh/D05PpH+21nZFgRwrgCQigSsjDFodDBD5PYu/lcdIcTlg16xtuWgASUQCV8YY
/T0UIHKVxQ+6bDZ4ct8qklAASUQCV0Z5jGIgQKQZ8efy71Jv/cgag3UYu+op2VYUyM+fj6uH
4qDPD5ddxgogiUjgymg1Uk+AyOfV97/NXVarLIzDy1MV7bFeY4kC+fktfyYp3eipeg5QL4JM
AElEAldykf77z+2fk0iDASLT0//x+XdDpKd/6aOx2eeDKVIrCuRLuuHlQuwlXfzw5/PPw1gB
JBEJXBmn126oO+7le7rVy2d5vv/+rKMutJp2n+VcMfstu8T6lz4Gm4WXTF8S86ltpyU8Akgi
Ergy2n2kobP194+HMhyDGYjLTBlRIKtoXlUm3SL5BZBEJHBltJgN3QEiS7JQqw4imVEgnUQS
CSCJSODKaCJ1B4g0LmOGRGpFgWxrYxFJJIAkIoErMwSI/FYsqMJCWkX6rWnWtOFbfYsoT/62
3kcSCCCJSODK9AEi0/P+56W19zszzCZSM0hk3hWhRYHMuuo+f6adDS8dvXZCASQRCVyZPkBk
Hcbx6dMuUh0kMv3XigJZHDir8LJbSt81kRpj7XwDSCISuDJ9gMjPfFh4scYmUh0kMptrRYFM
RzaU93R/tEY2ZIrKBJBcmEgKkeaD55F6WJZIiteazwgi9YBI4Aoi9eAr0us6SU7raP0u9Wu1
s6hR6T8lmhO4g0g9eIp0jC5zcXRB1KQekbhImg1E6sFTpE30lnxE6+Qt2oj9XkmXSKrxCdOD
SD14ipRWSB/RPk/IYT2YKiZKMidwB5F6EBBpGx0nFYk6aSYQqQfvpt3HMYqTKZp2qp4qy2oY
HUTqwb+zIYoOaYV0FPu9EkQKE0Tqwbv7O06vkJL1m9CPZckiR2mI5gZuIFIPy7ohm6LGzx6s
CInUfDb2szvGajla1fet4/bx260hd74BVxEJXBEVaTDGavVwq6dJjiL5Blz1ECntqItqRH+y
nnVKMiO4AjGR0s/BGKvFqf5sxkG9LbuepdpjSYM7doFI4IqoSA4xVvWpZ3bdS+sHZX0CrtK0
A1dkRSqexBuMsVpM64ipaXDIp+ycr2OhrtoBWJ/rVZ8TBFxFJHBlhBqpJ8aq3rSrI6b+zK9o
fmqxUNsBWH80n6udIODqokZ/ZyjRnMCdMtLqf1kg1aHpgEiDMVarzoZUtUbE1Id0wa88PFcj
OKoZgDUP0PArz2KCgKuLGv2doSQzgiuQ7rXrj7Fadn9nVVYjYurKdinTDsCabvSSbz9JwNUl
jf5OYaTdfIxwH6lXpMvH48NLY59VEeDk25+iQdiKq1p+NoLY5f+q3bvD25VV5W0BVxc0+jsD
keZDuLMhpSfGahmx7m8+U5mQxjApbvq046qWn7eI5BdwdUGjvzMUbbvZGEGk7hirn2WEx2/m
Phfpnh/TzTviqlpFMnO3iOQXcHU5o79zFCLNxggidcdYLTb7k3c2fDMujGo9ukTKr4S+l9dI
owdcXc7o7xyFSLMxgkjdMVbLzfIqqREx9THvRnvMbfnTdY2U98295LMTBFxdzOjvAoVIszGG
SN0xVqvbTVmVVEdM/VWNwGvEQm2LlN8uKheOH3B1aTdkFSLNxigidcZYLTd7zq+S6oip2ciG
7LyuY6FarpG+XS5u6oWjB1z1FGl9OIn9Th1Z6ChEmo0FPY803BSTxrvXLhrDJUQKEUTqwVOk
89tuDJcQKUQQqQeBa6T3w1rapZ5ScUd2NhCpB5nOho90uN2r/0/Vk0UJIs3GgkSaHhGRjpvs
0T65e7KIFCKI1IO/SOfDpTpaH88Xm7b9O552UXxIn7yI4v1VWTRBpNlApB58RXpPOxv2H/mK
/oOds8ctXg8OtRcihQgi9eB7H+lSGb2eyxVx7377dAzEPo525+Scpd2yMECk2UCkHnzvI23d
x9jF2Y5RlHnXLx0ihQgi9eB7H+ma/aL609IMjCKnkESINBudIqkpz9hA8QrH5XjuF8QNkc7U
SMujSyR1i0nNR/U+00gMreHf3Su6Q6C2htD5BlB1ZkKRymuk/blID2dhAZFmY0SRXlZmKIYC
24qeEKhdg7rd9vZhwtHfUr12mDQTHSKpz1sad/rYg++r51aEyM4VQ4EemknfAKrOTPkYhdB9
JESaiRFFurTfHqwnuG3FNSL5BlB1Rkqk94GbsQJZ5ChEmgu7SKrxeQXa+ZyGS3heWR5K0lY8
P6ye/hZtNu0RJC3iqpmBXwBVZ3xF2k8d+1sh0lxYRVLG1BVNpKfV78/ftmj5zRVPRTC8lkha
xNVWBn4BVJ3xFKn2aKKYDYg0H7lIyo2hE6/Z15BFL7404VodbM0Vv9Joqd/r8KzNx2GN6Kif
n82kdRPXAKrOeIoUR2/JJjqdNpNFWlWINBe2GklZUk40RcoCYdnads0VWYjUf3k84vwA9WeV
6hLJvonk9ZL3E7JJcrjURh9TheNCpPkQFqlOZ4GwPv+0QttpKwwftM9WdNTPlkg3BlB1RkCk
Y/okEtdI949FpKtacxqNc/hvdU/JuOWjregRqRUdtZmBXwBVZzxF2l6adqdonbwj0v0jOtau
cQ7/qHz5oW+jregWqRUdVcvAL4CqM54iZW+jyB7r24n9Xkn/rSqGNszFWCI9FjXRX7Ntp614
sl0j/a767zpF8gug6oxv9/chndtF/SN+/LIwQKS5GEmkP1XMOOO1Y/qKn2mv3XPea5f69bj6
mb7yaGWJuNrIwDeAqjNTjmyQyQKR5mIkkZ6roQcv+n0dY0V5H6kIgZq9uS97814r4mqRQWOs
3c0BVJ1BJHBlJJEe6uHdD9pIb3NF+mak1IoiBOqPh9X3/DBmxNUigxTfAKrOIBK4woN9PXhf
I60nHiKESLOBSD14inS44nmkG7MwQaS5QKQevIcICYaFtGdhgkhzgUg9CIxsGAFEChFE6sF7
ZMM14U9uysIEkeYCkXrwFOkUb0SHfVuyMFGMEZoJROrBu2k3Q2cDIs0DIvWASOAKIvWwxBuy
iDQPiNQDIoErwkOEmnHtugJE/nt+XK2eGi9V7j6cPt8RBXK8AJKLiyKESLMxokgdASL/PRQD
T/9dKVJXFMjxAkguLooQIs3GWM8jfXYGiPyexd/6+zQc7ccQqcu68QJILi6KECLNxogidQSI
XOWBhf4Nn9y3iiQWQHJxUYS4Izsb44nUFSDSjPhz+fdj9fAje6KiDmNXPSXbigL5+fNx9fAz
3//54bLLeAEkFxdFCJFmYzyRugJEXtp7f5s7rFZZGIeXpyraY73GEgXy81v+TFKeRfkcoFEA
oQCSi4sihEizkYv0339u/wZFqvsaOgNEpqf/4/PvhkhP/9JHY7PPB1OkVhTIl3TDf09p2+3X
6uHP55+H8QJILi6KECLNxii9dmm6M0DkxYXvacfay2d5vv/+rKMutJp2n+VcMfstU/Nf+hhs
Fl4y7Rz81LbTEl4BJJcXRQiR5mK0pl1ngMiM3z8eynAMZiAuM2VEgWx0sRfZdYvkG0ByeVGE
EGkuxhKpM0BkSaaYg0hmFEgnkYQCSC5vZAMizcXkASKNy5ghkVpRINvaWEQSCiCJSODK5AEi
v63yrusqLKRVpN+aZk0bvtW3iPLkb+t9JJEAkogErkwdIDI973/+u0yeUqFsIjWDROZdEVoU
yKyr7vNnevyXjl47sQCSiASuTB0gsg7j+PRpF6kOEpn+a0WBLC50stF02S2l75pIjbF2/gEk
FykSJs3C1AEiL/z5fqlEnn6VO5gi1UEis7lWFMh0ZEN5T/dHa2RDpqhUAElEAld4HqkHRAJX
EKkH35ENsvePbFm0UIg0D4jUg8BYuxFApBBBpB48RVpPH9cOkeYCkXrwFOm8nTyuHSLNBSL1
sLxwXIg0F4jUAyKBK4jUwwK7vxnaMBOI1AMigSuI1IO3SG/pY33bN6HfypqFCSLNAyL14CvS
prhCEo19gkhBIiRS43G7FHuM1XK0qu9bx+3jt1tD7vwDrnqK9BrFaUC7o/Cb+xApREYRqSPG
arWRp0mOIvkHXPW+IfuRTT+itcyP1c6iDSLNg5hIzbmOGKvFRs+WGF0emVmWao8lDe7YjdQQ
oQm7vxFpJkYRqTPGqm1j38wsS+sHZf0CrorVSLHMj9XOog0izcMYIg3FWC2mdcTUNDjkU3bO
17FQV+0ArM/1qs9JAq5yjQSujCFSV4xVvWlXR0z9mV/R/NRiobYDsP5oPlc7ScDVZfbaYdIc
lJFW/8sCqQ5Ne0Sq+xo6Y6xWnQ1pFIdGxNSHdMGvPDxXIziqGYA1D9DwK/dkkoCr/veRtjPc
R0KkORDutUvTnTFWy+7vLBpKI2Kq2cVXnPRmANZ0o5d8+4kCri5zZAMizcEITbvOGKvZRo8P
L8VM5d7lKujbnyLWUCuuavnZCGKX/6t27w5vV3bf3RpwFZHAFXmRumOslhHr/uYzdSX246G8
6dOOq1p+3iKSb8BVqe7veNJeO0SaA3mROmOsVhEevxl7XHh5fkxrsI64qlaRzLwtIvkGXBUS
6TTtfSREmgN5kTpjrBYb/ck7G76ZYx8qPbpEyq+EvpfXSBMEXPUQ6Rg1mXRkAyLNgbhI3TFW
y43yKqkRMfUx70Z7zG3503WNlPfNveSzkwRc9amR1k2Ppnv1JSLNhLhIPTFWi43+5VVSHTH1
VzUCrxELtS1SfruoXDhFwNUFRhFCpJkQF6knxmq50XNeadURU7ORDdl5XcdCtVwjfbtc3NQL
Jwi4Sq8duLKY55FcmmLS+Ip03qfddfFeNirXQKkYIzQLiNSDp0inOGvcRVF8kvq5zCwsINIs
IFIPniJtol1aF5330Vbq5zKzsIBIs4BIPSzxeSREmofFiDQHniLFRcjiMyLdP4jUg6dI+ygL
Wfy+kX2tOSKFCCL1sMTnkRBpHhCpB5nnkTaiz8ciUpggUg9LvCGLSPOASD0sVCRMmgFE6kFK
pPdp7yMh0gwgUg++Iu2nf60LIs0EIvXg3f1dchT7vRJEChNE6sH7huxbsolOp0004fNIiDQP
iNSDwBChw6U2+pC9kYRIIYJIPQiIdEyjrHKNdP8gUg+eIm0vTbtTtE7eEen+iaCHG77PRvqY
HiAbJrQT+70SRILF4dv9fUjndpHsmFVEgqXhIdL+IFoSWxZdMEYIAsNDpLRVN0sUIUSC4PAS
6YRIABkeIu18ey1uLxUiQWB4iHTeIhJAjlTwE1kQCRYGIgEIsMgH+xAJQmOpImESBAUiAQiA
SAACIBKAAIgEIAAiAQjgNdZuxpENiARBgUgAAvg27bZxGofrPRZ9QBaRYGl4irSPPrLpx6Sv
dUEkCA2psXbTNu0YIwSB4SlSXNVIsUx52llYQSQIC++mXZyGWD3GkWgAB0SCheHb2VC+sU/0
ZRSIBEvD+4Zs9sa+rWgIfUSCxbHMkQ2IBIGBSAACIBKAAL4iHdZzDBHijiwEhqdIh3nG2iES
BIb3DdlXsaJ0ZGFFIRIExZThuM77dPhD2hjcvHmWSiESBIWnSNvo7LzfKb5od47zhmD/mzIR
CRaGp0ineOP8FuZdtD1fPnany267/tHiiAQLw7tp597ZEKW1V5RXYef+Qa6IBAtjUpGStHei
MWOsvuJgCpEgKCa8IbtLH7k45M9dnPsvkhAJFsaEIn1E8f4j2cYXk47rqHeYKyLBwpAS6d3h
OYpjXLfd+h9fQiRYGL4i7a8a2fC2y0YUbQ8n31Ix2A6CwvsJ2RLRJ5IQCRaG9xCht2QTnU6b
yPl20rVZ2EEkCAqBIUKHS230MTBUwSMLO4gEQSEg0jEduDrx6G9EgrDwHmv3lpyidfKOSPCl
8RTpmAqURRISjVmMSLAwvJ+QTed2kWzEYieRMAkCYqExGxAJwgKRAARAJAABEAlAAEQCEACR
AARAJAABEAlAAF+RXtdJclpHa9HB34gES0NiiFD24Ovkj1EgEgSEp0ib6C35iNbJ2/SPUSAS
BITAYxQf6UC7qUd/M2oVgkJApG36mDkiwZfGu2n3cUyDpk7etEMkCAr/zoYstNbkwU8QCYLC
u/s7zh5FWg+8p8UnCzuIBCGx1BuyiARBgUgAAgj02mXEva9p8cmiA8WNJAgIIZFOM3R/IxKE
g4dIR+2NRuuJS6UQCQLCp0ZaNz2aeKwdIkFQSF0jyYJIsDAW3GuHSBAOiAQggK9Ih+pCSapE
rSzsKESCgPAU6XDFW81vzKIDhUgQEJ4ixekrXeRBJFgY9NoBCOAp0jY6ixWlI4suGGwHAeEp
0ineyMYPamfRBSJBQHg37ebqbEAkCAlEAhBgsTdkEQlCApEABPAW6bjNQnKdhMpjy8IOIkFA
+Iq0yS+PoljUJDeRMAmCwVOk12hzTkV6jXZiRUoQCRaH9xChcz66YY5eO0SCYBAYIoRIAJ4i
rYsa6WPymA2IBCEhc410FB4FjkiwMHx77bbFuAbRGPqIBEtD5D5StJUN/Y1IsDSWPLIBkSAY
EAlAAEQCEGCxUYQQCUJisVGEGLUKIbHYKEKIBCGx2ChCiAQhsdwoQogEAbHcKEKIBAGx3OAn
iAQBsWiRMAlCYbk3ZBEJAgKRAATwFultM9Pob0SCgBCJIjTL80iIBAHh/YRsfLxM5nhCFpEg
ILxjNnxk0xliNiASBITUECG6v+FLI1YjxTLlaWfRiUIkCAaukQAEoNcOQAD/+0hzRRFCJAgI
RjYACLBgkRj+DeEg07TbHYWKY82iC0SCYJDqbNhKFaidRSeIBMHgKdJ+xu5vRIJw8I4iNN8Q
IUSCcFjwECG67SAcvJt2ZY0kepGESLAwfDsbDtk10ns8w8gGRIJwkAt+IhkABZFgYSASgADL
HtmASBAIiAQgACIBCOAr0j6eLdIqIkE4eN9Hmi9kMSJBOHj32s33ojEsgnCQGiIkCyLBwvBu
2s33ojFEgnDwfh5pc5IqSlcWnagRcga4CV+RjjN2NmASBIOnSIc5e+0QCYLB+8G+GXvtEAmC
YcG9dtUDsmqUMgBcgXfTbs5eO5VPxigBwFV4P9i3eZcqSlcW3ajqA2Be5J5HEitS4i6SwiMI
g2WLVLbuAGZmyY9RIBIEw7JFUjTtIAy8RXrbzPVaFzobIBy8x9rN96Ixur8hHDxFmvfVl6pM
AMyMp0jrOWN/YxAEg9QQIQatwpdGrEaKZcrTzqIHJZkngAcLvkZCJAiHJffaIRIEg/99pO31
95EGL6hcS6WuyhZgNGYZ2SAqkvIqCoAIE4p0xZsrEAkWhq9I533aXRfvHZ7ve48RCe4VT5FO
cWZEFMUOUbnO2ygP3mW16JYXLanqA2BWPEXaRLu0Ljrv3d4h+xZFabeEbGeDctwYYDwmHtlw
2kTbMyLB3eEdjiu/ODo7t8YOUXxEJLg3PEXaR1nwk/dNtHfd/WM9fA2ESLAw5hjZsEMkuDdk
RjZshOOtXjeyQclmDnA9i47ZgEgQCogEIAAiAQiASAACIBKAAPcgEibB7CASgACIBCCAh4hO
CcgAABkhSURBVEhXPKg3WqlU4xNgPhAJQIBlN+0QCQLhLkTCJJgbKZHenZ6Q9crChtImAHPh
K9I+hGskRIK58X6wr+QoVqQEkWBxeD9q/pZsotNpE72LFSlBJFgcAsFPDpfa6EM2+Pe1ImES
zIyASMf0TRTzXiMhEsyMp0jbS9PuFK2T95lEKlGSuQNcjadIWWStLADKTqxICSLB4vDt/j6k
c7vIPRrX9Vm4oESzB7iWhY9sKFEjFALAnaWLpIwpwCzIxLXbid6OvarXTpUJgBnxFamMtCo6
1A6RYGl4DxGa863mmT6qSgHMhvcQoY9s+hGtZcrTzqIPlSASBIHAyAY9IYLjwVT9qSSzB7gW
76ZdWSPN8DySak5V11YAE+B9Qza7RnqPRcesIhIsDe+m3SgBUJwOpLSU6tgKYAoQCUCA5Y5s
UE0QCeZluSIZKPFCALjjIVLakps7QGQDJVkAgOtAJAAB7qZph0gwJ4gEIICvSOd9fPmM92eh
8liycESJlgDgKjxFOsXZxVEUxSepEplZOKMkSwBwFZ4ibaJdWhed93OMtTNQkiUAuIpFj/7O
UK0EwOR4P4+UXxydEQm+NN6PUWzSoN/vG9l4XIgEC0MqZsMcj1EUKGMKMD0yUYQ2ohEbbhQJ
k2A+7uCGrGolAKYGkQAEQCQAAbxjNqxnH/2tWgmAqfEU6RDCYxTKmAJMjvcNWeH+unYWw6hW
AmBipIYIyYJIsDA8RdpGss9PWLIYRrUSABPj/RhFNkRIGkSChSEX106sSMnVpVLGFGBqEAlA
gDu4IUvbDuYHkQAEuIu4dqqVAJiW+xIJYCbuommHSTA3vjdkRZ8wt2bhghqjEADu3MMQIUSC
2fEUaR3CECFEgtnxFOm8DWCIECLB7NzFyAbzfcyqYyuAsbgnkUqBlFxBANy4j+5vTSElWBAA
N+5LpHSqurcCGAtvkbIAkbujUHGsWTigqoTq3ghgNHxFKkMWi77VBZFgaXiKtI/itDI6CgdB
ub5UqpooyYIAuOEdRegjm35Ea5nytLNwQ9WfSrAgAG5IDRGauftbU0jJFQTADe+mXVkjzfzq
S5VwQxZmxLez4ZBdI73HM74fKUOJ5g9wJXIjGyRHN9xwICWVN8ANIBKAAHcysgGRYF4QCUCA
uxRJdWwDMBaIBCDA3YiESTAniAQggIdI+4NoSWxZXIPqnAEYHQ+R8kiroqUxs7gG1TkDMDpe
Ip2CFQmTYFo8RNqNM6rhxlJRJcGceIh03gYsEibBpEg9jyQLIsHCuFuRMAmm5H7uIyESzIh/
OK40jtD2Tag41iwcUQPzAOMhFY5r7idkU1TvLMCIeIr0Gko4rhQ1MA8wGp4irUMJx5WiJIsA
cA33Eo4rRUkWAeAaxGqkWKY87SzcUZJFALiGe7pGwiSYjXvqtUMkmA3/+0jbUO4jdfd/G8sB
xLmjkQ2d/d/mYgBx7lqkfEFrKYA49y1SuqS9EECcuxLJZlJ7EYA8iAQgwJ2LpGjawSTcl0gm
qvoAGJUpRTrvomhzLA7SexQhkZQ2ARgPX5Fe10lyWkfr9+H9znE2BiJ/R+YUIqlWAmAkPEU6
pkJkggybtE/H451f87dkTlIjAUyFp0ib6C17FunNYbBdnO94iten8URSN+8J4IHA80gf0d7p
eaRyk/NmY9teJEiewiSYBQGRttHRSaR1dC5Tm7FqJESCefBu2n0c02f6XJp2r9GuSJ2izTgi
qYTGHcyCf2dDFB3SCuk4vOO+suc40HpDJFgY3t3fcXqFlKydHkj62Jap024MkVTj07ICYDTu
amSDanza1gCMxT2JpIypZRXAOHiLdNxmPXcnofLYsnBFGVPbOoBREAl+clkWi5p0k0jKkupb
BCCHp0iv0eacilR3bYsgLxImwah4ihRH5/ze6vyRVlUTy2rPIgH0ITCyIRCRhlAjHBOgwFOk
dVEjBRFEvx81wjEBCmSukQIJWdyPGuOgABm+vXbbkEIW96PGOChAhsh9pFBCFg+gRjkqQHJf
IxsAZuMuRVIihQBwB5EABPC+j+T9dPhQFregJAoB4A4iAQgg07R732zbCz3wtlL1L7WvBrgZ
oWukcwCDVpuo3sUdawFuRqqzIaymXb9JHesAbkdIpNc0lJAco4mULu9aBXA7Yp0NB7EiJSI9
7KpredcKAA+ERFqLjllFJFgad3lDNkN1LbWvAPDhi4mkOtcA+OAhUqQzc6naqM5FljUAXnwp
kVT3KgAv7rdphy4wIfcs0iCqlQC4DSmR3kUH2031YJ/SJgA34yvSPtxrJBdU9QHgg6dItUcO
70e6LQtP1ODqgS0AHPAUKY7ekk10Om0c3mp+YxZ+DL4Lk5EOIIH3EKEkOVxqow/ZeFydpfov
5ZojIRJMgoBIxzQ45JTXSP+5C6WSgaabomkHEniKtL007U7ROnmfo7OhFKrHKJX0i6KSgQ0A
nPAU6ZgKlL0jad4nZDuEUo1PGyoZ2ADADd/u70M6t4uyNzLLcXP1pguljKmJMhLmPIArdzqy
IRdKFXOqe0MddeX2AAWeIp3FCtKZxe2oqoZSrl19qvoAuAbfXruN6I1YWxa3o5qpvMk31DlB
Lx7chqdI6+hyeSR6L7aVxc2oJsa6bqG4rwS34HuNdDpcXFofhJt4k421K4VqdFCoqfKGe0Kg
s+G0jyPhJt5kItXoTb/rhk8AyPTavS519LeJqj4QC65BokbKWnei7+ybSySlTTTaYuEW1Ihc
I8X7k1R5LFlMh2olumkIhVQg0Wu3C7TXbhryzr+qppq7ODAT3veRhF/D3M5CBOW5m+v+ddPv
2uc9YNnc88iGJsprtxv3Rqyvw52OtWuhfPa7dec2iHWveI/+Xi8k+Im6fb9bd3UAse4FT5EO
AUda1VE373jznjeAWEvFU6Q4kn2fiyULKdSt+zV2VK3EyCDWUvDttRvngikgkZS2p/I6lj+I
FSqeIm2jUfrtRtFT3byT6p6fGcQKBU+RTvFG/nZsYr7oAiB4bjjL7We8mEXJSDXSDahWwux8
UK31YWHUWNRaY/HlRFLeB9CPoEQOOhmINRJf5YZsyWDk1eEDGEfQFqhWInRMsRpjBxNEcweR
rt0/aZmkzPUL8siCNqqdGswRb5GO27RVt5V9jmI0kVTid5IrbVLMqNYWPlkEDmLZ8RVpk18e
RbGoSaGKpFqJdlvPnL9zECvHU6TXaHNORXqdOWSxI6rxKXlI7Yi2pt7XkeuriuUpUhyd89EN
i+i1U8ZU7pDNI/b2j6vETNw59s6M+8O7+zv50iKpVsLW1LP068kVYXncpVieIq2LGukjWosV
KRlLJGVJjZOJMpaozg3aia/HXYglc410FB4FvlyRlCUDva1niGXb4Ytj6XYPH99eu20xrkH0
zZcjDVptMkYGSUcFY+bW219uPcKXJ3yxRO4jRVvhECijdX/Pgeq9g2tv+uFRP+0bxXM3Db1F
GoV7EklVH/UCpW9hzJo7wFXoQ52myhWRRkZpk8TmSavpR1NPgmnvZ3l3f1dsBN9+eT8iKTOh
jOW2oRCaWeYO7QQMYhszKCuYnEhRFEsValKR1IR5dfqgF6K3N4KLKAG0Ue4iQvk27XZx+j6X
Yxy9J1u5NzJPKJKa+YxU2qRMq6R/nhpKFv9R7p4i7aOPbPoRbZKz3E3ZryOSaiWuvYiihhqD
1vNZQzt4N+0aCblhQtOJpJLQTkGlTfK0MrfRFihzexDFqVvdU6S4qpHiRYqkqo9QUK3EUG8E
3XwT0WuSd9OuvEbaJ29ywxumFSnwU05VH40lSt9E6Wu1PdoJkMe3s2FTDxGK5MbbTSWS0iZh
orRJlVTaNvqcbftWB3zgf/bS8BWpGCKUVkvRQaZIrSzGQxnTAFFdCaVt05gb6uZbxH8fi8Nb
pFGYSCTVSiThn1/KnlDaNtpcyyxjh/YR4Wq+skjKlpz7vtL1KG2Sp5W5jb5AWf/vUPosYl2D
h0h5j/fSAkQ2ULbk4kRSrcRwN9/AOFmlH9CSA5h8YZG055NUuTBZ/vmiqo/GEqVvos+6tf2U
Mb/8b0qQr9y0s6GSxZ8eSptUSaVv0pxNnNp+5gGpspogkoZqfC4U1ZVQ+ib1bDanzw6IRZXV
wlukZUVaHUAZ07tAmQllLE9c2n7mPFWWjq9IC4u0OoAypneJaiUc2n5X9gMOVVn3h6dIC4u0
OoCypL4CSpskLmL5VVntxOLxFGlZkVaHUJbUHf3WXSgzoYzlt/QD9ldZZg5mor0idDxFypp1
E4o06rP3tu7wyy+puve4T5SZUMZygSrLPICZRcd8uGZ5ijR9pNWp4y19QZFMVFdC6ZvUs8lw
lWWuN49gnQ+4+0LmGmnySKuTyaSScH6rYFBmQhnLb+m+MDewzPd1X7QT0+Ip0nyRVieqmVSC
SIOoroTSN6lnk+GLqoX1uPuKNHOk1bFD2KrGJzijzIQylt/SDxh2j7u3SKNwdRYjCaWMKdyG
sieUsU3vvHeVNS53IlKJcGRNZUzbM3ATSptUyZ55h6usWUNZ3JlINRJvL1CWVEI3nj+qK9E5
79J90V9ldeQgZdbdiqRzm1DKkkKkMVBGwpx3677or7LMHYwjtrO8ii8iUslVTb+OG7Stn0uw
fNCBMhPKWO5QZVnm+3on2ok+vphINbc2/VTS+jmUbTsYFdWVUPom9Wxyde9E64h9fFmRdJyF
Uo3PcomybAcTo8yEMpYnvh3q7UQTRNIYavopY2pr6kEIqFbi2t6J1h7KPGATROrA/nYClehT
RFoOqvpoLFH6JvrsUHdFE0RyJBNKtd5VqpL6E0JGaZMqqfRNmrPJYNuvASJdg9JSdY2lgn3Z
NhSoroTSN6lnE5cqqwKRrqDdH67KNemH1hTErMBRZkIZy136/WoQyQtlTPN0PWe+sxSCRXUl
lLZNY04DkXxQllS7Pzybrd/8hlTLQNkTytwuB5F8UB0ppW/UnJV+CTBMhtImBojkgdsYor47
tlxTLQfVSjRBJGFU47NKKssmVhBrqUwq0vshfzJ9u38fK4vZUY3PKqn0LbRZc7U2335tvetb
tmFaJhTpvG68u6I/xsNyRVLGtC2WrTNC9c43qDsr9N5ArrfmZkKR9lH8lr8D/ZS9vHmELOZH
DUztTb3++V4anRcJNdd8TChSHH1U6Y8oHiOL2VFmSllX1HOWBa0NbuU/Q7AEscZjQpGiqGum
WDLSW8umRBkpc97e1BsUyZi9kv9sTUK632WhRpKk1R+u6lXaZNC4ej7pusUrA2LJMO010jF/
+csdXyPpDAzO61lQzeYz2ux1F1FXgli3MWX396bRdlufR8kidFQrNTRNXDrQjVlJEMuNKUVK
3vfZfaR4e7jf+0j9KDPVuaBeMXwRNXxnSg7EsjOpSCFlMQvdTb3u3ol2W08lwzWUsWA8um4Y
f7VeQkSaleHeCYeLKJWYVdCEIrXRbxJb7nPNV7QRQaSgaFdZjiL1t/1mRRfLMjJj7gKKgEiB
o8yUMlcoYzt7FdV52JmxDHWau0g3gUiBo8yUSvqndrOac/YFM2MZ4tSYhg8ihY37Ld4es4ZF
mrLf7xbCFwuRFoYyUqq1Slm3bGx3U7+fMTsz4YmFSMuip4ZqmaWaicZ2DnemhqssY3Zm0+YX
C5GWjeoxS2lTlRgrjAWNWYcqa85bVw5MLxYi3RntDnRVrkmMhLGgmnXpUF9YW3B8sRDp7lHG
dLjfTxnzDm1BywJt1lxtmZ8SebEQ6d5RZqpzgTKWD1RZjdlbqqyg2oL+z2kh0r2jzJS5oHsD
1TG95arrhu6LOTHGDg6DSHdOuzNiqHdCJcaC4bbgDVWWZQNt1lw9I05CIdJX54YOdaXPulRZ
N4gUXJXVaxIigc5wh7o5f0uVZc7f0hYMqgpDJBhg+E5V5wJlLJ+4+8KYHRVEgivpvlPlUWWZ
Ozq0BQdFmvSqC5HAF7kqq95ulO6LMassRAJpbqiylD4rUmWZ8+NWWYgEo+NeZSlj+RVVln2D
xvwtVZY7iASTM0n3hTl/S5V1BYgE86PMVOcCZSx378644arrGhAJZqddQ/lXWSqxJ8wd63lE
grtHmSlzQff8QJ3W2rCx4BoQCcLn+ipLJcaCW9p+11iFSHAHmGKpQdMcui+s3XpdIBJ8CZSR
MuedROrp1UMk+Arc0H3hcuOpBpHgK9ISy0Ukc74JIgEkNrM6ui1UYgWRAOyoxqdtXgORAKwo
Y9q+mGqCSABW1MBUB5EAbCgzpdprGiASgA1lpJRtVQ0iAVjoufG0JJEAwkIfdGTZ4IazXNyb
MPK6Fcoow5crIyLpUEYZvlwZEUmHMsrw5cqISDqUUYYvV0ZE0qGMMny5MiKSDmWU4cuVEZF0
KKMMX66MiKRDGWX4cmVEJB3KKMOXKyMi6VBGGb5cGRFJhzLK8OXKiEg6lFGGL1fGJfzBAMGD
SAACIBKAAIgEIAAiAQiASAACIBKAAIgEIAAiAQiASAACIBKAAIgEIAAiAQiASAACIBKAAIgE
IMBkIu3jKN6fp8rtBl7LryLYkr6uq4KFWsbzLop2H3k61DKmvBc/tlwZpxJpkwX5X0+U2w18
lK8gCLak+6xgcfqzB1vGOCtYZlKwZbxwjvMfW7CME4n0HsUfyUccvU+T3fVcypZ/FcGW9CPa
ndN6cxdwGfdp6fbRNgm4jCnb/MeWLONEIu2j4+XzLTpMk93VvEabQqRgS7rNy5cWM9gyxlFa
X2bfZLBlTNJS5T+2ZBknEmkbnZL0P9XtNNldTbRPCpGCL2kUfBmjOAm6jKfyf03JMk4kUhQ1
J+HxYRYx1JKeo03oZdxHr0nQZdxEp7xYkmVEpJJliPSatkZCLuOl2bRPp+GW8RC9JYg0HosQ
6RSnzZCQy/i6jbNrjmDLmLXkEGk8liDSOd6kk6DLmCS7tG0XbBnX6Q2ExYoUh/q11hRlC7qk
m/yWR9BlTK/j4nDLuMt66vJiSZZx0l67U4h9OCVar12QJT2tN6csEXAZM+qexfDKGFXIlnEi
kQ7Z/wPH/Do0TAqRwi3pMdoUqWDLmN9HOqVjBUItY1MkyTIysqEk9JENp8qjcMuYjWw4b9Nr
pGDLmLHYkQ3JOvtPYDO84WyULeVQS7qr/ycNtozFWLusYMGWMaX4sQXLOJVI52yc7USZ3UQp
UqglbTRJgi1jNpx6/Zqlwi1jUv3YgmUMrVMFYJEgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIB
CIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAA
iAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgEoAAiAQgACIBCIBIAAIgkgjN1+lZOXatOKdv
Lk6Sj10c7To38qNx2Cg6lom+PfrXggW+MRGGRFp3G5a9U3uf770+jVC2Zt5RFJeJvl0Q6Wr4
xkQYOvO61+/SOuIQxZfP82UygkmRJlJ0GCjQ8FqwwDcmwu0ixZc1p1KgXbQTLVYr70ulF536
CzS8FizwjYnQPPOO26h6VfY+jjanouWXzr+ui5d+R9F5HW1ThzZpwy6vJpLz9tXYKq2sDlnT
b5/P76tjN7Y6bbOt8oWxvlBrcUbRR5prXuB8cZHqzSfWygwWEEmEhkiH/HInOws3aSo+Vydz
Np+qc1myzbZ5Sx3aRB/NgzW3yo523BRHLOY35lZxVLTZtu2FhkiXWu89aYtk5rOt8tm2ygwW
EEmERl9DFL2lfqTJt2hzvpy4+/KcfYvij+QjTjeI0lVJepq+my0pc6vX4jPO7ChXtbdapz0X
l9R5k1521Qv1pl3RTWiK1JmPeUiwg0gitDrtsmRmyTk/M5NsPu18Pqb/vUdZvVCs0EVqbRVl
lzX5CZ+v2lq3ShemZ/o5XR8ZFU9drNfotS1STz7GIcEKIomgqXA6HjaGIPU5m2hnd37FoovU
2kq7lulM5qlK6NZO9bHX0bl9jdSdj3FIsMJ3I0LzHNs0Wnn6+rYHh7R2uPyvX14jHc8TiPQe
7RBJGr4bERrn2C5avx5PjiJtstbUoey1e29c09wmklEiq0ipuFeIZPsjwYTvRgTzdMtF2nRc
I23rUzUbaFDdR9qkFVRrK/Na5pjebbJutY2OWjE6RDpF62rFu12k90S7FjP/SDDhuxFBE+k9
+civkV7Tbq593muXmqL1tKXbvhc3YHfZyIb0xo9tq+YJnq86dmyVLbxku9V3OpnlPGRNtfVF
2/PGLpKeT/OQYIXvRoTGOVYMm8v+Ty/vI13O2cyR5r2ffNu3fKdNc6yduVXzBM/WbTu3yhfG
p+bCdTXAri5nnCZes2PZRdoZ+dSHBCt8NyI0z7HLObh5P+Zn4cWqbSrH+zo/mV/jesxCkvef
5bxdTuhNYZW5ldZK2xarOrZ6vWizO2kLy7yb5TxmiUPc6nao8tmXYyXMQ4IVvpslwakcLPwy
SwKRgoVfZkkgUrDwyywJRAoWfhkAARAJQABEAhAAkQAEQCQAARAJQABEAhAAkQAEQCQAARAJ
QABEAhAAkQAEQCQAARAJQABEAhAAkQAEQCQAARAJQABEAhAAkQAEQCQAARAJQABEAhAAkQAE
QCQAARAJQABEAhDgf4fZtVhPvk/aAAAAAElFTkSuQmCC"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Three different methods were explored to determine the amount of factors, $m$, to use. Unfortunately, it seems right from the beginning that four factors will not be enough to adequately describe the variables. The three methods provide very different numbers Ã¢â¬â 19, 8 and 9. I will use 9, as it appears to be a middle ground between the two, and the scree plot does flatten out significantly. Also, we do not wish to use nearly half of the number of variables as factors! Even 9 factors is quite high.</p>
<p>This is where factor analysis and principle component analysis begin to differ. In PCA, we simply use the eigenvectors of the correlation matrix as variables, or 'principle components'. This does not account for the noise in the principle components themselves -- it is not trying to find any signal here.</p>
<p>Factor analysis considers that these components are representations of some latent variables, that which are the true 'signal' in the correlation matrix, while the remainder is noise. Factor analysis is controversial because it attempts to estimate this latency by approximating correlation matrices, as we saw above.</p>
<p>Now that we have determined m, the number of factors, we will aim to create the loadings matrix to estimate the covariance matrix, as explained above. There are four methods by which we can estimate the loadings matrices. The loadings matrix in each case will be a $p x m$ matrix, and will use the eigenvalues of the correlation matrix. The first method we will use is the principal components method.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Principal-Components-Method">Principal Components Method<a class="anchor-link" href="#Principal-Components-Method">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[113]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>eigenvectors <span class="o">&lt;-</span> <span class="kp">eigen</span><span class="p">(</span>R<span class="p">)</span><span class="o">$</span>vectors<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">]</span></span>
<span class="code-line">loadings <span class="o">&lt;-</span> eigenvectors <span class="o">%*%</span> <span class="p">(</span><span class="kp">diag</span><span class="p">(</span><span class="m">1</span><span class="p">,</span>nrow <span class="o">=</span> <span class="m">9</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="kp">eigen</span><span class="p">(</span>R<span class="p">)</span><span class="o">$</span>values<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">])</span><span class="o">^</span><span class="m">.5</span><span class="p">)</span></span>
<span class="code-line">loadings</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<table>
<tbody>
	<tr><td>-0.6277856   </td><td> 0.119554084 </td><td> 0.33171585  </td><td>-0.0517100760</td><td> 0.19589620  </td><td>-0.256426344 </td><td>-0.0051466920</td><td>-0.1050080325</td><td>-0.217875830 </td></tr>
	<tr><td>-0.6628787   </td><td>-0.132540591 </td><td> 0.11886326  </td><td> 0.2214725941</td><td> 0.22140567  </td><td> 0.132387083 </td><td>-0.2481776856</td><td> 0.0048103856</td><td> 0.073391368 </td></tr>
	<tr><td>-0.6390134   </td><td>-0.197196788 </td><td> 0.05624741  </td><td> 0.2311725131</td><td> 0.23664371  </td><td> 0.106747616 </td><td>-0.3142731034</td><td> 0.0725529294</td><td>-0.009258277 </td></tr>
	<tr><td>-0.6038332   </td><td>-0.001172604 </td><td> 0.25194869  </td><td> 0.0287228625</td><td> 0.19716616  </td><td>-0.096650456 </td><td> 0.2237402385</td><td> 0.3933113732</td><td> 0.175740568 </td></tr>
	<tr><td>-0.5623461   </td><td>-0.075542614 </td><td>-0.04215597  </td><td> 0.0889017298</td><td> 0.31622038  </td><td> 0.101831072 </td><td>-0.3414194020</td><td> 0.0325078492</td><td> 0.085362217 </td></tr>
	<tr><td>-0.5588305   </td><td>-0.202629294 </td><td>-0.02405988  </td><td> 0.1321338363</td><td> 0.30155513  </td><td> 0.073081367 </td><td>-0.2777839481</td><td>-0.0712005483</td><td> 0.093765748 </td></tr>
	<tr><td> 0.5610817   </td><td>-0.101431288 </td><td>-0.01542818  </td><td>-0.4286785091</td><td>-0.17991988  </td><td>-0.065822588 </td><td> 0.1121413950</td><td> 0.1320328403</td><td> 0.243711028 </td></tr>
	<tr><td> 0.4195786   </td><td>-0.089834730 </td><td> 0.04744726  </td><td>-0.4219788203</td><td>-0.09239017  </td><td> 0.008179850 </td><td>-0.1017128152</td><td> 0.1404356750</td><td> 0.359181871 </td></tr>
	<tr><td> 0.3666707   </td><td>-0.121246001 </td><td> 0.12235117  </td><td>-0.2673284202</td><td>-0.19228688  </td><td>-0.331672078 </td><td>-0.1723038527</td><td> 0.3330872957</td><td>-0.293884580 </td></tr>
	<tr><td> 0.2834756   </td><td>-0.191213944 </td><td> 0.13406289  </td><td>-0.2440643282</td><td>-0.18539624  </td><td>-0.106304884 </td><td>-0.4565341789</td><td>-0.0821946123</td><td>-0.171057932 </td></tr>
	<tr><td>-0.5196860   </td><td> 0.270565545 </td><td> 0.30577781  </td><td>-0.3423029175</td><td>-0.11245766  </td><td>-0.001271796 </td><td>-0.1320537729</td><td>-0.2283180107</td><td> 0.265527940 </td></tr>
	<tr><td>-0.6564641   </td><td> 0.068827634 </td><td> 0.31328652  </td><td>-0.2478632919</td><td>-0.34403345  </td><td> 0.087823904 </td><td>-0.1132566054</td><td> 0.0377935294</td><td>-0.011283238 </td></tr>
	<tr><td>-0.5998907   </td><td> 0.074604175 </td><td> 0.26507843  </td><td>-0.1892944458</td><td>-0.07212877  </td><td> 0.033343619 </td><td> 0.0308950899</td><td> 0.1426224411</td><td>-0.200220801 </td></tr>
	<tr><td>-0.6074783   </td><td> 0.096622639 </td><td> 0.34093184  </td><td>-0.1742184243</td><td> 0.20372042  </td><td>-0.252897043 </td><td> 0.0281370136</td><td>-0.0346131237</td><td>-0.254485357 </td></tr>
	<tr><td>-0.6248483   </td><td> 0.011604348 </td><td> 0.28844967  </td><td>-0.1296150311</td><td> 0.17592820  </td><td>-0.039241251 </td><td> 0.1844820354</td><td> 0.4216051360</td><td> 0.098786819 </td></tr>
	<tr><td> 0.5974964   </td><td>-0.234896621 </td><td>-0.29156288  </td><td> 0.0140733681</td><td> 0.45380545  </td><td>-0.094810107 </td><td> 0.0093078526</td><td> 0.0096644094</td><td> 0.016502011 </td></tr>
	<tr><td> 0.5405337   </td><td>-0.286677830 </td><td>-0.22708170  </td><td>-0.1085333297</td><td>-0.06070240  </td><td>-0.083413069 </td><td>-0.3147783298</td><td>-0.2285523728</td><td>-0.186028731 </td></tr>
	<tr><td> 0.5065189   </td><td>-0.420628297 </td><td>-0.27874397  </td><td> 0.0715082012</td><td> 0.18189460  </td><td> 0.002006571 </td><td> 0.0463524348</td><td> 0.3018104199</td><td>-0.260546935 </td></tr>
	<tr><td> 0.5780675   </td><td>-0.232627930 </td><td>-0.26378683  </td><td>-0.0556981294</td><td> 0.44299196  </td><td>-0.109840640 </td><td> 0.0355312724</td><td> 0.0666607568</td><td> 0.098413082 </td></tr>
	<tr><td> 0.5219373   </td><td>-0.216594655 </td><td>-0.13173681  </td><td>-0.1032620489</td><td>-0.04359939  </td><td> 0.070382456 </td><td>-0.2508050668</td><td> 0.2867068313</td><td> 0.083829363 </td></tr>
	<tr><td>-0.3992462   </td><td> 0.160965015 </td><td>-0.36037555  </td><td>-0.3690507373</td><td> 0.03464572  </td><td>-0.123678525 </td><td>-0.1924745951</td><td> 0.1507108412</td><td>-0.019742963 </td></tr>
	<tr><td>-0.3394289   </td><td> 0.150689800 </td><td>-0.26903653  </td><td>-0.5548752336</td><td> 0.16905090  </td><td> 0.011646505 </td><td>-0.0474249609</td><td>-0.1572321905</td><td> 0.166900898 </td></tr>
	<tr><td>-0.3320934   </td><td>-0.027645815 </td><td>-0.28907100  </td><td>-0.5582492767</td><td> 0.25327472  </td><td>-0.005346193 </td><td> 0.1102181121</td><td>-0.1334792270</td><td>-0.157127058 </td></tr>
	<tr><td>-0.3658667   </td><td> 0.108273914 </td><td>-0.38501438  </td><td>-0.4940891181</td><td> 0.19559323  </td><td> 0.064851965 </td><td>-0.0003570599</td><td> 0.0709592952</td><td>-0.068600025 </td></tr>
	<tr><td> 0.3796437   </td><td>-0.447822215 </td><td> 0.39727206  </td><td>-0.1408538430</td><td> 0.10110562  </td><td> 0.076120098 </td><td> 0.0523492570</td><td>-0.1131530027</td><td> 0.095050608 </td></tr>
	<tr><td> 0.3772945   </td><td>-0.495954442 </td><td> 0.47234291  </td><td>-0.1033727528</td><td> 0.11469191  </td><td> 0.018459564 </td><td> 0.1824386999</td><td>-0.1389626180</td><td>-0.114702885 </td></tr>
	<tr><td> 0.3799486   </td><td>-0.444134704 </td><td> 0.49194137  </td><td>-0.1387764590</td><td> 0.13915859  </td><td> 0.013283974 </td><td> 0.1229109140</td><td>-0.0973772037</td><td>-0.116962488 </td></tr>
	<tr><td> 0.2576489   </td><td>-0.483342987 </td><td> 0.37297134  </td><td>-0.1416818547</td><td> 0.21231287  </td><td> 0.117125266 </td><td> 0.1369392140</td><td>-0.1222056233</td><td> 0.146276756 </td></tr>
	<tr><td> 0.2062744   </td><td>-0.354617033 </td><td> 0.26543767  </td><td>-0.0007295079</td><td> 0.06470126  </td><td> 0.099806178 </td><td>-0.2945216155</td><td> 0.1256130100</td><td>-0.077929380 </td></tr>
	<tr><td> 0.3334748   </td><td>-0.402402498 </td><td> 0.39061932  </td><td>-0.1902953021</td><td> 0.08669907  </td><td> 0.083796843 </td><td>-0.1520357393</td><td>-0.0181765447</td><td> 0.107031691 </td></tr>
	<tr><td>-0.4064146   </td><td>-0.347591815 </td><td>-0.19534506  </td><td>-0.1439862302</td><td>-0.08445037  </td><td> 0.611418809 </td><td> 0.1300478000</td><td>-0.0035088423</td><td>-0.175680364 </td></tr>
	<tr><td>-0.4213233   </td><td>-0.421047564 </td><td>-0.21614195  </td><td>-0.1475954934</td><td>-0.14140091  </td><td> 0.569277998 </td><td> 0.1274642030</td><td> 0.0120766707</td><td>-0.147817867 </td></tr>
	<tr><td>-0.3177238   </td><td>-0.558528507 </td><td>-0.21188225  </td><td>-0.0260375231</td><td>-0.11058613  </td><td>-0.064299580 </td><td> 0.1599389824</td><td> 0.0008566237</td><td> 0.011078551 </td></tr>
	<tr><td>-0.4535206   </td><td>-0.568930452 </td><td>-0.13969631  </td><td> 0.0740933171</td><td>-0.20728453  </td><td>-0.111993462 </td><td> 0.0128182266</td><td> 0.0851071945</td><td> 0.058119259 </td></tr>
	<tr><td>-0.3733466   </td><td>-0.534136456 </td><td>-0.20889608  </td><td> 0.0462874732</td><td>-0.18328216  </td><td>-0.242376556 </td><td> 0.0930813931</td><td>-0.0879622294</td><td> 0.120472821 </td></tr>
	<tr><td>-0.4519620   </td><td>-0.486894058 </td><td> 0.02868335  </td><td>-0.0237170894</td><td>-0.22067245  </td><td>-0.121746325 </td><td> 0.0211133535</td><td> 0.0949324784</td><td>-0.117122331 </td></tr>
	<tr><td>-0.4792771   </td><td>-0.359058583 </td><td>-0.17026707  </td><td> 0.0243226733</td><td> 0.01058587  </td><td>-0.329825553 </td><td> 0.0814961168</td><td>-0.2338333561</td><td> 0.027777255 </td></tr>
	<tr><td>-0.4884591   </td><td>-0.361002718 </td><td>-0.23670362  </td><td>-0.0576248626</td><td> 0.04542125  </td><td>-0.266631231 </td><td> 0.1774163361</td><td>-0.1496133363</td><td>-0.028050715 </td></tr>
	<tr><td>-0.3794872   </td><td>-0.555721465 </td><td>-0.09478251  </td><td> 0.1039076241</td><td>-0.19174868  </td><td>-0.083409801 </td><td>-0.1393220930</td><td>-0.0449142959</td><td> 0.188743726 </td></tr>
	<tr><td>-0.4024135   </td><td>-0.549377337 </td><td>-0.11356106  </td><td> 0.1869168187</td><td>-0.12464387  </td><td>-0.136300169 </td><td>-0.0389545949</td><td> 0.0957233294</td><td> 0.133687572 </td></tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We first determine the loadings by multiplying the first $m$ eigenvectors (in this case, 9) by the square roots of their corresponding eigenvalues. This produces the above matrix, a unsightly $40x9$ matrix. Each column corresponds to a different loading.</p>
<p>To determine the communalities and specific variances of the variables, we can square the entries in each of the rows of the loadings matrix. This will give us the $h^2_i$ vector, from which we can also determine the specific variances.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[114]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>hisq <span class="o">&lt;-</span> <span class="kp">apply</span><span class="p">(</span><span class="kp">apply</span><span class="p">(</span>loadings<span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> x<span class="o">^</span><span class="m">2</span><span class="p">),</span> <span class="m">1</span><span class="p">,</span> <span class="kp">sum</span><span class="p">)</span></span>
<span class="code-line">psi <span class="o">&lt;-</span> <span class="kp">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">40</span><span class="p">)</span> <span class="o">-</span> hisq</span>
<span class="code-line"></span>
<span class="code-line">hisq</span>
<span class="code-line">psi</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<ol class=list-inline>
	<li>0.683770176904431</li>
	<li>0.653702160695683</li>
	<li>0.675341735731149</li>
	<li>0.712773153297998</li>
	<li>0.566896037890536</li>
	<li>0.558690262100049</li>
	<li>0.635211516458175</li>
	<li>0.532115981531147</li>
	<li>0.609566835025406</li>
	<li>0.484574181409319</li>
	<li>0.706671375970135</li>
	<li>0.735721820684866</li>
	<li>0.539231950202743</li>
	<li>0.69716387017627</li>
	<li>0.744607322341919</li>
	<li>0.712766093667313</li>
	<li>0.634277201980493</li>
	<li>0.710513684222417</li>
	<li>0.684661714950165</li>
	<li>0.506335359710202</li>
	<li>0.528022968048179</li>
	<li>0.60172739948512</li>
	<li>0.625084947046191</li>
	<li>0.590145642396035</li>
	<li>0.562934286491778</li>
	<li>0.701361913962161</li>
	<li>0.660692998827501</li>
	<li>0.573063322313833</li>
	<li>0.361502179731792</li>
	<li>0.511368640368655</li>
	<li>0.773637765079673</li>
	<li>0.805610905334176</li>
	<li>0.500542243541453</li>
	<li>0.620662490871121</li>
	<li>0.593723463371302</li>
	<li>0.529414785891513</li>
	<li>0.559200271826264</li>
	<li>0.556067291455054</li>
	<li>0.573394271904066</li>
	<li>0.574252742505073</li>
</ol>

</div>

</div>

<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<ol class=list-inline>
	<li>0.316229823095569</li>
	<li>0.346297839304317</li>
	<li>0.324658264268851</li>
	<li>0.287226846702002</li>
	<li>0.433103962109464</li>
	<li>0.441309737899951</li>
	<li>0.364788483541825</li>
	<li>0.467884018468853</li>
	<li>0.390433164974594</li>
	<li>0.515425818590681</li>
	<li>0.293328624029865</li>
	<li>0.264278179315134</li>
	<li>0.460768049797257</li>
	<li>0.30283612982373</li>
	<li>0.255392677658081</li>
	<li>0.287233906332687</li>
	<li>0.365722798019507</li>
	<li>0.289486315777583</li>
	<li>0.315338285049835</li>
	<li>0.493664640289798</li>
	<li>0.471977031951821</li>
	<li>0.39827260051488</li>
	<li>0.374915052953809</li>
	<li>0.409854357603965</li>
	<li>0.437065713508222</li>
	<li>0.298638086037839</li>
	<li>0.339307001172499</li>
	<li>0.426936677686167</li>
	<li>0.638497820268208</li>
	<li>0.488631359631345</li>
	<li>0.226362234920327</li>
	<li>0.194389094665824</li>
	<li>0.499457756458547</li>
	<li>0.379337509128879</li>
	<li>0.406276536628698</li>
	<li>0.470585214108487</li>
	<li>0.440799728173736</li>
	<li>0.443932708544946</li>
	<li>0.426605728095934</li>
	<li>0.425747257494927</li>
</ol>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above are the communalities and the specific variances of each of the variables.</p>
<p>To determine the amount of the total variance of which each loading contributes, we can divide the eigenvalues of $R$ by the number of variables, $p = 40$.</p>
<p>Thus, all nine factors account for about 60% of the variance of the variables. This is not very good, especially considering there were supposed to be only four underlying factors!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Rotating-the-Loadings">Rotating the Loadings<a class="anchor-link" href="#Rotating-the-Loadings">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Because we have $m = 9$, I will use the varimax rotations from the psych package to rotate the loadings we have above. This will make it easier to see which variables correspond to the factors.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[115]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="c1">## varimax includes a print method which makes the loading distributions easier to see.</span></span>
<span class="code-line">loadingsrot <span class="o">&lt;-</span> <span class="kp">print</span><span class="p">(</span>varimax<span class="p">(</span>loadings<span class="p">)</span><span class="o">$</span>loadings<span class="p">,</span>cutoff <span class="o">=</span> <span class="m">.3</span><span class="p">,</span>sort <span class="o">=</span> <span class="bp">T</span><span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="code-line"></span>
<span class="code-line">Loadings:</span>
<span class="code-line">      [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  </span>
<span class="code-line"> [1,] -0.681                                                        </span>
<span class="code-line"> [2,] -0.713                                                        </span>
<span class="code-line"> [3,] -0.681                                                        </span>
<span class="code-line"> [4,] -0.655                                                        </span>
<span class="code-line"> [5,]  0.506                                                   0.486</span>
<span class="code-line"> [6,]        -0.657                                                 </span>
<span class="code-line"> [7,]        -0.742                                                 </span>
<span class="code-line"> [8,]        -0.765                                                 </span>
<span class="code-line"> [9,]        -0.616                                                 </span>
<span class="code-line">[10,]        -0.642                                                 </span>
<span class="code-line">[11,]        -0.623                                                 </span>
<span class="code-line">[12,]        -0.685                                                 </span>
<span class="code-line">[13,]        -0.698                                                 </span>
<span class="code-line">[14,]                0.716                                          </span>
<span class="code-line">[15,]                0.784                                          </span>
<span class="code-line">[16,]                0.769                                          </span>
<span class="code-line">[17,]                0.731                                          </span>
<span class="code-line">[18,]                0.643                                          </span>
<span class="code-line">[19,]               -0.397 -0.534                                   </span>
<span class="code-line">[20,]                      -0.718                                   </span>
<span class="code-line">[21,]                      -0.747                                   </span>
<span class="code-line">[22,]                      -0.688                                   </span>
<span class="code-line">[23,]                      -0.313 -0.728                            </span>
<span class="code-line">[24,]                             -0.753                            </span>
<span class="code-line">[25,]                              0.759                            </span>
<span class="code-line">[26,]                              0.727                0.301       </span>
<span class="code-line">[27,]                              0.722                            </span>
<span class="code-line">[28,]                                     0.794                     </span>
<span class="code-line">[29,]        -0.349                       0.786                     </span>
<span class="code-line">[30,]                                            0.744              </span>
<span class="code-line">[31,]                                            0.745              </span>
<span class="code-line">[32,]                              0.338        -0.618              </span>
<span class="code-line">[33,]  0.302                                            0.681       </span>
<span class="code-line">[34,]                             -0.408                      -0.522</span>
<span class="code-line">[35,]                                                          0.578</span>
<span class="code-line">[36,]                             -0.396         0.337        -0.501</span>
<span class="code-line">[37,]                                           -0.407  0.481       </span>
<span class="code-line">[38,]                             -0.478         0.355              </span>
<span class="code-line">[39,]                              0.360                0.327  0.457</span>
<span class="code-line">[40,]                0.391                              0.326       </span>
<span class="code-line"></span>
<span class="code-line">                [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]</span>
<span class="code-line">SS loadings    2.970 4.308 3.507 2.557 3.954 1.746 2.289 1.437 1.819</span>
<span class="code-line">Proportion Var 0.074 0.108 0.088 0.064 0.099 0.044 0.057 0.036 0.045</span>
<span class="code-line">Cumulative Var 0.074 0.182 0.270 0.334 0.432 0.476 0.533 0.569 0.615</span>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see from the above that the factors do not separate so easily. The values of the loadings below .3 have been omitted to make it clearer what the underling relationships are. Although they do not separate cleanly into the four theorized factors, we can see that the first factor, for instance, largely is between the first and tenth variables (which was assertiveness), the second factor between the 30th and 40th variables (which was dominance). Similarly, the fifth factor is between the 20th and 30th variables (social confidence) and the third and factor is largely between the 30th and 40th variables (adventurousness). However, the split is not so simple as we assumed before the analysis, as the other factors show relationships between variables which are not in the same groups.</p>
<p>The second function simply sorts the variables by factor, so we can get a better sense of how they separate. We can see that towards the top there are larger values for the loading matrix, which indicates a stronger association with the factor. We can also see that the 6th-9th factors also have some large values, which means that their correlations cannot be ignored. This suggests that the initial separation into only four factors may have been overly simplistic.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Principle-Factors-Method">Principle Factors Method<a class="anchor-link" href="#Principle-Factors-Method">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the previous method, we had ignored the structure of the specific variances. Now, using the principal factors method, we will take the $\Phi$ matrix into account.</p>
<p>We first make an initial estimate of the $h_i$s, and use this to account for the specific variances. Below I create a diagonal matrix which has the specific variances in each of the diagonal elements. We will then subtract this matrix from the covariance matrix, and use the new matrix to compute our loadings.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[116]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>hinit <span class="o">&lt;-</span> <span class="m">1</span> <span class="o">-</span> <span class="m">1</span><span class="o">/</span><span class="kp">diag</span><span class="p">(</span><span class="kp">solve</span><span class="p">(</span>R<span class="p">))</span></span>
<span class="code-line">psiint <span class="o">&lt;-</span> <span class="m">1</span> <span class="o">-</span> hinit</span>
<span class="code-line">hinit</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<dl class=dl-horizontal>
	<dt>AS1</dt>
		<dd>0.703351716235528</dd>
	<dt>AS2</dt>
		<dd>0.547924923423483</dd>
	<dt>AS3</dt>
		<dd>0.55110592594616</dd>
	<dt>AS4</dt>
		<dd>0.505277553186238</dd>
	<dt>AS5</dt>
		<dd>0.390968237137091</dd>
	<dt>AS6</dt>
		<dd>0.417887373144211</dd>
	<dt>AS7</dt>
		<dd>0.479120494482409</dd>
	<dt>AS8</dt>
		<dd>0.327259567877729</dd>
	<dt>AS9</dt>
		<dd>0.267933784282943</dd>
	<dt>AS10</dt>
		<dd>0.219370957003131</dd>
	<dt>SC1</dt>
		<dd>0.578039342631155</dd>
	<dt>SC2</dt>
		<dd>0.655687388651141</dd>
	<dt>SC3</dt>
		<dd>0.456950572036036</dd>
	<dt>SC4</dt>
		<dd>0.695887704065272</dd>
	<dt>SC5</dt>
		<dd>0.572176977540937</dd>
	<dt>SC6</dt>
		<dd>0.639285723548227</dd>
	<dt>SC7</dt>
		<dd>0.486094063924925</dd>
	<dt>SC8</dt>
		<dd>0.57955658432216</dd>
	<dt>SC9</dt>
		<dd>0.572181489668999</dd>
	<dt>SC10</dt>
		<dd>0.377978608918933</dd>
	<dt>AD1</dt>
		<dd>0.362251502864202</dd>
	<dt>AD2</dt>
		<dd>0.382357316270689</dd>
	<dt>AD3</dt>
		<dd>0.402193273797794</dd>
	<dt>AD4</dt>
		<dd>0.412581346196776</dd>
	<dt>AD5</dt>
		<dd>0.428481448115908</dd>
	<dt>AD6</dt>
		<dd>0.610754177794417</dd>
	<dt>AD7</dt>
		<dd>0.579359418895454</dd>
	<dt>AD8</dt>
		<dd>0.392654029176029</dd>
	<dt>AD9</dt>
		<dd>0.230305720741996</dd>
	<dt>AD10</dt>
		<dd>0.360903362125066</dd>
	<dt>DO1</dt>
		<dd>0.618517766838695</dd>
	<dt>DO2</dt>
		<dd>0.657506921900043</dd>
	<dt>DO3</dt>
		<dd>0.424117606047785</dd>
	<dt>DO4</dt>
		<dd>0.538841247151321</dd>
	<dt>DO5</dt>
		<dd>0.438784438295507</dd>
	<dt>DO6</dt>
		<dd>0.41461456936309</dd>
	<dt>DO7</dt>
		<dd>0.442354015793423</dd>
	<dt>DO8</dt>
		<dd>0.462192532311484</dd>
	<dt>DO9</dt>
		<dd>0.45847071523956</dd>
	<dt>DO10</dt>
		<dd>0.484904277176995</dd>
</dl>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[117]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>psiint</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<dl class=dl-horizontal>
	<dt>AS1</dt>
		<dd>0.296648283764472</dd>
	<dt>AS2</dt>
		<dd>0.452075076576517</dd>
	<dt>AS3</dt>
		<dd>0.44889407405384</dd>
	<dt>AS4</dt>
		<dd>0.494722446813762</dd>
	<dt>AS5</dt>
		<dd>0.609031762862909</dd>
	<dt>AS6</dt>
		<dd>0.582112626855789</dd>
	<dt>AS7</dt>
		<dd>0.520879505517591</dd>
	<dt>AS8</dt>
		<dd>0.672740432122271</dd>
	<dt>AS9</dt>
		<dd>0.732066215717057</dd>
	<dt>AS10</dt>
		<dd>0.780629042996869</dd>
	<dt>SC1</dt>
		<dd>0.421960657368845</dd>
	<dt>SC2</dt>
		<dd>0.344312611348859</dd>
	<dt>SC3</dt>
		<dd>0.543049427963964</dd>
	<dt>SC4</dt>
		<dd>0.304112295934728</dd>
	<dt>SC5</dt>
		<dd>0.427823022459063</dd>
	<dt>SC6</dt>
		<dd>0.360714276451773</dd>
	<dt>SC7</dt>
		<dd>0.513905936075075</dd>
	<dt>SC8</dt>
		<dd>0.42044341567784</dd>
	<dt>SC9</dt>
		<dd>0.427818510331001</dd>
	<dt>SC10</dt>
		<dd>0.622021391081067</dd>
	<dt>AD1</dt>
		<dd>0.637748497135798</dd>
	<dt>AD2</dt>
		<dd>0.617642683729311</dd>
	<dt>AD3</dt>
		<dd>0.597806726202206</dd>
	<dt>AD4</dt>
		<dd>0.587418653803224</dd>
	<dt>AD5</dt>
		<dd>0.571518551884092</dd>
	<dt>AD6</dt>
		<dd>0.389245822205583</dd>
	<dt>AD7</dt>
		<dd>0.420640581104546</dd>
	<dt>AD8</dt>
		<dd>0.607345970823971</dd>
	<dt>AD9</dt>
		<dd>0.769694279258004</dd>
	<dt>AD10</dt>
		<dd>0.639096637874934</dd>
	<dt>DO1</dt>
		<dd>0.381482233161305</dd>
	<dt>DO2</dt>
		<dd>0.342493078099957</dd>
	<dt>DO3</dt>
		<dd>0.575882393952215</dd>
	<dt>DO4</dt>
		<dd>0.461158752848679</dd>
	<dt>DO5</dt>
		<dd>0.561215561704493</dd>
	<dt>DO6</dt>
		<dd>0.58538543063691</dd>
	<dt>DO7</dt>
		<dd>0.557645984206577</dd>
	<dt>DO8</dt>
		<dd>0.537807467688516</dd>
	<dt>DO9</dt>
		<dd>0.54152928476044</dd>
	<dt>DO10</dt>
		<dd>0.515095722823005</dd>
</dl>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[118]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kp">head</span><span class="p">(</span><span class="kp">diag</span><span class="p">(</span>psiint<span class="p">,</span>nrow <span class="o">=</span> <span class="m">40</span><span class="p">))</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<table>
<tbody>
	<tr><td>0.2966483</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td></tr>
	<tr><td>0.0000000</td><td>0.4520751</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td></tr>
	<tr><td>0.0000000</td><td>0.0000000</td><td>0.4488941</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td></tr>
	<tr><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.4947224</td><td>0.0000000</td><td>0.0000000</td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td></tr>
	<tr><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.6090318</td><td>0.0000000</td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td></tr>
	<tr><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.0000000</td><td>0.5821126</td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>...      </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td><td>0        </td></tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[119]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>New <span class="o">&lt;-</span> R <span class="o">-</span> <span class="kp">diag</span><span class="p">(</span>psiint<span class="p">,</span>nrow <span class="o">=</span> <span class="m">40</span><span class="p">)</span></span>
<span class="code-line">eigs <span class="o">&lt;-</span> <span class="kp">eigen</span><span class="p">(</span>New<span class="p">)</span></span>
<span class="code-line">eigs</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre><span class="code-line">eigen() decomposition</span>
<span class="code-line">$values</span>
<span class="code-line"> [1]  8.72480786  3.73122125  2.22214484  1.55959578  1.13192575  0.94924084</span>
<span class="code-line"> [7]  0.69195388  0.57596597  0.51777057  0.43404257  0.27746408  0.24907073</span>
<span class="code-line">[13]  0.20382247  0.18319157  0.14466391  0.10130984  0.08749738  0.06797501</span>
<span class="code-line">[19]  0.03771954  0.01208288 -0.01256549 -0.03911447 -0.04221706 -0.06783310</span>
<span class="code-line">[25] -0.09186545 -0.09808676 -0.10163170 -0.11804259 -0.12715993 -0.13788099</span>
<span class="code-line">[31] -0.15202202 -0.15751357 -0.17112680 -0.18008415 -0.18710930 -0.19541343</span>
<span class="code-line">[37] -0.20017634 -0.21596482 -0.23296518 -0.24850889</span>
<span class="code-line"></span>
<span class="code-line">$vectors</span>
<span class="code-line">             [,1]          [,2]         [,3]         [,4]        [,5]</span>
<span class="code-line"> [1,] -0.21191434  0.0638645803  0.216822536 -0.022468256 -0.25615848</span>
<span class="code-line"> [2,] -0.21927351 -0.0651230428  0.060958567  0.176672739 -0.13311399</span>
<span class="code-line"> [3,] -0.21134289 -0.0975909567  0.024092374  0.184194194 -0.14976161</span>
<span class="code-line"> [4,] -0.19896989  0.0003466393  0.146503753  0.036124043 -0.16574130</span>
<span class="code-line"> [5,] -0.18252625 -0.0364514037 -0.031953933  0.068121563 -0.18846025</span>
<span class="code-line"> [6,] -0.18190149 -0.0973139299 -0.019771354  0.098589581 -0.18447426</span>
<span class="code-line"> [7,]  0.18387677 -0.0482107759  0.007793947 -0.307751553  0.10121267</span>
<span class="code-line"> [8,]  0.13501250 -0.0404225650  0.039020411 -0.270733036  0.05035865</span>
<span class="code-line"> [9,]  0.11713155 -0.0533301025  0.077886468 -0.154472414  0.05325027</span>
<span class="code-line">[10,]  0.08984830 -0.0840485083  0.080677025 -0.136809325  0.08195542</span>
<span class="code-line">[11,] -0.17310669  0.1374663028  0.188035682 -0.242264396  0.07688027</span>
<span class="code-line">[12,] -0.22051610  0.0377714836  0.197260508 -0.180360079  0.30285600</span>
<span class="code-line">[13,] -0.19681859  0.0374233202  0.152409585 -0.121605209  0.05178017</span>
<span class="code-line">[14,] -0.20495512  0.0521895454  0.225533522 -0.118908627 -0.26503696</span>
<span class="code-line">[15,] -0.20756081  0.0070522330  0.175144989 -0.077991784 -0.15090784</span>
<span class="code-line">[16,]  0.20030461 -0.1209811419 -0.174818628  0.002661036 -0.39996267</span>
<span class="code-line">[17,]  0.17770036 -0.1395591708 -0.122119769 -0.085951004  0.01937474</span>
<span class="code-line">[18,]  0.16857986 -0.2115737663 -0.162210441  0.043223444 -0.14326556</span>
<span class="code-line">[19,]  0.19221725 -0.1173746987 -0.150999335 -0.048170270 -0.37524166</span>
<span class="code-line">[20,]  0.16932886 -0.1022850999 -0.066391823 -0.073364420  0.04165247</span>
<span class="code-line">[21,] -0.12898946  0.0734429712 -0.202814551 -0.244825607 -0.06795927</span>
<span class="code-line">[22,] -0.10993024  0.0695263365 -0.147704369 -0.370765189 -0.12523340</span>
<span class="code-line">[23,] -0.10769927 -0.0152920098 -0.156792280 -0.379661784 -0.18761962</span>
<span class="code-line">[24,] -0.11878337  0.0488835014 -0.218973571 -0.342253282 -0.14086453</span>
<span class="code-line">[25,]  0.12344763 -0.2112173729  0.245815037 -0.085344833 -0.03827756</span>
<span class="code-line">[26,]  0.12525971 -0.2461001017  0.316800429 -0.065964297 -0.07122012</span>
<span class="code-line">[27,]  0.12562028 -0.2179364719  0.324647616 -0.089517877 -0.09262179</span>
<span class="code-line">[28,]  0.08339879 -0.2266499762  0.226451086 -0.083557483 -0.10456494</span>
<span class="code-line">[29,]  0.06544192 -0.1581824758  0.146234944  0.008942834 -0.01682762</span>
<span class="code-line">[30,]  0.10746484 -0.1855239018  0.233367104 -0.110238782 -0.02908694</span>
<span class="code-line">[31,] -0.13547116 -0.1786365421 -0.126289225 -0.136896947  0.18515118</span>
<span class="code-line">[32,] -0.14099901 -0.2177440388 -0.139574002 -0.142611136  0.22845087</span>
<span class="code-line">[33,] -0.10330488 -0.2681499356 -0.116866275 -0.026874693  0.05623212</span>
<span class="code-line">[34,] -0.14963116 -0.2806943436 -0.081503597  0.050448938  0.12386953</span>
<span class="code-line">[35,] -0.12160025 -0.2569026085 -0.116685857  0.026558886  0.07581466</span>
<span class="code-line">[36,] -0.14725857 -0.2314500666  0.020682137 -0.013222301  0.11979588</span>
<span class="code-line">[37,] -0.15631314 -0.1727513006 -0.095991992  0.015281278 -0.07316327</span>
<span class="code-line">[38,] -0.15951395 -0.1755079663 -0.136005589 -0.044236436 -0.08708976</span>
<span class="code-line">[39,] -0.12405265 -0.2680220873 -0.053469497  0.070947368  0.11829684</span>
<span class="code-line">[40,] -0.13191686 -0.2673459106 -0.066980180  0.130023170  0.06007734</span>
<span class="code-line">             [,6]         [,7]          [,8]         [,9]         [,10]</span>
<span class="code-line"> [1,] -0.15827430 -0.024370544  0.4224781068  0.283846440  1.031011e-01</span>
<span class="code-line"> [2,]  0.16948247  0.256608475 -0.0821893620 -0.025852984  2.220461e-02</span>
<span class="code-line"> [3,]  0.15429878  0.315258322 -0.1168124788  0.053865031 -9.834273e-02</span>
<span class="code-line"> [4,]  0.00831336 -0.267711489 -0.3441884719  0.106418967  1.731900e-01</span>
<span class="code-line"> [5,]  0.13620105  0.290255310 -0.0718212342  0.021728576  6.960383e-03</span>
<span class="code-line"> [6,]  0.11876169  0.264451319 -0.0550391735 -0.111677533 -2.946530e-02</span>
<span class="code-line"> [7,] -0.11983914 -0.081808089 -0.0885797717  0.103757761  2.917757e-01</span>
<span class="code-line"> [8,] -0.03638548  0.106832218 -0.1216425503  0.081192391  2.767154e-01</span>
<span class="code-line"> [9,] -0.22758313  0.068913132 -0.1462576486  0.251324530 -1.503272e-01</span>
<span class="code-line">[10,] -0.09836163  0.284579531  0.0295665354  0.096691319 -3.174537e-02</span>
<span class="code-line">[11,] -0.04159560  0.199690466  0.0811955025 -0.248526910  3.737468e-01</span>
<span class="code-line">[12,] -0.03143641  0.138431767 -0.1064316538  0.071034512 -1.555814e-01</span>
<span class="code-line">[13,]  0.01845562 -0.036382777 -0.0557317087  0.143148630 -1.820674e-01</span>
<span class="code-line">[14,] -0.15512880 -0.069879416  0.3514228501  0.344208761  3.102509e-02</span>
<span class="code-line">[15,]  0.04932728 -0.252791620 -0.4134395758  0.136705403  1.063398e-01</span>
<span class="code-line">[16,]  0.04852405 -0.009191454  0.0540441676 -0.009051929  2.187696e-01</span>
<span class="code-line">[17,] -0.11391111  0.331796110  0.2845588774  0.116429272 -9.469127e-02</span>
<span class="code-line">[18,]  0.04685364 -0.097220774 -0.1245846052  0.380314139 -3.207187e-01</span>
<span class="code-line">[19,]  0.02598492 -0.030379206 -0.0187931360  0.036064034  2.647564e-01</span>
<span class="code-line">[20,]  0.02356316  0.173181864 -0.2096544144  0.213152551  6.651396e-02</span>
<span class="code-line">[21,] -0.08933338  0.132017255 -0.1016462713  0.106055687 -1.563673e-02</span>
<span class="code-line">[22,]  0.01747455  0.092299153 -0.0589124181 -0.213043710 -1.151562e-02</span>
<span class="code-line">[23,]  0.03002665 -0.039697120 -0.0007127983 -0.176397646 -2.957268e-01</span>
<span class="code-line">[24,]  0.05825359  0.022250604 -0.0835513124  0.011648537 -1.694231e-01</span>
<span class="code-line">[25,]  0.07225976  0.019548111 -0.0320634032 -0.158535811  8.227759e-05</span>
<span class="code-line">[26,]  0.06007824 -0.130169484  0.0882919492 -0.179487793 -2.022112e-01</span>
<span class="code-line">[27,]  0.06157881 -0.083945667  0.0822447224 -0.112769681 -1.448531e-01</span>
<span class="code-line">[28,]  0.12506919 -0.036460762 -0.0398695032 -0.198717192  1.904711e-02</span>
<span class="code-line">[29,]  0.05963406  0.195333527 -0.0375618776  0.127083107  1.367631e-02</span>
<span class="code-line">[30,]  0.06016990  0.165641225 -0.0829931136 -0.020241198  3.118090e-04</span>
<span class="code-line">[31,]  0.49629699 -0.139568268  0.2334919364  0.136470107  1.228746e-01</span>
<span class="code-line">[32,]  0.45331974 -0.133403153  0.2036658111  0.142013496  1.058988e-01</span>
<span class="code-line">[33,] -0.09906670 -0.123998263 -0.0123462892 -0.033555908 -3.513423e-02</span>
<span class="code-line">[34,] -0.17200932 -0.018014320 -0.0525556731  0.076674017  7.605789e-02</span>
<span class="code-line">[35,] -0.24447264 -0.073476737  0.0037701821 -0.119654641  5.546094e-02</span>
<span class="code-line">[36,] -0.15091680 -0.032103762 -0.0375143314  0.091121696 -1.053736e-01</span>
<span class="code-line">[37,] -0.25136231 -0.057850425  0.1352130623 -0.188409724 -1.135128e-02</span>
<span class="code-line">[38,] -0.19474644 -0.153848312  0.0485388559 -0.217494610 -1.182452e-01</span>
<span class="code-line">[39,] -0.13910063  0.137555935 -0.0240529211 -0.047495780  1.745509e-01</span>
<span class="code-line">[40,] -0.14829209  0.011484563 -0.0505739246  0.069005274  2.200400e-01</span>
<span class="code-line">             [,11]        [,12]         [,13]        [,14]        [,15]</span>
<span class="code-line"> [1,] -0.093106049  0.023953605  0.1117642781 -0.113668062  0.004013874</span>
<span class="code-line"> [2,] -0.091594959  0.116164513  0.1234670649  0.120932190 -0.058282880</span>
<span class="code-line"> [3,] -0.096311382  0.023670802  0.0301013676  0.171924685  0.174719738</span>
<span class="code-line"> [4,]  0.031169404 -0.041063817  0.1284634151 -0.056395911  0.084647510</span>
<span class="code-line"> [5,]  0.089108922  0.056604682 -0.1898358272 -0.141490134  0.187404140</span>
<span class="code-line"> [6,] -0.021100498  0.006001516  0.0726063633  0.014045167  0.331118635</span>
<span class="code-line"> [7,] -0.142377583  0.221895979 -0.0831367706 -0.058015962  0.250495434</span>
<span class="code-line"> [8,]  0.066598083  0.326645997 -0.1283547910  0.134837419  0.312362155</span>
<span class="code-line"> [9,]  0.138863707 -0.245192228  0.0005251174  0.222729453  0.061742042</span>
<span class="code-line">[10,]  0.096860572 -0.239634419  0.4166085223  0.129274248 -0.109238876</span>
<span class="code-line">[11,]  0.032137040 -0.072744124 -0.0210042302  0.195457424 -0.031786573</span>
<span class="code-line">[12,]  0.113908667  0.160259725  0.1018486933 -0.090273594  0.110726129</span>
<span class="code-line">[13,]  0.262175090  0.114901214 -0.3062113567  0.075555168  0.015794822</span>
<span class="code-line">[14,] -0.095782673  0.050590974  0.0380508460 -0.104868927 -0.040408956</span>
<span class="code-line">[15,]  0.127620058 -0.120741802  0.0185095025  0.089821657 -0.077724697</span>
<span class="code-line">[16,]  0.097562858 -0.152357458 -0.1851300772  0.253259537 -0.112013067</span>
<span class="code-line">[17,]  0.084887200  0.064189354 -0.1251991747 -0.002436892  0.146522710</span>
<span class="code-line">[18,] -0.008751875  0.152683667  0.0731774456 -0.172994806  0.111272080</span>
<span class="code-line">[19,]  0.040925231  0.035538330  0.1281941236  0.071947140  0.020408208</span>
<span class="code-line">[20,]  0.151121452 -0.113893518  0.3176877023 -0.215341692 -0.036916168</span>
<span class="code-line">[21,]  0.027220596 -0.279414760 -0.0433390425  0.107478575  0.035253394</span>
<span class="code-line">[22,] -0.169510734 -0.159674756  0.1943993309 -0.211848067  0.002522462</span>
<span class="code-line">[23,] -0.104679536 -0.042392807 -0.0338028763 -0.063781597 -0.159117917</span>
<span class="code-line">[24,] -0.140280185  0.123130792 -0.2595967733 -0.081837727 -0.150508676</span>
<span class="code-line">[25,] -0.035231678  0.030111977  0.0951462131 -0.269069359 -0.097736756</span>
<span class="code-line">[26,] -0.078007435 -0.268096562 -0.0732707829  0.018338435  0.264014738</span>
<span class="code-line">[27,]  0.005860812 -0.238214479 -0.1359315757  0.149958479  0.212755146</span>
<span class="code-line">[28,] -0.002991652  0.258861576  0.0072169464 -0.119524778 -0.069977754</span>
<span class="code-line">[29,]  0.089399076  0.168638263 -0.1681037648  0.222630026 -0.512673304</span>
<span class="code-line">[30,] -0.037932190  0.125090822  0.0568209367 -0.143333016 -0.304207052</span>
<span class="code-line">[31,]  0.137616759 -0.053774593  0.1118868527  0.091599985 -0.054671924</span>
<span class="code-line">[32,]  0.064922713 -0.098253612  0.0245284558 -0.009400531  0.076538119</span>
<span class="code-line">[33,] -0.270183667  0.267178022  0.1643686377  0.270876943  0.013175597</span>
<span class="code-line">[34,] -0.401859145 -0.028585352 -0.0340428235  0.081695820 -0.003376735</span>
<span class="code-line">[35,] -0.122327060  0.001947819  0.1762680714  0.094229249  0.013548891</span>
<span class="code-line">[36,] -0.133373550 -0.070487115 -0.1131474580  0.212853198 -0.136479841</span>
<span class="code-line">[37,]  0.425845448  0.178438174  0.2011172845 -0.069139919  0.015441866</span>
<span class="code-line">[38,]  0.449148328  0.129750421  0.0331814001  0.102540454  0.022067505</span>
<span class="code-line">[39,]  0.005658654 -0.179248687 -0.2686955684 -0.311721069 -0.059326690</span>
<span class="code-line">[40,]  0.104881177 -0.221573698 -0.2569202640 -0.335994013 -0.050981529</span>
<span class="code-line">             [,16]        [,17]        [,18]        [,19]       [,20]</span>
<span class="code-line"> [1,]  0.051720282 -0.020238235 -0.058414767  0.051386778 -0.07469648</span>
<span class="code-line"> [2,]  0.007900105 -0.060657392 -0.204351746  0.356363407 -0.36586927</span>
<span class="code-line"> [3,] -0.059353877 -0.041045516 -0.100604716  0.113254414  0.04586721</span>
<span class="code-line"> [4,]  0.163214149  0.022029493 -0.115407241  0.160971104  0.26392717</span>
<span class="code-line"> [5,]  0.026535895 -0.031813851 -0.003240182 -0.481812626  0.13138548</span>
<span class="code-line"> [6,]  0.023463697 -0.177477204  0.323295238  0.010728552  0.29721500</span>
<span class="code-line"> [7,]  0.095408883 -0.190247303  0.010081957  0.170243516  0.09287607</span>
<span class="code-line"> [8,]  0.118784719 -0.196428690 -0.224053731 -0.045563986 -0.12468882</span>
<span class="code-line"> [9,] -0.347625830 -0.265593306  0.089249789 -0.046668060  0.05152229</span>
<span class="code-line">[10,] -0.121290117 -0.370258195 -0.035792352 -0.070014074 -0.16248416</span>
<span class="code-line">[11,] -0.062675487  0.220098506  0.160339101  0.043641668  0.03028082</span>
<span class="code-line">[12,]  0.050279390  0.169227620  0.036932560  0.151684871 -0.07978067</span>
<span class="code-line">[13,] -0.384984033  0.294069805  0.166008709  0.099372197  0.02033137</span>
<span class="code-line">[14,] -0.002739694 -0.087999264  0.078464334 -0.087310380  0.07619189</span>
<span class="code-line">[15,] -0.018256737  0.004952343  0.101991017 -0.194234539 -0.12371531</span>
<span class="code-line">[16,] -0.062219141 -0.006547805  0.033942989  0.126531607  0.08430365</span>
<span class="code-line">[17,] -0.023548163  0.269649746 -0.029525599  0.046919843  0.15406323</span>
<span class="code-line">[18,] -0.003740704  0.077740029 -0.088847153  0.053133996 -0.10928175</span>
<span class="code-line">[19,] -0.205485270  0.258116841  0.098378726  0.128843426 -0.08405342</span>
<span class="code-line">[20,]  0.260010856  0.254843891  0.331655352  0.081325147  0.03186721</span>
<span class="code-line">[21,]  0.045060363  0.314590398 -0.385126163 -0.094414285 -0.08571515</span>
<span class="code-line">[22,] -0.002137638  0.058724250 -0.080568516  0.011762663 -0.02423440</span>
<span class="code-line">[23,] -0.011364031 -0.159136904  0.093150561  0.142806160  0.04425149</span>
<span class="code-line">[24,]  0.143093618 -0.128972864  0.123444664 -0.019143156 -0.11418887</span>
<span class="code-line">[25,] -0.205453327 -0.033657825  0.119336213  0.081793613  0.28913437</span>
<span class="code-line">[26,]  0.097058514  0.133415650 -0.078565994  0.104596997 -0.09782283</span>
<span class="code-line">[27,]  0.365188226  0.043768308  0.099117058 -0.151138200 -0.20757132</span>
<span class="code-line">[28,] -0.384396969 -0.002547885 -0.159472994  0.010463457 -0.16205344</span>
<span class="code-line">[29,]  0.340774417 -0.014595412  0.127879953  0.002344641  0.02043592</span>
<span class="code-line">[30,]  0.028675413  0.114443712 -0.314578858 -0.295229956  0.17182108</span>
<span class="code-line">[31,] -0.011902622 -0.022681539 -0.041731986 -0.117132033  0.03326848</span>
<span class="code-line">[32,] -0.034003731 -0.064140786 -0.017126434  0.086641437  0.04895536</span>
<span class="code-line">[33,] -0.037773697  0.062852864  0.290491093 -0.240911252 -0.12450871</span>
<span class="code-line">[34,] -0.110736030  0.159814187  0.148828191 -0.091103479 -0.17246856</span>
<span class="code-line">[35,] -0.010102437  0.116545622 -0.184509835 -0.191199381  0.25110941</span>
<span class="code-line">[36,]  0.104349502 -0.015312409 -0.185172637  0.344170979  0.38552672</span>
<span class="code-line">[37,]  0.153071645  0.085194208  0.051589122  0.079933406 -0.14949080</span>
<span class="code-line">[38,]  0.060750415 -0.184128812 -0.106072097 -0.032739581  0.05910251</span>
<span class="code-line">[39,]  0.039153548 -0.153044940  0.133559195  0.160187270 -0.18260341</span>
<span class="code-line">[40,] -0.104029738 -0.077892510 -0.090346363 -0.059860307 -0.12365209</span>
<span class="code-line">              [,21]        [,22]       [,23]         [,24]        [,25]</span>
<span class="code-line"> [1,]  0.0579591307 -0.039740203 -0.10091982  0.1942841006  0.017966313</span>
<span class="code-line"> [2,] -0.0638452228  0.070692539  0.09664590  0.0291234192 -0.237945831</span>
<span class="code-line"> [3,]  0.1526559936  0.235933268 -0.24627306  0.1702130370  0.192466940</span>
<span class="code-line"> [4,] -0.0641744493 -0.001977639 -0.14865577  0.2282540880 -0.129454561</span>
<span class="code-line"> [5,]  0.0123882163  0.045289069  0.12488489 -0.0217560204  0.159394251</span>
<span class="code-line"> [6,]  0.0080916563 -0.338532991  0.04980414 -0.1720542752 -0.210683128</span>
<span class="code-line"> [7,]  0.1495902663  0.022563021  0.11404634  0.0479750596 -0.205737785</span>
<span class="code-line"> [8,] -0.0612781453 -0.006327463 -0.07999877  0.0008769464  0.166442897</span>
<span class="code-line"> [9,]  0.1518713102 -0.040987409 -0.18438643  0.1862782857 -0.007132275</span>
<span class="code-line">[10,] -0.1582276401 -0.031648033  0.09735055 -0.0766521956 -0.017719865</span>
<span class="code-line">[11,] -0.1252435964  0.223314076  0.02084079 -0.1231135815 -0.033634954</span>
<span class="code-line">[12,] -0.0913193836  0.104541665 -0.10844077 -0.3977057666  0.117024755</span>
<span class="code-line">[13,] -0.0744130173 -0.187769187  0.15818975  0.1559301723 -0.105192037</span>
<span class="code-line">[14,] -0.0471932126  0.020402393  0.12184407 -0.2430700333 -0.034092675</span>
<span class="code-line">[15,] -0.1872153412  0.178775678  0.09625183  0.0906899853  0.045560988</span>
<span class="code-line">[16,] -0.1351770406  0.165135611  0.15147667 -0.0224691708 -0.191520189</span>
<span class="code-line">[17,] -0.2144382862  0.177457606 -0.07553808  0.2853070234 -0.098217811</span>
<span class="code-line">[18,] -0.1520261334  0.121036994  0.23578525 -0.1815775576 -0.006242295</span>
<span class="code-line">[19,]  0.0332746045 -0.166374126 -0.17678741 -0.2939967800  0.376315640</span>
<span class="code-line">[20,] -0.0327359512 -0.062531184 -0.08401832  0.0557358929 -0.110850195</span>
<span class="code-line">[21,]  0.3460286380 -0.043548404  0.14738032 -0.0996739946 -0.105962853</span>
<span class="code-line">[22,]  0.0701799548 -0.159000336  0.31439556  0.1608637114  0.131615590</span>
<span class="code-line">[23,] -0.1468553043  0.166848819 -0.27498667  0.0586445156  0.116787012</span>
<span class="code-line">[24,] -0.0504228670 -0.050956329 -0.27627230 -0.0379906188 -0.144433418</span>
<span class="code-line">[25,]  0.3623962226  0.295260837  0.07526132 -0.0513925862  0.043945684</span>
<span class="code-line">[26,] -0.0156444481  0.204781606 -0.08037619 -0.1463832699 -0.216720488</span>
<span class="code-line">[27,] -0.0065058861 -0.221096313  0.01618676  0.0867471177  0.203277324</span>
<span class="code-line">[28,] -0.0819656322 -0.344715215 -0.03475543  0.0547417753  0.002297038</span>
<span class="code-line">[29,]  0.1670186892 -0.090315359 -0.03359917 -0.0570117185  0.017178761</span>
<span class="code-line">[30,] -0.0200453377  0.009832483  0.00737685  0.1035461618 -0.171514697</span>
<span class="code-line">[31,] -0.0345718072 -0.077443600 -0.12451561 -0.0538730712 -0.073891870</span>
<span class="code-line">[32,]  0.0847929955  0.048745471  0.06597151  0.1345667287  0.055963832</span>
<span class="code-line">[33,]  0.0565325365  0.321406605  0.24899408  0.1211265296  0.124184236</span>
<span class="code-line">[34,]  0.2464053688 -0.198852184 -0.16066676 -0.0363620636 -0.298624927</span>
<span class="code-line">[35,] -0.4500559845 -0.087885134 -0.20970486 -0.0359137120 -0.032942481</span>
<span class="code-line">[36,]  0.0005874973 -0.179565570  0.21562362 -0.0862699484  0.363402967</span>
<span class="code-line">[37,]  0.2039633972 -0.044234455 -0.05975796  0.3037845599  0.099813485</span>
<span class="code-line">[38,]  0.1312358804  0.053820235  0.09237845 -0.2532880646 -0.258180296</span>
<span class="code-line">[39,] -0.2691440461 -0.034644630  0.26806242  0.1180734442  0.053118305</span>
<span class="code-line">[40,]  0.1130941349  0.156533276 -0.25197774 -0.1806635672  0.104414035</span>
<span class="code-line">             [,26]         [,27]       [,28]        [,29]         [,30]</span>
<span class="code-line"> [1,]  0.039467582 -0.2840391273 -0.20725439  0.396697101  0.1531317689</span>
<span class="code-line"> [2,] -0.164260013 -0.1757583326  0.01613049 -0.179027156 -0.1447680728</span>
<span class="code-line"> [3,]  0.088154522  0.3573144007 -0.14554754  0.156740995  0.0348901311</span>
<span class="code-line"> [4,]  0.257136663 -0.2418886890  0.16263086 -0.300708024 -0.0031598058</span>
<span class="code-line"> [5,] -0.268030695 -0.3908587166  0.06133227 -0.109763843 -0.0175360338</span>
<span class="code-line"> [6,]  0.154210185  0.1218706436  0.06392335  0.173228391 -0.0696772699</span>
<span class="code-line"> [7,] -0.049756894 -0.0928175534  0.01610963  0.180927225 -0.2311703627</span>
<span class="code-line"> [8,] -0.067008171  0.2449493781 -0.16530560 -0.139570319  0.0924849396</span>
<span class="code-line"> [9,]  0.171561158 -0.1706096878  0.06632165  0.029143109 -0.0406118330</span>
<span class="code-line">[10,] -0.151751824 -0.0806612466  0.10251604 -0.066704537 -0.0936753047</span>
<span class="code-line">[11,]  0.127085186 -0.0461831830 -0.10114725  0.061750919 -0.0656478754</span>
<span class="code-line">[12,]  0.183492246 -0.1906804528  0.27356471  0.093910554  0.3154096903</span>
<span class="code-line">[13,] -0.091485207  0.0828372705 -0.06444023  0.051063898 -0.2284318788</span>
<span class="code-line">[14,] -0.080831262  0.3892270526  0.11937920 -0.320671538 -0.0372443758</span>
<span class="code-line">[15,] -0.161500881  0.1329170244 -0.13223510  0.037488170  0.0691655905</span>
<span class="code-line">[16,]  0.022149174  0.0005670124  0.27821076  0.093749595  0.3740361205</span>
<span class="code-line">[17,]  0.094636341 -0.0600021737  0.02429151 -0.346922837  0.0788245119</span>
<span class="code-line">[18,]  0.235973241 -0.0730378063 -0.17793189  0.089854503 -0.0617591477</span>
<span class="code-line">[19,]  0.009347526 -0.1362944664  0.01096128  0.057720156 -0.2619759050</span>
<span class="code-line">[20,] -0.271918209  0.1984913894 -0.08552990  0.030609516  0.1648268665</span>
<span class="code-line">[21,] -0.071109758 -0.0147363652 -0.12442636  0.076172377  0.0285799179</span>
<span class="code-line">[22,]  0.308506643  0.0800603849  0.04222100 -0.163563630  0.0004563894</span>
<span class="code-line">[23,]  0.023351604  0.0758914792 -0.16851410  0.009459511  0.0186627422</span>
<span class="code-line">[24,] -0.223907224 -0.1472371042  0.19168939  0.039139536 -0.0734021999</span>
<span class="code-line">[25,] -0.190027999 -0.1764417426 -0.29011323 -0.171285663  0.0720009315</span>
<span class="code-line">[26,] -0.142975531  0.0445287376  0.09381860  0.046245505 -0.2877081566</span>
<span class="code-line">[27,]  0.097024422 -0.0756465548 -0.03746653 -0.057450427  0.1295984615</span>
<span class="code-line">[28,] -0.023809170  0.0256009249  0.04767220 -0.053399281  0.3204161680</span>
<span class="code-line">[29,]  0.191606867 -0.0860759422 -0.14228592 -0.048424723 -0.0777383369</span>
<span class="code-line">[30,]  0.141634385  0.1870734867  0.24219103  0.333110008 -0.1477013237</span>
<span class="code-line">[31,]  0.206258329 -0.0283817646 -0.16758729 -0.199333304 -0.2312462101</span>
<span class="code-line">[32,] -0.182214064 -0.0003276381  0.17397209  0.212858478  0.2092743862</span>
<span class="code-line">[33,]  0.126038496 -0.0350938799  0.20003242  0.039024884 -0.0558366229</span>
<span class="code-line">[34,] -0.030164624  0.0213708929 -0.08269342 -0.183877174  0.2181854614</span>
<span class="code-line">[35,] -0.168249868 -0.0934655884 -0.20540797  0.112733677 -0.0535145938</span>
<span class="code-line">[36,] -0.224599291  0.0157637932  0.11664186 -0.041537521 -0.0033759093</span>
<span class="code-line">[37,] -0.142615264  0.0704992312  0.22991204 -0.001423657 -0.1830070638</span>
<span class="code-line">[38,]  0.096976806  0.0109747239 -0.23421904  0.008052716  0.1780495448</span>
<span class="code-line">[39,]  0.126995085 -0.0206763301 -0.21606228  0.111897033 -0.0555356867</span>
<span class="code-line">[40,]  0.145516718  0.1265183082  0.21774350 -0.050439024 -0.0890523114</span>
<span class="code-line">            [,31]        [,32]        [,33]        [,34]        [,35]</span>
<span class="code-line"> [1,] -0.03929150  0.039036327  0.234407800 -0.195661904 -0.052521020</span>
<span class="code-line"> [2,] -0.11410580  0.267195808 -0.257214476  0.084579620  0.006368062</span>
<span class="code-line"> [3,]  0.30324362 -0.249997605 -0.011329700  0.048299141 -0.029114530</span>
<span class="code-line"> [4,] -0.02117228 -0.178660580  0.015950290 -0.006459983  0.166166465</span>
<span class="code-line"> [5,] -0.06221996 -0.049952025  0.123985606  0.033326042  0.296758825</span>
<span class="code-line"> [6,] -0.16322995  0.129372845  0.012958367 -0.090902394 -0.224304742</span>
<span class="code-line"> [7,]  0.13541207  0.050125630 -0.084493015  0.012540594 -0.064918089</span>
<span class="code-line"> [8,] -0.02413885  0.073915394  0.135172816 -0.081482425  0.159089202</span>
<span class="code-line"> [9,] -0.15934722 -0.027654017 -0.175117527  0.214110801  0.073510123</span>
<span class="code-line">[10,]  0.16674656 -0.081900480  0.158700561 -0.194846626 -0.006962392</span>
<span class="code-line">[11,] -0.29966159 -0.310919518 -0.123272047  0.073955519 -0.035311762</span>
<span class="code-line">[12,]  0.24948402  0.151517812  0.070383035  0.107911339  0.014310545</span>
<span class="code-line">[13,]  0.25068044  0.060387453 -0.018235376 -0.199337687  0.192125531</span>
<span class="code-line">[14,]  0.02726727 -0.076621955 -0.241997116  0.194677831  0.093976893</span>
<span class="code-line">[15,] -0.07277121  0.286385315  0.121980271 -0.075536247 -0.403937140</span>
<span class="code-line">[16,]  0.32301848 -0.008218561  0.100811942  0.058990914  0.093713908</span>
<span class="code-line">[17,] -0.14102919  0.193622173  0.046182068 -0.100545741 -0.322998914</span>
<span class="code-line">[18,] -0.10973798 -0.208461930 -0.111034664  0.009747904 -0.037118914</span>
<span class="code-line">[19,] -0.01586770  0.197562797  0.001992001  0.077403368  0.039732073</span>
<span class="code-line">[20,] -0.12926193 -0.148019368  0.028648592 -0.018656041  0.141698662</span>
<span class="code-line">[21,] -0.06016887 -0.129882691 -0.063795887  0.110965005 -0.098773847</span>
<span class="code-line">[22,]  0.19986776  0.069895464 -0.025077302 -0.311423521  0.054581792</span>
<span class="code-line">[23,] -0.23085512  0.230768851  0.054992877  0.077971457  0.313556372</span>
<span class="code-line">[24,]  0.13299918 -0.271090970 -0.032899761  0.027187211 -0.338681646</span>
<span class="code-line">[25,]  0.22191455  0.057180120 -0.088281763  0.068086131 -0.176956001</span>
<span class="code-line">[26,] -0.05263206 -0.126225701  0.270004872 -0.210473938  0.174811705</span>
<span class="code-line">[27,]  0.11276382  0.095453625 -0.302387164  0.161153966 -0.117786040</span>
<span class="code-line">[28,] -0.20507650 -0.368068524  0.021768376 -0.045942986 -0.112949390</span>
<span class="code-line">[29,]  0.02978577 -0.054301412 -0.113058660 -0.337538715  0.124733149</span>
<span class="code-line">[30,] -0.06848216  0.246895579  0.050212859  0.265827875  0.049590355</span>
<span class="code-line">[31,]  0.14714742 -0.057497700  0.300217752  0.216599437 -0.137425732</span>
<span class="code-line">[32,] -0.16679376  0.076976793 -0.350715085 -0.139544967  0.151896964</span>
<span class="code-line">[33,] -0.09636997 -0.109962092  0.049938161 -0.077857333  0.006201032</span>
<span class="code-line">[34,]  0.03193342  0.169985320  0.189404721  0.085796054  0.161790917</span>
<span class="code-line">[35,]  0.26575723 -0.024247239 -0.360733154 -0.111191978  0.033302669</span>
<span class="code-line">[36,] -0.18351059 -0.033070204  0.240490321  0.042284383 -0.140254105</span>
<span class="code-line">[37,]  0.07640285 -0.063540523  0.032207550  0.176138205 -0.046229884</span>
<span class="code-line">[38,] -0.09016278  0.052711591  0.009825214 -0.044902345  0.046250358</span>
<span class="code-line">[39,]  0.07322608 -0.088340720  0.102709992  0.326530028  0.089667710</span>
<span class="code-line">[40,] -0.07695258  0.046396648 -0.120292879 -0.346951521 -0.123820962</span>
<span class="code-line">             [,36]         [,37]         [,38]        [,39]        [,40]</span>
<span class="code-line"> [1,]  0.073202913 -1.077204e-01  0.0345802185  0.104614721 -0.076167249</span>
<span class="code-line"> [2,]  0.198232783 -1.040005e-01 -0.0192821416  0.089306752 -0.166691951</span>
<span class="code-line"> [3,] -0.041753424  2.922844e-01 -0.0391627344  0.011321615 -0.061030741</span>
<span class="code-line"> [4,] -0.236022109 -9.898293e-05 -0.0247717853  0.174975976  0.171245715</span>
<span class="code-line"> [5,]  0.154228208  1.732424e-01  0.0095106520 -0.103838575 -0.047414803</span>
<span class="code-line"> [6,] -0.030888773 -2.512679e-01 -0.0226852931 -0.059528257  0.182444231</span>
<span class="code-line"> [7,]  0.272912634  4.014193e-01 -0.2097037990 -0.090019612  0.101995852</span>
<span class="code-line"> [8,] -0.106243548 -4.020676e-01  0.1897921825  0.069072795 -0.021338197</span>
<span class="code-line"> [9,]  0.083475574 -1.653481e-01  0.0001245743 -0.182592710 -0.339590220</span>
<span class="code-line">[10,] -0.164952791  1.813943e-01  0.0765198942  0.167280468  0.333154785</span>
<span class="code-line">[11,]  0.070571458  1.127840e-01  0.3575193052 -0.005139065  0.042449463</span>
<span class="code-line">[12,]  0.019442145 -3.542866e-02 -0.1302258754 -0.134030914 -0.009336972</span>
<span class="code-line">[13,]  0.037861033 -2.590358e-02  0.0572470569  0.272662399  0.089389713</span>
<span class="code-line">[14,] -0.051600053  1.955630e-02 -0.1280507899 -0.086309797 -0.044150474</span>
<span class="code-line">[15,] -0.035926493  1.104654e-01 -0.0269238696 -0.284802531 -0.023849878</span>
<span class="code-line">[16,]  0.204555192 -1.368484e-01  0.0733278916 -0.048265671  0.046456671</span>
<span class="code-line">[17,] -0.159228113  1.487147e-01 -0.1197965511 -0.036850825  0.009388998</span>
<span class="code-line">[18,]  0.080428443  1.173415e-02  0.3975204223 -0.120504237  0.155859971</span>
<span class="code-line">[19,] -0.275776537  1.430277e-01 -0.0827326452  0.064815790 -0.097957552</span>
<span class="code-line">[20,]  0.218625473 -1.149067e-02 -0.1009829827  0.210638253 -0.227552606</span>
<span class="code-line">[21,] -0.066242087 -2.031058e-01 -0.3168660745  0.010797991  0.285122314</span>
<span class="code-line">[22,]  0.068359850  3.288600e-02  0.1293247779 -0.141740559 -0.335193371</span>
<span class="code-line">[23,]  0.192684257  3.243411e-02 -0.1169638845  0.009704912  0.346784585</span>
<span class="code-line">[24,] -0.217948709 -5.095151e-02  0.1585513406  0.144993941 -0.195741663</span>
<span class="code-line">[25,] -0.073696928 -2.514598e-01  0.0975879256  0.069962507  0.064542102</span>
<span class="code-line">[26,] -0.109721694 -7.949213e-02 -0.0800515099 -0.223622600 -0.175548323</span>
<span class="code-line">[27,]  0.124446753  1.114992e-01  0.0738133630  0.280378745  0.098140781</span>
<span class="code-line">[28,]  0.111534411  8.447453e-02 -0.2510605616 -0.104287972  0.055827450</span>
<span class="code-line">[29,] -0.071723658 -1.946262e-02 -0.1319386672 -0.306431211  0.053495476</span>
<span class="code-line">[30,] -0.094872498  3.238229e-02  0.1370211150  0.158518104 -0.053164040</span>
<span class="code-line">[31,]  0.269372698 -7.619546e-02 -0.0600342023  0.011702940 -0.071291291</span>
<span class="code-line">[32,] -0.317314799  6.019473e-02  0.0780420915 -0.102948282  0.025152779</span>
<span class="code-line">[33,]  0.005330222 -1.848071e-01 -0.2660664678  0.260488877 -0.027685999</span>
<span class="code-line">[34,] -0.073545879  1.827986e-01  0.3098538087 -0.114334372  0.091532208</span>
<span class="code-line">[35,]  0.079847028 -1.366967e-01 -0.0549126282 -0.132991312 -0.045998504</span>
<span class="code-line">[36,]  0.141395354  5.203679e-02  0.1255613006  0.075523547 -0.103298166</span>
<span class="code-line">[37,]  0.074960387 -8.705884e-02  0.1962094947 -0.310635257  0.190894168</span>
<span class="code-line">[38,] -0.131712458  2.801683e-01 -0.0300976347  0.235480701 -0.288824106</span>
<span class="code-line">[39,] -0.293872107 -6.678347e-02 -0.1953074869 -0.040274101 -0.048995780</span>
<span class="code-line">[40,]  0.278086759 -4.131369e-02  0.0221214957  0.201924497  0.069240683</span>
</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will then use this new matrix to compute our eigenvalues and vectors, and subsequently create our new loadings matrix. In a similar way, we will determine the specific variances and communalities, and then rotate the loadings matrix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[120]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>eigenvectors2 <span class="o">&lt;-</span> <span class="kp">eigen</span><span class="p">(</span>New<span class="p">)</span><span class="o">$</span>vectors<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">]</span></span>
<span class="code-line">loadings2 <span class="o">&lt;-</span> eigenvectors2 <span class="o">%*%</span> <span class="p">(</span><span class="kp">diag</span><span class="p">(</span><span class="m">1</span><span class="p">,</span>nrow <span class="o">=</span> <span class="m">9</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="kp">eigen</span><span class="p">(</span>New<span class="p">)</span><span class="o">$</span>values<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">])</span><span class="o">^</span><span class="m">.5</span><span class="p">)</span></span>
<span class="code-line">hisq2 <span class="o">&lt;-</span> <span class="kp">apply</span><span class="p">(</span><span class="kp">apply</span><span class="p">(</span>loadings2<span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> x<span class="o">^</span><span class="m">2</span><span class="p">),</span> <span class="m">1</span><span class="p">,</span> <span class="kp">sum</span><span class="p">)</span></span>
<span class="code-line">psi <span class="o">&lt;-</span> <span class="kp">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">40</span><span class="p">)</span> <span class="o">-</span> hisq</span>
<span class="code-line">loadings2rot <span class="o">&lt;-</span> <span class="kp">print</span><span class="p">(</span>varimax<span class="p">(</span>loadings2<span class="p">)</span><span class="o">$</span>loadings<span class="p">,</span>cutoff <span class="o">=</span> <span class="m">.35</span><span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="code-line"></span>
<span class="code-line">Loadings:</span>
<span class="code-line">      [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  </span>
<span class="code-line"> [1,]                                                   0.713       </span>
<span class="code-line"> [2,] -0.635                                                        </span>
<span class="code-line"> [3,] -0.662                                                        </span>
<span class="code-line"> [4,]                                           -0.574              </span>
<span class="code-line"> [5,] -0.549                                                        </span>
<span class="code-line"> [6,] -0.554                                                        </span>
<span class="code-line"> [7,]  0.500                                                        </span>
<span class="code-line"> [8,]                                                               </span>
<span class="code-line"> [9,]                                                          0.368</span>
<span class="code-line">[10,]                                                               </span>
<span class="code-line">[11,]                              0.636                            </span>
<span class="code-line">[12,]                              0.729                            </span>
<span class="code-line">[13,]                              0.460                            </span>
<span class="code-line">[14,]                                                   0.717       </span>
<span class="code-line">[15,]                                           -0.620              </span>
<span class="code-line">[16,]                             -0.739                            </span>
<span class="code-line">[17,]                             -0.360         0.505              </span>
<span class="code-line">[18,]                             -0.650                            </span>
<span class="code-line">[19,]                             -0.688                            </span>
<span class="code-line">[20,]                                                          0.371</span>
<span class="code-line">[21,]                      -0.466                                   </span>
<span class="code-line">[22,]                      -0.625                                   </span>
<span class="code-line">[23,]                      -0.627                                   </span>
<span class="code-line">[24,]                      -0.597                                   </span>
<span class="code-line">[25,]                0.649                                          </span>
<span class="code-line">[26,]                0.754                                          </span>
<span class="code-line">[27,]                0.732                                          </span>
<span class="code-line">[28,]                0.635                                          </span>
<span class="code-line">[29,]                0.362                                          </span>
<span class="code-line">[30,]                0.578                                          </span>
<span class="code-line">[31,]                                     0.727                     </span>
<span class="code-line">[32,]        -0.375                       0.730                     </span>
<span class="code-line">[33,]        -0.609                                                 </span>
<span class="code-line">[34,]        -0.711                                                 </span>
<span class="code-line">[35,]        -0.686                                                 </span>
<span class="code-line">[36,]        -0.588                                                 </span>
<span class="code-line">[37,]        -0.582                                                 </span>
<span class="code-line">[38,]        -0.581                                                 </span>
<span class="code-line">[39,]        -0.631                                                 </span>
<span class="code-line">[40,]        -0.651                                                 </span>
<span class="code-line"></span>
<span class="code-line">                [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]</span>
<span class="code-line">SS loadings    2.508 3.963 3.089 1.957 3.511 1.279 1.401 1.402 0.995</span>
<span class="code-line">Proportion Var 0.063 0.099 0.077 0.049 0.088 0.032 0.035 0.035 0.025</span>
<span class="code-line">Cumulative Var 0.063 0.162 0.239 0.288 0.376 0.408 0.443 0.478 0.503</span>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see a similar pattern to that of the principal component method above. The variance is not as well explained using this method, but when we perform the next step (using iterations of the above method), it will become a better representation of the data. Again, we can see similar relationships between the first five factors and the variables, but it is not all encompassing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Principal-Factors-Method">Principal Factors Method<a class="anchor-link" href="#Principal-Factors-Method">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will now use a similar method to above, but iterate it each time and update it with the new initial squared loadings. That is, when we calculate the new $h^2_i$ values, we do the iterate until it converges. For completeness, I perform the process 100 times below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[121]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>hinit2 <span class="o">&lt;-</span> <span class="m">1</span> <span class="o">-</span> <span class="m">1</span><span class="o">/</span><span class="kp">diag</span><span class="p">(</span><span class="kp">solve</span><span class="p">(</span>R<span class="p">))</span></span>
<span class="code-line"><span class="kr">for</span> <span class="p">(</span>i <span class="kr">in</span> <span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">)</span> <span class="p">{</span></span>
<span class="code-line">  New2 <span class="o">&lt;-</span> R <span class="o">-</span> <span class="kp">diag</span><span class="p">((</span><span class="m">1</span> <span class="o">-</span> hinit2<span class="p">),</span>nrow <span class="o">=</span> <span class="m">40</span><span class="p">)</span></span>
<span class="code-line">  eigenvectors3 <span class="o">&lt;-</span> <span class="kp">eigen</span><span class="p">(</span>New2<span class="p">)</span><span class="o">$</span>vectors<span class="p">[,</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">]</span></span>
<span class="code-line">  loadings3 <span class="o">&lt;-</span> eigenvectors3 <span class="o">%*%</span> <span class="p">(</span><span class="kp">diag</span><span class="p">(</span><span class="m">1</span><span class="p">,</span>nrow <span class="o">=</span> <span class="m">9</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="kp">eigen</span><span class="p">(</span>New2<span class="p">)</span><span class="o">$</span>values<span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">9</span><span class="p">])</span><span class="o">^</span><span class="m">.5</span><span class="p">)</span></span>
<span class="code-line">  hinit2 <span class="o">&lt;-</span> <span class="kp">apply</span><span class="p">(</span><span class="kp">apply</span><span class="p">(</span>loadings3<span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="kr">function</span><span class="p">(</span>x<span class="p">)</span> x<span class="o">^</span><span class="m">2</span><span class="p">),</span> <span class="m">1</span><span class="p">,</span> <span class="kp">sum</span><span class="p">)</span></span>
<span class="code-line"><span class="p">}</span></span>
<span class="code-line">hisq3 <span class="o">=</span> hinit2</span>
<span class="code-line">loadings3</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<table>
<tbody>
	<tr><td>-0.6336865   </td><td> 0.129510159 </td><td> 0.342334002 </td><td>-0.038023630 </td><td>-0.298855098 </td><td>-0.106461077 </td><td> 0.004012698 </td><td> 0.4158842813</td><td>-0.135351810 </td></tr>
	<tr><td>-0.6502007   </td><td>-0.125780976 </td><td> 0.094612300 </td><td> 0.224797379 </td><td>-0.119888358 </td><td> 0.183656923 </td><td>-0.219213531 </td><td>-0.0853302346</td><td>-0.005595223 </td></tr>
	<tr><td>-0.6291168   </td><td>-0.190720725 </td><td> 0.040105304 </td><td> 0.241398639 </td><td>-0.146635370 </td><td> 0.173629622 </td><td>-0.280919773 </td><td>-0.1084499357</td><td>-0.062148373 </td></tr>
	<tr><td>-0.5908644   </td><td> 0.002653042 </td><td> 0.226497992 </td><td> 0.040334761 </td><td>-0.168819189 </td><td> 0.045883497 </td><td> 0.264423699 </td><td>-0.2146298828</td><td>-0.139428187 </td></tr>
	<tr><td>-0.5403052   </td><td>-0.069776558 </td><td>-0.046541079 </td><td> 0.088730496 </td><td>-0.184540364 </td><td> 0.153429789 </td><td>-0.241690414 </td><td>-0.0715201038</td><td>-0.043392862 </td></tr>
	<tr><td>-0.5383186   </td><td>-0.187930368 </td><td>-0.027921243 </td><td> 0.127559024 </td><td>-0.182887463 </td><td> 0.135235784 </td><td>-0.224915024 </td><td>-0.0776985624</td><td> 0.061374907 </td></tr>
	<tr><td> 0.5427781   </td><td>-0.094736535 </td><td> 0.006660377 </td><td>-0.381431291 </td><td> 0.087614276 </td><td>-0.135960537 </td><td> 0.081417002 </td><td>-0.0316562359</td><td>-0.128293688 </td></tr>
	<tr><td> 0.3964542   </td><td>-0.078623909 </td><td> 0.051902429 </td><td>-0.326084495 </td><td> 0.044529009 </td><td>-0.049494223 </td><td>-0.071142504 </td><td>-0.0747379454</td><td>-0.122691640 </td></tr>
	<tr><td> 0.3450905   </td><td>-0.102407731 </td><td> 0.114416779 </td><td>-0.188480115 </td><td> 0.024731098 </td><td>-0.224110839 </td><td>-0.036705193 </td><td>-0.0697731205</td><td>-0.189402920 </td></tr>
	<tr><td> 0.2642319   </td><td>-0.161843476 </td><td> 0.115746565 </td><td>-0.166040194 </td><td> 0.068724964 </td><td>-0.112025835 </td><td>-0.220779261 </td><td> 0.0120029682</td><td>-0.083149450 </td></tr>
	<tr><td>-0.5066480   </td><td> 0.260520630 </td><td> 0.262478846 </td><td>-0.290267019 </td><td> 0.080632270 </td><td>-0.054378180 </td><td>-0.154096849 </td><td> 0.0042190639</td><td> 0.110051557 </td></tr>
	<tr><td>-0.6545789   </td><td> 0.074217208 </td><td> 0.290576461 </td><td>-0.232408027 </td><td> 0.333829685 </td><td>-0.097500210 </td><td>-0.134497695 </td><td>-0.0869242753</td><td>-0.052456320 </td></tr>
	<tr><td>-0.5774633   </td><td> 0.071893130 </td><td> 0.217354904 </td><td>-0.150535446 </td><td> 0.064325771 </td><td> 0.004985414 </td><td> 0.017801279 </td><td>-0.0225716598</td><td>-0.066759174 </td></tr>
	<tr><td>-0.6108433   </td><td> 0.105405443 </td><td> 0.349790991 </td><td>-0.163838651 </td><td>-0.300910254 </td><td>-0.103460721 </td><td> 0.045917678 </td><td> 0.3512154313</td><td>-0.173798335 </td></tr>
	<tr><td>-0.6225742   </td><td> 0.015818600 </td><td> 0.276931601 </td><td>-0.112348412 </td><td>-0.156714309 </td><td> 0.086461623 </td><td> 0.283676355 </td><td>-0.3076787991</td><td>-0.195371434 </td></tr>
	<tr><td> 0.5917724   </td><td>-0.235014489 </td><td>-0.255368899 </td><td> 0.006650969 </td><td>-0.424432473 </td><td> 0.116896158 </td><td> 0.026640961 </td><td> 0.0407768581</td><td>-0.024693231 </td></tr>
	<tr><td> 0.5279568   </td><td>-0.274680621 </td><td>-0.187219522 </td><td>-0.101549741 </td><td>-0.006358423 </td><td>-0.131489718 </td><td>-0.307936400 </td><td> 0.2109326646</td><td>-0.053543527 </td></tr>
	<tr><td> 0.4940006   </td><td>-0.403112322 </td><td>-0.229288093 </td><td> 0.053826480 </td><td>-0.149052853 </td><td> 0.058937557 </td><td> 0.079736682 </td><td>-0.0232532548</td><td>-0.217180572 </td></tr>
	<tr><td> 0.5679135   </td><td>-0.227968636 </td><td>-0.220179480 </td><td>-0.057163025 </td><td>-0.401467891 </td><td> 0.088122223 </td><td> 0.046636312 </td><td> 0.0008630064</td><td>-0.070659460 </td></tr>
	<tr><td> 0.4988083   </td><td>-0.198023532 </td><td>-0.098685578 </td><td>-0.087517556 </td><td> 0.037379190 </td><td> 0.007864417 </td><td>-0.112906050 </td><td>-0.1261029356</td><td>-0.214195713 </td></tr>
	<tr><td>-0.3811688   </td><td> 0.142742173 </td><td>-0.307097139 </td><td>-0.296217542 </td><td>-0.089586852 </td><td>-0.090118823 </td><td>-0.097821370 </td><td>-0.0685454038</td><td>-0.104261189 </td></tr>
	<tr><td>-0.3269154   </td><td> 0.136263759 </td><td>-0.233410980 </td><td>-0.474756404 </td><td>-0.138868985 </td><td> 0.016461123 </td><td>-0.091602431 </td><td>-0.0939518707</td><td> 0.145870493 </td></tr>
	<tr><td>-0.3203851   </td><td>-0.030166568 </td><td>-0.245768083 </td><td>-0.487202590 </td><td>-0.205275498 </td><td> 0.037694710 </td><td> 0.008704829 </td><td>-0.0416043160</td><td> 0.183271179 </td></tr>
	<tr><td>-0.3520775   </td><td> 0.095211884 </td><td>-0.336299557 </td><td>-0.428447845 </td><td>-0.149642422 </td><td> 0.056262149 </td><td>-0.029658115 </td><td>-0.0706184986</td><td> 0.004097048 </td></tr>
	<tr><td> 0.3651857   </td><td>-0.412331552 </td><td> 0.368944213 </td><td>-0.117631028 </td><td>-0.026511108 </td><td> 0.073337166 </td><td>-0.028056229 </td><td>-0.0530975791</td><td> 0.108645037 </td></tr>
	<tr><td> 0.3703239   </td><td>-0.479742202 </td><td> 0.476104604 </td><td>-0.096721625 </td><td>-0.057825485 </td><td> 0.071261480 </td><td> 0.081462466 </td><td> 0.0360994696</td><td> 0.185563020 </td></tr>
	<tr><td> 0.3699674   </td><td>-0.421161776 </td><td> 0.480916968 </td><td>-0.124121065 </td><td>-0.078094691 </td><td> 0.074598867 </td><td> 0.047942227 </td><td> 0.0369657600</td><td> 0.117259474 </td></tr>
	<tr><td> 0.2463836   </td><td>-0.441672543 </td><td> 0.339399682 </td><td>-0.114323666 </td><td>-0.089602239 </td><td> 0.133700006 </td><td> 0.015520059 </td><td>-0.0628408544</td><td> 0.141265238 </td></tr>
	<tr><td> 0.1920995   </td><td>-0.302887469 </td><td> 0.214310995 </td><td> 0.008630304 </td><td>-0.010576560 </td><td> 0.051437400 </td><td>-0.149282018 </td><td>-0.0246684699</td><td>-0.103773907 </td></tr>
	<tr><td> 0.3177906   </td><td>-0.361924210 </td><td> 0.349591276 </td><td>-0.146596477 </td><td>-0.020838225 </td><td> 0.053217935 </td><td>-0.144213470 </td><td>-0.0810473995</td><td>-0.010744364 </td></tr>
	<tr><td>-0.4021482   </td><td>-0.351959582 </td><td>-0.201011400 </td><td>-0.176729613 </td><td> 0.263077182 </td><td> 0.444156811 </td><td> 0.088841988 </td><td> 0.1869857115</td><td>-0.057679182 </td></tr>
	<tr><td>-0.4265001   </td><td>-0.447290807 </td><td>-0.237762488 </td><td>-0.201792383 </td><td> 0.345792026 </td><td> 0.460313960 </td><td> 0.104878412 </td><td> 0.2129963667</td><td>-0.087767881 </td></tr>
	<tr><td>-0.3041925   </td><td>-0.513476732 </td><td>-0.171039751 </td><td>-0.023635033 </td><td> 0.034697829 </td><td>-0.114633165 </td><td> 0.097987560 </td><td>-0.0090573562</td><td> 0.030682748 </td></tr>
	<tr><td>-0.4423233   </td><td>-0.542324765 </td><td>-0.120056095 </td><td> 0.074356156 </td><td> 0.095296276 </td><td>-0.199403042 </td><td> 0.027929674 </td><td>-0.0202432667</td><td>-0.069660173 </td></tr>
	<tr><td>-0.3607007   </td><td>-0.500592080 </td><td>-0.174513283 </td><td> 0.046947376 </td><td> 0.037358826 </td><td>-0.267136034 </td><td> 0.074407466 </td><td>-0.0093759739</td><td> 0.080036468 </td></tr>
	<tr><td>-0.4350645   </td><td>-0.446475795 </td><td> 0.030254396 </td><td>-0.010627044 </td><td> 0.101759433 </td><td>-0.174325196 </td><td> 0.029505768 </td><td>-0.0120425589</td><td>-0.050011012 </td></tr>
	<tr><td>-0.4599254   </td><td>-0.329350786 </td><td>-0.137864380 </td><td> 0.028873337 </td><td>-0.110769493 </td><td>-0.226812362 </td><td> 0.044696510 </td><td> 0.0707773605</td><td> 0.146141059 </td></tr>
	<tr><td>-0.4702438   </td><td>-0.336331662 </td><td>-0.199380424 </td><td>-0.044648421 </td><td>-0.119115633 </td><td>-0.177227803 </td><td> 0.122626669 </td><td> 0.0060156884</td><td> 0.175156554 </td></tr>
	<tr><td>-0.3655473   </td><td>-0.514596691 </td><td>-0.078419787 </td><td> 0.097432633 </td><td> 0.093421477 </td><td>-0.160987378 </td><td>-0.093535945 </td><td>-0.0298388671</td><td>-0.005968257 </td></tr>
	<tr><td>-0.3888139   </td><td>-0.512946164 </td><td>-0.095166329 </td><td> 0.169291414 </td><td> 0.033699196 </td><td>-0.152326147 </td><td> 0.016936469 </td><td>-0.0188479397</td><td>-0.086933077 </td></tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[122]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span><span class="kp">print</span><span class="p">(</span>hisq3<span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="code-line"> [1] 0.8289141 0.6015374 0.6382330 0.5680873 0.4298459 0.4542829 0.4993689</span>
<span class="code-line"> [8] 0.3025154 0.2711171 0.2100527 0.5230468 0.7217756 0.4179823 0.7903544</span>
<span class="code-line">[15] 0.7225057 0.6674738 0.5590656 0.5417629 0.6023525 0.3814098 0.3890036</span>
<span class="code-line">[22] 0.4633676 0.4802789 0.4611304 0.4748262 0.6541179 0.5900188 0.4340915</span>
<span class="code-line">[29] 0.2110671 0.4064326 0.6699060 0.8747538 0.4109744 0.5645937 0.4981439</span>
<span class="code-line">[36] 0.4339110 0.4319218 0.4673458 0.4583968 0.4845446</span>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[123]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>loadings3rot <span class="o">&lt;-</span> <span class="kp">print</span><span class="p">(</span>varimax<span class="p">(</span>loadings3<span class="p">)</span><span class="o">$</span>loadings<span class="p">,</span> cutoff<span class="o">=</span><span class="m">.3</span><span class="p">)</span></span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre><span class="code-line"></span>
<span class="code-line">Loadings:</span>
<span class="code-line">      [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  </span>
<span class="code-line"> [1,]                              0.328                0.771       </span>
<span class="code-line"> [2,] -0.639                                                        </span>
<span class="code-line"> [3,] -0.688 -0.300                                                 </span>
<span class="code-line"> [4,]                                            0.593              </span>
<span class="code-line"> [5,] -0.551                                                        </span>
<span class="code-line"> [6,] -0.555                                                        </span>
<span class="code-line"> [7,]  0.487                                                  -0.362</span>
<span class="code-line"> [8,]                                                         -0.357</span>
<span class="code-line"> [9,]                                                         -0.376</span>
<span class="code-line">[10,]                                                         -0.326</span>
<span class="code-line">[11,]                              0.605                            </span>
<span class="code-line">[12,]                              0.755                            </span>
<span class="code-line">[13,]                              0.463                            </span>
<span class="code-line">[14,]                              0.324                0.749       </span>
<span class="code-line">[15,]                              0.300         0.703              </span>
<span class="code-line">[16,]                             -0.757                            </span>
<span class="code-line">[17,]                             -0.361        -0.509        -0.340</span>
<span class="code-line">[18,]                             -0.624                            </span>
<span class="code-line">[19,]                             -0.706                            </span>
<span class="code-line">[20,]                             -0.327                      -0.398</span>
<span class="code-line">[21,]               -0.350 -0.455                                   </span>
<span class="code-line">[22,]                      -0.647                                   </span>
<span class="code-line">[23,]                      -0.662                                   </span>
<span class="code-line">[24,]                      -0.606                                   </span>
<span class="code-line">[25,]                0.653                                          </span>
<span class="code-line">[26,]                0.771                                          </span>
<span class="code-line">[27,]                0.734                                          </span>
<span class="code-line">[28,]                0.641                                          </span>
<span class="code-line">[29,]                0.347                                          </span>
<span class="code-line">[30,]                0.573                                          </span>
<span class="code-line">[31,]                                     0.730                     </span>
<span class="code-line">[32,]        -0.367                       0.834                     </span>
<span class="code-line">[33,]        -0.606                                                 </span>
<span class="code-line">[34,]        -0.717                                                 </span>
<span class="code-line">[35,]        -0.698                                                 </span>
<span class="code-line">[36,]        -0.592                                                 </span>
<span class="code-line">[37,]        -0.575                                                 </span>
<span class="code-line">[38,]        -0.578                                                 </span>
<span class="code-line">[39,]        -0.628                                                 </span>
<span class="code-line">[40,]        -0.649                                                 </span>
<span class="code-line"></span>
<span class="code-line">                [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]</span>
<span class="code-line">SS loadings    2.456 3.967 3.049 1.971 3.529 1.417 1.485 1.511 1.206</span>
<span class="code-line">Proportion Var 0.061 0.099 0.076 0.049 0.088 0.035 0.037 0.038 0.030</span>
<span class="code-line">Cumulative Var 0.061 0.161 0.237 0.286 0.374 0.410 0.447 0.485 0.515</span>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, we can see a similar pattern to the two above methods. The iterated method also created loadings which are closer to the principal component method. There are a few differences; for instance, in the iterated method we see that the third variable is part of both the 1st and 2nd factor, which wasnÃ¢â¬â¢t present in the principal component method nor the principal factor method. Still, the analysis from before holds: much of the expected separation is contained in the factors (or some combination of them), but there are other correlations which those four factors cannot account for.</p>
<p>I will use the rotated matrix above to make some considerations about the level of effectiveness of the analysis. We know that the main goal of factor analysis is to acquire factors for which each variable can be loaded into only one factor. If we can do this, we have essentially reached our goal Ã¢â¬â we have separated the variables completely into a smaller number of factors which accounts for the covariates between them.</p>
<p>However, in practice, this is very difficult to achieve. In the loading matrix above, there are many instance where the variables correlate strongly with more than one factor at once -- that is there is no clear distinction between what these questions relate to in terms of our factors. This indicates that factor analysis is not doing a very good job at separating the factors with the number of factors we have used.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Some-Considerations">Some Considerations<a class="anchor-link" href="#Some-Considerations">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In an ideal situation, we would use a very small value of $m$ relative to the amount of variables $p$. Although $m = 9$ performed decently well, it is quite a large number of factors. Even with nine factors, we see that the factor analysis was only able to account for around 60% of the covariation of the original variables. A fundamental issue with factor analysis is that the correlation matrix $R$ contains both structure and error, and factor analysis is not able to separate the two. Thus, the original assumptions of no relationships between the errors and the factors are too optimistic, as this rarely happens in practice.</p>
<p>As a result, the loadings above are quite difficult to interpret. It is difficult to say what the exact factors are in plain English. Here, we can see some of the difficulty in working with factor analysis in practice Ã¢â¬â it is often questioned if these factors truly exist.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusions-and-Relation-to-Psychological-Tests">Conclusions and Relation to Psychological Tests<a class="anchor-link" href="#Conclusions-and-Relation-to-Psychological-Tests">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[124]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-r"><pre><span class="code-line"><span></span>loadings</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>



<div class="output_html rendered_html output_subarea ">
<table>
<tbody>
	<tr><td>-0.6277856   </td><td> 0.119554084 </td><td> 0.33171585  </td><td>-0.0517100760</td><td> 0.19589620  </td><td>-0.256426344 </td><td>-0.0051466920</td><td>-0.1050080325</td><td>-0.217875830 </td></tr>
	<tr><td>-0.6628787   </td><td>-0.132540591 </td><td> 0.11886326  </td><td> 0.2214725941</td><td> 0.22140567  </td><td> 0.132387083 </td><td>-0.2481776856</td><td> 0.0048103856</td><td> 0.073391368 </td></tr>
	<tr><td>-0.6390134   </td><td>-0.197196788 </td><td> 0.05624741  </td><td> 0.2311725131</td><td> 0.23664371  </td><td> 0.106747616 </td><td>-0.3142731034</td><td> 0.0725529294</td><td>-0.009258277 </td></tr>
	<tr><td>-0.6038332   </td><td>-0.001172604 </td><td> 0.25194869  </td><td> 0.0287228625</td><td> 0.19716616  </td><td>-0.096650456 </td><td> 0.2237402385</td><td> 0.3933113732</td><td> 0.175740568 </td></tr>
	<tr><td>-0.5623461   </td><td>-0.075542614 </td><td>-0.04215597  </td><td> 0.0889017298</td><td> 0.31622038  </td><td> 0.101831072 </td><td>-0.3414194020</td><td> 0.0325078492</td><td> 0.085362217 </td></tr>
	<tr><td>-0.5588305   </td><td>-0.202629294 </td><td>-0.02405988  </td><td> 0.1321338363</td><td> 0.30155513  </td><td> 0.073081367 </td><td>-0.2777839481</td><td>-0.0712005483</td><td> 0.093765748 </td></tr>
	<tr><td> 0.5610817   </td><td>-0.101431288 </td><td>-0.01542818  </td><td>-0.4286785091</td><td>-0.17991988  </td><td>-0.065822588 </td><td> 0.1121413950</td><td> 0.1320328403</td><td> 0.243711028 </td></tr>
	<tr><td> 0.4195786   </td><td>-0.089834730 </td><td> 0.04744726  </td><td>-0.4219788203</td><td>-0.09239017  </td><td> 0.008179850 </td><td>-0.1017128152</td><td> 0.1404356750</td><td> 0.359181871 </td></tr>
	<tr><td> 0.3666707   </td><td>-0.121246001 </td><td> 0.12235117  </td><td>-0.2673284202</td><td>-0.19228688  </td><td>-0.331672078 </td><td>-0.1723038527</td><td> 0.3330872957</td><td>-0.293884580 </td></tr>
	<tr><td> 0.2834756   </td><td>-0.191213944 </td><td> 0.13406289  </td><td>-0.2440643282</td><td>-0.18539624  </td><td>-0.106304884 </td><td>-0.4565341789</td><td>-0.0821946123</td><td>-0.171057932 </td></tr>
	<tr><td>-0.5196860   </td><td> 0.270565545 </td><td> 0.30577781  </td><td>-0.3423029175</td><td>-0.11245766  </td><td>-0.001271796 </td><td>-0.1320537729</td><td>-0.2283180107</td><td> 0.265527940 </td></tr>
	<tr><td>-0.6564641   </td><td> 0.068827634 </td><td> 0.31328652  </td><td>-0.2478632919</td><td>-0.34403345  </td><td> 0.087823904 </td><td>-0.1132566054</td><td> 0.0377935294</td><td>-0.011283238 </td></tr>
	<tr><td>-0.5998907   </td><td> 0.074604175 </td><td> 0.26507843  </td><td>-0.1892944458</td><td>-0.07212877  </td><td> 0.033343619 </td><td> 0.0308950899</td><td> 0.1426224411</td><td>-0.200220801 </td></tr>
	<tr><td>-0.6074783   </td><td> 0.096622639 </td><td> 0.34093184  </td><td>-0.1742184243</td><td> 0.20372042  </td><td>-0.252897043 </td><td> 0.0281370136</td><td>-0.0346131237</td><td>-0.254485357 </td></tr>
	<tr><td>-0.6248483   </td><td> 0.011604348 </td><td> 0.28844967  </td><td>-0.1296150311</td><td> 0.17592820  </td><td>-0.039241251 </td><td> 0.1844820354</td><td> 0.4216051360</td><td> 0.098786819 </td></tr>
	<tr><td> 0.5974964   </td><td>-0.234896621 </td><td>-0.29156288  </td><td> 0.0140733681</td><td> 0.45380545  </td><td>-0.094810107 </td><td> 0.0093078526</td><td> 0.0096644094</td><td> 0.016502011 </td></tr>
	<tr><td> 0.5405337   </td><td>-0.286677830 </td><td>-0.22708170  </td><td>-0.1085333297</td><td>-0.06070240  </td><td>-0.083413069 </td><td>-0.3147783298</td><td>-0.2285523728</td><td>-0.186028731 </td></tr>
	<tr><td> 0.5065189   </td><td>-0.420628297 </td><td>-0.27874397  </td><td> 0.0715082012</td><td> 0.18189460  </td><td> 0.002006571 </td><td> 0.0463524348</td><td> 0.3018104199</td><td>-0.260546935 </td></tr>
	<tr><td> 0.5780675   </td><td>-0.232627930 </td><td>-0.26378683  </td><td>-0.0556981294</td><td> 0.44299196  </td><td>-0.109840640 </td><td> 0.0355312724</td><td> 0.0666607568</td><td> 0.098413082 </td></tr>
	<tr><td> 0.5219373   </td><td>-0.216594655 </td><td>-0.13173681  </td><td>-0.1032620489</td><td>-0.04359939  </td><td> 0.070382456 </td><td>-0.2508050668</td><td> 0.2867068313</td><td> 0.083829363 </td></tr>
	<tr><td>-0.3992462   </td><td> 0.160965015 </td><td>-0.36037555  </td><td>-0.3690507373</td><td> 0.03464572  </td><td>-0.123678525 </td><td>-0.1924745951</td><td> 0.1507108412</td><td>-0.019742963 </td></tr>
	<tr><td>-0.3394289   </td><td> 0.150689800 </td><td>-0.26903653  </td><td>-0.5548752336</td><td> 0.16905090  </td><td> 0.011646505 </td><td>-0.0474249609</td><td>-0.1572321905</td><td> 0.166900898 </td></tr>
	<tr><td>-0.3320934   </td><td>-0.027645815 </td><td>-0.28907100  </td><td>-0.5582492767</td><td> 0.25327472  </td><td>-0.005346193 </td><td> 0.1102181121</td><td>-0.1334792270</td><td>-0.157127058 </td></tr>
	<tr><td>-0.3658667   </td><td> 0.108273914 </td><td>-0.38501438  </td><td>-0.4940891181</td><td> 0.19559323  </td><td> 0.064851965 </td><td>-0.0003570599</td><td> 0.0709592952</td><td>-0.068600025 </td></tr>
	<tr><td> 0.3796437   </td><td>-0.447822215 </td><td> 0.39727206  </td><td>-0.1408538430</td><td> 0.10110562  </td><td> 0.076120098 </td><td> 0.0523492570</td><td>-0.1131530027</td><td> 0.095050608 </td></tr>
	<tr><td> 0.3772945   </td><td>-0.495954442 </td><td> 0.47234291  </td><td>-0.1033727528</td><td> 0.11469191  </td><td> 0.018459564 </td><td> 0.1824386999</td><td>-0.1389626180</td><td>-0.114702885 </td></tr>
	<tr><td> 0.3799486   </td><td>-0.444134704 </td><td> 0.49194137  </td><td>-0.1387764590</td><td> 0.13915859  </td><td> 0.013283974 </td><td> 0.1229109140</td><td>-0.0973772037</td><td>-0.116962488 </td></tr>
	<tr><td> 0.2576489   </td><td>-0.483342987 </td><td> 0.37297134  </td><td>-0.1416818547</td><td> 0.21231287  </td><td> 0.117125266 </td><td> 0.1369392140</td><td>-0.1222056233</td><td> 0.146276756 </td></tr>
	<tr><td> 0.2062744   </td><td>-0.354617033 </td><td> 0.26543767  </td><td>-0.0007295079</td><td> 0.06470126  </td><td> 0.099806178 </td><td>-0.2945216155</td><td> 0.1256130100</td><td>-0.077929380 </td></tr>
	<tr><td> 0.3334748   </td><td>-0.402402498 </td><td> 0.39061932  </td><td>-0.1902953021</td><td> 0.08669907  </td><td> 0.083796843 </td><td>-0.1520357393</td><td>-0.0181765447</td><td> 0.107031691 </td></tr>
	<tr><td>-0.4064146   </td><td>-0.347591815 </td><td>-0.19534506  </td><td>-0.1439862302</td><td>-0.08445037  </td><td> 0.611418809 </td><td> 0.1300478000</td><td>-0.0035088423</td><td>-0.175680364 </td></tr>
	<tr><td>-0.4213233   </td><td>-0.421047564 </td><td>-0.21614195  </td><td>-0.1475954934</td><td>-0.14140091  </td><td> 0.569277998 </td><td> 0.1274642030</td><td> 0.0120766707</td><td>-0.147817867 </td></tr>
	<tr><td>-0.3177238   </td><td>-0.558528507 </td><td>-0.21188225  </td><td>-0.0260375231</td><td>-0.11058613  </td><td>-0.064299580 </td><td> 0.1599389824</td><td> 0.0008566237</td><td> 0.011078551 </td></tr>
	<tr><td>-0.4535206   </td><td>-0.568930452 </td><td>-0.13969631  </td><td> 0.0740933171</td><td>-0.20728453  </td><td>-0.111993462 </td><td> 0.0128182266</td><td> 0.0851071945</td><td> 0.058119259 </td></tr>
	<tr><td>-0.3733466   </td><td>-0.534136456 </td><td>-0.20889608  </td><td> 0.0462874732</td><td>-0.18328216  </td><td>-0.242376556 </td><td> 0.0930813931</td><td>-0.0879622294</td><td> 0.120472821 </td></tr>
	<tr><td>-0.4519620   </td><td>-0.486894058 </td><td> 0.02868335  </td><td>-0.0237170894</td><td>-0.22067245  </td><td>-0.121746325 </td><td> 0.0211133535</td><td> 0.0949324784</td><td>-0.117122331 </td></tr>
	<tr><td>-0.4792771   </td><td>-0.359058583 </td><td>-0.17026707  </td><td> 0.0243226733</td><td> 0.01058587  </td><td>-0.329825553 </td><td> 0.0814961168</td><td>-0.2338333561</td><td> 0.027777255 </td></tr>
	<tr><td>-0.4884591   </td><td>-0.361002718 </td><td>-0.23670362  </td><td>-0.0576248626</td><td> 0.04542125  </td><td>-0.266631231 </td><td> 0.1774163361</td><td>-0.1496133363</td><td>-0.028050715 </td></tr>
	<tr><td>-0.3794872   </td><td>-0.555721465 </td><td>-0.09478251  </td><td> 0.1039076241</td><td>-0.19174868  </td><td>-0.083409801 </td><td>-0.1393220930</td><td>-0.0449142959</td><td> 0.188743726 </td></tr>
	<tr><td>-0.4024135   </td><td>-0.549377337 </td><td>-0.11356106  </td><td> 0.1869168187</td><td>-0.12464387  </td><td>-0.136300169 </td><td>-0.0389545949</td><td> 0.0957233294</td><td> 0.133687572 </td></tr>
</tbody>
</table>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We know that the original variables were constructed considering four groups (assertiveness, social confidence, adventurousness, and dominance). These were the intended characteristics that the statements were testing for. Above, we can see how factors 1-5 capture a lot of the variables in each set. So, we could say that factor 1 represents assertiveness, factor 2 represents dominance, factor 3 and 4 represent adventurousness, and factor 5 represents social confidence.</p>
<p>However, this would not be so accurate, as there are other factors to consider as well. We can see that factors 6-9 have members from all the different groups. This suggests that the relationship between the variables was not so simple as we once thought. Additionally, variable 10 failed to have a loading greater than .3 into any of the factors, which suggests that even 9 factors are inadequate to capture the variation in the data.</p>
<p>In this example, which seemed relatively straightforward, we see that problems emerged quite quickly. What, for instance, is factor six? And why are factors 3 and 4 separated? It is not so clear that our prior ideas about the factors are accurate.</p>
<p>One thing is clear, though. The four factors used to create the questionnaire are insufficient to truly describe the structure of the data. To say that these statements measure these factors would be jumping to conclusions too quickly. Doing so is, in a way, oversimplifying the data.</p>
<p>In conclusion, we have learned that this dataset cannot be so simply separated into the four purported factors. In fact, even separating the variables into 9 factors can only account for 60% of the variation of the variables. We can see that while factor analysis is a powerful technique, it is very dependent on the dataset which it is performed on. The self-checking property of the assumptions of factor analysis ensures that only models which fit the assumptions will produce results which separate the variables adequately to the different factors.</p>

</div>
</div>
</div>
 


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['$','$'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        " linebreaks: { automatic: true, width: '95% container' }, " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            <div>
            <span class="author_blurb"><a href="https://seanammirati.github.io/pages/about"><span class="author_name">Sean Ammirati</span></a> -
                 creator of Stats Works. He can be reached on Github, LinkedIn and email.</span><br />
</div>

            
            <section>
<div class="accordion" id="accordion2">
    <div class="accordion-group">
        <div class="accordion-heading">
            <a class="accordion-toggle disqus-comment-count" data-toggle="collapse" data-parent="#accordion2"
                href="../../category/projects/factor_analysis.html#disqus_thread",
                id="disqus-accordion-toggle">
                Comments
            </a>
        </div>
        <div id="disqus_thread" class="accordion-body collapse">
            <div class="accordion-inner">
                <div class="comments">
                    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'statsworks';
        var disqus_identifier = '../../category/projects/factor_analysis.html';
    var disqus_url = '../../category/projects/factor_analysis.html';

    (function() {
         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
</script>
<noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

                </div>
            </div>
        </div>
    </div>
</div>
</section>

            <hr/>
            <aside>
            <nav>
                <h3> Articles in Projects</h3>
                <ul class="articles-timeline">
                <li class="previous-article">Â« <a href="../../category/projects/nypd_stop_frisk.html" title="Previous: NYPD Stop and Frisk">NYPD Stop and Frisk</a></li>
            </ul>
            <h3> All Articles</h3>
            <ul class='articles-timeline'>
                <li class="previous-article">Â« <a href="../../category/beginner-statistics/sumstatsmeasuresofspread.html" title="Previous: Summary Statistics Part 2: Measures of Spread">Summary Statistics Part 2: Measures of Spread</a></li>
                <li class="next-article"><a href="../../category/linear-models/anova.html" title="Next: ANOVA - Robustness to Non-Normality">ANOVA <small>Robustness to Non-Normality</small></a> Â»</li>

            </ul>
            </nav>
            </aside>
        </div>
        <section>
        <div class="span2" style="float:right;font-size:0.9em;">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2018-09-16T00:00:00-04:00">Sep 16, 2018</time>
            <h4>Category</h4>
            <a class="category-link" href="../../categories.html#projects-ref">Projects</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="../../tags/#abstract-ref">abstract
                    <span>1</span>
</a></li>
                <li><a href="../../tags/#factor-analysis-ref">factor analysis
                    <span>1</span>
</a></li>
                <li><a href="../../tags/#feature-reduction-ref">feature reduction
                    <span>1</span>
</a></li>
            </ul>
<h4>Contact</h4>
    <a href="https://github.com/SeanAmmirati" title="My GitHub Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-github sidebar-social-links"></i></a>
    <a href="https://www.linkedin.com/in/sean-ammirati-4795a4ba/" title="My LinkedIn Profile" class="sidebar-social-links" target="_blank">
    <i class="fa fa-linkedin sidebar-social-links"></i></a>
    <a href="ammirati.sean@gmail.com" title="My Email Address" class="sidebar-social-links" target="_blank">
    <i class="fa fa-envelope sidebar-social-links"></i></a>
        </div>
        </section>
</div>
</article>
                </div>
                <div class="span1"></div>
            </div>
        </div>
        <div id="push"></div>
    </div>
<footer>
<div id="footer">
    <ul class="footer-content">
        <li class="elegant-subtitle"><span class="site-name">Stats Works</span> - Making Statistics Work for You</li>
        <li class="elegant-power">Based on Elegant Theme: <a href="https://github.com/Pelican-Elegant/elegant/" title="Theme Elegant Home Page">Elegant</a></li>

    </ul>
</div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

            <script type="text/javascript">
var disqus_shortname = 'statsworks';
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
<script  language="javascript" type="text/javascript">
function uncollapse() {
    if (window.location.hash.match(/^#comment-\d+$/)) {
        $('#disqus_thread').collapse('show');
    }
}
</script>
<script type="text/javascript" language="JavaScript">
uncollapse();
window.onhashchange=function(){
    if (window.location.hash.match(/^#comment-\d+$/))
        window.location.reload(true);
}
</script>
<script>
$('#disqus_thread').on('shown', function () {
    var link = document.getElementById('disqus-accordion-toggle');
    var old_innerHTML = link.innerHTML;
    $(link).fadeOut(500, function() {
        $(this).text('Click here to hide comments').fadeIn(500);
    });
    $('#disqus_thread').on('hidden', function () {
        $(link).fadeOut(500, function() {
            $(this).text(old_innerHTML).fadeIn(500);
        });
    })
})
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
    License : MIT -->
</html>